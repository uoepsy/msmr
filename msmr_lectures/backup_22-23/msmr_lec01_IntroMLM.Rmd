---
title: "<b>Week 1: Introduction to Multilevel Modeling</b>"
subtitle: "Multivariate Statistics and Methodology using R (MSMR)<br><br> "
author: "Dan Mirman"
institute: "Department of Psychology<br>The University of Edinburgh"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  extra_css = list(".scroll-output" = list("height"="70%","overflow-y"="scroll"))
) 
```

# What are nested data?

Data that have hierarchical or clustered structure

* Clustered observations (ex: multiple children in one family or classroom or neighborhood)
* Repeated measures (ex: multiple questions on an exam, any within-subject manipulation)
* Longitudinal data (ex: height of children)

--

**Very common in psychological science**

```{r setup2, include=FALSE, message=FALSE}
options(digits = 4)
library(tidyverse)
library(knitr)
library(kableExtra)
library(patchwork)
load("./data/WordLearnEx.rda")
load("./data/VisualSearchEx.rda")
```

---
# Gradual change (based on a true story)

```{r echo=FALSE, fig.height=4, fig.width=6}
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP, fill=TP)) + 
  stat_summary(fun=mean, geom="line") +
  stat_summary(fun.data = mean_se, geom="ribbon", alpha=0.5, color=NA) +
  scale_x_continuous(breaks=1:10) +
  theme_bw() 
```

--

* Novel word learning is faster for high TP than low TP words

--

* But, t-test on overall mean accuracy is marginal (p=0.096)

--

* Repeated measures ANOVA shows main effect of Block, marginal effect of TP, and no interaction (F<1)

--

* Block-by-block t-test significant only in block 4 and marginal in block 5

---
# Two key features

### Nested data are not independent

* A child that is taller-than-average at time *t*, is likely to be taller-than-average at time *t+1*
* Non-independence is related to individual differences
* Nesting can happen on multiple levels: children within families or hospitals

--

### Can be related by continuous variable (i.e., time, but could be [letter] size, number of distractors, etc.)

* Ought to model this variable as continuous
* Can quantify trajectories/shapes of change

---
# Why use multilevel models for nested data?

* Correctly model non-independence of observations
* Estimate group-level and individual-level effects (same for other levels)
* Model trajectories of change

--

### Nomenclature note

All of these terms refer to the same family of statistical methods, with slight differences in term preference across application domains and in implied data type:

* Multilevel models (MLM), Multilevel regression (MLR)
* Hierarchical linear modeling (HLM)
* Mixed effects models (MEM), Mixed effects regression (MER), Linear mixed models (LMM)
* Growth curve analysis (GCA; implies longitudinal data with non-linear change)


---
# Linear regression: A brief review
```{r echo=FALSE, message=FALSE, fig.height=4, fig.width=4}
library(grid)
set.seed(1)
lr.dat <- data.frame(Time = 0:10, Y = 5:15 + runif(11, -2, 2))
#basic plot
f1 <- ggplot(lr.dat, aes(Time, Y)) + geom_point(shape=21) + 
  stat_smooth(method="lm", se=F, color="black") + 
  expand_limits(y=c(0,16), x=c(-1,11)) + 
  theme_bw() + theme(axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank()) +
  #add betas and dashed lines
  annotate("segment", x=0, xend=10, y=5.2, yend=5.2, linetype="dashed", color="red") + 
  annotate("segment", x=10, xend=10, y=5.2, yend=14.9, linetype="dashed", color="red") + 
  annotate("text", x=-0.2, y=5.2, label = "beta[0]", parse=T, hjust=1, color="blue", size=8) + 
  annotate("text", x=10.2, y=10, label = "beta[1]", parse=T, hjust=0, color="red", size=8) 
#add Yij and epsilon
f1 <- f1 + geom_point(data = subset(lr.dat, Time==5), shape=16) + 
  annotate("text", x=4.9, y=12.1, label = "Y[ij]", parse=T, hjust=0, size=8) + 
  annotate("text", x=4.8, y=11, label = "epsilon[ij]", parse=T, hjust=1, size=8) + 
  geom_segment(aes(x = 5, y = 10.25, xend = 5, yend = 11.4), arrow = arrow(length = unit(0.1, "cm"), ends="both"))
# render the image
f1
```

--

$Y = \beta_{0} + \beta_{1} \cdot Time$

--

$Y_{ij} = \beta_{0i} + \beta_{1i} \cdot Time_{j} + \epsilon_{ij}$

--

__Fixed effects__: <span style="color:blue"> $\beta_{0}$ </span> (Intercept), <span style="color:red"> $\beta_{1}$ </span> (Slope)

--

__Random effects__: $\epsilon_{ij}$ (Residual error)

---
# Multilevel models: Fixed effects
Level 1: $Y_{ij} = \beta_{0i} + \beta_{1i} \cdot Time_{j} + \epsilon_{ij}$

--

Level 2: model of the Level 1 parameter(s) 

$\beta_{0i} = \gamma_{00} + \zeta_{0i}$ 

* $\gamma_{00}$ is the population mean 
* $\zeta_{0i}$ is individual deviation from the mean

--

$\beta_{0i} = \gamma_{00} + \gamma_{0C} \cdot C  + \zeta_{0i}$ 

* $\gamma_{0C}$ is the fixed effect of condition $C$ on the *intercept*

--

$\beta_{1i} = \gamma_{10} + \gamma_{1C} \cdot C  + \zeta_{1i}$ 

* $\gamma_{1C}$ is the fixed effect of condition $C$ on the *slope*

---
# Multilevel models: Random effects
Level 1: $Y_{ij} = \beta_{0i} + \beta_{1i} \cdot Time_{j} + \epsilon_{ij}$

Level 2: 

$\beta_{0i} = \gamma_{00} + \gamma_{0C} \cdot C  + \zeta_{0i}$

$\beta_{1i} = \gamma_{10} + \gamma_{1C} \cdot C  + \zeta_{1i}$ 

Residual errors

* $\zeta_{0i}$ unexplained variance in *intercept*
* $\zeta_{1i}$ unexplained variance in *slope*
* Unexplained variance reflects individual differences
* Random effects require a lot of data to estimate

---
# Fixed vs. Random effects

**Fixed effects**

* Interesting in themselves
* Reproducible fixed properties of the world (nouns vs. verbs, WM load, age, etc.)
* <span style="color:red"> *Unique, unconstrained parameter estimate for each condition* </span>

--

**Random effects**

* Randomly sampled observational units over which you intend to generalize (particular nouns/verbs, particular individuals, etc.)
* Unexplained variance
* <span style="color:red"> *Drawn from normal distribution with mean 0* </span>

---
# Maximum Likelihood Estimation
* Find an estimate of parameters that maximizes the likelihood of observing the actual data
* Simple regression: OLS produces MLE parameters by solving an equation
* Multilevel models: use iterative algorithm to gradually converge to MLE estimates

--

Goodness of fit measure: log likelihood (LL)

* Not inherently meaningful (unlike $R^2$)
* Change in LL indicates improvement of the fit of the model
* Changes in $-2\Delta LL$ (aka "Likelihood Ratio") are distributed as $\chi^2$
* Requires models be nested (parameters added or removed)
* DF = number of parameters added

---
# MLM: The core steps

1. Load the pacakge: `library(lme4)`
2. Fit the model(s): `lmer(formula, data, options)`
3. Evaluate the model(s): compare models, examine parameter estimates, plot model fit(s), etc.
4. Improve/adjust model(s), rinse, repeat

---
# A simple MLM example
```{r fig.height=3, fig.width=5}
str(VisualSearchEx)
ggplot(VisualSearchEx, aes(Set.Size, RT, color=Dx)) + stat_summary(fun.data=mean_se, geom="pointrange")
```

---
# A simple MLM example: Fit the models
```{r message=FALSE, warning=FALSE}
library(lme4)
# a null, intercept-only model
vs.null <- lmer(RT ~ 1 + (1 + Set.Size | Participant), 
                data=VisualSearchEx, REML=FALSE)
# add effect of set size
vs <- lmer(RT ~ Set.Size + (Set.Size | Participant), 
           data=VisualSearchEx, REML=F)
```

--

```{r message=FALSE, warning=FALSE}
# add effect of diagnosis
vs.0 <- lmer(RT ~ Set.Size + Dx + (Set.Size | Participant),
             data=VisualSearchEx, REML=F)
# add set size by diagnosis interaction
vs.1 <- lmer(RT ~ Set.Size + Dx + Set.Size:Dx + (Set.Size | Participant),
             data=VisualSearchEx, REML=F)
```

---
# A simple MLM example: Compare the model fits
```{r message=FALSE, warning=FALSE}
anova(vs.null, vs, vs.0, vs.1)
```

---
# A simple MLM example: Interpet results
```{r echo=FALSE}
kable(anova(vs.null, vs, vs.0, vs.1)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

Compared to null model, adding set size (`vs`) substantially improves model fit: response times are affected by number of distractors

--

Adding effect of Diagnosis on intercept (`vs.0`) significantly improves model fit: stroke survivors respond more slowly than control participants do

--

Adding interaction of set size and Diagnosis, i.e., effect of Diagnosis on slope (`vs.1`), does not significantly improve model fit: stroke survivors are not more affected by distractors than control participants are

---
# A simple MLM example: Inspect model
.scroll-output[
```{r}
summary(vs.1)
```
]

---
# A simple MLM example: Parameter p-values
Oh no! `summary()` for models fit by `lme4::lmer` (`lmerMod` objects) does not include p-values.
```{r echo=FALSE}
coef(summary(vs.1))
```

Those p-values are one-sample t-tests of whether $Est \neq 0$ with $t = Est/SE$

--

The `df` for these t-tests are not simple to determine because random effects are not free parameters (estimated under constraints). 

But `df` can be estimated, and the two most common estimations are "Kenward-Roger" and "Satterthwaite". These approximations are implemented in a few different packages (`afex`, `lmerTest`, `pbkrtest`).

---
# A simple MLM example: Parameter p-values

One of the easiest to use is the `lmerTest` package: you can fit the model the same way (it just passes your call to `lmer`) and it will calculate the Satterthwaite approximation and add those `df` and p-values to the model summary

--

.scroll-output[
```{r message=FALSE, warning=FALSE}
library(lmerTest)
vs.1 <- lmer(RT ~ Set.Size + Dx + Set.Size:Dx + (Set.Size | Participant), 
             data=VisualSearchEx, REML=F)
summary(vs.1)
```
]

---
# A simple MLM example: Plot model fit
```{r fig.height=4, fig.width=6}
ggplot(VisualSearchEx, aes(Set.Size, RT, color=Dx)) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(aes(y=fitted(vs.0)), fun=mean, geom="line")
```

---
# Another way to plot model fit

The `effects` package provides a convenient way to pull estimated values from a model (along with SE and confidence intervals). Very useful for plotting effects from complicated models, or when there are missing data, etc.

```{r message=FALSE}
library(effects)
ef <- as.data.frame(effect("Set.Size:Dx", vs.0))
head(ef)
```

---
# Another way to plot model fit

The `effects` package provides a convenient way to pull estimated values from a model (along with SE and confidence intervals). Very useful for plotting effects from complicated models, or when there are missing data, etc.

```{r fig.height=4, fig.width=6, message=FALSE}
ggplot(ef, aes(Set.Size, fit, color=Dx)) + geom_line()
```

---
# Some general advice

This semester you will be learning statistical methods that don't have "cookbook" recipes. You'll need to actively engage with the data and research question in order to come up with a good model for answering the question, then to defend/explain that model.

--

**Practice is absolutely critical** to learning how to do this. You can't learn it just from the lectures; you have to try it with real data. You will make mistakes, run into problems, etc. Identifying the mistakes and solving those problems is how you'll master this material. *We have made all of the example data sets and code available to you for exactly this reason.*

--

Do the lab exercises! If you're not sure, **try something** then try to figure out whether it worked or not. Ask questions when you're stuck -- we're here to help you learn, but it will only work if you engage in **active, hands-on learning**.

![gym](figs/stockvault-fitness-center106595.jpg)
---
# Live R

Questions?

--

### Use MLM to run a 2x2 repeated-measures ANOVA

--

Recall: 

1. ANOVA is a particular kind of GLM (linear regression with categorical predictors).
2. Repeated-measures ANOVA is used for within-participant manipulations; aka, nested observations.

--

MLM is an extension of GLM with random effects to capture within-participant nesting, so we should be able to implement a 2x2 rm-ANOVA using MLM.

--

**Example 1**: Fazio (2020) Repetition Increases Perceived Truth Even for Known Falsehoods. *Collabra: Psychology, 6*(1):38. https://doi.org/10.1525/collabra.347. Data available at: https://osf.io/kt2p8/

Participants had to determine whether statements were true or false. The manipulations were (1) whether the fact was known or unknown to the participant and (2) whether the participant had been previously exposed to the statement. We know repeated exposure increases "truthiness" of false statements, the key question is whether prior knowledge is a protective factor.

That is, does *knowing* that the statement is false protect people from the repetition effect?