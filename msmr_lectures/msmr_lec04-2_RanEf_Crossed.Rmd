---
title: "<b>Week 4, part 2: Crossed Random Effects</b>"
subtitle: "Multivariate Statistics and Methodology using R (MSMR)<br><br> "
author: "Dan Mirman"
institute: "Department of Psychology<br>The University of Edinburgh"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 4)
library(knitr)
library(kableExtra)
library(patchwork)

library(tidyverse)
library(broom)
library(broom.mixed)
library(gt)
library(lme4)
library(lmerTest)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
) 
```

# Crossed random effects

In most statistical tests, we are evaluating the reliability of some effect in the context of the variability in the data. The goal is to make an inference from the sample to the population from which from that sample was drawn.

Usually, we want to make inferences about reliability/variability across **subjects**: i.e., in general, do participants tend to show this effect or is it just one or two people showing it? That is, would we expect the rest of the participant population to also behave this way?

--

We might also ask about reliability/variability across **items**. This often comes up in laboratory experimental contexts where we ask participants to solve N problems, or answer N questions, or recognize N words, etc. There will be variability across those N "items" and we should test whether our effects of interest are reliable in the context of that variability. That is, would we expect the same outcome for other problems, questions, words, etc. from the same population?

--

Historically, this was done by conducting separate by-subjects ("F1") and by-items ("F2") analyses and journals sometimes even required this (a long time ago, I had a paper rejected because a key result was statistically significant by subjects but only marginal by items).

--

**Multilevel models provide a better solution to this problem.**

---
# Improving problem solving
Made-up data on an intervention to improve problem solving ability
```{r}
load("./data/problem_solving.Rdata")
summary(problem_solving)
```

* `Item`: word problem, can be *`Hard`* or *`Easy`*
* `Prob_Type`: difficulty level of word problem (16 hard problems, 14 easy problems)
* `Subject`: Participant ID (N=120)
* `Condition`: whether the participant received the `Treatment` or not
* `RT`: time to solve the problem

Note: there is some missing data because trials where the participants failed to solve the problem are excluded.

---
# A traditional analysis approach

.pull-left[
Calculate subject-by-condition means

```{r, message=FALSE}
ps_subj <- problem_solving %>%
  group_by(Subject, Condition, Prob_Type) %>%
  summarise(RT = mean(RT))
```
]

.pull-right[
```{r, fig.width=6, fig.height=4, message=FALSE, echo=FALSE}
# Plot the subject-level data
ggplot(ps_subj, aes(Condition, RT, fill=Prob_Type)) + 
  geom_boxplot()
```
]

--

* Everyone solves `Easy` problems faster than `Hard` ones
* `Treatment` group seems faster at problem solving, esp. for `Hard` problems
---
# Repeated-measures ANOVA
```{r message=FALSE}
afex::aov_ez(id = "Subject", dv = "RT",
  within = "Prob_Type", between = "Condition",
  data = ps_subj)
```

--

Looks like there is an overall problem difficulty effect, and no effect(s) of the intervention.

--

But hang on: not all word problems are the same and we're going to make inferences about solving problems **of this type**, not just about solving these particular problems.

---
# By-items analysis
Calculate item-by-condition means

```{r, fig.width=6, fig.height=4, message=FALSE}
ps_item <- problem_solving %>%
  group_by(Item, Condition, Prob_Type) %>%
  summarise(RT = mean(RT))
# plot the item-level data
ggplot(ps_item, aes(Condition, RT, fill=Prob_Type)) + geom_boxplot()
```

---
# By-items repeated-measures ANOVA
```{r, message=FALSE}
afex::aov_ez(id = "Item", dv = "RT", 
             between = "Prob_Type", within = "Condition", 
             data = ps_item)
```

All effects significant. **Hol up, why the difference?**

---
# By-items repeated-measures ANOVA
```{r, message=FALSE, echo=FALSE}
afex::aov_ez(id = "Item", dv = "RT", 
             between = "Prob_Type", within = "Condition", 
             data = ps_item)
```

* Problem type (difficulty) has a large effect, it is significant in both analyses
* Condition (intervention) effect and its interaction with problem type are small

--

Condition is between-subjects but within-items, so the between-subjects variability is strong in the by-subjects analysis but averaged away in the by-items analysis. 

This makes the by-items analysis look overly strong (subject variability is missing) and the by-subjects analysis look overly weak (items consistency is missing).

--

Also, the idea that effects should be significant in separate by-items and by-subjects analyses (aka F1 and F2) is generally thought to be overly conservative.

---
# A multilevel modeling approach 

Multilevel models provide a way to simultaneously model random variability at subject and item levels, as well as the group-level effects that we are interested in.

These data are nested (clustered observations that are not independent), but, unlike prior examples, they are not **hierarchical**. Items are clustered within subjects (subjects solved multiple problems); but also subjects are clustered within items (same problems were presented to multiple subjects). This can be modeled with **crossed random effects**.

```{r}
mod_ps <- lmer(RT ~ Prob_Type*Condition + 
                 (Prob_Type | Subject) + (Condition | Item), 
                 data=problem_solving, REML=FALSE)
```

---
# A multilevel modeling approach 
```{r}
gt(tidy(mod_ps))
```

---
# Plotting results

Computing means and SE is a little tricky for trial-level data

Conveniently, the `effects` package will do that based on the fitted model

```{r, message=FALSE}
library(effects)
efx <- effect("Prob_Type:Condition", mod_ps) %>% as.data.frame(.)
efx
```

---
# Plotting results
```{r, fig.width=5, fig.height=4, message=FALSE}
ggplot(efx, aes(Condition, fit, color=Prob_Type)) + 
  geom_point(size=3) + 
  geom_linerange(aes(ymin=fit-se, ymax=fit+se), size=2) +
  geom_linerange(aes(ymin=lower, ymax=upper), size=1) +
  theme_bw() + labs(y="Response Time", color="Problem\nType")
```

---
# Not so different after all
This week we covered some more complex random effect structures

* 3-level nesting
* Crossed random effects

These are just kinds of MLM and the principles from other lectures also apply to these analyses

* p-value (df) estimation using Satterthwaite method; model comparisons would've been a good alternative
* use full random effect structure, where "full" is defined by study design; can simplify if model doesn't converge by removing correlations and random "slopes"
* be aware of how your categorical variables as coded; can conduct pairwise comparisons using a single model
* use logistic regression for binary outcomes
