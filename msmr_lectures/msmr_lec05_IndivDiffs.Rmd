---
title: "<b>Week 5: Individual Differences</b>"
subtitle: "Multivariate Statistics and Methodology using R (MSMR)<br><br> "
author: "Dan Mirman"
institute: "Department of Psychology<br>The University of Edinburgh"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 4)
library(knitr)
library(kableExtra)
library(patchwork)

library(tidyverse)
library(broom)
library(broom.mixed)
library(gt)
library(lme4)
library(lmerTest)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
) 
```

# Individual differences

**Individual differences provide an additional level of analysis for understanding phenomena**

--

* At a group level, a treatment works better than a placebo, but why does it work better for some people than for others?
* People solve easy problems faster than hard problems, but why are some people a lot faster on the easy problems and other people only a little faster?

--

t-test and ANOVA methods treat individual differences as noise

Multilevel models provide two ways to quantify and analyse individual differences

1. Add individual differences as **fixed effects**
2. Quantify individual differences using **random effects**

---
# Individual differences can be added as fixed effects

Example: USA National Youth Survey longitudinal data on tolerance for deviant behaviour, exposure to deviant behaviour, and gender. Subset of 16 subjects printed in Willett (1997, Table 11.1). (`DeviantBehavior`)

`Tolerance` was measured by asking whether it is wrong for someone their age to: 

* cheat on tests
* purposely destroy property of others
* hit or threaten someone without reason
* use alcohol
* use marijuana
* sell hard drugs
* steal something worth less than $5
* steal something worth more than $50
* break into a building or vehicle to steal

---
# Tolerance for deviant behaviour

```{r}
load("./data/DeviantBehavior.RData")
summary(DeviantBehavior)
```

```{r echo=FALSE, fig.height=3.25, fig.width=5}
# The overall pattern (ignoring gender for now)
ggplot(DeviantBehavior, aes(Age, Tolerance, color=Gender)) + 
  stat_summary(fun=mean, geom="point", size=3) +
  stat_summary(fun.data=mean_se, geom="errorbar", width=0.2) + 
  stat_summary(fun=mean, geom="line") +
  labs(x="Age", y="Tolerance for deviant behavior") + 
  theme_bw()
```

---
## Research question 1

**Does tolerance increase with age and is it modulated by gender?**

```{r, warning=FALSE}
DeviantBehavior$Age0 <- DeviantBehavior$Age - 11 #adjust Age so 0 is start of study
# fit model
m.base <- lmer(Tolerance ~ Age0*Gender + (Age0 | SubjID),
                 contrasts=list(Gender="contr.sum"), data=DeviantBehavior)
```
```{r echo=FALSE}
gt(tidy(m.base, effects="fixed"))
```

Significant increase with age, no significant effects of gender

---
## Research question 2

**Is this modulated by exposure to deviant behavior?**

`Exposure` scale is not clear and other effects would be estimated at `Exposure=0`, which is not an attested value. 

If we center `Exposure`, its effects and the effects of other predictors will be easier to interpret.

```{r}
# center Exposure, but do not re-scale it
DeviantBehavior$Exposure.center <- scale(DeviantBehavior$Exposure, scale=FALSE)
```

--

Fit the full model

```{r}
m.exp1 <- lmer(Tolerance ~ Age0*Gender*Exposure.center + (Age0 | SubjID), 
               data=DeviantBehavior, contrasts=list(Gender="contr.sum"))
```

---

```{r}
gt(tidy(m.exp1, effects="fixed"))
```

* Significant main effect of **Age**
* Significant interactions: **Age-by-Exposure** and **Age-by-Gender-by-Exposure**

---
# How to plot the three-way interaction?

The three-way Age-by-Gender-by-Exposure interaction is a relationship among four variables (Tolerance for deviant behaviour, Exposure to deviant behaviour, Age, and Gender), three of which are continuous variables. **This is hard to visualize.**

To make it easier to visualize, can split the `Exposure` into levels

--

Option 1: Median split
```{r}
DeviantBehavior$ExposureMedSpl <- ifelse(
  DeviantBehavior$Exposure >= median(DeviantBehavior$Exposure), "High", "Low")
```

---

```{r fig.height=3.5, fig.width=7}
ggplot(DeviantBehavior, aes(Age, Tolerance, color=ExposureMedSpl)) + 
  facet_wrap(~ Gender) + 
  stat_summary(fun=mean, geom="point", size=3) +
  stat_summary(fun.data=mean_se, geom="errorbar", width=0.2) + 
  stat_summary(aes(y=fitted(m.exp1)), fun=mean, geom="line") +
  labs(x="Age", y="Tolerance for deviant behavior", 
       color="Exposure\n(Median Split)") + 
  theme_bw() 
```

Adolescents with higher Exposure to deviant behaviour tend to have increased Tolerance for deviant behaviour as they get older, and more so for males than females.

---
# How to plot the three-way interaction?

Option 2: tertile split provides a little more info than median split

```{r warning=FALSE}
# define break points
b <- quantile(DeviantBehavior$Exposure, probs=seq(0, 1, by=1/3)) 
# split continuous predictor and provide level labels
DeviantBehavior$Exposure3 <- cut(DeviantBehavior$Exposure, 
                                 breaks=b, include.lowest=T, 
                                 labels=c("Low", "Medium", "High"))
```

---
```{r fig.height=4, fig.width=8}
ggplot(DeviantBehavior, aes(Age, Tolerance, color=Exposure3)) + 
  facet_wrap(~ Gender) + 
  stat_summary(fun=mean, geom="point", size=3) + 
  stat_summary(fun.data=mean_se, geom="errorbar", width=0.2) + 
  stat_summary(aes(y=fitted(m.exp1)), fun=mean, geom="line") +
  labs(x="Age", y="Tolerance for deviant behavior", 
       color="Exposure\n(Tertiles)") + 
  theme_bw() 
```

**Warning**: reviewers and readers may get confused about whether your *model* used continuous or categorical predictors, so you will need to be extra clear about this in your write-up.

---
# Quantify individual differences using random effects

.pull-left[
### A simple example

<img src="./figs/IndivDiffsDemo.png" width="75%" />
]

.pull-right[
**Random effects provide a way to quantify individual effect sizes in the context of a model of overall group performance**:

Participant A: $\zeta_{A1} - \zeta_{A0} = 1 - (-1) = 2$

Participant B: $\zeta_{B1} - \zeta_{B0} = (-1) - 1 = -2$
]

---
# Example 
Data (Made-up): Effect of school mental health services on educational achievement (`EducMH`)

```{r}
load("./data/EducMH.RData")
summary(EducMH)
```

* `Condition` = Treatment (students who received mental health services) vs. Control (academically matched group of students who did not receive services)
* `SDQ` = Strengths and Difficulties Questionnaire: a brief behavioural screening for mental health, only available for Treatment group. Lower scores are better (Total difficulties).
* `Math` = Score on standardised math test

---

```{r fig.width=6, fig.height=3.5}
ggplot(EducMH, aes(Year, Math, color=Condition, fill=Condition)) + 
  stat_summary(fun=mean, geom="line") + 
  stat_summary(fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) + 
  labs(y="Math Achievement Score") + theme_bw(base_size=12) + 
  scale_color_manual(values=c("red", "blue")) + 
  scale_fill_manual(values=c("red", "blue"))
```

**Question 1**: Did the school mental health services improve academic achievement? That is, did the two groups differ on math achievement at baseline and over the 6 years of the study?

**Question 2**: For the treatment group, was individual-level improvement in mental health associated with improvement in math scores?

---
# Question 1

**Did the school mental health services improve academic achievement? That is, did the two groups differ on math achievement at baseline and over the 6 years of the study?**

```{r}
# adjust time variable to have a sensible intercept
EducMH$Time <- EducMH$Year - 2009
# fit the models
m.base <- lmer(Math ~ Time + (Time | ID), data=EducMH, REML=F)
m.0 <- lmer(Math ~ Time + Condition + (Time | ID), data=EducMH, REML=F)
m.1 <- lmer(Math ~ Time*Condition + (Time | ID), data=EducMH, REML=F)
```

---
# Question 1

**Did the school mental health services improve academic achievement? That is, did the two groups differ on math achievement at baseline and over the 6 years of the study?**

Compare the models

```{r}
anova(m.base, m.0, m.1)
```


There was no group difference overall, but there was a group difference on slope. That is, math achievement increased more quickly in the Treatment group.

---
```{r}
gt(tidy(m.1))
```

---
# Question 2

**For the treatment group, was individual-level improvement in mental health associated with improvement in math scores?**

First make a plot of what we're interested in: the treatment group's change in the SDQ over time showing both group mean (black line with error bars) and individual variability (grey lines)

```{r fig.height=3.5, fig.width=5, echo=FALSE}
ggplot(subset(EducMH, Condition == "Treatment"), aes(Year, SDQ)) + 
  geom_line(aes(group=ID), color="gray") +
  stat_summary(fun=mean, geom="line") +
  stat_summary(fun.data=mean_se, geom="errorbar", width=0.3) +
  theme_bw(base_size=12)
```

Within the treatment group, there is not an overall change in mental health (SDQ), but it looks like there is lots of variability in response to the mental health services. Some people responded really well (big decreases in difficulties on SDQ), some people didn't respond well (increased difficulties according to SDQ).

--

We want to know whether this variability is associated with variability in improved math achievement.

---
# Analysis strategy

1. Build separate models for change in SDQ and change in Math scores over time
2. Use random effects to quantify individual differences in change over time for the two scores
3. Test the correlation between change in SDQ and in Math achievement (and make a scatterplot showing this).

---
# Analysis

1. Build separate models for change in SDQ and change in Math scores over time

```{r}
m.math <- lmer(Math ~ Time + (Time | ID), 
               data=subset(EducMH, Condition == "Treatment"), REML=F)
m.sdq <- lmer(SDQ ~ Time + (Time | ID), 
              data=subset(EducMH, Condition == "Treatment"), REML=F)
```

---
# Analysis

1. Build separate models for change in SDQ and change in Math scores over time
2. Use random effects to quantify individual differences in change over time for the two scores

```{r}
source("get_ranef.R") # get_ranef() will extract the named random effect and clean them up a bit
re.math <- get_ranef(m.math, "ID")
re.sdq <- get_ranef(m.sdq, "ID")
# merge() will combine those into one data frame, but needs some help because the variable names are all the same
re <- merge(re.math, re.sdq, by="ID", suffixes = c(".math", ".sdq"))
summary(re)
head(re)
```

---
# Analysis

1. Build separate models for change in SDQ and change in Math scores over time
2. Use random effects to quantify individual differences in change over time for the two scores
3. Test the correlation between change in SDQ and in Math achievement (and make a scatterplot showing this).

```{r}
cor.test(re$Time.math, re$Time.sdq)
```

Strong correlation ( $r = -0.77, p < 0.0001$ ) indicating that response to mental health intervention (decreased difficulties) was associated with larger increases in math achievement. Note that the key quantities here are **slopes**. That is, the **rate** of decreased mental health difficulties is associated with a higher **rate** of math achievement.

---
# Analysis

1. Build separate models for change in SDQ and change in Math scores over time
2. Use random effects to quantify individual differences in change over time for the two scores
3. Test the correlation between change in SDQ and in Math achievement (and **make a scatterplot showing this**).

```{r fig.height=4, fig.width=4, message=FALSE}
ggplot(re, aes(Time.math, Time.sdq)) + geom_point() + stat_smooth(method="lm") + 
  labs(x="Relative Rate of Increase in\nMath Score", 
       y="Relative Rate of Decrease in\nSDQ Total Difficulties Score") + 
  theme_bw(base_size=12)
```

---
# Why use random effects instead of individual models?

.pull-left[
<img src="./figs/shrinkage-plot-1.png" width="75%"/>

http://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/
]

.pull-right[
An individual's performance (on the math test, on the SDQ) is their actual level plus some noise. 

Individual models (no pooling) don't make that distinction, so you have a noisy estimate of individual differences. 

Multilevel models reduce the noise component using the mean and variance of the rest of the group (partial pooling). **This produces a better estimate of true individual differences.**

See also: Efron, B. & Morris, C. (1977). Stein's Paradox in Statistics. Scientific American, 236:5, 119-127.
]

---
# A word of warning

This approach to quantifying individual differences is sometimes called **"Best Linear Unbiased Predictors"** (BLUPs)

They are **estimates**; like all estimates, they are made with some degree of error. But this error is not carried forward into the individual difference analysis, which can produce false positives.

.pull-left[
```{r echo=FALSE, fig.height=5, fig.width=5, message=FALSE}
# get random effects with SD
re.math2 <- as.data.frame(ranef(m.math)) %>% 
  filter(term=="Time") %>% 
  select(-grpvar, -term)
re.sdq2 <- as.data.frame(ranef(m.sdq)) %>% 
  filter(term=="Time") %>% 
  select(-grpvar, -term)
re2 <- merge(re.math2, re.sdq2, by="grp", suffixes = c(".math", ".sdq"))

# plot with variance
ggplot(re2, aes(condval.math, condval.sdq)) + 
  geom_pointrange(aes(ymin=condval.sdq-condsd.sdq, 
                      ymax=condval.sdq+condsd.sdq)) + 
  geom_linerange(aes(xmin=condval.math-condsd.math, 
                     xmax=condval.math+condsd.math)) +
  stat_smooth(method="lm") + 
  labs(x="Relative Rate of Increase in\nMath Score", 
       y="Relative Rate of Decrease in\nSDQ Total Difficulties Score") + 
  theme_bw(base_size=12)
```
]

.pull-right[
Multivariate models provide an alternative approach to modeling data with multiple outcomes (e.g., Math and SDQ scores).
]

---
# Key points

.pull-left[
* Individual differences provide deeper insights into group-level phenomena
* Can assess them by adding them to the multilevel model
* Dividing a continuous variable into levels can help to visualise complex interactions
]
.pull-right[
* Random effects provide a useful way to quantify individual differences in the context of a group-level model
* Partial pooling / shrinkage improves individual difference estimates
* But be careful about using these in subsequent analyses without considering the uncertainly associated with those estimates
]