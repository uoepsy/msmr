```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
require(patchwork)
source("plottingmixedmods.R")
```

# Recap & Individual differences 

### Packages {-}  

+ lme4  

```{r message=FALSE,warning=FALSE}
require(tidyverse)
require(lme4)
library(lmerTest)
library(effects)
```


### Lecture Slides {-}  

+ Lecture slides are available [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/Lecture05_IndivDiffs.html)  

## Mixed model recap {-} 

In a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.  
<br>
Multi-level (or 'mixed-effects') approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them. 
<br>
We can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).  
<br>
The first part of this lab will guide you through a walkthrough of these concepts, before you tackle some exercises.  


### Simple regression {-}

<div class="noteBox">
**Formula:**  
  
+ $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$  
  
**R command:**  
  
+ `lm(outcome ~ predictor, data = dataframe)`  
  
*Note:* this is the same as `lm(outcome ~ 1 + predictor, data = dataframe)`. The `1 +` is always there unless we specify otherwise (e.g., by using `0 +`).
</div>
<br>
```{r echo=FALSE}
plot_data + plot_lm
```



### Clustered (multi-level) data structures {-}

When our data is clustered (or 'grouped') such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.  

If we separate out our data to show an individual plot for each subject, we can see how the fitted regression line from `lm()` is assumed to be the same for each subject.  
  
```{r echo=FALSE, fig.height=10}
plot_lm_fac
```

### Random intercept models {-} 

By including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.

<div class="noteBox">
**Formula:**  
Level 1:  
  
+ $Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}$  
  
Level 2:  
  
+ $\beta_{0j} = \gamma_{00} + u_{0j}$   

Where the expected values of $u_0$, and $\epsilon$ are 0, and their variances are $\sigma_{u_0}^2$ and $\sigma_\epsilon^2$ respectively.  We will further assume that these are normally distributed.

We can now see that the $\beta_0$ estimate for a particular group $j$ is represented by the combination of a mean estimate for the parameter ($\gamma_{00}$) and a random effect for that group ($u_{0j}$).


**R command:**  
  
+ `lmer(outcome ~ predictor + (1 | grouping), data = dataframe)`  
  
</div>

Notice how the fitted line of the random intercept model has an adjustment for each subject.  
Each subject's line has been moved up or down accordingly. 

```{r echo=FALSE, fig.height=8, fig.width=12}
plot_lm_fac + plot_ri_fac
```

<div class="lo">

#### Shrinkage {-}  

If you think about it, we might have done a similar thing with the tools we already had at our disposal, by using `lm(y~x+subject)`.  
This would give us a coefficient for the difference between each subject and the reference level intercept.  
  
However, the estimate of these models will be slightly different:  

```{r echo=FALSE}
plot_shrinkage
```

**Why?** One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters. 

</div>

### Random slopes {-} 


<div class="noteBox">
**Formula:**  
Level 1:  
  
+ $Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}$  
  
Level 2:  
  
+ $\beta_{0j} = \gamma_{00} + u_{0j}$  
+ $\beta_{1j} = \gamma_{10} + u_{1j}$  

Where the expected values of $u_0$, $u_1$, and $\epsilon$ are 0, and their variances are $\sigma_{u_0}^2$, $\sigma_{u_1}^2$, $\sigma_\epsilon^2$ respectively. We will further assume that these are normally distributed.

As with the intercept $\beta_0$, the slope of the predictor $\beta_1$ is now modelled by a mean and a random effect for each group ($u_{1j}$). 


**R command:** 
  
+ `lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)`  
  
*Note:* this is the same as `lmer(outcome ~ predictor + (predictor | grouping), data = dataframe)` . Like in the fixed-effects part, the `1 +` is assumed in the random-effects part.
</div>

```{r echo=FALSE, fig.height=8, fig.width=12}
plot_ri_fac + plot_rs_fac
```


### Fixef(), Ranef, and Coef {-}

The plots below show the fitted values from each model for each subject: 

```{r echo=FALSE, fig.width=12}
plotdata2<-
  ggplot(dat, aes(x=x1,y=outcome, col=subject))+
  geom_point(alpha=0.5)+geom_path(alpha=0.5)+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")+
  scale_y_continuous(breaks=NULL)+scale_x_continuous(breaks=NULL)+
  labs(title="- The data (by subject) - ", y="y", x="x")+
  NULL

#(plot_data + plotdata2 ) / (plot_lm2 + plot_ri + plot_rs)
plot_lm2 + plot_ri + plot_rs
```

In the random-intercept model (center panel), the differences from each of the subjects' intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation $\sigma_u$.  The standard deviation (and variance, which is $\sigma_u^2$) is what we see in the random effects part of our model summary (or using the `VarCorr()` function).  
  
```{r echo=FALSE, out.width="400px"}
random_intercept_model = mod2
random_slopes_model = mod3
knitr::include_graphics("images/varcors.PNG")
```
  
In the random-slope model (right panel), the same is true for the differences from each subjects' slope to the fixed slope. 


`r msmbstyle::solution_begin(header = "&#x25BA; Fixed effects", hidden=FALSE)`
We can extract the *fixed effects* using the `fixef()` function:
```{r}
fixef(random_intercept_model)
```
`r msmbstyle::solution_end()` 

`r msmbstyle::solution_begin(header = "&#x25BA; Random effects", hidden=FALSE)`
We can extract the deviations for each group from these fixed effect estimates using the `ranef()` function.
```{r}
ranef(random_intercept_model)
```
`r msmbstyle::solution_end()` 

`r msmbstyle::solution_begin(header = "&#x25BA; Group-level coefficients", hidden=FALSE)`
We can also see the estimate for each subject directly, using the `coef()` function.  
These sometimes get referred to as "Best Linear Unbiased Estimates (BLUPS)". 

```{r}
coef(random_intercept_model)
```
`r msmbstyle::solution_end()` 

`r msmbstyle::solution_begin(header = "&#x25BA; Plotting random effects", hidden=FALSE)`
The quick and easy way to plot your random effects is to use the `dotplot.ranef.mer()` function in `lme4`. 

```{r}
randoms <- ranef(random_intercept_model, condVar=TRUE)

dotplot.ranef.mer(randoms)
```

<!-- Sometimes, however, we might want to have a bit more control over our plotting, we can extract the estimates and correlations for each subject using the `condVar=TRUE`: --> -->
<!-- ```{r} -->
<!-- #we can get the random effects: -->
<!-- #(note that we use $subject because there might be other groupings, and the ranef() function will give us a list, with one element for each grouping variable) -->
<!-- randoms <-  -->
<!--   ranef(random_slopes_model)$subject %>% -->
<!--   mutate(subject = row.names(randoms)) %>%  # the subject IDs are stored in the rownames, so lets add them as a variable -->
<!--   pivot_longer(cols=1:2, names_to="term",values_to="estimate") # finally, let's reshape it for plotting -->

<!-- #and the same for the standard errors (from the arm package): -->
<!-- randoms_se <-  -->
<!--   se.ranef(random_slopes_model)$subject %>% -->
<!--   enframe %>% -->
<!--   mutate(subject = row.names(randoms_se)) %>% -->
<!--   pivot_longer(cols=1:2, names_to="term",values_to="se") -->

<!-- # join them together: -->
<!-- left_join(randoms, randoms_se) -->




<!-- # it's easier for plotting if we  -->

<!-- ggplot(ranefs_plotting, aes(y=)) -->

<!-- ``` -->
`r msmbstyle::solution_end()` 


### More levels: Nested and Crossed random-effects {-}  

The same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups). 

`r msmbstyle::question_begin()`
Consider the example where we have observations for each student in every class within a number of schools:  

```{r echo=FALSE, out.width="1200px"}
knitr::include_graphics("images/structure_id.png")
```

Is "Class 1" in "School 1" the same as "Class 1" in "School 2"? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
No.  
<br>
The classes in one school are distinct from the classes in another *even though they are named the same*.  
  
The classes-within-schools example is a good case of **nested random effects** - one factor level (one group in a grouping varible) appears *only within* a particular level of another grouping variable.  
  
In R, we can specify this using:  
  
`(1 | school) + (1 | class:school)`  
  
or, more succinctly:  
  
`(1 | school/class)`  

`r msmbstyle::solution_end()` 

`r msmbstyle::question_begin()`
Consider another example, where we administer the same set of tasks at multiple time-points for every participant.  
Are tasks nested within participants?  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
No - tasks are seen by multiple participants (and participants see multiple tasks).  

We could visualise this as the below:  
```{r echo=FALSE, out.width="400px"}
knitr::include_graphics("images/structure_crossed.png")
```

In the sense that these are not nested, they are **crossed** random effects.  
  
In R, we can specify this using:  

`(1 | subject) + (1 | task)`  

`r msmbstyle::solution_end()` 

<div class="red">
#### Nested vs Crossed {-}  
*Nested:* Each group belongs uniquely to a higher-level group.   

*Crossed:* Not-nested. 
</div>


Note that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures `(1 | school) + (1 | class)` and `(1 | school/class)` would give the same results.  
```{r echo=FALSE, out.width="1200px"}
knitr::include_graphics("images/structure_nested.png")
```

<!-- ## Fitting models   -->

<!-- A simple regression can be solved using *Ordinary Least Squares (OLS)*, to find the values of $\beta_0$ and $\beta_1$ which minimise the sum of the squared residuals (the distance from the predicted values of $y$ to the observed values of $y$). -->

<!-- We saw in USMR that we could extend the principles of linear regression to non-numeric outcomes such as a binomial variable. In this case, we modelled the log-odds, and we left OLS behind in favour of *Maximum Likelihood Estimation (MLE)*. MLE can only be found by *numerical optimization*. Optimisation consists of maximising/minimising a function through iteration (choosing a set of input values, computing the output of the function, and refitting). In its simplest form, think of it as finding the highest or lowest point of a curve.   -->

<!-- Maximum likelihood estimation:  -->

<!-- 1. Begin with some an initial guess (start values) for parameter values. -->
<!-- 1. Compute the log-likelihood, and adjust the guess at parameter values. (Each time this is done is an *iteration*) -->
<!-- 1. Recompute the likelihood. -->
<!-- 1. Repeat until improvements in likelihood value are neglible across iterations -->

<!-- When we have multiple parameters, this problem becomes a lot more complex as likelihood is not a simple curve but a mult-dimensional  -->


<!-- There are different approaches to optimisation. For our purposes, this means that we have a choice of optimisers. Some may converge (find a solution) when other's don't. See [Week 4](#other-random-effects-structures) for more details on convergence.   -->

  
<hr />
 
   
<hr />
 

## THE LAB


<div class="red">
#### The data {-} 

44 participants across 4 groups (between-subjects) were tested 5 times (waves) in 11 domains. 
In each wave, participants received a score (on a 20-point scale) for each domain and a set of questions which were they answered either correctly or incorrectly.  



```{r}
load(url("https://edin.ac/2Hd6V4Q"))

summary(dat5)
```

</div>


### Exercise 1 

<div class="lo">
#### Research question {-}   
Did the groups differ in overall performance?
</div>
  
There are different ways to test this: use the 20-point score or the accuracy? Keep the domains separate or calculate an aggregate across all domains? Which way makes the most sense to you?  
  
`r msmbstyle::question_begin(header = "&#x25BA; Question 1")`
Make a plot that corresponds to the reseach question. Does it look like there's a difference?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Lots of options for this one, here is one that shows Group and Domain differences:

```{r}
ggplot(dat5, aes(Domain, Score, color=Group)) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  coord_flip()
```

Looks like there are group differences and domain differences, but not much in the way of group-by-domain differences.
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 2")`
Use a mixed-effects model to test the difference.  
  
+ Will you use a linear or logistic model?
+ What should the fixed(s) effect be? 
+ What should the random effect(s) be? We have observations clustered by subjects and by domains - are they nested?  
  
  
*Tip:* For now, forget about the longitudinal aspect to the data.
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
We're interested in the amount to which Groups vary in their overall performance, so we want a fixed effect of Group. Subjects and Domains are not nested - each subject sees different domains, and each domain is seen by multiple subjects.  

```{r}
# maximal model doesn't converge, removed random Group slopes for Domain
mod_grp <- lmer(Score ~ Group + 
                   (1 | Anonymous_Subject_ID) + 
                   (1 | Domain), 
                 data=dat5, REML=FALSE)
summary(mod_grp)
```

Yes, substantial Group differences: overall, group A does the best, group B is slightly behind, group C next, and group W does the worst. 
`r msmbstyle::solution_end()` 

<hr />


## Exercise 2  

<div class="lo">
#### Research question {-}   
Did performance change over time (across waves)? Did the groups differ in pattern of change?
</div>

`r msmbstyle::question_begin(header = "&#x25BA; Question 3")`
Make a plot that corresponds to the reseach question. Does it look like there was a change? A group difference?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`

```{r, fig.width=5, fig.height=4}
ggplot(dat5, aes(Wave, Score, color=Group, fill=Group)) +
  stat_summary(fun.data=mean_se, geom="ribbon", alpha=0.3, color=NA) +
  stat_summary(fun.y=mean, geom="line")
```

Yes, looks like groups A, C, and W are improving, but group B is getting worse.
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 4")`
Use mixed-effects model(s) to test this.  
<br>
*Hint:* Fit a baseline model in which scores change over time (wave), then assess improvement in model fit due to inclusion of overall group effect and finally the interaction of group with time.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
mod_wv <- lmer(Score ~ Wave + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = "bobyqa"))

mod_wv_grp <- lmer(Score ~ Wave+Group + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = "bobyqa"))

mod_wv_x_grp <- lmer(Score ~ Wave*Group + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = "bobyqa"))

anova(mod_wv, mod_wv_grp, mod_wv_x_grp)
summary(mod_wv_x_grp)
```
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 5")`
Plot the group-level data (see Question 3) and model fitted values for each group from your final model from Question 4.  
<br>
*Hint:* using `fortify(model)` or `broom::augment(model)` as your starting point will help.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  stat_summary(aes(y=.fitted), fun.y=mean, geom="line")
```

*We fit a linear model, but the model fit lines are not straight lines. Why is that?*  
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 6")`
Create individual subject plots for the data and the model's fitted values. Will these show straight lines?  
<br>
*Hint:* make use of `facet_wrap()` to create a different panel for each level of a grouping variable.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) +
  facet_wrap(~ Anonymous_Subject_ID) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  stat_summary(aes(y=.fitted), fun.y=mean, geom="line")
```

The individual subject plots show linear fits, which is a better match to the model. But now we see the missing data -- some participants only completed the first few waves.  
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 7")`
Make a plot of the actual (linear) model prediction.  
<br>
*Hint:* Use the `effect()` function from the `effects` package.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`

```{r}
ef <- as.data.frame(effect("Wave:Group", mod_wv_x_grp))
ggplot(ef, aes(Wave, fit, color=Group, fill=Group)) +
  geom_ribbon(aes(ymax=fit+se, ymin=fit-se), color=NA, alpha=0.1) +
  geom_line()
```

`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 8")`
What important things are different between the plot from question 7 and that from question 5?  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE, toggle=FALSE)`
*Group B was not actually getting worse. The appearance that it was getting worse is an artifact of selective drop-out: there's only a few people in this group and the better-performing ones only did the first few waves so they are not represented in the later waves, but the worse-performing ones are contributing to the later waves. The model estimates how the better-performing ones would have done in later waves based on their early-wave performance and the pattern of performance of other participants in the study.*  

```{r echo=FALSE}
cfs <- coef(summary(mod_wv_x_grp))
```

```{r}
summary(mod_wv_x_grp)$coefficients
```

Note that the Group A slope (coefficient for `Wave`) is `r round(cfs[2, 1], 3)` and, relative to that slope, the Group B slope is `r round(cfs[6, 1], 3)` (coefficient for `Wave:GroupB`). This means that the model-estimated slope for Group B is `r round(cfs[2, 1] + cfs[6, 1], 3)`, which is very slightly positive, not strongly negative as appeared in the initial plots.

One of the valuable things about mixed-effects (aka multilevel) modeling is that individual-level and group-level trajectories are estimated. This helps the model overcome missing data in a sensible way. In fact, MLM/MLR models are sometimes used for imputing missing data. However, one has to think carefully about *why* data are missing. Group B is small and it might just be a coincidence that the better-performing participants dropped out after the first few waves, which would make it easier to generalize the patterns to them. On the other hand, it might be the case that there is something about the study that makes better-performing members of Group B drop out, which should make us suspicious of generalizing to them.
`r msmbstyle::solution_end()` 

<hr />

`r msmbstyle::question_begin(header = "&#x25BA; Question 9")`
Create a plot of the subject and domain random effects. 
Notice the pattern between the random intercept and random slope estimates for the 11 domains - what in our model is this pattern representing?  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
randoms <- ranef(mod_wv_x_grp, condVar=TRUE)
dotplot.ranef.mer(randoms)
```

Notice that the domains with the lower relative intercept tend to have a higher relative slope (and vice versa). This is the negative correlation between random intercepts and slopes for domain in our model: 
```{r}
VarCorr(mod_wv_x_grp)
```

Try removing the correlation (hint: use the `||`) to see what happens. Does it make sense that these would be correlated? (Answer: we don't really know enough about the study, but it's something to think about!)
`r msmbstyle::solution_end()` 


