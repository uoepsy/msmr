---
title: "Contrasts - A quick overview"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
source('assets/setup.R')
library(patchwork)
library(tidyverse)
library(lme4)
library(broom.mixed)
library(effects)
```
 

We largely avoided the discussion of contrasts in USMR because they can be quite incomprehensible at first, and we wanted to focus on really consolidating our learning of the linear model basics.  

:::frame
**What are contrasts and when are they relevant?**  
  
Recall when we have a categorical predictors? To continue the example in USMR, Martin's lectures considered the utility of different types of toys (Playmo, SuperZings, and Lego).  
```{r echo=FALSE, out.width="400px", fig.cap="[USMR Week 9 Lecture](https://uoepsy.github.io/usmr/lectures/lecture_8.html#36) utility of different toy_types"}
knitr::include_graphics("images/loglong/leccontrasts.png")
```

When we put a categorical variable into our model as a predictor, we are always relying on **contrasts**, which are essentially just combinations of 0s & 1s which we use to define the differences which we wish to test.  

The default, *treatment* coding, treats one level as a **reference group**, and results in coefficients which compare each group to that reference.  
When a variable is defined as a **factor** in R, there will always be an attached "attribute" defining the contrasts:  
```{r}
#read the data
toy_utils <- read_csv("https://uoepsy.github.io/msmr/data/toyutility.csv")

#make type a factor
toy_utils <- toy_utils %>%
  mutate(
    type = factor(type)
  )
```


```{r}
# look at the contrasts
contrasts(toy_utils$type)
```
Currently, the resulting coefficients from the contrasts above will correspond to:

- Intercept = estimated mean utility for Lego (reference group)
- typeplaymo = estimated difference in mean utility Playmo vs Lego
- typezing = estimated different in mean utility Zing vs Lego

:::

`r qbegin("1")`
Read in the toy utility data from [https://uoepsy.github.io/msmr/data/toyutility.csv](https://uoepsy.github.io/msmr/data/toyutility.csv).  
We're going to switch back to the normal `lm()` world here for a bit, so no multi-level stuff while we learn more about contrasts.

1. Fit the model below 
    ```{r}
    model_treatment <- lm(UTILITY ~ type, data = toy_utils)
    ```
2. Calculate the means for each group, and the differences between them, to match them to the coefficients from the model above

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

You don't have to do it this way. This was just the first way to calculate the difference in means that came to my head. `group_by()` will definitely come in handy though. 
```{r}
toy_utils %>% group_by(type) %>%
  summarise(
    mutil = mean(UTILITY)
  ) %>% 
  mutate(
    diff = mutil - mutil[type == "lego"]
  )

coef(model_treatment)
```
`r solend()`


:::frame
**How can we change the way contrasts are coded?**  

In our regression model we are fitting an intercept plus a coefficient for each explanatory variable, and any categorical predictors are coded as 0s and 1s.  

We can add constraints to our the way our contrasts are coded. In fact, with the default **treatment** coding, we already have applied one - we constrained them such that the intercept ($\beta_0$) = the mean of one of the groups. 

`r optbegin("Optional: Why constraints?", olabel=FALSE, toggle=params$TOGGLE)`

Why do we not have:  

- `Intercept`
- `typelego`
- `typeplaymo`
- `typezing`

The model has would become **not identifiable**. We cannot distinguish any more whether the group mean is due to a group effect or not. For example, we can get the same group mean $μ_{lego}=10$, say, either by setting:

- $\beta_0=0$ and $\beta_1=10$, which interprets as "there is an effect due to being lego"
- $\beta_0=10$ and $\beta_1=0$, which interprets as "there is no effect due to being lego"

`r optend()`

The other common constraint that is used is the **sum-to-zero** constraint. This approach gets called "effects coding".  
Under this constraint the interpretation of the coefficients becomes:

- $\beta_0$ represents the global mean (sometimes referred to as the "grand mean"). 
- $\beta_1$ the effect due to group $i$ — that is, the mean response in group $i$ minus the global mean.  

**Changing contrasts in R**  
There are two main ways to change the contrasts in R. 

1. Change them in your data object.
    ```{r}
    contrasts(toy_utils$type) <- "contr.sum"
    ```
2. Change them *only* in the fit of your model (don't change them in the data object)
    ```{r}
    model_sum <- lm(UTILITY ~ type, contrasts = list(type = "contr.sum"), data = toy_utils)
    ```

:::

`r qbegin("2")`

1. Refit the same model of utility by toy-type, using effects coding for the `type` variable
2. Then calculate the global mean, and the differences from each mean to the global mean, to match them to the coefficients

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
model_sum <- lm(UTILITY ~ type, contrasts = list(type = "contr.sum"), data = toy_utils)

globmean <- mean(toy_utils$UTILITY)
globmean

toy_utils %>% group_by(type) %>%
  summarise(
    mutil = mean(UTILITY)
  ) %>% 
  mutate(
    diff = mutil - globmean
  )

coef(model_sum)
```

`r solend()`
