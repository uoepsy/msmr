{"title":"3A: Polynomial Growth","markdown":{"yaml":{"title":"3A: Polynomial Growth","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Linear vs Non-Linear","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n```\n\n\n:::lo\nThis reading:  \n\n\n:::\n\n\nWe have already seen in the last couple of weeks that we can use MLM to study something 'over the course of X'. In the novel word learning experiment from last week's lab, we investigated the change in the probability of answering correctly over the course of the experimental blocks. \n\nWe've talked about how **\"longitudinal\"** is the term commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opens up the potential for observations to be unevenly spaced, or missing at certain points. It also, as will be the focus of this week, opens the door to thinking about how many effects of interest are likely to display **non-linear patterns**. These exercises focus on including higher-order polynomials in the multi-level model to capture non-linearity. \n  \n\n```{r include=F}\nres <- MASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% lm(y~x,.) %>% broom::tidy() %>% mutate(across(estimate:statistic,~round(.,2)))\n```\n\n<div style=\"display:inline-block;width:45%;margin:5px;vertical-align:middle\">\nSuppose we had collected the data in Figure \\@ref(fig:mcycle), and we wanted to fit a model to predict $y$ based on the values of $x$.  \n\nLet's use our old friend linear regression, $y = \\beta_0 + \\beta_1(x) + \\varepsilon$.  \n\nWe'll get out some estimated coefficients, some standard errors, and some p-values:  \n\n- The intercept:  \n  $\\beta_0$ = `r res[1,2]`, SE = `r res[1,3]`, p < .001  \n- The estimated coefficient of x:  \n  $\\beta_1$ = `r res[2,2]`, SE = `r res[2,3]`, p < .001   \n\nJob done? Clearly not - we need only overlay model upon raw data (Figure \\@ref(fig:mcycle3)) to see we are missing some key parts of the pattern.  \n</div>\n<div style=\"display:inline-block;width:45%;margin:5px;vertical-align:middle\">\n```{r mcycle, echo=FALSE, fig.cap = \"A clearly non-linear pattern\"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>%\n  ggplot(.,aes(x=x,y=y))+\n  geom_point(size=2)\n```\n```{r mcycle3, echo=FALSE, fig.asp=.7,fig.cap=\"Uh-oh... \"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x,.) %>%\n  sjPlot::plot_model(type=\"pred\", show.data = TRUE) -> p0\np0$x\n```\n\n</div>  \n\n:::statbox\n**Thoughts about Model + Error**  \n\nAll our work here is in aim of making models of the world.  \n\n1. Models are just models. They are simplifications, and so they don't perfectly fit to the observed world (indeed, how well a model fits to the world is often our metric for comparing models).  \n2. $y - \\widehat{y}$. Our observed data minus our model predicted values (i.e. in linear regression our \"residuals\") reflect everything that we don't account for in our model\n3. In an ideal world, our model accounts for all the systematic relationships, and what is left over (our residuals) is just randomness. If our model is mis-specified, or misses out something systematic, then our residuals will reflect this. \n4. We check for this by examining how much like randomness the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d (\"independent and identically distributed\") - i.e., what gets referred to as the \"assumptions\"). \n5. We will **never** know whether our residuals contain only randomness, because we can never observe *everything*. \n\n<h/>\n\nLet's just do a quick `plot(model)` for some diagnostic plots of my linear model:\n```{r echo=FALSE, out.width = \"100%\"}\npar(mfrow=c(2,2))\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>%\nlm(y~x,.) %>% plot()\npar(mfrow=c(1,1))\n```\n\nDoes it look like the residuals are independently distributed? Not really. \nWe need to find some way of incorporating the non-linear relationship between y and x into our model. \n\n:::\n\n\n# What is a polynomial?  \n\n:::statbox\nPolynomials are mathematical expressions which involve a sum of powers. For instance:\n\n- $y = 4 + x + x^2$ is a second-order polynomial as the highest power of $x$ that appears is $x^2$  \n- $y = 9x + 2x^2 + 4x^3$ is a third-order polynomial as the highest power of $x$ that appears is $x^3$  \n- $y = x^6$ is a sixth-order polynomial as the highest power of $x$ that appears is $x^6$  \n\n:::\n\nFor our purposes, extending our model to include higher-order terms can fit non-linear relationships between two variables. For instance, fitting models with linear and quadratic terms ($y_i$ = $\\beta_0 + \\beta_1 x_{i} \\ + \\beta_2 x^2_i + \\varepsilon_i$) and extending these to cubic ($y_i$ = $\\beta_0 + \\beta_1 x_{i} \\ + \\beta_2 x^2_i + \\beta_3 x^3_i + \\varepsilon_i$) (or beyond), may aid in modelling nonlinear patterns.\n\n```{r echo=FALSE, out.width = \"100%\", fig.heigth = 4}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x+I(x^2),.) %>%\n  sjPlot::plot_model(type=\"pred\",show.data=TRUE) -> p\n\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x+I(x^2)+I(x^3),.) %>% \n  sjPlot::plot_model(type=\"pred\",show.data=TRUE) -> p1\n\np0$x + labs(title=\"\",subtitle=\"y ~ x + e\") +\np$x + labs(title=\"\",subtitle=\"y ~ x + x^2 + e\") +\np1$x + labs(title=\"\",subtitle=\"y ~ x + x^2 + x^3 + e\") & theme_bw(base_size=12)\n```\n\n:::statbox\n**What are we interested in here?**  \n\nAs the order of polynomials increases, we tend to be less interested in these terms in our model. Linear change is the easiest to think about: are things going up over the course of $x$, or down? (or neither?). Quadratic change is the next most interesting, and it may help to think of this as the \"rate of change\". For instance, in the plot below, it is the quadratic term which differs between the two groups trajectories. \n\n```{r echo=FALSE}\nx=1:10\ntibble(\n  y=c(x*3, (x*1.1+(x^2)*.2)),\n  xx=rep(x,2),\n  g = rep(letters[1:2],each=10)\n) %>% ggplot(.,aes(x=xx,y=y,col=g))+geom_line()+\n  labs(x=\"x\")\n```\n\n`r optbegin(\"Positive and negative quadratic terms\",olabel=FALSE,toggle=params$TOGGLE)`\n```{r quadfig, echo=FALSE}\ntibble(\n  x = -10:10,\n  y = x^2,\n  y1 = -x^2\n) %>% pivot_longer(y:y1) %>%\n  mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n Y = X^2\", \"quadratic term is negative \\n Y = -X^2\")) %>%\n  ggplot(.,aes(x=x,y=value,col=name)) +\n  geom_line()+\n  guides(col=FALSE)+\n  facet_wrap(~name, scales=\"free_y\")\n```\n\n<!-- `r optbegin(\"Code to create plot\", olabel=FALSE, toggle=params$TOGGLE)` -->\n<!-- ```{r eval=FALSE} -->\n<!-- tibble( -->\n<!--   x = -10:10, -->\n<!--   y = x^2, -->\n<!--   y1 = -x^2 -->\n<!-- ) %>% pivot_longer(y:y1) %>% -->\n<!--   mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n Y = X^2\", \"quadratic term is negative \\n Y = -X^2\")) %>% -->\n<!--   ggplot(.,aes(x=x,y=value,col=name)) + -->\n<!--   geom_line()+ -->\n<!--   guides(col=FALSE)+ -->\n<!--   facet_wrap(~name, scales=\"free_y\") -->\n<!-- ``` -->\n<!-- `r optend()` -->\n\n`r optend()`\n:::\n\n\n\n## Raw Polynomials\n\n<div style=\"display:inline-block; width: 60%; vertical-align: top\">\nThere are two types of polynomial we can construct. \"Raw\" (or \"Natural\") polynomials are the straightforward ones you might be expecting the table to the right to be filled with.  \n\nThese are simply the original values of the x variable to the power of 2, 3 and so on.  \n  \nWe can quickly get these in R using the `poly()` function, with `raw = TRUE`.  \n\nIf you want to create \"raw\" polynomials, make sure to specify `raw = TRUE` or you will not get what you want because the default behaviour is `raw = FALSE`.\n</div>\n<div style=\"display:inline-block; width: 30%;\">\n<center>\n```{r echo=FALSE}\nMASS::mcycle %>%\n    rename(y=accel,x=times) %>% filter(x>20) -> df\ncbind(x=1:5,`x^2`=rep(\"?\",5),`x^3`=rep(\"?\",5)) %>% rbind(\"...\") %>% \n    kableExtra::kable() %>%\n    kableExtra::kable_styling(full_width = FALSE)\n```\n</center>\n</div>\n\n```{r}\npoly(1:10, degree = 3, raw=TRUE)\n```\n\n\n<div style=\"background-color: #eafaff; border-radius: 5px; padding: 20px 20px 10px 20px; margin-top: 20px; margin-bottom: 20px;\">\n**Raw polynomials are correlated**  \n\nWith raw (or \"natural\") polynomials, the terms `poly1`, `poly2` and `poly3` are correlated.  \nThink think about why this might be - by definition, as $x^1$ increases, so will $x^2$, and so will $x^3$ and so on.  \n\nWe can visualise them: \n```{r}\nmatplot(poly(1:10, 3, raw=T), type=\"l\")\n```\nAnd measure the correlation coefficients:\n```{r}\ncor(poly(1:10, 3, raw=T)) %>% round(2)\n```\n\nWhy might this be a problem?  \nWell, this multicollinearity can lead to estimation problems, and means that our parameter estimates may change considerably depending upon what terms we include in our model, and it becomes more difficult to determine which ones are important, and what the effect sizes are.  \nTable \\@ref(tab:rawpolytab) below shows the coefficients for models fitted to a randomly generated dataset, with `poly1`, `poly1+poly2`, and `poly1+poly2+poly3` as predictors (where `poly1`-`poly3` are **natural** polynomials). Notice that they change with the addition of each term.\n```{r rawpolytab, echo=FALSE}\nset.seed(754)\ndf<-tibble(\n    x = 1:10,\n    y = x + \n        rnorm(1, mean = 100) * (x) +\n        rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n        rnorm(1, mean = -1) * (x) ^ 3 + \n        rnorm(10)\n)\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, orthogonal = FALSE, draw.poly = FALSE)\nfull_join(\n    broom::tidy(lm(y~poly1, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"*\",\"**\"))) %>% \n        rename(`y~poly1` = estimate),\n    broom::tidy(lm(y~poly1+poly2, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"*\",\"***\",\"***\"))) %>% \n        rename(`y~poly1+poly2` = estimate)\n) %>% full_join(.,\n                broom::tidy(lm(y~poly1+poly2+poly3, df))[,1:2] %>% \n                    #mutate(estimate = paste(round(estimate,2), c(\"\",\"***\",\"\",\"***\"))) %>% \n                    rename(`y~poly1+poly2+poly3` = estimate)\n) %>% mutate_if(is.numeric,~round(.,2)) %>% \n    mutate_all(~ifelse(is.na(.), \"-\", .)) %>% \n    kableExtra::kbl(caption = \"Incremental addition of raw polynomial terms\") %>%\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n</div>\n\n## Orthogonal Polynomials  \n\n\"Orthogonal\" polynomials are uncorrelated (hence the name). \nWe can get these for $x = 1,2,...,9,10$ using the following code:\n```{r}\npoly(1:10, degree = 3, raw = FALSE)\n```\nNotice that the first order term has been scaled, so instead of the values 1 to 10, we have values ranging from -0.5 to +0.5, centered on 0. \n\n```{r}\ncolMeans(poly(1:10, degree = 3, raw = FALSE)) %>%\n    round(2)\n```\n\nAs you can see from the output above, which computes the mean of each column, each predictor has mean 0, so they are mean-centred. __This is a key fact and will affect the interpretation of our predictors later on__.\n\nThink about what this means for $x^2$. It will be uncorrelated with $x$ (because $-0.5^2 = 0.5^2$)!  \n```{r}\nmatplot(poly(1:10, 3, raw=F), type=\"l\")\n```\nThe correlations are zero!\n```{r}\ncor(poly(1:10, 3, raw=F)) %>% round(2)\n```\n\nWe can then fit the same models `y~poly1`, `y~poly1+poly2`, and `y~poly1+poly2+poly3` as predictors (where `poly1`-`poly3` are now **orthogonal** polynomials), and see that estimated coefficients do not change between models: \n```{r orthpolytab, echo=FALSE}\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, orthogonal = TRUE, draw.poly = FALSE)\nfull_join(\n    broom::tidy(lm(y~poly1, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"\",\"**\"))) %>% \n        rename(`y~poly1` = estimate),\n    broom::tidy(lm(y~poly1+poly2, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"***\",\"***\",\"***\"))) %>% \n        rename(`y~poly1+poly2` = estimate)\n) %>% full_join(.,\n                broom::tidy(lm(y~poly1+poly2+poly3, df))[,1:2] %>% \n                    #mutate(estimate = paste(round(estimate,2), c(\"***\",\"***\",\"***\",\"***\"))) %>% \n                    rename(`y~poly1+poly2+poly3` = estimate)\n) %>% mutate_if(is.numeric,~round(.,2)) %>% \n    mutate_all(~ifelse(is.na(.), \"-\", .)) %>% \n    kableExtra::kable(caption = \"Incremental addition of orthogonal polynomial terms\") %>%\n    kableExtra::kable_styling(full_width = FALSE)\n\n```\n\n\n:::statbox\n**Remember what zero is!** \n\nWith orthogonal polynomials, you need to be careful about interpreting coefficients. For raw polynomials the intercept remains the y-intercept (i.e., where the line hits the y-axis). The higher order terms can then be thought of from that starting point - e.g., \"where $x$ is 2, $\\widehat{y}$ is $\\beta_0 + \\beta_1 \\cdot 2 + \\beta_2 \\cdot 2^2 + \\beta_3 \\cdot 2^3 ...$\".  \n<br>\nFor orthogonal polynomials, the interpretation becomes more tricky. The intercept is the overall average of y, the linear predictor is the linear change pivoting around the mean of $x$ (rather than $x = 0$), the quadratic term corresponds to the steepness of the quadratic curvature (\"how curvy is it?\"), the cubic term to the steepness at the inflection points (\"how wiggly is it?\"), and so on. \n\n::: \n\n## Some useful code from Dan\n\n:::rtip\n\nIt's possible to use `poly()` internally in fitting our linear model, if we want:  \n```{r eval=FALSE}\nlm(y ~ poly(x, 3, raw = T), data = df)\n```\n<br>\nUnfortunately, the coefficients will end up having long messy names `poly(x, 3, raw = T)[1]`, `poly(x, 3, raw = T)[2]` etc.   \n<br>\nIt is probably nicer if we add the polynomials to our data itself. As it happens, Dan has provided a nice little function which attaches these as columns to our data, naming them `poly1`, `poly2`, etc. \n```{r include=FALSE}\nset.seed(754)\nmydata<-tibble(\n  x = 1:10,\n  y = x + \n  rnorm(1, mean = 100) * (x) +\n  rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n  rnorm(1, mean = -1) * (x) ^ 3 + \n  rnorm(10)\n) %>% rename(time=x)\n```\n\n```{r}\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nmydata <- code_poly(df = mydata, predictor = 'time', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(mydata)\n```\n\nBoth will produce the same model output (but Dan's method produces these nice neat names for the coefficients!), and we can just put the terms into our model directly as `lm(y ~ poly1 + poly2 + poly3, data = mydata)`.  \n\n`r optbegin(\"Demonstration\", olabel=F,toggle=params$TOGGLE)`\n```{r echo=FALSE}\nset.seed(4)\ndf<-tibble(\n  x = 1:10,\n  y = x + \n  rnorm(1, mean = 100) * (x) +\n  rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n  rnorm(1, mean = -1) * (x) ^ 3 + \n  rnorm(10)\n)\n```\nOur data:  \n```{r}\nhead(df)\n```\n\nA messy model:  \n```{r}\nm.messy <- lm(y ~ poly(x, 3, raw=T), data = df)\n```\n\n```{r echo=FALSE}\n# broom::tidy(m.messy) %>% pander::pander()\n```\n\n```{r echo=FALSE}\nbroom::tidy(m.messy) %>% gt::gt()\n```\n\nDan's code, and a neat model:  \n\n```{r}\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, \n                orthogonal = FALSE, draw.poly = FALSE)\nmDan <- lm(y ~ poly1 + poly2 + poly3, data = df)\n```\n\n```{r echo=FALSE}\n# broom::tidy(mDan) %>% pander::pander()\n```\n\n```{r echo=FALSE}\nbroom::tidy(mDan) %>% gt::gt()\n```\n\n`r optend()`\n:::\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n```\n\n\n:::lo\nThis reading:  \n\n\n:::\n\n\nWe have already seen in the last couple of weeks that we can use MLM to study something 'over the course of X'. In the novel word learning experiment from last week's lab, we investigated the change in the probability of answering correctly over the course of the experimental blocks. \n\nWe've talked about how **\"longitudinal\"** is the term commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opens up the potential for observations to be unevenly spaced, or missing at certain points. It also, as will be the focus of this week, opens the door to thinking about how many effects of interest are likely to display **non-linear patterns**. These exercises focus on including higher-order polynomials in the multi-level model to capture non-linearity. \n  \n# Linear vs Non-Linear\n\n```{r include=F}\nres <- MASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% lm(y~x,.) %>% broom::tidy() %>% mutate(across(estimate:statistic,~round(.,2)))\n```\n\n<div style=\"display:inline-block;width:45%;margin:5px;vertical-align:middle\">\nSuppose we had collected the data in Figure \\@ref(fig:mcycle), and we wanted to fit a model to predict $y$ based on the values of $x$.  \n\nLet's use our old friend linear regression, $y = \\beta_0 + \\beta_1(x) + \\varepsilon$.  \n\nWe'll get out some estimated coefficients, some standard errors, and some p-values:  \n\n- The intercept:  \n  $\\beta_0$ = `r res[1,2]`, SE = `r res[1,3]`, p < .001  \n- The estimated coefficient of x:  \n  $\\beta_1$ = `r res[2,2]`, SE = `r res[2,3]`, p < .001   \n\nJob done? Clearly not - we need only overlay model upon raw data (Figure \\@ref(fig:mcycle3)) to see we are missing some key parts of the pattern.  \n</div>\n<div style=\"display:inline-block;width:45%;margin:5px;vertical-align:middle\">\n```{r mcycle, echo=FALSE, fig.cap = \"A clearly non-linear pattern\"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>%\n  ggplot(.,aes(x=x,y=y))+\n  geom_point(size=2)\n```\n```{r mcycle3, echo=FALSE, fig.asp=.7,fig.cap=\"Uh-oh... \"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x,.) %>%\n  sjPlot::plot_model(type=\"pred\", show.data = TRUE) -> p0\np0$x\n```\n\n</div>  \n\n:::statbox\n**Thoughts about Model + Error**  \n\nAll our work here is in aim of making models of the world.  \n\n1. Models are just models. They are simplifications, and so they don't perfectly fit to the observed world (indeed, how well a model fits to the world is often our metric for comparing models).  \n2. $y - \\widehat{y}$. Our observed data minus our model predicted values (i.e. in linear regression our \"residuals\") reflect everything that we don't account for in our model\n3. In an ideal world, our model accounts for all the systematic relationships, and what is left over (our residuals) is just randomness. If our model is mis-specified, or misses out something systematic, then our residuals will reflect this. \n4. We check for this by examining how much like randomness the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d (\"independent and identically distributed\") - i.e., what gets referred to as the \"assumptions\"). \n5. We will **never** know whether our residuals contain only randomness, because we can never observe *everything*. \n\n<h/>\n\nLet's just do a quick `plot(model)` for some diagnostic plots of my linear model:\n```{r echo=FALSE, out.width = \"100%\"}\npar(mfrow=c(2,2))\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>%\nlm(y~x,.) %>% plot()\npar(mfrow=c(1,1))\n```\n\nDoes it look like the residuals are independently distributed? Not really. \nWe need to find some way of incorporating the non-linear relationship between y and x into our model. \n\n:::\n\n\n# What is a polynomial?  \n\n:::statbox\nPolynomials are mathematical expressions which involve a sum of powers. For instance:\n\n- $y = 4 + x + x^2$ is a second-order polynomial as the highest power of $x$ that appears is $x^2$  \n- $y = 9x + 2x^2 + 4x^3$ is a third-order polynomial as the highest power of $x$ that appears is $x^3$  \n- $y = x^6$ is a sixth-order polynomial as the highest power of $x$ that appears is $x^6$  \n\n:::\n\nFor our purposes, extending our model to include higher-order terms can fit non-linear relationships between two variables. For instance, fitting models with linear and quadratic terms ($y_i$ = $\\beta_0 + \\beta_1 x_{i} \\ + \\beta_2 x^2_i + \\varepsilon_i$) and extending these to cubic ($y_i$ = $\\beta_0 + \\beta_1 x_{i} \\ + \\beta_2 x^2_i + \\beta_3 x^3_i + \\varepsilon_i$) (or beyond), may aid in modelling nonlinear patterns.\n\n```{r echo=FALSE, out.width = \"100%\", fig.heigth = 4}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x+I(x^2),.) %>%\n  sjPlot::plot_model(type=\"pred\",show.data=TRUE) -> p\n\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) %>% \n  lm(y~x+I(x^2)+I(x^3),.) %>% \n  sjPlot::plot_model(type=\"pred\",show.data=TRUE) -> p1\n\np0$x + labs(title=\"\",subtitle=\"y ~ x + e\") +\np$x + labs(title=\"\",subtitle=\"y ~ x + x^2 + e\") +\np1$x + labs(title=\"\",subtitle=\"y ~ x + x^2 + x^3 + e\") & theme_bw(base_size=12)\n```\n\n:::statbox\n**What are we interested in here?**  \n\nAs the order of polynomials increases, we tend to be less interested in these terms in our model. Linear change is the easiest to think about: are things going up over the course of $x$, or down? (or neither?). Quadratic change is the next most interesting, and it may help to think of this as the \"rate of change\". For instance, in the plot below, it is the quadratic term which differs between the two groups trajectories. \n\n```{r echo=FALSE}\nx=1:10\ntibble(\n  y=c(x*3, (x*1.1+(x^2)*.2)),\n  xx=rep(x,2),\n  g = rep(letters[1:2],each=10)\n) %>% ggplot(.,aes(x=xx,y=y,col=g))+geom_line()+\n  labs(x=\"x\")\n```\n\n`r optbegin(\"Positive and negative quadratic terms\",olabel=FALSE,toggle=params$TOGGLE)`\n```{r quadfig, echo=FALSE}\ntibble(\n  x = -10:10,\n  y = x^2,\n  y1 = -x^2\n) %>% pivot_longer(y:y1) %>%\n  mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n Y = X^2\", \"quadratic term is negative \\n Y = -X^2\")) %>%\n  ggplot(.,aes(x=x,y=value,col=name)) +\n  geom_line()+\n  guides(col=FALSE)+\n  facet_wrap(~name, scales=\"free_y\")\n```\n\n<!-- `r optbegin(\"Code to create plot\", olabel=FALSE, toggle=params$TOGGLE)` -->\n<!-- ```{r eval=FALSE} -->\n<!-- tibble( -->\n<!--   x = -10:10, -->\n<!--   y = x^2, -->\n<!--   y1 = -x^2 -->\n<!-- ) %>% pivot_longer(y:y1) %>% -->\n<!--   mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n Y = X^2\", \"quadratic term is negative \\n Y = -X^2\")) %>% -->\n<!--   ggplot(.,aes(x=x,y=value,col=name)) + -->\n<!--   geom_line()+ -->\n<!--   guides(col=FALSE)+ -->\n<!--   facet_wrap(~name, scales=\"free_y\") -->\n<!-- ``` -->\n<!-- `r optend()` -->\n\n`r optend()`\n:::\n\n\n\n## Raw Polynomials\n\n<div style=\"display:inline-block; width: 60%; vertical-align: top\">\nThere are two types of polynomial we can construct. \"Raw\" (or \"Natural\") polynomials are the straightforward ones you might be expecting the table to the right to be filled with.  \n\nThese are simply the original values of the x variable to the power of 2, 3 and so on.  \n  \nWe can quickly get these in R using the `poly()` function, with `raw = TRUE`.  \n\nIf you want to create \"raw\" polynomials, make sure to specify `raw = TRUE` or you will not get what you want because the default behaviour is `raw = FALSE`.\n</div>\n<div style=\"display:inline-block; width: 30%;\">\n<center>\n```{r echo=FALSE}\nMASS::mcycle %>%\n    rename(y=accel,x=times) %>% filter(x>20) -> df\ncbind(x=1:5,`x^2`=rep(\"?\",5),`x^3`=rep(\"?\",5)) %>% rbind(\"...\") %>% \n    kableExtra::kable() %>%\n    kableExtra::kable_styling(full_width = FALSE)\n```\n</center>\n</div>\n\n```{r}\npoly(1:10, degree = 3, raw=TRUE)\n```\n\n\n<div style=\"background-color: #eafaff; border-radius: 5px; padding: 20px 20px 10px 20px; margin-top: 20px; margin-bottom: 20px;\">\n**Raw polynomials are correlated**  \n\nWith raw (or \"natural\") polynomials, the terms `poly1`, `poly2` and `poly3` are correlated.  \nThink think about why this might be - by definition, as $x^1$ increases, so will $x^2$, and so will $x^3$ and so on.  \n\nWe can visualise them: \n```{r}\nmatplot(poly(1:10, 3, raw=T), type=\"l\")\n```\nAnd measure the correlation coefficients:\n```{r}\ncor(poly(1:10, 3, raw=T)) %>% round(2)\n```\n\nWhy might this be a problem?  \nWell, this multicollinearity can lead to estimation problems, and means that our parameter estimates may change considerably depending upon what terms we include in our model, and it becomes more difficult to determine which ones are important, and what the effect sizes are.  \nTable \\@ref(tab:rawpolytab) below shows the coefficients for models fitted to a randomly generated dataset, with `poly1`, `poly1+poly2`, and `poly1+poly2+poly3` as predictors (where `poly1`-`poly3` are **natural** polynomials). Notice that they change with the addition of each term.\n```{r rawpolytab, echo=FALSE}\nset.seed(754)\ndf<-tibble(\n    x = 1:10,\n    y = x + \n        rnorm(1, mean = 100) * (x) +\n        rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n        rnorm(1, mean = -1) * (x) ^ 3 + \n        rnorm(10)\n)\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, orthogonal = FALSE, draw.poly = FALSE)\nfull_join(\n    broom::tidy(lm(y~poly1, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"*\",\"**\"))) %>% \n        rename(`y~poly1` = estimate),\n    broom::tidy(lm(y~poly1+poly2, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"*\",\"***\",\"***\"))) %>% \n        rename(`y~poly1+poly2` = estimate)\n) %>% full_join(.,\n                broom::tidy(lm(y~poly1+poly2+poly3, df))[,1:2] %>% \n                    #mutate(estimate = paste(round(estimate,2), c(\"\",\"***\",\"\",\"***\"))) %>% \n                    rename(`y~poly1+poly2+poly3` = estimate)\n) %>% mutate_if(is.numeric,~round(.,2)) %>% \n    mutate_all(~ifelse(is.na(.), \"-\", .)) %>% \n    kableExtra::kbl(caption = \"Incremental addition of raw polynomial terms\") %>%\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n</div>\n\n## Orthogonal Polynomials  \n\n\"Orthogonal\" polynomials are uncorrelated (hence the name). \nWe can get these for $x = 1,2,...,9,10$ using the following code:\n```{r}\npoly(1:10, degree = 3, raw = FALSE)\n```\nNotice that the first order term has been scaled, so instead of the values 1 to 10, we have values ranging from -0.5 to +0.5, centered on 0. \n\n```{r}\ncolMeans(poly(1:10, degree = 3, raw = FALSE)) %>%\n    round(2)\n```\n\nAs you can see from the output above, which computes the mean of each column, each predictor has mean 0, so they are mean-centred. __This is a key fact and will affect the interpretation of our predictors later on__.\n\nThink about what this means for $x^2$. It will be uncorrelated with $x$ (because $-0.5^2 = 0.5^2$)!  \n```{r}\nmatplot(poly(1:10, 3, raw=F), type=\"l\")\n```\nThe correlations are zero!\n```{r}\ncor(poly(1:10, 3, raw=F)) %>% round(2)\n```\n\nWe can then fit the same models `y~poly1`, `y~poly1+poly2`, and `y~poly1+poly2+poly3` as predictors (where `poly1`-`poly3` are now **orthogonal** polynomials), and see that estimated coefficients do not change between models: \n```{r orthpolytab, echo=FALSE}\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, orthogonal = TRUE, draw.poly = FALSE)\nfull_join(\n    broom::tidy(lm(y~poly1, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"\",\"**\"))) %>% \n        rename(`y~poly1` = estimate),\n    broom::tidy(lm(y~poly1+poly2, df))[,1:2] %>% \n        #mutate(estimate = paste(round(estimate,2), c(\"***\",\"***\",\"***\"))) %>% \n        rename(`y~poly1+poly2` = estimate)\n) %>% full_join(.,\n                broom::tidy(lm(y~poly1+poly2+poly3, df))[,1:2] %>% \n                    #mutate(estimate = paste(round(estimate,2), c(\"***\",\"***\",\"***\",\"***\"))) %>% \n                    rename(`y~poly1+poly2+poly3` = estimate)\n) %>% mutate_if(is.numeric,~round(.,2)) %>% \n    mutate_all(~ifelse(is.na(.), \"-\", .)) %>% \n    kableExtra::kable(caption = \"Incremental addition of orthogonal polynomial terms\") %>%\n    kableExtra::kable_styling(full_width = FALSE)\n\n```\n\n\n:::statbox\n**Remember what zero is!** \n\nWith orthogonal polynomials, you need to be careful about interpreting coefficients. For raw polynomials the intercept remains the y-intercept (i.e., where the line hits the y-axis). The higher order terms can then be thought of from that starting point - e.g., \"where $x$ is 2, $\\widehat{y}$ is $\\beta_0 + \\beta_1 \\cdot 2 + \\beta_2 \\cdot 2^2 + \\beta_3 \\cdot 2^3 ...$\".  \n<br>\nFor orthogonal polynomials, the interpretation becomes more tricky. The intercept is the overall average of y, the linear predictor is the linear change pivoting around the mean of $x$ (rather than $x = 0$), the quadratic term corresponds to the steepness of the quadratic curvature (\"how curvy is it?\"), the cubic term to the steepness at the inflection points (\"how wiggly is it?\"), and so on. \n\n::: \n\n## Some useful code from Dan\n\n:::rtip\n\nIt's possible to use `poly()` internally in fitting our linear model, if we want:  \n```{r eval=FALSE}\nlm(y ~ poly(x, 3, raw = T), data = df)\n```\n<br>\nUnfortunately, the coefficients will end up having long messy names `poly(x, 3, raw = T)[1]`, `poly(x, 3, raw = T)[2]` etc.   \n<br>\nIt is probably nicer if we add the polynomials to our data itself. As it happens, Dan has provided a nice little function which attaches these as columns to our data, naming them `poly1`, `poly2`, etc. \n```{r include=FALSE}\nset.seed(754)\nmydata<-tibble(\n  x = 1:10,\n  y = x + \n  rnorm(1, mean = 100) * (x) +\n  rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n  rnorm(1, mean = -1) * (x) ^ 3 + \n  rnorm(10)\n) %>% rename(time=x)\n```\n\n```{r}\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nmydata <- code_poly(df = mydata, predictor = 'time', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(mydata)\n```\n\nBoth will produce the same model output (but Dan's method produces these nice neat names for the coefficients!), and we can just put the terms into our model directly as `lm(y ~ poly1 + poly2 + poly3, data = mydata)`.  \n\n`r optbegin(\"Demonstration\", olabel=F,toggle=params$TOGGLE)`\n```{r echo=FALSE}\nset.seed(4)\ndf<-tibble(\n  x = 1:10,\n  y = x + \n  rnorm(1, mean = 100) * (x) +\n  rnorm(1, mean = 0, sd = .01) * (x) ^ 2 +\n  rnorm(1, mean = -1) * (x) ^ 3 + \n  rnorm(10)\n)\n```\nOur data:  \n```{r}\nhead(df)\n```\n\nA messy model:  \n```{r}\nm.messy <- lm(y ~ poly(x, 3, raw=T), data = df)\n```\n\n```{r echo=FALSE}\n# broom::tidy(m.messy) %>% pander::pander()\n```\n\n```{r echo=FALSE}\nbroom::tidy(m.messy) %>% gt::gt()\n```\n\nDan's code, and a neat model:  \n\n```{r}\ndf <- code_poly(df = df, predictor = 'x', poly.order = 3, \n                orthogonal = FALSE, draw.poly = FALSE)\nmDan <- lm(y ~ poly1 + poly2 + poly3, data = df)\n```\n\n```{r echo=FALSE}\n# broom::tidy(mDan) %>% pander::pander()\n```\n\n```{r echo=FALSE}\nbroom::tidy(mDan) %>% gt::gt()\n```\n\n`r optend()`\n:::\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"03a_poly.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"3A: Polynomial Growth","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}