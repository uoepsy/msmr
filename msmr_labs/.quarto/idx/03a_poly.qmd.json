{"title":"3A: Polynomial Growth","markdown":{"yaml":{"title":"3A: Polynomial Growth","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Linear vs Non-Linear","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\ndfeg = MASS::mcycle |>\n  rename(y=accel,x=times) |> filter(x>20) |>\n  mutate(y=scale(y)[,1]*3,\n         y=y-min(y),\n         x=(x-20)/2\n         ) |>\n  transmute(\n    age = round(x,2),\n    syndens = pmax(.1,y)\n  ) \nwrite_csv(dfeg, file=\"../../data/msmr_synapticdens.csv\")\nsyndat <- dfeg\nlinmod <- lm(syndens ~ age, data = syndat)\nres <- broom::tidy(linmod) |> mutate(across(estimate:statistic,~round(.,2)))\n\n\n\n```\n\n\n:::lo\nThis reading:  \n\n- The basics of modelling non-linear change via polynomial terms. \n- An example with MLM\n\nFor additional reading, [Winter & Wieling, 2016](https://doi.org/10.1093/jole/lzv003) is pretty good (mainly focus on sections 1-3)\n\n:::\n\n\nWe have already seen in the last couple of weeks that we can use MLM to study something 'over the course of X'. This might be \"over the course of adolescence\" (i.e. `y ~ age`), or \"over the course of an experiment\" (`y ~ trial_number`). The term **\"longitudinal\"** is commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opened up the potential for observations to be unevenly spaced, or missing at certain points. \n\nIt also, as will be the focus of this week, opens the door to thinking about how many effects of interest may display patterns that are **non-linear**. There are lots of techniques to try and summarise non-linear trajectories, and here we are going to focus on the method of including higher-order polynomials as predcitors.  \n  \n\nForget about multilevel models for a little while, as we can think about this in the single level world.  \n\nSuppose we had collected the data on `r nrow(syndat)` children, and measured the number of pathways in their brains (\"synaptic density\"), see @fig-dfeg. We wanted to fit a model to predict synaptic density based on the values of age.  \n\n```{r}\n#| label: fig-dfeg\n#| fig-cap: \"A clearly non-linear pattern\"\n#| fig.height: 3.5\nsyndat <- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nggplot(syndat, aes(x=age,y=syndens))+\n  geom_point(size=2)+\n  labs(x=\"Age (years)\",y=\"Synaptic Density\")\n```\n\nWe'll use our old friend linear regression, $y = b_0 + b_1 \\cdot x + \\varepsilon$.  \n\n```{r}\nlinmod <- lm(syndens ~ age, data = syndat)\n```\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\nWe'll get out some estimated coefficients, some $t$ statistics, and some p-values:  \n\n- The intercept: $b_0$ = `r res[1,2]`, t(`r nrow(syndat)-2`)=`r res[1,4]`, p < .001  \n- The estimated coefficient of x: $b_1$ = `r res[2,2]`, t(`r nrow(syndat)-2`)=`r res[1,4]`, p < .001   \n\nJob done? Clearly not - we need only overlay model upon raw data (@fig-dfeg2) to see we are missing some key parts of the pattern.  \n\n:::\n::: {.column width=\"40%\"}\n\n```{r}\n#| label: fig-dfeg2\n#| fig-cap: \"Uh-oh... \"\n#| echo: false\nsjPlot::plot_model(linmod, type=\"pred\", terms=c(\"age\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,6,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",x=\"Age (years)\",y=\"Synaptic Density\")\n```\n\n:::\n::::\n\n::: {.callout-note collapse=\"true\"}\n#### We can see this in our residuals!!  \n\nLet's just do a quick `plot(model)` for some diagnostic plots of my linear model:\n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(linmod)\npar(mfrow=c(1,1))\n```\n\nDoes it look like the residuals are independently and identically distributed? Not really. We can see that the residuals do not have a constant mean of zero across the fitted values. This makes sense, because the line is below all of the points for people 5 years old, and then it's above all the points when $age<3$ and $age>15$. We need to find some way of incorporating the clearly non-linear relationship between the two variables into our model. \n\n:::\n\n\n# What is a polynomial?  \n\nFor many purposes, extending the linear model to include higher-order terms can allow us to usefully summarise a non-linear relationships between two variables. For instance, fitting models with linear and quadratic terms ($y$ = $b_0 + b_1 \\cdot x \\ + b_2 \\cdot x^2 + \\varepsilon$) and extending these to cubic ($y$ = $b_0 + b_1 \\cdot x \\ + b_2 \\cdot x^2 + b_3 \\cdot x^3 + \\varepsilon$) (or beyond), may aid in modelling nonlinear patterns, as in @fig-dfeg3\n\n```{r}\n#| label: fig-dfeg3\n#| out-width: \"100%\"\n#| fig-height: 3.5\n#| fig-cap: \"Linear, quadratic and cubic models fitted to the same dataset.\"\n#| echo: false\np0 <- \n  sjPlot::plot_model(lm(syndens~age,syndat), type=\"pred\",\n                     terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + e\")\n\np1 <- \n  sjPlot::plot_model(lm(syndens~age+I(age^2),syndat), type=\"pred\", terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + x^2 + e\")\n\np2 <- \n  sjPlot::plot_model(lm(syndens~age+I(age^2)+I(age^3),syndat), type=\"pred\",terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + x^2 + x^3 + e\")\n\n\np0 + p1 + p2 & theme_bw(base_size=12)\n```\n\n\n\n:::sticky\nPolynomials are mathematical expressions which involve a sum of powers. For instance:\n\n- $y = 4 + x + x^2$ is a second-order polynomial as the highest power of $x$ that appears is $x^2$  \n- $y = 9x + 2x^2 + 4x^3$ is a third-order polynomial as the highest power of $x$ that appears is $x^3$  \n- $y = x^6$ is a sixth-order polynomial as the highest power of $x$ that appears is $x^6$  \n\n::: {.callout-note collapse=\"true\"}\n#### How does a polynomial term capture non-linearity?   \n\nSuppose we have the model $y = b_0 + b_1 \\cdot x + \\varepsilon$. \nLet's take a sequence of values for $x$, and write out the model equation for the predicted values $\\hat y$ from this model.  \n\nWe can see that for each row, the predicted value increases by $b_1$. In the column with example coefficients, each row increases by 1.  \n\n\n| x | predicted y | predicted y if e.g. $b_0=1$ and $b_1=1$ |\n| ---- | ---- | ---- |\n| 0 | $b_0 + b_1 \\cdot 0$ | $1+1 \\cdot 0 = 1$ |\n| 1 | $b_0 + b_1 \\cdot 1$ | $1+1 \\cdot 1 = 2$ |\n| 2 | $b_0 + b_1 \\cdot 2$ | $1+1 \\cdot 2 = 3$ |\n| 3 | $b_0 + b_1 \\cdot 3$ | $1+1 \\cdot 3 = 4$ |\n| ... | ... | ... |\n\n\nNow let's do the same for the model with the quadratic term in, $y = b_0 + b_1 \\cdot x + b_2 \\cdot x^2 + \\varepsilon$.  \nWe can see now that each row doesn't increase by the same amount! The increases are 2, 4, 6 for our example coefficients.  \n\n\n| x | predicted y | predicted y if e.g. $b_0=1$, $b_1=1$ and $b_2=1$ |\n| ---- | ---- | ---- |\n| 0 | $b_0 + b_1 \\cdot 0 + b_2 \\cdot 0^2$ | $1+1 \\cdot 0 + 1 \\cdot 0^2 = 1$ |\n| 1 | $b_0 + b_1 \\cdot 1 + b_2 \\cdot 1^2$ | $1+1 \\cdot 1 + 1 \\cdot 1^2 = 3$ |\n| 2 | $b_0 + b_1 \\cdot 2 + b_2 \\cdot 2^2$ | $1+1 \\cdot 2 + 1 \\cdot 2^2 = 7$ |\n| 3 | $b_0 + b_1 \\cdot 3 + b_2 \\cdot 3^2$ | $1+1 \\cdot 3 + 1 \\cdot 3^2 = 13$ |\n| ... | ... | ... |\n\nThis is because the value of $x^2$ is bigger for bigger values of $x$. Hence the difference between predicted values of $y$ when $x=9$ and $x=10$ is much bigger than it is between $x=0$ and $x=1$.  \n\nYou can see the linear and the quadratic lines we just talked about in the figure below:\n\n```{r}\n#| echo: false\n#| fig-height: 3.5\nggplot(data.frame(x=0:5,y=1:6),aes(x=x,y=y))+\n  stat_function(geom=\"line\",fun=function(.x) 1+1*.x, lwd=1,col=\"blue\")+\n  stat_function(geom=\"line\",fun=function(.x) 1+(1*.x)+(1*.x^2), lwd=1,col=\"green3\")+\n  annotate(geom=\"label\",label=\"y = 1 + 1*x\", x=4,y=3,col=\"blue\")+\n  annotate(geom=\"label\",label=\"y = 1 + 1*x + 1*x^2\", x=3.5,y=20,col=\"green3\")\n```\n\n\n:::\n:::\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nAs the order of polynomials increases, we tend to be less interested in these terms in our model. Linear change is the easiest to think about: are things going up? or down? (or neither?). Quadratic change is the next most interesting, and it may help to think of this as the \"rate of change\". For instance, in @fig-quad1, it is the quadratic term which differs between the two groups trajectories. \n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"40%\"}\n```{r echo=FALSE}\n#| echo: false\n#| label: fig-quad1\n#| fig-cap: \"Two lines for which the quadratic term differs\" \nx=1:10\ntibble(\n  y=c(x*3, (x*1.1+(x^2)*.2)),\n  xx=rep(x,2),\n  g = rep(letters[1:2],each=10)\n) %>% ggplot(.,aes(x=xx,y=y,col=g))+geom_line(lwd=1)+\n  labs(x=\"x\")\n```\n:::\n::::\n\n::: {.callout-note collapse=\"true\"}\n#### Positive and negative coefficients for quadratic terms\n\nOne thing to focus on is the sign of the quadratic coefficient. When a quadratic term is positive, the curve is $\\cup$ shaped, and when it is negative, the curve is $\\cap$ shaped. \nTo help illustrate, consider what each value from -10 to 10 is when squared (they're all positive, as in the RH side of @fig-quad3)\n\n```{r quadfig, echo=FALSE}\n#| label: fig-quad3\n#| echo: false\n#| fig-cap: \"Negative quadratic term (Left) and Positive quadratic term (Right) lead to curvatures that are either U shaped (positive) or inverted-U shaped (negative)\"  \n#| fig-height: 3.5\ntibble(\n  x = -10:10,\n  y = x^2,\n  y1 = -x^2\n) %>% pivot_longer(y:y1) %>%\n  mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n y = 1*(x^2)\", \"quadratic term is negative \\n y = -1*(x^2)\")) %>%\n  ggplot(.,aes(x=x,y=value,col=name)) +\n  geom_line(lwd=1)+\n  guides(col=FALSE)+\n  facet_wrap(~name, scales=\"free_y\")\n```\n\n:::\n\n\n\n## Raw Polynomials\n\n\nThere are two types of polynomial we can construct. \"Raw\" (or \"Natural\") polynomials are the straightforward ones that you would expect (example in the table below), where the original value of $x$ is squared/cubed. \n\n\n| $x$ | $x^2$ | $x^3$ |\n| ---- | ---- | ---- |\n| 1 | 1 | 1 |\n| 2 | 4 | 8 |\n| 3 | 9 | 27 |\n| 4 | 16 | 64 |\n| 5 | 25 | 125 |\n| ... | ... | ... |\n\n\nWe can quickly get these in R using the `poly()` function. As we want to create \"raw\" polynomials, we need to make sure to specify `raw = TRUE` or we get something else (we'll talk about what they are in a second!).  \n\n```{r}\npoly(1:10, degree = 3, raw=TRUE)\n```\n\nLet's now use these with our example data we had been plotting above.  \nFirst lets add new variables to the dataset, which are the polynomials of our $x$ variable:    \n```{r}\nsyndat <- \n  syndat |> \n    mutate(\n      # poly1 is the first column\n      poly1 = poly(age, degree = 3, raw = TRUE)[,1],\n      # poly2 is the second\n      poly2 = poly(age, degree = 3, raw = TRUE)[,2],\n      # poly3 is the third\n      poly3 = poly(age, degree = 3, raw = TRUE)[,3]\n    )\n```\n\nAnd now lets use them in our model as predictors:  \n```{r}\ncubicmod <- lm(syndens ~ poly1 + poly2 + poly3, data = syndat)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### other ways to get polynomials into the model\n\nAs we're working with raw polynomials, we could just do:  \n```{r}\n#| eval: false\nsyndat |> \n  mutate(\n    poly1 = age,\n    poly2 = age^2,\n    poly3 = age^3\n  )\n```\n\nOr we could even just specify the calculations for each term _inside_ the call to `lm()`:\n```{r}\n#| eval: false\nlm(syndens ~ age + I(age^2) + I(age^3), data = syndat)\n```\n\nOr even use the `poly()` function:  \n```{r}\n#| eval: false\nlm(syndens ~ poly(age, degree=3, raw=TRUE), data = syndat)\n```\n\n:::\n::: {.callout-note collapse=\"true\"}\n#### A handy function from Dan\n\nDan has a nice function that may be handy. It adds the polynomials to your dataset for you:  \n```{r}\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nsyndat <- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nsyndat <- code_poly(df = syndat, predictor = 'age', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(syndat)\n```\n:::\n\n\nJust to see it in action, let's take a look at the predicted values from our model.  \nTake for instance, the 9th row below. The predicted value of y (shown in the `.fitted` column) is:  \n$\\hat y_9 = b_0 + b_1 \\cdot x_9 + b_2 \\cdot x^2_9 + b_3 \\cdot x^3_9$  \n$\\hat y_9 = b_0 + b_1 \\cdot 2 + b_2 \\cdot 4 + b_3 \\cdot 8$  \n$\\hat y_9 = -1.843 + 3.375 \\cdot 2 + -0.332 \\cdot 4 + 0.0097 \\cdot 8$  \n$\\hat y_9 = 3.66$.  \n```{r}\nlibrary(broom)\naugment(cubicmod) \n```\n\nIf we plot the predictions with `poly1` on the x-axis (`poly1` is just the same as our `age` variable with a different name!), we can see that we are able to model a non-linear relationship between y and x (between synaptic density and age), via a combination of linear parameters!  \n```{r}\n#| label: fig-fittedcubic\n#| fig-cap: \"a cubic model\"  \nlibrary(broom)\naugment(cubicmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=.fitted),col=\"darkorange\") +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper),fill=\"darkorange\", alpha=.2)+\n  labs(x=\"age\") # our x-axis, \"poly1\", is just age!  \n```\n\nNow lets look at our coefficients:   \n```{r}\n#| eval: false\nsummary(cubicmod)\n```\n```\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.842656   0.704193  -2.617   0.0109 *  \npoly1        3.375159   0.345570   9.767 1.06e-14 ***\npoly2       -0.331747   0.044664  -7.428 2.06e-10 ***\npoly3        0.009685   0.001614   6.001 7.79e-08 ***\n---\n```\n\nWith polynomials the interpretation is a little tricky because we have 3 coefficients that together explain the curvy line we see in @fig-fittedcubic, and these coefficients are all dependent upon one another.  \n\n- `(Intercept)` = When all predictors are zero, i.e. the synaptic density at age 0.  \n- `poly1` coefficient = The instantaneous change in $y$ when $x=0$. \n- `poly2` coefficient = Represents \"rate of change of the rate of change\" at $x=0$. In other words, the _curvature_ at $x=0$.  \n- `poly3` coefficient = Represents how the curvature is changing. It gets more abstract as the order of polynomials increase, so the easiest way to think about it is \"the wiggliness\"  \n\nI've tried to represent what each term adds in @fig-polycoef. The intercept is the purple point where age is zero. The `poly1` coefficient is represented by the dashed blue line - the tangent of the curve at age zero. The `poly2` coef, rperesented by the dashed green line, is how the angle of the blue line is changing at age zero. Finally, the `poly3` coefficient tells us how much this curvature is changing (which gets us to our dashed orange line).  \n\nNote that these interpretations are all dependent upon the others - e.g. the interpretation of `poly2` refers to how the angle of `poly1` is changing.  \n\n```{r}\n#| label: fig-polycoef\n#| echo: false\n#| fig-cap: \"the instantaneous rate of change at x=0 (blue), the rate of change in the rate of change (i.e. curvature, green), and 'rate of change in rate of change in rate of change' (i.e. wiggliness, orange)\"\ncc = coef(cubicmod)\naugment(cubicmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n\n    stat_function(geom=\"line\",fun=function(.x) cc[1]+(cc[2]*.x)+(cc[3]*.x^2)+(cc[4]*.x^3),\n                lty=\"dashed\",col=\"darkorange\",lwd=1)+  \n  stat_function(geom=\"line\",fun=function(.x) cc[1]+(cc[2]*.x)+(cc[3]*.x^2),\n                lty=\"dashed\",col=\"green4\",lwd=1)+\n  geom_abline(intercept=cc[1],slope=cc[2],lty=\"dashed\", col=\"blue\",lwd=1)+\n  geom_point(x=0,y=cc[1],size=5,col=\"purple\") + \n  scale_x_continuous(\"age\",limits=c(0,19))+\n  scale_y_continuous(limits=c(-4,12))\n```\n\n## Orthogonal Polynomials  \n\nThe `poly()` function also enables us to compute \"orthogonal polynomials\". This is the same information as the raw polynomials, re-expressed into a set of __uncorrelated__ variables.  \n\nRaw polynomials are correlated, which is what results makes their interpretation depend upon one another. For example, if we take the numbers 1,2,3,4,5, then these numbers are _by definition_ correlated with their squares 1,4,9,16,25. As we increase from 1 to 5, we necessarily increase from 1 to 25.  \nHowever, if we first _center_ the set of numbers, so that 1,2,3,4,5 becomes -2,1,0,1,2, then their squares are 4,1,0,1,4 - they're not correlated!  \n\nOrthogonal polynomials essentially do this centering and scaling for $k$ degrees of polynomial terms.  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\nSo while raw polynomials look like this:  \n```{r}\n#| fig-height: 4\nmatplot(poly(1:10, 3, raw=T), type=\"l\", lwd=2)\n```\n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"45%\"}\nOrthogonal polynomials look like this:\n```{r}\n#| fig-height: 4\nmatplot(poly(1:10, 3, raw=F), type=\"l\", lwd=2)\n```\n:::\n::::\n\nThis orthogonality allows us to essentially capture express the linear trend, curvature, and 'wiggliness' of the trajectory **independently** from one another, rather than relative to one another.  \n\nUltimately, models using raw polynomials and using orthogonal polynomials are identical, but the coefficients we get out represent different things.\n\nLet's overwrite our `poly` variables with orthogonal polynomials, by setting `raw = FALSE`:  \n```{r}\nsyndat <- \n  syndat |> \n    mutate(\n      poly1 = poly(age,degree = 3, raw=FALSE)[,1],\n      poly2 = poly(age,degree = 3, raw=FALSE)[,2],\n      poly3 = poly(age,degree = 3, raw=FALSE)[,3],\n    )\n```\n\nAnd fit our model:  \n```{r}\n#| eval: false\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\nsummary(Ocubicmod)\n```\n```{r}\n#| echo: false\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\n```\n\n```\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.5917     0.1902  34.649  < 2e-16 ***\npoly1        12.9161     1.6365   7.892 2.88e-11 ***\npoly2       -14.3156     1.6365  -8.748 7.68e-13 ***\npoly3         9.8212     1.6365   6.001 7.79e-08 ***\n---\n```\n\nThe interpretation of the estimates themselves are not really very tangible anymore, because the scaling of the orthogonal polynomials has lost a clear link back to \"age\".  \n\nAs the polynomial terms are centered on the mean of age, the intercept is the estimated synaptic density at the mean age (the purple dot in @fig-polycoeforth). The `poly1`, `poly2` and `poly3` coefficient represent the independent overall linear trend, centered curvature, and \"wiggliness\" of the relationship between synaptic density and age (as shown in the blue, green and orange lines in @fig-polycoeforth respectively).  \n\n```{r}\n#| label: fig-polycoeforth\n#| fig-cap: \"the independent rate of change (blue), curvature (green) and wiggliness (orange) of the y~x relationship\"   \n#| echo: false\nOlinmod <- lm(syndens ~ poly1,syndat)\nOquadmod <- lm(syndens ~ poly1+poly2,syndat)\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\n\nOcc = coef(Olinmod)\nlibrary(broom)\naugment(Olinmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  scale_y_continuous(limits=c(-4,12)) + \n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=predict(Olinmod)),lty=\"dashed\",col=\"blue\",lwd=1)+\n  geom_line(aes(y=predict(Oquadmod)),lty=\"dashed\",col=\"green4\",lwd=1)+\n  geom_line(aes(y=predict(Ocubicmod)),lty=\"dashed\",col=\"darkorange\",lwd=1)+\n  geom_point(x=0,y=coef(Ocubicmod)[1],size=5,col=\"purple\")\n```\n\n## Raw vs Orthognal  \n\nThe two models we have seen, one with raw polynomials, and one with orthogonal polynomials, are identical.  \n\nFor proof, compare the two:  \n```{r}\nanova(\n  lm(syndens ~ poly(age, 3, raw = TRUE), data = syndat),\n  lm(syndens ~ poly(age, 3, raw = FALSE), data = syndat)\n)\n```\n\nSo why would we choose one vs the other? \n\nThe main reason is if we are interested in evaluating things *relative to baseline*, in which case raw polynomials allow us to do just that. If we are instead interested in evaluating the trends **across** the timecourse, then we would want orthogonal polynomials. \n\nConsider two examples:  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\n__Example 1__  \n\nA student advisor who meets with students as they start university wants to know about how happiness evolves over the course of students' year at univeristy, and wonders if this is different between introverted and extraverted individuals.  \n\nIn this case, they would want *raw* polynomials, so that they can assess whether the two personality types differ when they first come to University, and how this is likely to evolve from that point.  \n```{r}\n#| echo: false\ntibble(\n  age = seq(1,4,.1),\n  int = .6*scale(age) + .3*(scale(age)^2),\n  ex =  1.5 + .5*scale(age) -.2*(scale(age)^2),\n) |> pivot_longer(int:ex) |>\n  mutate(personality = \n           factor(name,levels=,c(\"ex\",\"int\"),\n                  labels=c(\"extravert\",\"introvert\"))) |>\n  ggplot(aes(x=age,y=value,col=personality))+\n  geom_line(lwd=1)+\n  labs(x=\"Year of Study at University\", y=\"Happiness\")+\n  scale_y_continuous(breaks=NULL)\n\n```\n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"45%\"}\n__Example 2__  \n\nA company has four stores across the UK, and they want to know if the stores have differed in how variable their earnings have been across the year. \n\nIn this case, looking at change relative to month 1 isn't very useful. It would, for instance, tell us that the linear trend for store2's earnings is upwards, whereas the linear trend for store 1 is flat. This makes store2 look better.  \nIf we used orthogonal polynomials instead, we would see that the linear trend for store 2 is actually *negative* compared to store1. \n\n```{r}\n#| echo: false\ntibble(\n  month = seq(1,12,.3),\n  s1 = 1.4,#scale(month) + scale(month)^2,\n  s2 = 1.4 + .3*scale(month,center=F) + -.1*(scale(month)+1)^2 + -.2*scale(month,center=F)^3,\n  s3 = scale(month) + scale(month)^2\n) |> pivot_longer(s1:s3) |>\n  mutate(store=factor(name,levels=c(\"s1\",\"s2\",\"s3\"),\n                      labels=c(\"store1\",\"store2\",\"store3\"))) |>\n  ggplot(aes(x=month,y=value,col=store))+\n  geom_line(lwd=1)+\n  labs(x=\"Month\",y=\"Earnings\")+\n  scale_y_continuous(breaks=NULL)\n\n```\n\n:::\n::::\n\n:::sticky\n\nRaw? Orthogonal?  \n\nFor non-linear relationships, a good plot is usually the most important thing!  \n\n:::\n\n\n# Example in MLM\n\n\n:::frame\n__Data: midlife_ape.csv__  \n\n\n```{r}\n#| include: false\nss = 36460\n# ss = round(runif(1,1e3,1e6))\nset.seed(ss)\nn_groups = 200\nnpgroup = rep(10,200)\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nx = rep(1:10,200)\nb = sample(letters[1:2],n_groups,T,prob=c(.6,.4))\nb = b[g]\nres = MASS::mvrnorm(n=n_groups,\n                    mu=c(0,0),\n                    Sigma=diag(c(2,1))%*%matrix(c(1,.4,.4,1),nrow=2)%*%diag(c(2,1)))\nre0 = res[,1]\nre  = re0[g]\nrex = res[,2]\nre_x  = rex[g]\npoly1 = poly(x,2)[,1]\npoly2 = poly(x,2)[,2]\nlp = (0 + re) + (3 + re_x)*poly1 + \n  #+*poly1*(b==\"b\") +\n  9*poly2 + \n  .6*(b==\"b\") +\n  -5.5*(b==\"b\")*poly2\ny = lp + rnorm(N,0,1)\ndf = data.frame(x = x,g=g, b=b,y=round(scale(y)[,1],1))\n\nmnames = unique(randomNames::randomNames(1e5,which=\"first\"))\n\ndf = df |>\n  transmute(\n    apeID = mnames[g],\n    age = x,\n    species = as.character(factor(b, levels=letters[1:2],labels=c(\"chimp\",\"orangutan\"))),\n    HiP = round(scale(y)[,1]*2.4+9.4,2)\n  ) |>\n  mutate(\n    timepoint = age,\n    age = age*3 + 10\n  )\n\n\n\n# library(lme4)\n# df <- df |> mutate(\n#   poly1 = poly(timepoint,2,raw=F)[,1],\n#   poly2 = poly(timepoint,2,raw=F)[,2],\n# )\n# m = lmer(HiP ~ 1 + (poly1+poly2)*species+(1+poly1+poly2|apeID),df)\n# summary(m)\n# broom.mixed::augment(m) |>\n#   ggplot(aes(x=round(poly1,2),y=.fitted,col=species))+\n#   stat_summary(geom=\"pointrange\",aes(y=HiP))+\n#   stat_summary(geom=\"line\")\n# \n# write_csv(df, \"../../data/midlife_ape.csv\")\n\n```\n\nPrevious research has evidenced a notable dip in happiness for middle-aged humans. Interestingly, this phenomenon has even been observed in other primates, such as chimpanzees.  \n\nThe present study is interested in examining whether the 'middle-age slump' happens to a similar extent for Orangutans as it does for Chimpanzees. \n\n`r n_distinct(df$apeID)` apes (`r n_distinct(df$apeID[df$species==\"chimp\"])` Chimps and `r n_distinct(df$apeID[df$species==\"orangutan\"])` Orangutans) were included in the study. All apes were studied from early adulthood (10-12 years old for most great apes), and researchers administered the Happiness in Primates (HiP) scale to each participant every 3 years, up until the age of 40.  \n\nThe data are available at [https://uoepsy.github.io/data/midlife_ape.csv](https://uoepsy.github.io/data/midlife_ape.csv){target=\"_blank\"}.  \n\nThe dataset has already been cleaned, and the researchers have confirmed that it includes `r n_distinct(df$apeID[df$species==\"chimp\"])` Chimps and `r n_distinct(df$apeID[df$species==\"orangutan\"])` Orangutans, and every ape has complete data (i.e. 10 rows for each ape). A data dictionary can be found in @tbl-midlifedict\n\n```{r}\n#| echo: false\n#| label: tbl-midlifedict\n#| tbl-cap: \"Data Dictionary: midlife_ape.csv\"\nmlape <- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\ntibble(\n  variable=names(mlape),\n  description=c(\"Ape's Name (all names are chosen to be unique)\",\n                \"Age (in years) at assessment\",\n                \"Species (chimp v orangutan)\",\n                \"Happiness in Primate Scale (range 1 to 18)\",\n                \"Study visit (1 to 10)\")\n) |> gt::gt()\n```\n\n:::\n\n\nInitial plots are hard see when we plot a different line for each ape:  \n\n```{r}\nmlape <- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\n\nggplot(mlape, aes(x=age,y=HiP))+\n  geom_line(aes(group=apeID,col=species),alpha=.3) +\n  scale_color_manual(values=c(\"grey40\",\"darkorange\")) + \n  facet_wrap(~species)\n```\n\nWhat is often more useful for exploring is to plot the averages in y across values of x.  \nHere we can see some slight curvature - the chimps seem to have a notable dip in happiness at about 20 years old.  \n\n```{r}\nggplot(mlape, aes(x=age,y=HiP, col=species))+\n  scale_color_manual(values=c(\"grey40\",\"darkorange\")) + \n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Note\nThese plots can be misleading, especially in the event of missing data or incomplete trajectories.  For instance, suppose we had data on happy chimps when they are young and when they are old, but not in the middle of their lives. The average of all chimps at each data point would look non-linear!  \n\nThis illusion would not fool our multilevel model, however, which estimates the average of the individual lines\n\n```{r}\n#| echo: false\n#| label: fig-missing\n#| fig-cap: \"hypothetical: if happier apes have missing data in mid life, the average looks non-linear, even though for each ape it is linear!\"\n#| out-width: \"100%\"\n#| fig-height: 3.5\ntibble(\n  g = rep(1:20,e=10),\n  g0 = rep(sort(rnorm(20)),e=10),\n  age = rep(seq(13,40,3),20),\n  HiP = 9 + g0 + rnorm(200,0,.05)\n) |> \n  filter(!(g>15 & age>20 & age<35)) |>\n  filter(!(g>10 & g<=15 & age>25 & age<30))-> dd\n\n\nggplot(dd, aes(x=age,y=HiP,group=g))+\n  geom_point(size=3,alpha=.3)+\n  geom_line(alpha=.3) +\n   labs(title=\"individual chimp lines\") + \n  \nggplot(dd, aes(x=age,y=HiP))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")+\n  labs(title=\"naive averages\")\n```\n\n\n:::\n\n\n> The present study is interested in examining whether the 'middle-age slump' happens to a similar extent for Orangutans as it does for Chimpanzees. \n\nWe're interested in the difference in curvature across adulthood between Chimps and Orangutans. This is a case where we're not hugely interested in some specific point in time, so orthogonal polynomials might be easier.  \nBoth `age` and `timepoint` here contain the same information, so we can use either to create our polynomials:\n```{r}\nmlape <- mlape |> \n  mutate(\n    poly1 = poly(timepoint,2,raw=F)[,1],\n    poly2 = poly(timepoint,2,raw=F)[,2],\n  )\n```\n\nAnd then we'll fit our model.  \n\nWe know we're interested in the change in happiness across age, so our model is going to take the form: \n```\nlmer(HiP ~ age ....\n```\n. But we've just said that we're interested in the curvature of this, so we're going to re-express `age` as the two parts, the linear and the quadratic:  \n```\nlmer(HiP ~ poly1 + poly2 ....\n```\nWe're interested in the differences in the \"hapiness ~ age\" relationship between Orangutans and Chimps, so we can estimate species-differences in both aspects of the trajectroy (the linear and the quadratic).  \n```\nlmer(HiP ~ (poly1 + poly2) * species ....\n```\nthis is equivalent to\n```\nlmer(HiP ~ poly1 + poly2 + species + \n           poly1:species + poly2:species ....\n```\n\nFinally, we want to account for the variability in happiness due to random ape-to-ape differences (i.e. some are just happy, some are not).  \n```\nlmer(HiP ~ (poly1 + poly2) * species + \n           (1 + ... | apeID), data = mlape)\n```\n\nLastly, it's very likely that 'change over time' varies from ape-to-ape, so we want to allow the slopes of age to vary. We can allow one or both of the poly1 and poly2 terms to vary, so we'll start with both:  \n\n```{r}\nlibrary(lme4)\nmod1 = lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape)\n```\n\nand it converges (hooray!). \n\nNow let's plot the average value of happiness across time, alongside the average _predicted_ value.  \n\n```{r}\nbroom.mixed::augment(mod1) |>\n  mutate(poly1 = round(poly1,4)) |>\n  ggplot(aes(x=poly1,y=.fitted,col=species))+\n  # the average value across time: \n  stat_summary(geom=\"pointrange\",aes(y=HiP))+\n  # the average model predicted value\n  stat_summary(geom=\"line\") + \n  scale_color_manual(values=c(\"grey40\",\"darkorange\"))\n```\n\nIt looks like both species have a bit of curvature, and this may be more pronounced for chimps than for orangutans.  \n\nLet's look at our parameters to see:  \n```{r}\n#| eval: false\n# fitted with satterthwaite to get some p-values\nlmerTest::lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape) |>\n  summary()\n```\n```\n...\nFixed effects:\n                       Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)              9.1155     0.1955 198.0009  46.630  < 2e-16 ***\npoly1                    1.8796     1.5391 197.9811   1.221   0.2235    \npoly2                    9.4004     1.7454 197.9991   5.386 2.03e-07 ***\nspeciesorangutan         0.6853     0.3035 198.0009   2.258   0.0250 *  \npoly1:speciesorangutan   0.1104     2.3892 197.9811   0.046   0.9632    \npoly2:speciesorangutan  -6.2718     2.7093 197.9991  -2.315   0.0216 *  \n```\n```{r}\n#| echo: false\nlmerTest::lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape) |>\nbroom.mixed::tidy() |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    p = case_when(\n      p.value<.001 ~ \"***\",\n      p.value<.01 ~ \"**\",\n      p.value<.05 ~ \"*\",\n      TRUE ~ \"\"\n    ),\n    `what it tells us` = c(\n      \"the average happiness at the average age is 9.11 (not hugely interesting).\",\n      \"there is no significant linear trend of happiness over time for Chimpanzees (you can kind of see this in our plot above - they more or less end where they started)\",\n      \"there *is* a significant curvature to the chimpanzees' happiness trajectory. Because this estimate is positive, that means we know it is u-shaped and not n-shaped - i.e. - Chimpanzees have a mid-life slump in happiness!\",\n      \"at the average age, Orangutans are happier than Chimpanzees.\",\n      \"the linear trend for Orangutans is not significantly different from the linear trend for Chimpanzees\",\n      \"the curvature for Orangutans is significantly different from the curvature for Chimpanzees. Because the estimate is negative, that means it is less u-shaped than the Chimpanzee line - i.e. Orangutans do not have as much of a mid-life slump as Chimps do!\"\n    )\n  ) |> gt::gt()\n```\n\n\nAs we have two groups, the parameters provide us with all the tests we need to identify \"do species differ in the extent that they have a mid-life dip in happiness?\"  \nBut if we had $>2$ species and wanted to do test join \"species differences\", we could set up models for comparisons, i.e. allowing species to interact with different parts: \n\n- `HiP ~ (poly1 + poly2)*species`\n    + species differ in their linear and quadratic trends\n- `HiP ~ poly1*species + poly2`\n    + species differ in their linear trend only\n- `HiP ~ poly1 + poly2 + species`\n    + species differ in their overall happiness, but not in change over age\n\n\n```{r}\n#| eval: false\n#| echo: false\n\nmod1 = lmer(HiP ~ 1 + poly(timepoint,2,raw=TRUE)*species+\n            (1+poly(timepoint,2,raw=TRUE)|apeID), \n            data = mlape)\neffects::effect(term=c(\"species*timepoint\"),mod=mod1, xlevels=40) |>\n  as.data.frame() |>\n  ggplot(aes(x=timepoint,y=fit,ymin=lower,ymax=upper,col=species))+\n  geom_line()+geom_ribbon(alpha=.2,aes(fill=species),col=NA)\n\nmod1 = lmer(HiP ~ 1 + poly(timepoint,2,raw=FALSE)*species+\n            (1+poly(timepoint,2,raw=FALSE)|apeID), \n            data = mlape)\neffects::effect(term=c(\"species*timepoint\"),mod=mod1, xlevels=40) |>\n  as.data.frame() |>\n  ggplot(aes(x=timepoint,y=fit,ymin=lower,ymax=upper,col=species))+\n  geom_line()+geom_ribbon(alpha=.2,aes(fill=species),col=NA)\n\n\nmod1 = lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape)\n\npp1 <- bind_rows(poly(seq(1,10,length.out=50),2) |> as_tibble(),\n          poly(seq(1,10,length.out=50),2) |> as_tibble()\n          ) |>\n  mutate(species=rep(c(\"chimp\",\"orangutan\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"species\")\n\nbts = bootMer(mod1,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=500)\n\nbts$t\n\npp1$pred = predict(mod1, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\n\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=species,fill=species))+\n  geom_line()+\n  geom_ribbon(alpha=.2)\n  \n\n\n```\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### TIMTOWTDI \nAs always, **there is more than one way to do it (TIMTOWTDI)**  \n\nThe approach we are learning about in this course is only one of many approaches to studying non-linearity. \nSome alternatives, which you may come across in future work, are listed below.\n\n**Piecewise linear regression:** fit a linear model with cut point(s) along x (determine cut points by greatest reduction in mean squared error $\\sigma$)\n```{r echo=FALSE, out.width=\"350px\"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) -> df\nbreaks <- df$x[which(df$x >= 20 & df$x <= 40)]\nmse <- numeric(length(breaks))\nfor(i in 1:length(breaks)){\n piecewise1 <- lm(y ~ x*(x < breaks[i]) + x*(x>=breaks[i]),df)\n mse[i] <- summary(piecewise1)[6]\n}\nmse <- as.numeric(mse)\n#breaks[which(mse==min(mse))]\npiecewise2 <- lm(y ~ x*(x <= 33) + x*(x > 33), df)\ndf %>% mutate(\n  pred = predict(piecewise2),\n  se = predict(piecewise2,se.fit = T)$se\n) %>% \n  ggplot(.,aes(x=x,y=pred))+\n  geom_line(aes(group=x>33),lwd=1)+\n  #geom_ribbon(aes(ymin=pred-(1.96*se),ymax=pred+(1.96*se),group=x>33),alpha=.2)+\n  geom_point(aes(y=y))+labs(y=\"y\",title=\"Predicted values of y\", subtitle=\"y ~ x*(x < 33) + x*(x > 33)\")\n```\n\n**Piecewise polynomial** fit the model $y \\sim x + x^2 + x^3$ to equal chunks of x.  \n\n```{r echo=FALSE, out.width=\"350px\"}\nmod<-function(ddf){lm(y~poly(x,3), data=ddf)}\n\ndf %>% mutate(pieces = cut(x,3)) %>% \n  group_by(pieces) %>%\n  nest_legacy() %>%\n  mutate(\n    model = map(data, mod),\n    fit = map(model, ~fitted(.))\n  ) %>%\n  unnest_legacy(data,fit) %>%\n  ggplot(., aes(x=x))+\n  geom_point(aes(y=y))+\n  geom_line(aes(y=fit, col=pieces),lwd=1)+\n  theme_minimal() + guides(col=FALSE)+\n  labs(title=\"Predicted values of y\",subtitle=\"y~ x + x^2 + x^3 for 3 cuts of x\")\n```\n\n\n**Splines, penalised splines & GAMS** \n\nThis begins to open a huge can of worms, but if you foresee yourself needing these sort of tools, then Simon Wood, author of the **mgcv** R package for fitting generalised additive models (GAMS), has plenty of materials on [his webpage](https://www.maths.ed.ac.uk/~swood34/talks/snw-Koln.pdf) (Warning, these are fairly technical). There are also a reasonable number of tutorials [online which are really good](https://www.google.com/search?hl=&site=&q=gam+in+r+tutorial).  \n```{r echo=FALSE, out.width=\"350px\"}\nlibrary(mgcv)\ngam(y~s(x,bs=\"cr\"),df, family=\"gaussian\") -> m\ndf %>% mutate(\n  pred = predict(m),\n  se = predict(m,se.fit = T)$se\n) %>% \n  ggplot(.,aes(x=x,y=pred))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=pred-(1.96*se),ymax=pred+(1.96*se)),alpha=.2)+\n  geom_point(aes(y=y))+labs(y=\"y\",title=\"Predicted values of y\", subtitle=\"mgcv::gam(y ~ s(x, b = 'cr'))\")\n```\n\n\n:::\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\ndfeg = MASS::mcycle |>\n  rename(y=accel,x=times) |> filter(x>20) |>\n  mutate(y=scale(y)[,1]*3,\n         y=y-min(y),\n         x=(x-20)/2\n         ) |>\n  transmute(\n    age = round(x,2),\n    syndens = pmax(.1,y)\n  ) \nwrite_csv(dfeg, file=\"../../data/msmr_synapticdens.csv\")\nsyndat <- dfeg\nlinmod <- lm(syndens ~ age, data = syndat)\nres <- broom::tidy(linmod) |> mutate(across(estimate:statistic,~round(.,2)))\n\n\n\n```\n\n\n:::lo\nThis reading:  \n\n- The basics of modelling non-linear change via polynomial terms. \n- An example with MLM\n\nFor additional reading, [Winter & Wieling, 2016](https://doi.org/10.1093/jole/lzv003) is pretty good (mainly focus on sections 1-3)\n\n:::\n\n\nWe have already seen in the last couple of weeks that we can use MLM to study something 'over the course of X'. This might be \"over the course of adolescence\" (i.e. `y ~ age`), or \"over the course of an experiment\" (`y ~ trial_number`). The term **\"longitudinal\"** is commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opened up the potential for observations to be unevenly spaced, or missing at certain points. \n\nIt also, as will be the focus of this week, opens the door to thinking about how many effects of interest may display patterns that are **non-linear**. There are lots of techniques to try and summarise non-linear trajectories, and here we are going to focus on the method of including higher-order polynomials as predcitors.  \n  \n# Linear vs Non-Linear\n\nForget about multilevel models for a little while, as we can think about this in the single level world.  \n\nSuppose we had collected the data on `r nrow(syndat)` children, and measured the number of pathways in their brains (\"synaptic density\"), see @fig-dfeg. We wanted to fit a model to predict synaptic density based on the values of age.  \n\n```{r}\n#| label: fig-dfeg\n#| fig-cap: \"A clearly non-linear pattern\"\n#| fig.height: 3.5\nsyndat <- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nggplot(syndat, aes(x=age,y=syndens))+\n  geom_point(size=2)+\n  labs(x=\"Age (years)\",y=\"Synaptic Density\")\n```\n\nWe'll use our old friend linear regression, $y = b_0 + b_1 \\cdot x + \\varepsilon$.  \n\n```{r}\nlinmod <- lm(syndens ~ age, data = syndat)\n```\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\nWe'll get out some estimated coefficients, some $t$ statistics, and some p-values:  \n\n- The intercept: $b_0$ = `r res[1,2]`, t(`r nrow(syndat)-2`)=`r res[1,4]`, p < .001  \n- The estimated coefficient of x: $b_1$ = `r res[2,2]`, t(`r nrow(syndat)-2`)=`r res[1,4]`, p < .001   \n\nJob done? Clearly not - we need only overlay model upon raw data (@fig-dfeg2) to see we are missing some key parts of the pattern.  \n\n:::\n::: {.column width=\"40%\"}\n\n```{r}\n#| label: fig-dfeg2\n#| fig-cap: \"Uh-oh... \"\n#| echo: false\nsjPlot::plot_model(linmod, type=\"pred\", terms=c(\"age\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,6,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",x=\"Age (years)\",y=\"Synaptic Density\")\n```\n\n:::\n::::\n\n::: {.callout-note collapse=\"true\"}\n#### We can see this in our residuals!!  \n\nLet's just do a quick `plot(model)` for some diagnostic plots of my linear model:\n```{r echo=c(2)}\npar(mfrow=c(2,2))\nplot(linmod)\npar(mfrow=c(1,1))\n```\n\nDoes it look like the residuals are independently and identically distributed? Not really. We can see that the residuals do not have a constant mean of zero across the fitted values. This makes sense, because the line is below all of the points for people 5 years old, and then it's above all the points when $age<3$ and $age>15$. We need to find some way of incorporating the clearly non-linear relationship between the two variables into our model. \n\n:::\n\n\n# What is a polynomial?  \n\nFor many purposes, extending the linear model to include higher-order terms can allow us to usefully summarise a non-linear relationships between two variables. For instance, fitting models with linear and quadratic terms ($y$ = $b_0 + b_1 \\cdot x \\ + b_2 \\cdot x^2 + \\varepsilon$) and extending these to cubic ($y$ = $b_0 + b_1 \\cdot x \\ + b_2 \\cdot x^2 + b_3 \\cdot x^3 + \\varepsilon$) (or beyond), may aid in modelling nonlinear patterns, as in @fig-dfeg3\n\n```{r}\n#| label: fig-dfeg3\n#| out-width: \"100%\"\n#| fig-height: 3.5\n#| fig-cap: \"Linear, quadratic and cubic models fitted to the same dataset.\"\n#| echo: false\np0 <- \n  sjPlot::plot_model(lm(syndens~age,syndat), type=\"pred\",\n                     terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + e\")\n\np1 <- \n  sjPlot::plot_model(lm(syndens~age+I(age^2),syndat), type=\"pred\", terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + x^2 + e\")\n\np2 <- \n  sjPlot::plot_model(lm(syndens~age+I(age^2)+I(age^3),syndat), type=\"pred\",terms=c(\"age [all]\"), show.data = TRUE) +\n  scale_y_continuous(breaks=c(0,3,9,12))+\n  scale_x_continuous(breaks=seq(0,30,10))+\n  labs(title=\"\",subtitle=\"y ~ x + x^2 + x^3 + e\")\n\n\np0 + p1 + p2 & theme_bw(base_size=12)\n```\n\n\n\n:::sticky\nPolynomials are mathematical expressions which involve a sum of powers. For instance:\n\n- $y = 4 + x + x^2$ is a second-order polynomial as the highest power of $x$ that appears is $x^2$  \n- $y = 9x + 2x^2 + 4x^3$ is a third-order polynomial as the highest power of $x$ that appears is $x^3$  \n- $y = x^6$ is a sixth-order polynomial as the highest power of $x$ that appears is $x^6$  \n\n::: {.callout-note collapse=\"true\"}\n#### How does a polynomial term capture non-linearity?   \n\nSuppose we have the model $y = b_0 + b_1 \\cdot x + \\varepsilon$. \nLet's take a sequence of values for $x$, and write out the model equation for the predicted values $\\hat y$ from this model.  \n\nWe can see that for each row, the predicted value increases by $b_1$. In the column with example coefficients, each row increases by 1.  \n\n\n| x | predicted y | predicted y if e.g. $b_0=1$ and $b_1=1$ |\n| ---- | ---- | ---- |\n| 0 | $b_0 + b_1 \\cdot 0$ | $1+1 \\cdot 0 = 1$ |\n| 1 | $b_0 + b_1 \\cdot 1$ | $1+1 \\cdot 1 = 2$ |\n| 2 | $b_0 + b_1 \\cdot 2$ | $1+1 \\cdot 2 = 3$ |\n| 3 | $b_0 + b_1 \\cdot 3$ | $1+1 \\cdot 3 = 4$ |\n| ... | ... | ... |\n\n\nNow let's do the same for the model with the quadratic term in, $y = b_0 + b_1 \\cdot x + b_2 \\cdot x^2 + \\varepsilon$.  \nWe can see now that each row doesn't increase by the same amount! The increases are 2, 4, 6 for our example coefficients.  \n\n\n| x | predicted y | predicted y if e.g. $b_0=1$, $b_1=1$ and $b_2=1$ |\n| ---- | ---- | ---- |\n| 0 | $b_0 + b_1 \\cdot 0 + b_2 \\cdot 0^2$ | $1+1 \\cdot 0 + 1 \\cdot 0^2 = 1$ |\n| 1 | $b_0 + b_1 \\cdot 1 + b_2 \\cdot 1^2$ | $1+1 \\cdot 1 + 1 \\cdot 1^2 = 3$ |\n| 2 | $b_0 + b_1 \\cdot 2 + b_2 \\cdot 2^2$ | $1+1 \\cdot 2 + 1 \\cdot 2^2 = 7$ |\n| 3 | $b_0 + b_1 \\cdot 3 + b_2 \\cdot 3^2$ | $1+1 \\cdot 3 + 1 \\cdot 3^2 = 13$ |\n| ... | ... | ... |\n\nThis is because the value of $x^2$ is bigger for bigger values of $x$. Hence the difference between predicted values of $y$ when $x=9$ and $x=10$ is much bigger than it is between $x=0$ and $x=1$.  \n\nYou can see the linear and the quadratic lines we just talked about in the figure below:\n\n```{r}\n#| echo: false\n#| fig-height: 3.5\nggplot(data.frame(x=0:5,y=1:6),aes(x=x,y=y))+\n  stat_function(geom=\"line\",fun=function(.x) 1+1*.x, lwd=1,col=\"blue\")+\n  stat_function(geom=\"line\",fun=function(.x) 1+(1*.x)+(1*.x^2), lwd=1,col=\"green3\")+\n  annotate(geom=\"label\",label=\"y = 1 + 1*x\", x=4,y=3,col=\"blue\")+\n  annotate(geom=\"label\",label=\"y = 1 + 1*x + 1*x^2\", x=3.5,y=20,col=\"green3\")\n```\n\n\n:::\n:::\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nAs the order of polynomials increases, we tend to be less interested in these terms in our model. Linear change is the easiest to think about: are things going up? or down? (or neither?). Quadratic change is the next most interesting, and it may help to think of this as the \"rate of change\". For instance, in @fig-quad1, it is the quadratic term which differs between the two groups trajectories. \n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"40%\"}\n```{r echo=FALSE}\n#| echo: false\n#| label: fig-quad1\n#| fig-cap: \"Two lines for which the quadratic term differs\" \nx=1:10\ntibble(\n  y=c(x*3, (x*1.1+(x^2)*.2)),\n  xx=rep(x,2),\n  g = rep(letters[1:2],each=10)\n) %>% ggplot(.,aes(x=xx,y=y,col=g))+geom_line(lwd=1)+\n  labs(x=\"x\")\n```\n:::\n::::\n\n::: {.callout-note collapse=\"true\"}\n#### Positive and negative coefficients for quadratic terms\n\nOne thing to focus on is the sign of the quadratic coefficient. When a quadratic term is positive, the curve is $\\cup$ shaped, and when it is negative, the curve is $\\cap$ shaped. \nTo help illustrate, consider what each value from -10 to 10 is when squared (they're all positive, as in the RH side of @fig-quad3)\n\n```{r quadfig, echo=FALSE}\n#| label: fig-quad3\n#| echo: false\n#| fig-cap: \"Negative quadratic term (Left) and Positive quadratic term (Right) lead to curvatures that are either U shaped (positive) or inverted-U shaped (negative)\"  \n#| fig-height: 3.5\ntibble(\n  x = -10:10,\n  y = x^2,\n  y1 = -x^2\n) %>% pivot_longer(y:y1) %>%\n  mutate(name = ifelse(name==\"y\",\"quadratic term is positive \\n y = 1*(x^2)\", \"quadratic term is negative \\n y = -1*(x^2)\")) %>%\n  ggplot(.,aes(x=x,y=value,col=name)) +\n  geom_line(lwd=1)+\n  guides(col=FALSE)+\n  facet_wrap(~name, scales=\"free_y\")\n```\n\n:::\n\n\n\n## Raw Polynomials\n\n\nThere are two types of polynomial we can construct. \"Raw\" (or \"Natural\") polynomials are the straightforward ones that you would expect (example in the table below), where the original value of $x$ is squared/cubed. \n\n\n| $x$ | $x^2$ | $x^3$ |\n| ---- | ---- | ---- |\n| 1 | 1 | 1 |\n| 2 | 4 | 8 |\n| 3 | 9 | 27 |\n| 4 | 16 | 64 |\n| 5 | 25 | 125 |\n| ... | ... | ... |\n\n\nWe can quickly get these in R using the `poly()` function. As we want to create \"raw\" polynomials, we need to make sure to specify `raw = TRUE` or we get something else (we'll talk about what they are in a second!).  \n\n```{r}\npoly(1:10, degree = 3, raw=TRUE)\n```\n\nLet's now use these with our example data we had been plotting above.  \nFirst lets add new variables to the dataset, which are the polynomials of our $x$ variable:    \n```{r}\nsyndat <- \n  syndat |> \n    mutate(\n      # poly1 is the first column\n      poly1 = poly(age, degree = 3, raw = TRUE)[,1],\n      # poly2 is the second\n      poly2 = poly(age, degree = 3, raw = TRUE)[,2],\n      # poly3 is the third\n      poly3 = poly(age, degree = 3, raw = TRUE)[,3]\n    )\n```\n\nAnd now lets use them in our model as predictors:  \n```{r}\ncubicmod <- lm(syndens ~ poly1 + poly2 + poly3, data = syndat)\n```\n\n::: {.callout-note collapse=\"true\"}\n#### other ways to get polynomials into the model\n\nAs we're working with raw polynomials, we could just do:  \n```{r}\n#| eval: false\nsyndat |> \n  mutate(\n    poly1 = age,\n    poly2 = age^2,\n    poly3 = age^3\n  )\n```\n\nOr we could even just specify the calculations for each term _inside_ the call to `lm()`:\n```{r}\n#| eval: false\nlm(syndens ~ age + I(age^2) + I(age^3), data = syndat)\n```\n\nOr even use the `poly()` function:  \n```{r}\n#| eval: false\nlm(syndens ~ poly(age, degree=3, raw=TRUE), data = syndat)\n```\n\n:::\n::: {.callout-note collapse=\"true\"}\n#### A handy function from Dan\n\nDan has a nice function that may be handy. It adds the polynomials to your dataset for you:  \n```{r}\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nsyndat <- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nsyndat <- code_poly(df = syndat, predictor = 'age', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(syndat)\n```\n:::\n\n\nJust to see it in action, let's take a look at the predicted values from our model.  \nTake for instance, the 9th row below. The predicted value of y (shown in the `.fitted` column) is:  \n$\\hat y_9 = b_0 + b_1 \\cdot x_9 + b_2 \\cdot x^2_9 + b_3 \\cdot x^3_9$  \n$\\hat y_9 = b_0 + b_1 \\cdot 2 + b_2 \\cdot 4 + b_3 \\cdot 8$  \n$\\hat y_9 = -1.843 + 3.375 \\cdot 2 + -0.332 \\cdot 4 + 0.0097 \\cdot 8$  \n$\\hat y_9 = 3.66$.  \n```{r}\nlibrary(broom)\naugment(cubicmod) \n```\n\nIf we plot the predictions with `poly1` on the x-axis (`poly1` is just the same as our `age` variable with a different name!), we can see that we are able to model a non-linear relationship between y and x (between synaptic density and age), via a combination of linear parameters!  \n```{r}\n#| label: fig-fittedcubic\n#| fig-cap: \"a cubic model\"  \nlibrary(broom)\naugment(cubicmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=.fitted),col=\"darkorange\") +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper),fill=\"darkorange\", alpha=.2)+\n  labs(x=\"age\") # our x-axis, \"poly1\", is just age!  \n```\n\nNow lets look at our coefficients:   \n```{r}\n#| eval: false\nsummary(cubicmod)\n```\n```\n...\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.842656   0.704193  -2.617   0.0109 *  \npoly1        3.375159   0.345570   9.767 1.06e-14 ***\npoly2       -0.331747   0.044664  -7.428 2.06e-10 ***\npoly3        0.009685   0.001614   6.001 7.79e-08 ***\n---\n```\n\nWith polynomials the interpretation is a little tricky because we have 3 coefficients that together explain the curvy line we see in @fig-fittedcubic, and these coefficients are all dependent upon one another.  \n\n- `(Intercept)` = When all predictors are zero, i.e. the synaptic density at age 0.  \n- `poly1` coefficient = The instantaneous change in $y$ when $x=0$. \n- `poly2` coefficient = Represents \"rate of change of the rate of change\" at $x=0$. In other words, the _curvature_ at $x=0$.  \n- `poly3` coefficient = Represents how the curvature is changing. It gets more abstract as the order of polynomials increase, so the easiest way to think about it is \"the wiggliness\"  \n\nI've tried to represent what each term adds in @fig-polycoef. The intercept is the purple point where age is zero. The `poly1` coefficient is represented by the dashed blue line - the tangent of the curve at age zero. The `poly2` coef, rperesented by the dashed green line, is how the angle of the blue line is changing at age zero. Finally, the `poly3` coefficient tells us how much this curvature is changing (which gets us to our dashed orange line).  \n\nNote that these interpretations are all dependent upon the others - e.g. the interpretation of `poly2` refers to how the angle of `poly1` is changing.  \n\n```{r}\n#| label: fig-polycoef\n#| echo: false\n#| fig-cap: \"the instantaneous rate of change at x=0 (blue), the rate of change in the rate of change (i.e. curvature, green), and 'rate of change in rate of change in rate of change' (i.e. wiggliness, orange)\"\ncc = coef(cubicmod)\naugment(cubicmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n\n    stat_function(geom=\"line\",fun=function(.x) cc[1]+(cc[2]*.x)+(cc[3]*.x^2)+(cc[4]*.x^3),\n                lty=\"dashed\",col=\"darkorange\",lwd=1)+  \n  stat_function(geom=\"line\",fun=function(.x) cc[1]+(cc[2]*.x)+(cc[3]*.x^2),\n                lty=\"dashed\",col=\"green4\",lwd=1)+\n  geom_abline(intercept=cc[1],slope=cc[2],lty=\"dashed\", col=\"blue\",lwd=1)+\n  geom_point(x=0,y=cc[1],size=5,col=\"purple\") + \n  scale_x_continuous(\"age\",limits=c(0,19))+\n  scale_y_continuous(limits=c(-4,12))\n```\n\n## Orthogonal Polynomials  \n\nThe `poly()` function also enables us to compute \"orthogonal polynomials\". This is the same information as the raw polynomials, re-expressed into a set of __uncorrelated__ variables.  \n\nRaw polynomials are correlated, which is what results makes their interpretation depend upon one another. For example, if we take the numbers 1,2,3,4,5, then these numbers are _by definition_ correlated with their squares 1,4,9,16,25. As we increase from 1 to 5, we necessarily increase from 1 to 25.  \nHowever, if we first _center_ the set of numbers, so that 1,2,3,4,5 becomes -2,1,0,1,2, then their squares are 4,1,0,1,4 - they're not correlated!  \n\nOrthogonal polynomials essentially do this centering and scaling for $k$ degrees of polynomial terms.  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\nSo while raw polynomials look like this:  \n```{r}\n#| fig-height: 4\nmatplot(poly(1:10, 3, raw=T), type=\"l\", lwd=2)\n```\n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"45%\"}\nOrthogonal polynomials look like this:\n```{r}\n#| fig-height: 4\nmatplot(poly(1:10, 3, raw=F), type=\"l\", lwd=2)\n```\n:::\n::::\n\nThis orthogonality allows us to essentially capture express the linear trend, curvature, and 'wiggliness' of the trajectory **independently** from one another, rather than relative to one another.  \n\nUltimately, models using raw polynomials and using orthogonal polynomials are identical, but the coefficients we get out represent different things.\n\nLet's overwrite our `poly` variables with orthogonal polynomials, by setting `raw = FALSE`:  \n```{r}\nsyndat <- \n  syndat |> \n    mutate(\n      poly1 = poly(age,degree = 3, raw=FALSE)[,1],\n      poly2 = poly(age,degree = 3, raw=FALSE)[,2],\n      poly3 = poly(age,degree = 3, raw=FALSE)[,3],\n    )\n```\n\nAnd fit our model:  \n```{r}\n#| eval: false\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\nsummary(Ocubicmod)\n```\n```{r}\n#| echo: false\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\n```\n\n```\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.5917     0.1902  34.649  < 2e-16 ***\npoly1        12.9161     1.6365   7.892 2.88e-11 ***\npoly2       -14.3156     1.6365  -8.748 7.68e-13 ***\npoly3         9.8212     1.6365   6.001 7.79e-08 ***\n---\n```\n\nThe interpretation of the estimates themselves are not really very tangible anymore, because the scaling of the orthogonal polynomials has lost a clear link back to \"age\".  \n\nAs the polynomial terms are centered on the mean of age, the intercept is the estimated synaptic density at the mean age (the purple dot in @fig-polycoeforth). The `poly1`, `poly2` and `poly3` coefficient represent the independent overall linear trend, centered curvature, and \"wiggliness\" of the relationship between synaptic density and age (as shown in the blue, green and orange lines in @fig-polycoeforth respectively).  \n\n```{r}\n#| label: fig-polycoeforth\n#| fig-cap: \"the independent rate of change (blue), curvature (green) and wiggliness (orange) of the y~x relationship\"   \n#| echo: false\nOlinmod <- lm(syndens ~ poly1,syndat)\nOquadmod <- lm(syndens ~ poly1+poly2,syndat)\nOcubicmod <- lm(syndens ~poly1+poly2+poly3,syndat)\n\nOcc = coef(Olinmod)\nlibrary(broom)\naugment(Olinmod, interval=\"confidence\") |>\n  ggplot(aes(x=poly1))+\n  scale_y_continuous(limits=c(-4,12)) + \n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=predict(Olinmod)),lty=\"dashed\",col=\"blue\",lwd=1)+\n  geom_line(aes(y=predict(Oquadmod)),lty=\"dashed\",col=\"green4\",lwd=1)+\n  geom_line(aes(y=predict(Ocubicmod)),lty=\"dashed\",col=\"darkorange\",lwd=1)+\n  geom_point(x=0,y=coef(Ocubicmod)[1],size=5,col=\"purple\")\n```\n\n## Raw vs Orthognal  \n\nThe two models we have seen, one with raw polynomials, and one with orthogonal polynomials, are identical.  \n\nFor proof, compare the two:  \n```{r}\nanova(\n  lm(syndens ~ poly(age, 3, raw = TRUE), data = syndat),\n  lm(syndens ~ poly(age, 3, raw = FALSE), data = syndat)\n)\n```\n\nSo why would we choose one vs the other? \n\nThe main reason is if we are interested in evaluating things *relative to baseline*, in which case raw polynomials allow us to do just that. If we are instead interested in evaluating the trends **across** the timecourse, then we would want orthogonal polynomials. \n\nConsider two examples:  \n\n:::: {.columns}\n::: {.column width=\"45%\"}\n__Example 1__  \n\nA student advisor who meets with students as they start university wants to know about how happiness evolves over the course of students' year at univeristy, and wonders if this is different between introverted and extraverted individuals.  \n\nIn this case, they would want *raw* polynomials, so that they can assess whether the two personality types differ when they first come to University, and how this is likely to evolve from that point.  \n```{r}\n#| echo: false\ntibble(\n  age = seq(1,4,.1),\n  int = .6*scale(age) + .3*(scale(age)^2),\n  ex =  1.5 + .5*scale(age) -.2*(scale(age)^2),\n) |> pivot_longer(int:ex) |>\n  mutate(personality = \n           factor(name,levels=,c(\"ex\",\"int\"),\n                  labels=c(\"extravert\",\"introvert\"))) |>\n  ggplot(aes(x=age,y=value,col=personality))+\n  geom_line(lwd=1)+\n  labs(x=\"Year of Study at University\", y=\"Happiness\")+\n  scale_y_continuous(breaks=NULL)\n\n```\n:::\n::: {.column width=\"10%\"}\n:::\n::: {.column width=\"45%\"}\n__Example 2__  \n\nA company has four stores across the UK, and they want to know if the stores have differed in how variable their earnings have been across the year. \n\nIn this case, looking at change relative to month 1 isn't very useful. It would, for instance, tell us that the linear trend for store2's earnings is upwards, whereas the linear trend for store 1 is flat. This makes store2 look better.  \nIf we used orthogonal polynomials instead, we would see that the linear trend for store 2 is actually *negative* compared to store1. \n\n```{r}\n#| echo: false\ntibble(\n  month = seq(1,12,.3),\n  s1 = 1.4,#scale(month) + scale(month)^2,\n  s2 = 1.4 + .3*scale(month,center=F) + -.1*(scale(month)+1)^2 + -.2*scale(month,center=F)^3,\n  s3 = scale(month) + scale(month)^2\n) |> pivot_longer(s1:s3) |>\n  mutate(store=factor(name,levels=c(\"s1\",\"s2\",\"s3\"),\n                      labels=c(\"store1\",\"store2\",\"store3\"))) |>\n  ggplot(aes(x=month,y=value,col=store))+\n  geom_line(lwd=1)+\n  labs(x=\"Month\",y=\"Earnings\")+\n  scale_y_continuous(breaks=NULL)\n\n```\n\n:::\n::::\n\n:::sticky\n\nRaw? Orthogonal?  \n\nFor non-linear relationships, a good plot is usually the most important thing!  \n\n:::\n\n\n# Example in MLM\n\n\n:::frame\n__Data: midlife_ape.csv__  \n\n\n```{r}\n#| include: false\nss = 36460\n# ss = round(runif(1,1e3,1e6))\nset.seed(ss)\nn_groups = 200\nnpgroup = rep(10,200)\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nx = rep(1:10,200)\nb = sample(letters[1:2],n_groups,T,prob=c(.6,.4))\nb = b[g]\nres = MASS::mvrnorm(n=n_groups,\n                    mu=c(0,0),\n                    Sigma=diag(c(2,1))%*%matrix(c(1,.4,.4,1),nrow=2)%*%diag(c(2,1)))\nre0 = res[,1]\nre  = re0[g]\nrex = res[,2]\nre_x  = rex[g]\npoly1 = poly(x,2)[,1]\npoly2 = poly(x,2)[,2]\nlp = (0 + re) + (3 + re_x)*poly1 + \n  #+*poly1*(b==\"b\") +\n  9*poly2 + \n  .6*(b==\"b\") +\n  -5.5*(b==\"b\")*poly2\ny = lp + rnorm(N,0,1)\ndf = data.frame(x = x,g=g, b=b,y=round(scale(y)[,1],1))\n\nmnames = unique(randomNames::randomNames(1e5,which=\"first\"))\n\ndf = df |>\n  transmute(\n    apeID = mnames[g],\n    age = x,\n    species = as.character(factor(b, levels=letters[1:2],labels=c(\"chimp\",\"orangutan\"))),\n    HiP = round(scale(y)[,1]*2.4+9.4,2)\n  ) |>\n  mutate(\n    timepoint = age,\n    age = age*3 + 10\n  )\n\n\n\n# library(lme4)\n# df <- df |> mutate(\n#   poly1 = poly(timepoint,2,raw=F)[,1],\n#   poly2 = poly(timepoint,2,raw=F)[,2],\n# )\n# m = lmer(HiP ~ 1 + (poly1+poly2)*species+(1+poly1+poly2|apeID),df)\n# summary(m)\n# broom.mixed::augment(m) |>\n#   ggplot(aes(x=round(poly1,2),y=.fitted,col=species))+\n#   stat_summary(geom=\"pointrange\",aes(y=HiP))+\n#   stat_summary(geom=\"line\")\n# \n# write_csv(df, \"../../data/midlife_ape.csv\")\n\n```\n\nPrevious research has evidenced a notable dip in happiness for middle-aged humans. Interestingly, this phenomenon has even been observed in other primates, such as chimpanzees.  \n\nThe present study is interested in examining whether the 'middle-age slump' happens to a similar extent for Orangutans as it does for Chimpanzees. \n\n`r n_distinct(df$apeID)` apes (`r n_distinct(df$apeID[df$species==\"chimp\"])` Chimps and `r n_distinct(df$apeID[df$species==\"orangutan\"])` Orangutans) were included in the study. All apes were studied from early adulthood (10-12 years old for most great apes), and researchers administered the Happiness in Primates (HiP) scale to each participant every 3 years, up until the age of 40.  \n\nThe data are available at [https://uoepsy.github.io/data/midlife_ape.csv](https://uoepsy.github.io/data/midlife_ape.csv){target=\"_blank\"}.  \n\nThe dataset has already been cleaned, and the researchers have confirmed that it includes `r n_distinct(df$apeID[df$species==\"chimp\"])` Chimps and `r n_distinct(df$apeID[df$species==\"orangutan\"])` Orangutans, and every ape has complete data (i.e. 10 rows for each ape). A data dictionary can be found in @tbl-midlifedict\n\n```{r}\n#| echo: false\n#| label: tbl-midlifedict\n#| tbl-cap: \"Data Dictionary: midlife_ape.csv\"\nmlape <- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\ntibble(\n  variable=names(mlape),\n  description=c(\"Ape's Name (all names are chosen to be unique)\",\n                \"Age (in years) at assessment\",\n                \"Species (chimp v orangutan)\",\n                \"Happiness in Primate Scale (range 1 to 18)\",\n                \"Study visit (1 to 10)\")\n) |> gt::gt()\n```\n\n:::\n\n\nInitial plots are hard see when we plot a different line for each ape:  \n\n```{r}\nmlape <- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\n\nggplot(mlape, aes(x=age,y=HiP))+\n  geom_line(aes(group=apeID,col=species),alpha=.3) +\n  scale_color_manual(values=c(\"grey40\",\"darkorange\")) + \n  facet_wrap(~species)\n```\n\nWhat is often more useful for exploring is to plot the averages in y across values of x.  \nHere we can see some slight curvature - the chimps seem to have a notable dip in happiness at about 20 years old.  \n\n```{r}\nggplot(mlape, aes(x=age,y=HiP, col=species))+\n  scale_color_manual(values=c(\"grey40\",\"darkorange\")) + \n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Note\nThese plots can be misleading, especially in the event of missing data or incomplete trajectories.  For instance, suppose we had data on happy chimps when they are young and when they are old, but not in the middle of their lives. The average of all chimps at each data point would look non-linear!  \n\nThis illusion would not fool our multilevel model, however, which estimates the average of the individual lines\n\n```{r}\n#| echo: false\n#| label: fig-missing\n#| fig-cap: \"hypothetical: if happier apes have missing data in mid life, the average looks non-linear, even though for each ape it is linear!\"\n#| out-width: \"100%\"\n#| fig-height: 3.5\ntibble(\n  g = rep(1:20,e=10),\n  g0 = rep(sort(rnorm(20)),e=10),\n  age = rep(seq(13,40,3),20),\n  HiP = 9 + g0 + rnorm(200,0,.05)\n) |> \n  filter(!(g>15 & age>20 & age<35)) |>\n  filter(!(g>10 & g<=15 & age>25 & age<30))-> dd\n\n\nggplot(dd, aes(x=age,y=HiP,group=g))+\n  geom_point(size=3,alpha=.3)+\n  geom_line(alpha=.3) +\n   labs(title=\"individual chimp lines\") + \n  \nggplot(dd, aes(x=age,y=HiP))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")+\n  labs(title=\"naive averages\")\n```\n\n\n:::\n\n\n> The present study is interested in examining whether the 'middle-age slump' happens to a similar extent for Orangutans as it does for Chimpanzees. \n\nWe're interested in the difference in curvature across adulthood between Chimps and Orangutans. This is a case where we're not hugely interested in some specific point in time, so orthogonal polynomials might be easier.  \nBoth `age` and `timepoint` here contain the same information, so we can use either to create our polynomials:\n```{r}\nmlape <- mlape |> \n  mutate(\n    poly1 = poly(timepoint,2,raw=F)[,1],\n    poly2 = poly(timepoint,2,raw=F)[,2],\n  )\n```\n\nAnd then we'll fit our model.  \n\nWe know we're interested in the change in happiness across age, so our model is going to take the form: \n```\nlmer(HiP ~ age ....\n```\n. But we've just said that we're interested in the curvature of this, so we're going to re-express `age` as the two parts, the linear and the quadratic:  \n```\nlmer(HiP ~ poly1 + poly2 ....\n```\nWe're interested in the differences in the \"hapiness ~ age\" relationship between Orangutans and Chimps, so we can estimate species-differences in both aspects of the trajectroy (the linear and the quadratic).  \n```\nlmer(HiP ~ (poly1 + poly2) * species ....\n```\nthis is equivalent to\n```\nlmer(HiP ~ poly1 + poly2 + species + \n           poly1:species + poly2:species ....\n```\n\nFinally, we want to account for the variability in happiness due to random ape-to-ape differences (i.e. some are just happy, some are not).  \n```\nlmer(HiP ~ (poly1 + poly2) * species + \n           (1 + ... | apeID), data = mlape)\n```\n\nLastly, it's very likely that 'change over time' varies from ape-to-ape, so we want to allow the slopes of age to vary. We can allow one or both of the poly1 and poly2 terms to vary, so we'll start with both:  \n\n```{r}\nlibrary(lme4)\nmod1 = lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape)\n```\n\nand it converges (hooray!). \n\nNow let's plot the average value of happiness across time, alongside the average _predicted_ value.  \n\n```{r}\nbroom.mixed::augment(mod1) |>\n  mutate(poly1 = round(poly1,4)) |>\n  ggplot(aes(x=poly1,y=.fitted,col=species))+\n  # the average value across time: \n  stat_summary(geom=\"pointrange\",aes(y=HiP))+\n  # the average model predicted value\n  stat_summary(geom=\"line\") + \n  scale_color_manual(values=c(\"grey40\",\"darkorange\"))\n```\n\nIt looks like both species have a bit of curvature, and this may be more pronounced for chimps than for orangutans.  \n\nLet's look at our parameters to see:  \n```{r}\n#| eval: false\n# fitted with satterthwaite to get some p-values\nlmerTest::lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape) |>\n  summary()\n```\n```\n...\nFixed effects:\n                       Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)              9.1155     0.1955 198.0009  46.630  < 2e-16 ***\npoly1                    1.8796     1.5391 197.9811   1.221   0.2235    \npoly2                    9.4004     1.7454 197.9991   5.386 2.03e-07 ***\nspeciesorangutan         0.6853     0.3035 198.0009   2.258   0.0250 *  \npoly1:speciesorangutan   0.1104     2.3892 197.9811   0.046   0.9632    \npoly2:speciesorangutan  -6.2718     2.7093 197.9991  -2.315   0.0216 *  \n```\n```{r}\n#| echo: false\nlmerTest::lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape) |>\nbroom.mixed::tidy() |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    p = case_when(\n      p.value<.001 ~ \"***\",\n      p.value<.01 ~ \"**\",\n      p.value<.05 ~ \"*\",\n      TRUE ~ \"\"\n    ),\n    `what it tells us` = c(\n      \"the average happiness at the average age is 9.11 (not hugely interesting).\",\n      \"there is no significant linear trend of happiness over time for Chimpanzees (you can kind of see this in our plot above - they more or less end where they started)\",\n      \"there *is* a significant curvature to the chimpanzees' happiness trajectory. Because this estimate is positive, that means we know it is u-shaped and not n-shaped - i.e. - Chimpanzees have a mid-life slump in happiness!\",\n      \"at the average age, Orangutans are happier than Chimpanzees.\",\n      \"the linear trend for Orangutans is not significantly different from the linear trend for Chimpanzees\",\n      \"the curvature for Orangutans is significantly different from the curvature for Chimpanzees. Because the estimate is negative, that means it is less u-shaped than the Chimpanzee line - i.e. Orangutans do not have as much of a mid-life slump as Chimps do!\"\n    )\n  ) |> gt::gt()\n```\n\n\nAs we have two groups, the parameters provide us with all the tests we need to identify \"do species differ in the extent that they have a mid-life dip in happiness?\"  \nBut if we had $>2$ species and wanted to do test join \"species differences\", we could set up models for comparisons, i.e. allowing species to interact with different parts: \n\n- `HiP ~ (poly1 + poly2)*species`\n    + species differ in their linear and quadratic trends\n- `HiP ~ poly1*species + poly2`\n    + species differ in their linear trend only\n- `HiP ~ poly1 + poly2 + species`\n    + species differ in their overall happiness, but not in change over age\n\n\n```{r}\n#| eval: false\n#| echo: false\n\nmod1 = lmer(HiP ~ 1 + poly(timepoint,2,raw=TRUE)*species+\n            (1+poly(timepoint,2,raw=TRUE)|apeID), \n            data = mlape)\neffects::effect(term=c(\"species*timepoint\"),mod=mod1, xlevels=40) |>\n  as.data.frame() |>\n  ggplot(aes(x=timepoint,y=fit,ymin=lower,ymax=upper,col=species))+\n  geom_line()+geom_ribbon(alpha=.2,aes(fill=species),col=NA)\n\nmod1 = lmer(HiP ~ 1 + poly(timepoint,2,raw=FALSE)*species+\n            (1+poly(timepoint,2,raw=FALSE)|apeID), \n            data = mlape)\neffects::effect(term=c(\"species*timepoint\"),mod=mod1, xlevels=40) |>\n  as.data.frame() |>\n  ggplot(aes(x=timepoint,y=fit,ymin=lower,ymax=upper,col=species))+\n  geom_line()+geom_ribbon(alpha=.2,aes(fill=species),col=NA)\n\n\nmod1 = lmer(HiP ~ 1 + (poly1+poly2)*species+\n            (1+poly1+poly2|apeID), \n            data = mlape)\n\npp1 <- bind_rows(poly(seq(1,10,length.out=50),2) |> as_tibble(),\n          poly(seq(1,10,length.out=50),2) |> as_tibble()\n          ) |>\n  mutate(species=rep(c(\"chimp\",\"orangutan\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"species\")\n\nbts = bootMer(mod1,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=500)\n\nbts$t\n\npp1$pred = predict(mod1, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\n\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=species,fill=species))+\n  geom_line()+\n  geom_ribbon(alpha=.2)\n  \n\n\n```\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### TIMTOWTDI \nAs always, **there is more than one way to do it (TIMTOWTDI)**  \n\nThe approach we are learning about in this course is only one of many approaches to studying non-linearity. \nSome alternatives, which you may come across in future work, are listed below.\n\n**Piecewise linear regression:** fit a linear model with cut point(s) along x (determine cut points by greatest reduction in mean squared error $\\sigma$)\n```{r echo=FALSE, out.width=\"350px\"}\nMASS::mcycle %>%\n  rename(y=accel,x=times) %>% filter(x>20) -> df\nbreaks <- df$x[which(df$x >= 20 & df$x <= 40)]\nmse <- numeric(length(breaks))\nfor(i in 1:length(breaks)){\n piecewise1 <- lm(y ~ x*(x < breaks[i]) + x*(x>=breaks[i]),df)\n mse[i] <- summary(piecewise1)[6]\n}\nmse <- as.numeric(mse)\n#breaks[which(mse==min(mse))]\npiecewise2 <- lm(y ~ x*(x <= 33) + x*(x > 33), df)\ndf %>% mutate(\n  pred = predict(piecewise2),\n  se = predict(piecewise2,se.fit = T)$se\n) %>% \n  ggplot(.,aes(x=x,y=pred))+\n  geom_line(aes(group=x>33),lwd=1)+\n  #geom_ribbon(aes(ymin=pred-(1.96*se),ymax=pred+(1.96*se),group=x>33),alpha=.2)+\n  geom_point(aes(y=y))+labs(y=\"y\",title=\"Predicted values of y\", subtitle=\"y ~ x*(x < 33) + x*(x > 33)\")\n```\n\n**Piecewise polynomial** fit the model $y \\sim x + x^2 + x^3$ to equal chunks of x.  \n\n```{r echo=FALSE, out.width=\"350px\"}\nmod<-function(ddf){lm(y~poly(x,3), data=ddf)}\n\ndf %>% mutate(pieces = cut(x,3)) %>% \n  group_by(pieces) %>%\n  nest_legacy() %>%\n  mutate(\n    model = map(data, mod),\n    fit = map(model, ~fitted(.))\n  ) %>%\n  unnest_legacy(data,fit) %>%\n  ggplot(., aes(x=x))+\n  geom_point(aes(y=y))+\n  geom_line(aes(y=fit, col=pieces),lwd=1)+\n  theme_minimal() + guides(col=FALSE)+\n  labs(title=\"Predicted values of y\",subtitle=\"y~ x + x^2 + x^3 for 3 cuts of x\")\n```\n\n\n**Splines, penalised splines & GAMS** \n\nThis begins to open a huge can of worms, but if you foresee yourself needing these sort of tools, then Simon Wood, author of the **mgcv** R package for fitting generalised additive models (GAMS), has plenty of materials on [his webpage](https://www.maths.ed.ac.uk/~swood34/talks/snw-Koln.pdf) (Warning, these are fairly technical). There are also a reasonable number of tutorials [online which are really good](https://www.google.com/search?hl=&site=&q=gam+in+r+tutorial).  \n```{r echo=FALSE, out.width=\"350px\"}\nlibrary(mgcv)\ngam(y~s(x,bs=\"cr\"),df, family=\"gaussian\") -> m\ndf %>% mutate(\n  pred = predict(m),\n  se = predict(m,se.fit = T)$se\n) %>% \n  ggplot(.,aes(x=x,y=pred))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=pred-(1.96*se),ymax=pred+(1.96*se)),alpha=.2)+\n  geom_point(aes(y=y))+labs(y=\"y\",title=\"Predicted values of y\", subtitle=\"mgcv::gam(y ~ s(x, b = 'cr'))\")\n```\n\n\n:::\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"03a_poly.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"3A: Polynomial Growth","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}