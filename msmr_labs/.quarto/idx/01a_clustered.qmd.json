{"title":"1A: Clustered Data | LMM","markdown":{"yaml":{"title":"1A: Clustered Data | LMM","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Our Starting Point","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\n```\n\n\n:::lo\n\nTODO this reading\n\n:::\n\n\n\nWe're going to start this course with our good old friend the linear regression model. The content we're going to cover in this block can be seen as an extension of these methods, so keep in mind that the materials from previous courses you have taken will likely be very a useful resource. \n\n\n:::sticky\n__Linear Regression__\n\nThe linear regression model is a way of expressing an outcome (or \"dependent\") variable $y$ as the linear combination of various predictors $x_1,\\ ...\\ ,\\ x_p$ (or \"independent variables\").   \n\nWe can write the linear regression model as:  \n$$\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}\\\\ \n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n$$\nwhere the $b$'s (sometimes written $\\beta$) are the partial associations between each predictor and the outcome. For instance, $b_1$ represents the expected change in $y$ associated with a 1 unit change in $x_1$, while _holding constant_ other independent variables $x_2,\\ ...\\ ,\\ x_p$.  \nIn this way, we can use the regression model to isolate the association between each predictor and the outcome from the other predictors in the model. \n\nIn R, we fit these models using:\n```{r eval=F}\nlm(y ~ x1 + x2 + .... xp, data = mydata)  \n```\n\n`r optbegin(\"Optional: a general notation\", olabel=FALSE)`\n\nIf we wanted to write this more simply, we can express $x_1$ to $x_p$ as an $n \\times p$ matrix (sample size $\\times$ parameters), and $b_0$ to $b_p$ as a vector of coefficients:\n$$\n\\begin{align}\n& \\color{red}{\\mathbf{y}} = \\color{blue}{\\mathbf{X b}} + \\boldsymbol{\\varepsilon} \\\\\n& \\varepsilon \\sim N(0, \\sigma) \\text{ independently} \\\\\n\\end{align}\n$$\nYou can see below how we get between these two formulations. In the top line, we use an index $i$ to indicate that the model is fitted over a set of observations. In the middle we can see this expanded out to show the value for each individual observation on each $y$ and $x$. Because the $b$ coefficients are fixed - they are the same for each observation $i$, we can separate them out so that we are multiplying the $\\mathbf{X}$ matrix by the vector of $\\mathbf{b}$'s (i.e. the formulation we see at the bottom).  \n\n$$\n\\begin{align} \\color{red}{y_i} \\;\\;\\;\\; & = \\;\\;\\;\\;\\; \\color{blue}{b_0 \\cdot{} 1 + b_1 \\cdot{} x_{1i} + ... + b_p \\cdot x_{pi}} & + & \\;\\;\\;\\varepsilon_i \\\\ \\qquad \\\\ \\color{red}{\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ \\vdots \\\\ y_n \\end{bmatrix}} & = \\color{blue}{\\begin{bmatrix} 1 & x_{11} & x_{21} & \\dots & x_{p1} \\\\ 1 & x_{12} & x_{22} &  & x_{p2} \\\\ 1 & x_{13} & x_{23} &  & x_{p3} \\\\ 1 & x_{14} & x_{24} &  & x_{p4} \\\\ 1 & x_{15} & x_{25} &  & x_{p5} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{1n} & x_{2n} & \\dots & x_{pn} \\end{bmatrix} \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{bmatrix}} & + & \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\\\ \\varepsilon_4 \\\\ \\varepsilon_5 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix} \\\\ \\qquad \\\\ \\color{red}{\\boldsymbol y} \\;\\;\\;\\;\\; & = \\qquad \\qquad \\;\\;\\; \\mathbf{\\color{blue}{X \\qquad \\qquad \\qquad \\;\\;\\;\\: b}} & + & \\;\\;\\; \\boldsymbol \\varepsilon \\\\ \\end{align}\n$$\n\n`r optend()`\n\n<!-- We have seen various ways we can extend this model to capture different processes - predictors can be continuous or categorical; we can include _interactions_ (where the effect of $x_1$ on $y$ _depends_ on the level of $x_2$); and we can even extend the same model structure to model discrete outcomes (with a little trickery of using a function to link the expected value of $\\color{red}{y}$ to the linear prediction $\\color{blue}{b_0 + b_1(x_1) + ... + b_p(x_p)}$).  -->\n\n:::\n\n\nWhen we fit linear regression models, we are fitting a line (or a regression surface, when we add in more predictors), to a cloud of datapoints. The discrepancy between the fitted model and the observed data is taken up by the residuals. \n\n$$\n\\begin{align}\n\\color{red}{y} &= \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n\\color{red}{\\text{observed }y} &= \\color{blue}{\\text{fitted }\\hat y} \\,\\, \\color{black}{+ \\text{ residual }\\hat \\varepsilon}\\\\ \n\\end{align}\n$$\n\nWe are theorising that our model contains all the systematic relationships with our outcome variable, we assume that the residuals - the leftovers - are essentially random noise. This is the $\\epsilon \\sim N(0, \\sigma)$ bit, which is a way of specifying our assumption that the errors are normally distributed with a mean of zero (see @fig-slr2).   \n\n```{r}\n#| label: fig-slr2\n#| echo: false\n#| fig-cap: \"Simple linear regression model, with the systematic part of the model in blue, and residuals in red. The distributional assumption placed on the residuals is visualised by the orange normal curves - the residuals are normally distributed with a mean of zero, and this does not change across the fitted model.\"\n#| fig-height: 3.5\nset.seed(235)\ndf <- tibble(\n  x = rnorm(100,3,1),\n  y = 0.5+.8*x + rnorm(100,0,1)\n)\ndf$y=df$y+1\nmodel1 <- lm(y ~ x, data = df)\nbetas <- coef(model1)\nintercept <- betas[1]\nslope <- betas[2]\n\ndfd<-tibble(\n  x = c(0:4),\n  m = predict(model1,newdata=tibble(x=c(0:4))),\n  s = sigma(model1)\n)\n\nbroom::augment(model1) %>%\nggplot(., aes(x = x, y = y)) +\n  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=x,dist=\"norm\",arg1=m,arg2=s),alpha=.15, fill=\"orange\") + \n  geom_point(size=3,alpha=.5)+\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  #xlim(0,6)+ylim(0,7)+\n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(breaks=0:6)+\n  labs(x = \"X (predictor)\", \n       y = \"Y (outcome)\")+\n  geom_segment(aes(x=x, xend=x, y=y, yend=.fitted), col=\"red\",lty=\"dotted\", linewidth=0.7)\n```\n\nWe typically want to check our model residuals (by plotting or performing statistical tests) to determine if we have reason to believe our assumptions are violated. The easiest way to do this in R is with `plot(model)`, which provides us with a series of visuals to examine for unusual patterns and conspicuous observations.  \n\nWhen model assumptions appear problematic, then our inferential tools go out the window. While our specific point estimates for our regression coefficients are our best linear estimates for the sample that we have, our standard errors rely on the distributional assumptions of the residuals^[Why is this? It's because the formula to calculate the standard error involves $\\sigma^2$ - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient]. It is our standard errors that allow us to construct test statistics and compute p-values (@fig-inf1) and construct confidence intervals. Our assumptions underpin our ability to generalise from our specific sample to make statements about the broader population.  \n\n\n```{r}\n#| label: fig-inf1\n#| echo: false\n#| fig-cap: \"inference for regression coefficients\" \nknitr::include_graphics(\"images/sum4.png\")\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Refresher: Standard Error   \n\nTaking **samples** from a **population** involves an element of _randomness_. The mean height of 10 randomly chosen Scottish people will not be exactly equal to the mean height of the entire Scottish population. Take another sample of 10, and we get _another_ mean height (@fig-se).  \n\n```{r}\n#| echo: false\n#| label: fig-se\n#| fig-cap: \"Estimates from random samples vary randomly around the population parameter\"\nset.seed(nchar(\"i'm going dotty\"))\ntibble(\n  m = replicate(750, mean(rnorm(10,165,11)))\n) |>\n  ggplot(aes(x=m))+\n  geom_vline(xintercept = 165, col=\"red\",lty=\"dashed\", lwd=1) +\n  geom_dotplot(dotsize=.5,binwidth=.5) +\n  scale_y_continuous(NULL, breaks=NULL)+\n  scale_x_continuous(\"mean height (cm)\",breaks=seq(-10,10,5)+165) +\n  theme_minimal()+\n  annotate(\"text\",\n           x=170, y=.88, \n           label=\"Mean height of\\nentire Scottish population\", col=\"red\",\n           hjust=0)+\n  geom_curve(aes(x=170, xend=165, y=.88, yend=.88), col=\"red\", size=0.5, \n             curvature = 0, arrow = arrow(length = unit(0.03, \"npc\"))) +\n  \n  annotate(\"text\",x=156, y=.57, label=\"the mean height of\\n10 randomly selected\\nScottish people\", col=\"grey30\") +\n  geom_curve(aes(x=156, xend=160.5, y=.5, yend=.40), col=\"grey30\", size=0.5, curvature = 0.2, arrow = arrow(length = unit(0.03, \"npc\"))) +\n  \n  annotate(\"text\",x=173, y=.62, label=\"the mean height of\\nanother random sample of 10\", col=\"grey30\") +\n  annotate(\"text\",x=174, y=.45, label=\"and another\", col=\"grey30\") +\n  annotate(\"text\",x=175, y=.25, label=\"and another\", col=\"grey30\") +\n  geom_curve(aes(x=173, xend=167.5, y=.57, yend=.45), col=\"grey30\", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_curve(aes(x=174, xend=169, y=.43, yend=.35), col=\"grey30\", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_curve(aes(x=175, xend=173.9, y=.23, yend=.015), col=\"grey30\", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))\n```\n\nThe standard error of a statistic is the standard deviation of all the statistics we _might have_ computed from samples of that size (@fig-se2). We can calculate a standard error using formulae (e.g. for a mean, the standard error is $\\frac{\\sigma}{\\sqrt{n}}$) but we can also use more computationally intensive approaches such as \"bootstrapping\" to actually generate an empirical sampling distribution of statistics which we can then summarise.  \n\nWe use the standard error to quantify the uncertainty around our sample statistic as an estimate of the population parameter, or to construct standardised test statistics in order to perform tests against some null hypothesis.  \n\n```{r}\n#| echo: false\n#| label: fig-se2\n#| fig-cap: \"The standard error is the standard deviation of the 'sampling distribution' - the distribution of sample statistics that we _could_ see.\"\nset.seed(2394)\nsamplemeans <- replicate(2000, mean(rnorm(10,0,11)))\ng <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+\n  #geom_histogram(alpha=.3)+\n  stat_function(geom=\"line\",fun=~dnorm(.x, mean=0,sd=sd(samplemeans)),lwd=1)\n\nld <- layer_data(g) %>% filter(x <= (11/sqrt(10)) & x >= (-11/sqrt(10)))\nld2 <- layer_data(g) %>% filter(x <= 2*(11/sqrt(10)) & x >= 2*(-11/sqrt(10)))\ng + geom_area(data=ld,aes(x=x,y=y),fill=\"grey30\",alpha=.3) + \n  geom_area(data=ld2,aes(x=x,y=y),fill=\"grey30\",alpha=.1) +\n  geom_vline(xintercept = 0, col=\"red\",lty=\"dashed\", lwd=1) +\n  annotate(\"text\",\n           x=5, y=.12, \n           label=\"Mean height of\\nentire Scottish population\", col=\"red\",\n           hjust=0)+\n  geom_curve(aes(x=5, xend=0, y=.12, yend=.12), col=\"red\", size=0.5, \n             curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_segment(x=0,xend=(-10.5/sqrt(10)),y=.06,yend=.06) +\n  annotate(\"text\",x=-7, y=.07, label=\"standard error\\n(standard deviation of\\nsampling distribution)\", col=\"grey30\")+\n  geom_curve(aes(x=-7, xend=-1.5, y=.06, yend=.06), col=\"grey30\", size=0.5, curvature = 0.8, arrow = arrow(length = unit(0.03, \"npc\")))+\n\n  scale_y_continuous(NULL, limits=c(0,.135),breaks=NULL)+\n  labs(x=\"mean height (cm)\") +\n  theme_minimal()+\n  scale_x_continuous(\"mean height (cm)\",breaks=seq(-10,10,5),labels=seq(-10,10,5)+165)\n\n```\n\n\n:::\n\n\nIn the face of plots (or tests) that appear to show violations of the distributional assumptions (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. If you're interested, you can read brief explanations about some of these methods [here](00_lm_assumpt.html){target=\"_blank\"}. \n\nOne assumption that none of the above approaches address is our assumption of __\"independence\"__.^[With the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we _can_ actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).] Thinking broadly, if our observations are not independent from one another, this means that there are systematic differences in our outcome variable that our model is not capturing. \n\n:::sticky\n__Independence of observations__  \n\nIndividual observations in the sample are not related to one another in any way. The value of one observation does not influence the value of any other observation.  \n\n:::\n\nOne common way in which observations are not independent can come when the data has a _hierarchical_ structure - some form of __clustering__ of observations into different groups. These groups, or 'clusters', could be simply something we observe, or they could be the result of how we have designed our study.  \n\n\n# Clustered Data\n\n```{r}\n#| echo: false\n#| eval: false\n#| label: SCHOOLDATA\nschoolids = read_csv(\"data/schoolnames.csv\") |> janitor::clean_names() |> \n  filter(secondary_12==\"Secondary\") |> pull(school_name)\nset.seed(nchar(\"cool cool cool cool cool\")*1385)\nn_groups = 30\n# npgroup = round(runif(30,2,25))\nnpgroup = rep(30,30)\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nxd = rnorm(N)\nxm = rnorm(n_groups)[g]\nb = rnorm(n_groups, 6, 1.2)\nb = round(b[g],2)\nre0 = rnorm(n_groups, sd = 1)\nre  = re0[g]\nrex = rnorm(n_groups, sd = 1)\nre_x  = rex[g]\nlp = (0 + re) + 1*xm + (1 + re_x)*xd + 1*b - .1*b*(xd+xm)\ny = rnorm(N, mean = lp, sd = 2)\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf = data.frame(x = xm+xd, b, g=factor(g), y, y_bin)\n\nschoolmot = df |>\n  transmute(\n    motiv = round(x,2),\n    funding = b,\n    schoolid = sample(schoolids,n_groups)[as.numeric(g)],\n    grade = round(50.356 + scale(y)[,1]*14.53,2)\n  )\nschoolmot[schoolmot$schoolid==\"Fettes College\",\"funding\"] <- 9.1\n\n# library(ICC)\n# ICCbare(schoolid, grade, data = schoolmot)\n# library(lme4)\n# lmer(grade~1+motiv*funding+(1+motiv|schoolid),schoolmot) |> summary()\n#    sjPlot::plot_model(type=\"pred\",terms=c(\"motiv\",\"funding [4, 5, 6, 7]\"),show.data=TRUE)\n\n# write_csv(schoolmot,\"data/schoolmot.csv\")\n# schoolmot <- read_csv(\"data/schoolmot.csv\")\n# academic_performance ~ 1 + motiv * schoolfunding + (1 + motiv | school)\n# health_anxiety ~ 1 + education * GPpracticesize + (1 + education | GP)\n```\n\n```{r}\n#| include: false\nschoolmot <- read_csv(\"data/schoolmot.csv\") |> mutate(schoolid=factor(schoolid))\n```\n\nA common example to start thinking about clustered data is to think about conducting a study on school children. Each observation in our sample is a different child, and they come from a set of various schools (i.e. we might get 20 children from `r levels(schoolmot$schoolid)[10]`, 15 from `r levels(schoolmot$schoolid)[11]`, and so on). If we ignore the clustering of children into their respective schools, we can end up reaching completely different conclusions. \n\nThis can be seen in @fig-egschool - in which each datapoint is a single child. If we consider them to be independent of one another (Left-Hand plot), then we might think there is a strong positive relationship between `grade` and `motivation`. Once we colour them by which school they belong to, we can see that the observations in the top right of the plots are all high for some other reason - because they come from the green school. \n\n```{r}\n#| label: fig-egschool\n#| echo: false\n#| fig-cap: \"Observations (children) are clustered (into schools), and their grades are dependent upon which cluster they are in. Ignoring the clustering may lead to erroneous conclusions\"  \nset.seed(46)\nn_groups = 5\nnpgroup = round(runif(5,5,10))\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nxm = rnorm(n_groups,3,1)\nxm = xm[g]\nxd = rnorm(N)\nre0 = rnorm(n_groups, sd = 1)\nre  = re0[g]\nrex = rnorm(n_groups, sd = .1)\nre_x  = rex[g]\nlp = (0 + re) + 2*xm + (0 + re_x)*xd\ny = rnorm(N, mean = lp, sd = 1)*9\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf = data.frame(x=scale(xm+xd)[,1], g=factor(g), y, y_bin)\n\np1 = ggplot(df,aes(x=x,y=y))+\n  geom_point(size=3) +\n  geom_smooth(method=lm,se=F)+\n  labs(title=\"motivation and grade\",subtitle=\"ignoring school\",\n       x=\"motivation\",y=\"grade\")\n\np2 = ggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(size=3)+\n  geom_smooth(method=lm,se=F)+\n  guides(col=\"none\") +\n  labs(title=\"motivation and grade\",subtitle=\"separate lines for each school\",\n       x=\"motivation\",y=\"grade\")\n\np3 = df |> mutate(r=resid(lm(y~x,df)),f=fitted(lm(y~x,df))) |>\n  ggplot(aes(x=f,y=r)) +\n  geom_point(size=3, aes(col=g)) +\n  geom_hline(yintercept=0,lty=\"dotted\") +\n  geom_segment(aes(x=f,xend=f,y=0,yend=r),\n               lty=\"dotted\",col=\"red\")+\n  guides(col=\"none\") + \n  labs(x=\"Fitted Values\\nlm(grade ~ motivation)\",y=\"Residuals\")\n  \n(p1 + p2)\n\n```\n\nPut another way - the children are _not_ independent from one another, because they tend to be more similar to other children in the same school than they are to children from a different school. Our assumption of the normal linear model that $\\varepsilon \\sim N(0,\\sigma) \\text{ independently}$ is violated, because groups of residuals are related to one another - e.g. all the children from the green school have positive residuals (see residuals vs fitted plot in @fig-egschool2).  \n\n```{r}\n#| label: fig-egschool2\n#| echo: false\n#| fig-cap: \"Residuals vs Fitted plot from the linear model that ignores the school/children structure of the data\"\np3\n```\n\n## Clusters clusters everywhere\n\nThe idea of observing \"children in schools\" is just one such example of clustering that we might come across. This same _hierarchical data structure_ can be found in patients within medical practices, employees within departments, people within towns etc. These sort of groups exist 'in the wild' (i.e. the schools, departments and towns are all just occurrences of groups that we have observed). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a __Repeated Measures__ study we have individual experimental trials clustered within participants. __Longitudinal__ studies exhibit the same data structure but have _time-ordered_ observations clustered within people.  \n\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters^[It's [\"turtles all the way down\"](https://en.wikipedia.org/wiki/Turtles_all_the_way_down){target=\"_blank\"}]. @tbl-design shows just a few examples of different levels of clustering that may arise from different types of study.  \n\n<!-- Cross sectional studies are snapshots of a structure (and that structure may contain clusters); Repeated measures studies involve assessing the same person^[it doesn't have to be people, but in psychology it usually is!] multiple times across different experimental conditions/items (resulting in clusters of observation for each person);  Longitudinal studies also result in clusters of observations for each person, but there is a temporal sequence to these observations (e.g. at various ages).   -->\n\n```{r}\n#| echo: false\n#| label: tbl-design\n#| tbl-cap: \"Various different study designs will give rise to clustered data.\" \ntribble(\n  ~` `,~`Cross Sectional`,~`Repeated Measures`,~`Longitudinal`,\n  \"Level n\",\"...\",\"...\",\"...\",\n  \"...\",\"...\",\"...\",\"...\",\n  \"Level 3\",\"School\",\"...\",\"Families\",\n  \"Level 2\",\"Classroom\",\"Participants\",\"People\",\n  \"Level 1 (Observations)\",\"Children\",\"Experimental Stimuli\",\"Time\"\n) |> gt::gt()\n```\n\n## What are 'clusters'?  \n\nAt the fundamental level, we are using the term 'cluster' here to refer to __a grouping of observations.__ In fact, we will probably start using the terms \"clusters\" and \"groups\" interchangeably, so it's worth taking a bit of time to try and understand the _kind_ of groupings that we're talking about (and how we think about them).  \n\n:::sticky\n\n- \"Clusters\" are just \"groups\".  \n- When we talk about clustered data, the groups we are discussing can be thought of as a _random sample of higher level units_.  \n- More often than not, the specific group-differences are not of interest.  \n\n:::\n\nContrast the idea of 'clusters' with how we think about other sorts of groupings. In a study that looks at \"how do drugs placebo/aspirin/beta-blockers influence people's heart rate?\" (@fig-clgroup LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we'll use the same drugs (they're not just a random sample of drugs - the x-axis of our LH plot in @fig-clgroup will be the same).  \n\nIf we are interested in \"what is the average grade at GCSE?\", and we have children grouped into different schools (@fig-clgroup RH plot), we are probably not interested in all the specific differences between grades in `r levels(schoolmot$schoolid)[6]` vs `r levels(schoolmot$schoolid)[13]` etc. If we were to run our study again, we might not collect data from the same set of schools. We can view these schools as 'clusters' - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).  \n\n```{r}\n#| label: fig-clgroup\n#| fig-height: 3.5\n#| fig-cap: \"Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\"  \n#| echo: false\nset.seed(3)\nbp <- tibble(drug = rep(0:2,50),\n       hr = 65 + (drug==1)*-2 + (drug==2)*-3 + rnorm(150,0,2)\n) |>  mutate(\n  drug = fct_recode(factor(drug),\n                    placebo=\"0\",\n                    aspirin=\"1\",\n                    betablocker=\"2\")\n)\n\nset.seed(753)\npltd <- schoolmot |> filter(schoolid %in% sample(unique(schoolmot$schoolid),10)) |> droplevels()\nlevels(pltd$schoolid)[9:10] <- c(\"...\",\"... \")\n\nggplot(bp,aes(x=drug,y=hr))+\n  geom_boxplot()+\n  labs(x=\"- Drug -\", y=\"Resting Heart Rate (bpm)\") +\n\nggplot(pltd,aes(x=schoolid,y=grade))+\n  geom_boxplot()+\n  scale_x_discrete(labels=abbreviate)+\n  labs(x=\"- School -\", y=\"Grade (%)\")+\n  theme(axis.text.x = element_text(angle=45))\n\n```\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the level of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as \"mixed effects modelling\"), which is a regression modelling technique that allows us to explore questions such as these (and many more).^[Depending on the research question and design of the study, we may be only interested in things that occur at \"level 1\" (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply \"account for the nuisance clustering\".]  \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Optional \"univariate\"and \"multivariate\"\n\nIn \"univariate\" statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.  \n\nIn \"multivariate\" statistics, there are more sources of variation. For the \"children in schools\" example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another. \n\n:::\n::: {.callout-note collapse=\"true\"}\n#### Optional: \"Panel data\"\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as 'panel data'. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:    \n\n```{r}\n#| echo: false\n#| label: fig-panel1\n#| fig-cap: \"Panels of data\"\nknitr::include_graphics(\"images/panel1.png\")\n```\n```{r}\n#| echo: false\n#| label: fig-panel2\n#| fig-cap: \"Panels of panels of data\"\nknitr::include_graphics(\"images/panel2.png\")\n```\n\n:::\n\n# Working with Clustered Data\n\n:::frame\n__SchoolMot Data__  \n\nThis dataset contains information on 900 children from 30 different schools across Scotland. The data was collected as part of a study looking at whether education-related motivation is associated with school grades.  \nAll children completed an 'education motivation' questionnaire, and their end-of-year grade average has been recorded.    \n\nIt is available at [https://uoepsy.github.io/data/schoolmot.csv](https://uoepsy.github.io/data/schoolmot.csv).  \n\n```{r}\n#| echo: false\n#| label: tbl-schoolmotdict\n#| tbl-cap: \"SchoolMot Data Dictionary. Children are clustered in Schools, with this group-identifier visible in the schoolid column\"\ntibble(variable=names(schoolmot),\n       description = c(\n         \"Child's Education Motivation Score (Z-scored)\",\n         \"Annual School Funding (in 1000s of £)\",\n         \"Name of School that the child attends\",\n         \"Child's end-of-year grade average (0-100)\")\n) %>% gt::gt()\n```\n\n:::\n\nIn terms of the dataframes we are going to be working with, clusters are simply included in another column that contains a unique identifier for each cluster:^[Note, this is not true for a set of analytical methods called \"cluster analysis\", which attempts to identify clusters that haven't been measured/observed (or may not even 'exist' in any real sense of the word).]  \n\n```{r}\n#| echo: false\nschoolmot |> slice(1,46,105,2,4,50,80) |> \n  rbind(\"...\") |> mutate(schoolid=ifelse(is.na(schoolid),\"...\",\n                                         as.character(schoolid)))\n```\n\n## Determining Sample Sizes\n\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different _levels._  \nIn the description of the __SchoolMot__ data above we are told the relevant numbers:  \n\n```{r}\n#| echo: false\ntribble(\n  ~` `,~`Unit`,~`Sample Size`,\n  \"Level 2\",\"School\",30,\n  \"Level 1 (Observations)\",\"Children\",900,\n) |> gt::gt()\n```\n\nWe can check this in our data: \n```{r}\n# TODO\n# schoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n```\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from `r unique(schoolmot$schoolid)[5]` and only 10 from `r unique(schoolmot$schoolid)[10]`, or do we have the same number in each?  \n\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the `summary()` function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):  \n\n```{r}\nschoolmot %>%\n  count(schoolid) %>%\n  summary()\n```\n\n## ICC\n\nThe __IntraClass Correlation Coefficient (ICC)__ is a descriptive measure of how much variation in a variable is explained by the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol $\\rho$:^[although this symbol get used for lots of other correlation-y things too!]  \n\n$$\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n$$\n\nThis is illustrated in the @fig-schooliccplot below, in which our continuous outcome variable (children's grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the \"between-group variance\" as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the \"within-group variance\" as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):  \n\n```{r}\n#| code-fold: true\n#| fig-cap: \"Variance in grades between schools. Data from [https://uoepsy.github.io/data/schoolmot.csv](https://uoepsy.github.io/data/schoolmot.csv)\"\n#| label: fig-schooliccplot\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n```\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can estimate from a fitted model.  \nIn the data (visualised above), it's estimated that `r round(suppressWarnings(ICC::ICCbare(schoolid, grade, data = schoolmot)),2)*100`% of the variance in grades is due to school-related differences:  \n```{r}\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Optional: Calculating ICC manually  \n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it's still a bit tricky.  \n\nLet's take a look at the formula for ICC: \n\n$$\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\ \nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n = & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}} \n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n$$\n\nSo we're going to need to calculate the grand mean of $y$, the group means of $y$, and then the various squared differences between group means and grand mean, and between observations and their respective group means.  \n\nThe code below will give us a couple of new columns. The first is the overall mean of $y$, and the second is the mean of $y$ for each group. Note that we calculate this by first using `group_by` to make the subsequent operation (the `mutate`) be applied to each group. To ensure that the grouping does not persist after this, we've passed it to `ungroup` at the end.  \n\n```{r}\nschoolmot <- \n  schoolmot %>% \n  mutate(\n    grand_mean = mean(grade)\n  ) %>%\n  group_by(schoolid) %>%\n  mutate(\n    group_mean = mean(grade)\n  ) %>%\n  ungroup()\n```\n\nNow we need to create a column which is the squared differences between the observations $y_{ij}$ and the group means $\\bar{y_i}$.  \nWe also want a column which is the squared differences between the group means $\\bar{y_i}$ and the overall mean $\\bar{y}$.  \n```{r}\nschoolmot <- schoolmot %>% \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n```\n\nAnd then we want to sum them:\n```{r}\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n```\n\nFinally, we divide them by the degrees of freedom. \nOur degrees of freedom for our between group variance $30 \\text{ groups} - 1 \\text{ grand mean}=29$  \nOur degrees of freedom for our within group variance is $900 \\text{ observations} - 30 \\text{ groups}=870$\n```{r}\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n```\n\nAnd calculate the ICC!!!  \nThe 29 here is the $k-1$ in the formula above, where $k$ is the number of observations within each group.  \n```{r}\n# ICC\n(msb-mse) /(msb + (29*mse))\n```\n\n:::\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of \"correlation\" that you are familiar with. Pearson's correlation (e.g think about a typical scatterplot) operates on *pairs of observations* (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on *data which is structured in groups*.   \n\nSuppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (@fig-icccorplot). The ICC is the correlation between such pairs. \n\n```{r}\n#| echo: false\n#| label: fig-icccorplot\n#| fig-cap: \"ICC is the correlation of randomly drawn pairs from the same group\"\n#| fig-height: 3\n#| out-width: \"60%\"\nset.seed(23456)\nget_random_pair <- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\nsims <- replicate(15, get_random_pair())\nsims <- t(sims)\nplot(sims, xlab=\"Child 1\",ylab=\"Child 2\",main=\"15 randomly drawn pairs of children\")\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Optional: A Little Simulation\n\nWe can actually do the \"randomly drawn pair of observations from the same group\" via simulation.  \nThe code below creates a function for us to use. Can you figure out how it works?\n```{r}\nget_random_pair <- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n```\nTry it out, by running it several times.\n```{r}\nget_random_pair()\n```\n\nNow let's make our computer do it loads and loads of times:\n```{r}\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims <- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims <- t(sims)\ncor(sims[,1], sims[,2])\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Optional: correlations from group-structured data\n\nLet's suppose we had only 2 observations in each group.  \n```{r echo=FALSE}\ntempdat <- read.csv(\"https://uoepsy.github.io/data/iccexplainer.csv\")\nhead(tempdat) %>% rbind(.,rep(\"...\", 3))\n```\n\n```{r include=F}\nlibrary(nlme)\nres <- lme(y ~ 1, random = ~ 1 | cluster, data=tempdat, method=\"ML\")\nic <- getVarCov(res)[1] / (getVarCov(res)[1] + res$sigma^2)\n```\n\n__The ICC for this data is `r round(ic,2)`.__    \n\nNow suppose we *reshape* our data so that we have one row per group, and one column for each observation to look like this:\n```{r echo=F}\ntempdat_wide <- tempdat %>% \n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") \ntempdat_wide %>% head %>% rbind(.,rep(\"...\", 3))\n```\nCalculating Pearson's correlation on those two columns yields `r cor(tempdat_wide$obs1, tempdat_wide$obs2) %>% round(.,2)`, which isn't quite right. It's close, but not quite.. \n\n:::imp \nThe crucial thing here is that it is completely arbitrary which observations get called \"obs1\" and which get called \"obs2\".  \nThe data aren't paired, they're just random draws from a __group.__\n:::\n\nEssentially, there are lots of different combinations of \"pairs\" here. \nThere are the ones we have shown above:\n```{r echo=F}\nhead(tempdat_wide) %>% rbind(., rep(\"...\",3))\n```\nBut we might have equally chosen any of these:  \n\n:::panelset\n:::panel\n#### ...\n```{r echo=F}\nset.seed(12)\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::panel\n#### ...\n```{r echo=F}\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::panel\n#### ...\n```{r echo=F}\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of `r round(ic, 2)`!\n\n__ICC = the expected correlation of a *randomly drawn pair* of observations from the same group.__\n\n:::\n\n## Visualisations\n\nWhen we're visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. \nFor instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens _within_ a given school (see e.g. @fig-egschool from earlier).  \n\nSome useful __ggplot__ tools here are:  \n\n- `facet_wrap()` - make a separate little plot for each level of a grouping variable\n- the `group` aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n:::panelset\n\n:::panel\n##### facets\n```{r}\nggplot(schoolmot, aes(x=motiv,y=grade,col))+\n  geom_point() +\n  facet_wrap(~schoolid)\n```\n:::\n:::panel\n##### group\n```{r}\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)\n```\n:::\n:::\n\n\n# Information, pooled\n\nWith our current toolset (linear regression), there are two avenues for us with respect to how we analyse clustered data. We can either ignore the clustering completely (and violate our assumptions), we can add the cluster-level differences in as another predictor. These reflect different ways in which we can \"pool\" information from across the different clusters.  \n\n:::sticky\n__Complete Pooling__  \n\nAll information from different clusters is pooled together to estimate the relevant association.  \n\n```{r}\n#| eval: false\nlm(grade ~ motiv, data = schoolmot)\n```\nTake all the children, fit a regression line for `grade ~ motivation`.  \n\n:::\n\n:::sticky\n__No Pooling__  \n\nInformation from each cluster contributes **only** to that specific cluster's estimate.  \n\n```{r}\n#| eval: false\nlm(grade ~ motiv*schoolid, data = schoolmot)\n```\nEstimate school-specific differences in grades, and school-specific differences in the effect of motivation on grades. Get _loads_ of coefficients that aren't really of interest.   \n\n\n\n:::\n\n\nEstimating the school-specific differences in our model (the no-pooling approach) is clearly better than simply ignoring them, but it does mean that we are treating the schools as if they are completely independent entities.  \nSuppose we had another school - School X - for which we had only _three_ children's data (@fig-borrowstrength). Intuitively, we don't want to trust the line for School X as much as we trust the others (where we have 30 children's data). \n\n```{r}\n#| echo: false\n#| label: fig-borrowstrength\n#| fig-cap: \"Blue lines show the estimated fitted values from the no-pooling approach lm(grade ~ motiv * schoolid). School X's estimate is based on just the 3 datapoints from that school, and does not take into account any of the other schools in anyway.\"\nset.seed(123)\nbind_rows(\n  schoolmot |> filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4,6)]),\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4),\n    grade = 50 + 10*motiv + rnorm(3,0,10)\n  )\n) -> tdf \n\ntdf |> \n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  geom_line(data=broom::augment(lm(grade~motiv*schoolid,tdf)),aes(y=.fitted),\n            col=\"blue\",lwd=.5)+\n  facet_wrap(~schoolid)\n \n```\n\nIn the no-pooling approach, it is only those three children from School X that contribute to the School X line. What would be great is if we could _somehow_ use the information from the other schools to inform our estimation of what is going on in School X. This is what multi-level modelling achieves, _partially_ pooling information across the groups, and this is where we'll turn to next.    \n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\n```\n\n\n:::lo\n\nTODO this reading\n\n:::\n\n\n# Our Starting Point\n\nWe're going to start this course with our good old friend the linear regression model. The content we're going to cover in this block can be seen as an extension of these methods, so keep in mind that the materials from previous courses you have taken will likely be very a useful resource. \n\n\n:::sticky\n__Linear Regression__\n\nThe linear regression model is a way of expressing an outcome (or \"dependent\") variable $y$ as the linear combination of various predictors $x_1,\\ ...\\ ,\\ x_p$ (or \"independent variables\").   \n\nWe can write the linear regression model as:  \n$$\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}\\\\ \n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n$$\nwhere the $b$'s (sometimes written $\\beta$) are the partial associations between each predictor and the outcome. For instance, $b_1$ represents the expected change in $y$ associated with a 1 unit change in $x_1$, while _holding constant_ other independent variables $x_2,\\ ...\\ ,\\ x_p$.  \nIn this way, we can use the regression model to isolate the association between each predictor and the outcome from the other predictors in the model. \n\nIn R, we fit these models using:\n```{r eval=F}\nlm(y ~ x1 + x2 + .... xp, data = mydata)  \n```\n\n`r optbegin(\"Optional: a general notation\", olabel=FALSE)`\n\nIf we wanted to write this more simply, we can express $x_1$ to $x_p$ as an $n \\times p$ matrix (sample size $\\times$ parameters), and $b_0$ to $b_p$ as a vector of coefficients:\n$$\n\\begin{align}\n& \\color{red}{\\mathbf{y}} = \\color{blue}{\\mathbf{X b}} + \\boldsymbol{\\varepsilon} \\\\\n& \\varepsilon \\sim N(0, \\sigma) \\text{ independently} \\\\\n\\end{align}\n$$\nYou can see below how we get between these two formulations. In the top line, we use an index $i$ to indicate that the model is fitted over a set of observations. In the middle we can see this expanded out to show the value for each individual observation on each $y$ and $x$. Because the $b$ coefficients are fixed - they are the same for each observation $i$, we can separate them out so that we are multiplying the $\\mathbf{X}$ matrix by the vector of $\\mathbf{b}$'s (i.e. the formulation we see at the bottom).  \n\n$$\n\\begin{align} \\color{red}{y_i} \\;\\;\\;\\; & = \\;\\;\\;\\;\\; \\color{blue}{b_0 \\cdot{} 1 + b_1 \\cdot{} x_{1i} + ... + b_p \\cdot x_{pi}} & + & \\;\\;\\;\\varepsilon_i \\\\ \\qquad \\\\ \\color{red}{\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\\\ \\vdots \\\\ y_n \\end{bmatrix}} & = \\color{blue}{\\begin{bmatrix} 1 & x_{11} & x_{21} & \\dots & x_{p1} \\\\ 1 & x_{12} & x_{22} &  & x_{p2} \\\\ 1 & x_{13} & x_{23} &  & x_{p3} \\\\ 1 & x_{14} & x_{24} &  & x_{p4} \\\\ 1 & x_{15} & x_{25} &  & x_{p5} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{1n} & x_{2n} & \\dots & x_{pn} \\end{bmatrix} \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{bmatrix}} & + & \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\\\ \\varepsilon_4 \\\\ \\varepsilon_5 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix} \\\\ \\qquad \\\\ \\color{red}{\\boldsymbol y} \\;\\;\\;\\;\\; & = \\qquad \\qquad \\;\\;\\; \\mathbf{\\color{blue}{X \\qquad \\qquad \\qquad \\;\\;\\;\\: b}} & + & \\;\\;\\; \\boldsymbol \\varepsilon \\\\ \\end{align}\n$$\n\n`r optend()`\n\n<!-- We have seen various ways we can extend this model to capture different processes - predictors can be continuous or categorical; we can include _interactions_ (where the effect of $x_1$ on $y$ _depends_ on the level of $x_2$); and we can even extend the same model structure to model discrete outcomes (with a little trickery of using a function to link the expected value of $\\color{red}{y}$ to the linear prediction $\\color{blue}{b_0 + b_1(x_1) + ... + b_p(x_p)}$).  -->\n\n:::\n\n\nWhen we fit linear regression models, we are fitting a line (or a regression surface, when we add in more predictors), to a cloud of datapoints. The discrepancy between the fitted model and the observed data is taken up by the residuals. \n\n$$\n\\begin{align}\n\\color{red}{y} &= \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n\\color{red}{\\text{observed }y} &= \\color{blue}{\\text{fitted }\\hat y} \\,\\, \\color{black}{+ \\text{ residual }\\hat \\varepsilon}\\\\ \n\\end{align}\n$$\n\nWe are theorising that our model contains all the systematic relationships with our outcome variable, we assume that the residuals - the leftovers - are essentially random noise. This is the $\\epsilon \\sim N(0, \\sigma)$ bit, which is a way of specifying our assumption that the errors are normally distributed with a mean of zero (see @fig-slr2).   \n\n```{r}\n#| label: fig-slr2\n#| echo: false\n#| fig-cap: \"Simple linear regression model, with the systematic part of the model in blue, and residuals in red. The distributional assumption placed on the residuals is visualised by the orange normal curves - the residuals are normally distributed with a mean of zero, and this does not change across the fitted model.\"\n#| fig-height: 3.5\nset.seed(235)\ndf <- tibble(\n  x = rnorm(100,3,1),\n  y = 0.5+.8*x + rnorm(100,0,1)\n)\ndf$y=df$y+1\nmodel1 <- lm(y ~ x, data = df)\nbetas <- coef(model1)\nintercept <- betas[1]\nslope <- betas[2]\n\ndfd<-tibble(\n  x = c(0:4),\n  m = predict(model1,newdata=tibble(x=c(0:4))),\n  s = sigma(model1)\n)\n\nbroom::augment(model1) %>%\nggplot(., aes(x = x, y = y)) +\n  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=x,dist=\"norm\",arg1=m,arg2=s),alpha=.15, fill=\"orange\") + \n  geom_point(size=3,alpha=.5)+\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  #xlim(0,6)+ylim(0,7)+\n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(breaks=0:6)+\n  labs(x = \"X (predictor)\", \n       y = \"Y (outcome)\")+\n  geom_segment(aes(x=x, xend=x, y=y, yend=.fitted), col=\"red\",lty=\"dotted\", linewidth=0.7)\n```\n\nWe typically want to check our model residuals (by plotting or performing statistical tests) to determine if we have reason to believe our assumptions are violated. The easiest way to do this in R is with `plot(model)`, which provides us with a series of visuals to examine for unusual patterns and conspicuous observations.  \n\nWhen model assumptions appear problematic, then our inferential tools go out the window. While our specific point estimates for our regression coefficients are our best linear estimates for the sample that we have, our standard errors rely on the distributional assumptions of the residuals^[Why is this? It's because the formula to calculate the standard error involves $\\sigma^2$ - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient]. It is our standard errors that allow us to construct test statistics and compute p-values (@fig-inf1) and construct confidence intervals. Our assumptions underpin our ability to generalise from our specific sample to make statements about the broader population.  \n\n\n```{r}\n#| label: fig-inf1\n#| echo: false\n#| fig-cap: \"inference for regression coefficients\" \nknitr::include_graphics(\"images/sum4.png\")\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Refresher: Standard Error   \n\nTaking **samples** from a **population** involves an element of _randomness_. The mean height of 10 randomly chosen Scottish people will not be exactly equal to the mean height of the entire Scottish population. Take another sample of 10, and we get _another_ mean height (@fig-se).  \n\n```{r}\n#| echo: false\n#| label: fig-se\n#| fig-cap: \"Estimates from random samples vary randomly around the population parameter\"\nset.seed(nchar(\"i'm going dotty\"))\ntibble(\n  m = replicate(750, mean(rnorm(10,165,11)))\n) |>\n  ggplot(aes(x=m))+\n  geom_vline(xintercept = 165, col=\"red\",lty=\"dashed\", lwd=1) +\n  geom_dotplot(dotsize=.5,binwidth=.5) +\n  scale_y_continuous(NULL, breaks=NULL)+\n  scale_x_continuous(\"mean height (cm)\",breaks=seq(-10,10,5)+165) +\n  theme_minimal()+\n  annotate(\"text\",\n           x=170, y=.88, \n           label=\"Mean height of\\nentire Scottish population\", col=\"red\",\n           hjust=0)+\n  geom_curve(aes(x=170, xend=165, y=.88, yend=.88), col=\"red\", size=0.5, \n             curvature = 0, arrow = arrow(length = unit(0.03, \"npc\"))) +\n  \n  annotate(\"text\",x=156, y=.57, label=\"the mean height of\\n10 randomly selected\\nScottish people\", col=\"grey30\") +\n  geom_curve(aes(x=156, xend=160.5, y=.5, yend=.40), col=\"grey30\", size=0.5, curvature = 0.2, arrow = arrow(length = unit(0.03, \"npc\"))) +\n  \n  annotate(\"text\",x=173, y=.62, label=\"the mean height of\\nanother random sample of 10\", col=\"grey30\") +\n  annotate(\"text\",x=174, y=.45, label=\"and another\", col=\"grey30\") +\n  annotate(\"text\",x=175, y=.25, label=\"and another\", col=\"grey30\") +\n  geom_curve(aes(x=173, xend=167.5, y=.57, yend=.45), col=\"grey30\", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_curve(aes(x=174, xend=169, y=.43, yend=.35), col=\"grey30\", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_curve(aes(x=175, xend=173.9, y=.23, yend=.015), col=\"grey30\", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))\n```\n\nThe standard error of a statistic is the standard deviation of all the statistics we _might have_ computed from samples of that size (@fig-se2). We can calculate a standard error using formulae (e.g. for a mean, the standard error is $\\frac{\\sigma}{\\sqrt{n}}$) but we can also use more computationally intensive approaches such as \"bootstrapping\" to actually generate an empirical sampling distribution of statistics which we can then summarise.  \n\nWe use the standard error to quantify the uncertainty around our sample statistic as an estimate of the population parameter, or to construct standardised test statistics in order to perform tests against some null hypothesis.  \n\n```{r}\n#| echo: false\n#| label: fig-se2\n#| fig-cap: \"The standard error is the standard deviation of the 'sampling distribution' - the distribution of sample statistics that we _could_ see.\"\nset.seed(2394)\nsamplemeans <- replicate(2000, mean(rnorm(10,0,11)))\ng <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+\n  #geom_histogram(alpha=.3)+\n  stat_function(geom=\"line\",fun=~dnorm(.x, mean=0,sd=sd(samplemeans)),lwd=1)\n\nld <- layer_data(g) %>% filter(x <= (11/sqrt(10)) & x >= (-11/sqrt(10)))\nld2 <- layer_data(g) %>% filter(x <= 2*(11/sqrt(10)) & x >= 2*(-11/sqrt(10)))\ng + geom_area(data=ld,aes(x=x,y=y),fill=\"grey30\",alpha=.3) + \n  geom_area(data=ld2,aes(x=x,y=y),fill=\"grey30\",alpha=.1) +\n  geom_vline(xintercept = 0, col=\"red\",lty=\"dashed\", lwd=1) +\n  annotate(\"text\",\n           x=5, y=.12, \n           label=\"Mean height of\\nentire Scottish population\", col=\"red\",\n           hjust=0)+\n  geom_curve(aes(x=5, xend=0, y=.12, yend=.12), col=\"red\", size=0.5, \n             curvature = 0, arrow = arrow(length = unit(0.03, \"npc\")))+\n  geom_segment(x=0,xend=(-10.5/sqrt(10)),y=.06,yend=.06) +\n  annotate(\"text\",x=-7, y=.07, label=\"standard error\\n(standard deviation of\\nsampling distribution)\", col=\"grey30\")+\n  geom_curve(aes(x=-7, xend=-1.5, y=.06, yend=.06), col=\"grey30\", size=0.5, curvature = 0.8, arrow = arrow(length = unit(0.03, \"npc\")))+\n\n  scale_y_continuous(NULL, limits=c(0,.135),breaks=NULL)+\n  labs(x=\"mean height (cm)\") +\n  theme_minimal()+\n  scale_x_continuous(\"mean height (cm)\",breaks=seq(-10,10,5),labels=seq(-10,10,5)+165)\n\n```\n\n\n:::\n\n\nIn the face of plots (or tests) that appear to show violations of the distributional assumptions (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. If you're interested, you can read brief explanations about some of these methods [here](00_lm_assumpt.html){target=\"_blank\"}. \n\nOne assumption that none of the above approaches address is our assumption of __\"independence\"__.^[With the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we _can_ actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).] Thinking broadly, if our observations are not independent from one another, this means that there are systematic differences in our outcome variable that our model is not capturing. \n\n:::sticky\n__Independence of observations__  \n\nIndividual observations in the sample are not related to one another in any way. The value of one observation does not influence the value of any other observation.  \n\n:::\n\nOne common way in which observations are not independent can come when the data has a _hierarchical_ structure - some form of __clustering__ of observations into different groups. These groups, or 'clusters', could be simply something we observe, or they could be the result of how we have designed our study.  \n\n\n# Clustered Data\n\n```{r}\n#| echo: false\n#| eval: false\n#| label: SCHOOLDATA\nschoolids = read_csv(\"data/schoolnames.csv\") |> janitor::clean_names() |> \n  filter(secondary_12==\"Secondary\") |> pull(school_name)\nset.seed(nchar(\"cool cool cool cool cool\")*1385)\nn_groups = 30\n# npgroup = round(runif(30,2,25))\nnpgroup = rep(30,30)\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nxd = rnorm(N)\nxm = rnorm(n_groups)[g]\nb = rnorm(n_groups, 6, 1.2)\nb = round(b[g],2)\nre0 = rnorm(n_groups, sd = 1)\nre  = re0[g]\nrex = rnorm(n_groups, sd = 1)\nre_x  = rex[g]\nlp = (0 + re) + 1*xm + (1 + re_x)*xd + 1*b - .1*b*(xd+xm)\ny = rnorm(N, mean = lp, sd = 2)\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf = data.frame(x = xm+xd, b, g=factor(g), y, y_bin)\n\nschoolmot = df |>\n  transmute(\n    motiv = round(x,2),\n    funding = b,\n    schoolid = sample(schoolids,n_groups)[as.numeric(g)],\n    grade = round(50.356 + scale(y)[,1]*14.53,2)\n  )\nschoolmot[schoolmot$schoolid==\"Fettes College\",\"funding\"] <- 9.1\n\n# library(ICC)\n# ICCbare(schoolid, grade, data = schoolmot)\n# library(lme4)\n# lmer(grade~1+motiv*funding+(1+motiv|schoolid),schoolmot) |> summary()\n#    sjPlot::plot_model(type=\"pred\",terms=c(\"motiv\",\"funding [4, 5, 6, 7]\"),show.data=TRUE)\n\n# write_csv(schoolmot,\"data/schoolmot.csv\")\n# schoolmot <- read_csv(\"data/schoolmot.csv\")\n# academic_performance ~ 1 + motiv * schoolfunding + (1 + motiv | school)\n# health_anxiety ~ 1 + education * GPpracticesize + (1 + education | GP)\n```\n\n```{r}\n#| include: false\nschoolmot <- read_csv(\"data/schoolmot.csv\") |> mutate(schoolid=factor(schoolid))\n```\n\nA common example to start thinking about clustered data is to think about conducting a study on school children. Each observation in our sample is a different child, and they come from a set of various schools (i.e. we might get 20 children from `r levels(schoolmot$schoolid)[10]`, 15 from `r levels(schoolmot$schoolid)[11]`, and so on). If we ignore the clustering of children into their respective schools, we can end up reaching completely different conclusions. \n\nThis can be seen in @fig-egschool - in which each datapoint is a single child. If we consider them to be independent of one another (Left-Hand plot), then we might think there is a strong positive relationship between `grade` and `motivation`. Once we colour them by which school they belong to, we can see that the observations in the top right of the plots are all high for some other reason - because they come from the green school. \n\n```{r}\n#| label: fig-egschool\n#| echo: false\n#| fig-cap: \"Observations (children) are clustered (into schools), and their grades are dependent upon which cluster they are in. Ignoring the clustering may lead to erroneous conclusions\"  \nset.seed(46)\nn_groups = 5\nnpgroup = round(runif(5,5,10))\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nxm = rnorm(n_groups,3,1)\nxm = xm[g]\nxd = rnorm(N)\nre0 = rnorm(n_groups, sd = 1)\nre  = re0[g]\nrex = rnorm(n_groups, sd = .1)\nre_x  = rex[g]\nlp = (0 + re) + 2*xm + (0 + re_x)*xd\ny = rnorm(N, mean = lp, sd = 1)*9\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf = data.frame(x=scale(xm+xd)[,1], g=factor(g), y, y_bin)\n\np1 = ggplot(df,aes(x=x,y=y))+\n  geom_point(size=3) +\n  geom_smooth(method=lm,se=F)+\n  labs(title=\"motivation and grade\",subtitle=\"ignoring school\",\n       x=\"motivation\",y=\"grade\")\n\np2 = ggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(size=3)+\n  geom_smooth(method=lm,se=F)+\n  guides(col=\"none\") +\n  labs(title=\"motivation and grade\",subtitle=\"separate lines for each school\",\n       x=\"motivation\",y=\"grade\")\n\np3 = df |> mutate(r=resid(lm(y~x,df)),f=fitted(lm(y~x,df))) |>\n  ggplot(aes(x=f,y=r)) +\n  geom_point(size=3, aes(col=g)) +\n  geom_hline(yintercept=0,lty=\"dotted\") +\n  geom_segment(aes(x=f,xend=f,y=0,yend=r),\n               lty=\"dotted\",col=\"red\")+\n  guides(col=\"none\") + \n  labs(x=\"Fitted Values\\nlm(grade ~ motivation)\",y=\"Residuals\")\n  \n(p1 + p2)\n\n```\n\nPut another way - the children are _not_ independent from one another, because they tend to be more similar to other children in the same school than they are to children from a different school. Our assumption of the normal linear model that $\\varepsilon \\sim N(0,\\sigma) \\text{ independently}$ is violated, because groups of residuals are related to one another - e.g. all the children from the green school have positive residuals (see residuals vs fitted plot in @fig-egschool2).  \n\n```{r}\n#| label: fig-egschool2\n#| echo: false\n#| fig-cap: \"Residuals vs Fitted plot from the linear model that ignores the school/children structure of the data\"\np3\n```\n\n## Clusters clusters everywhere\n\nThe idea of observing \"children in schools\" is just one such example of clustering that we might come across. This same _hierarchical data structure_ can be found in patients within medical practices, employees within departments, people within towns etc. These sort of groups exist 'in the wild' (i.e. the schools, departments and towns are all just occurrences of groups that we have observed). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a __Repeated Measures__ study we have individual experimental trials clustered within participants. __Longitudinal__ studies exhibit the same data structure but have _time-ordered_ observations clustered within people.  \n\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters^[It's [\"turtles all the way down\"](https://en.wikipedia.org/wiki/Turtles_all_the_way_down){target=\"_blank\"}]. @tbl-design shows just a few examples of different levels of clustering that may arise from different types of study.  \n\n<!-- Cross sectional studies are snapshots of a structure (and that structure may contain clusters); Repeated measures studies involve assessing the same person^[it doesn't have to be people, but in psychology it usually is!] multiple times across different experimental conditions/items (resulting in clusters of observation for each person);  Longitudinal studies also result in clusters of observations for each person, but there is a temporal sequence to these observations (e.g. at various ages).   -->\n\n```{r}\n#| echo: false\n#| label: tbl-design\n#| tbl-cap: \"Various different study designs will give rise to clustered data.\" \ntribble(\n  ~` `,~`Cross Sectional`,~`Repeated Measures`,~`Longitudinal`,\n  \"Level n\",\"...\",\"...\",\"...\",\n  \"...\",\"...\",\"...\",\"...\",\n  \"Level 3\",\"School\",\"...\",\"Families\",\n  \"Level 2\",\"Classroom\",\"Participants\",\"People\",\n  \"Level 1 (Observations)\",\"Children\",\"Experimental Stimuli\",\"Time\"\n) |> gt::gt()\n```\n\n## What are 'clusters'?  \n\nAt the fundamental level, we are using the term 'cluster' here to refer to __a grouping of observations.__ In fact, we will probably start using the terms \"clusters\" and \"groups\" interchangeably, so it's worth taking a bit of time to try and understand the _kind_ of groupings that we're talking about (and how we think about them).  \n\n:::sticky\n\n- \"Clusters\" are just \"groups\".  \n- When we talk about clustered data, the groups we are discussing can be thought of as a _random sample of higher level units_.  \n- More often than not, the specific group-differences are not of interest.  \n\n:::\n\nContrast the idea of 'clusters' with how we think about other sorts of groupings. In a study that looks at \"how do drugs placebo/aspirin/beta-blockers influence people's heart rate?\" (@fig-clgroup LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we'll use the same drugs (they're not just a random sample of drugs - the x-axis of our LH plot in @fig-clgroup will be the same).  \n\nIf we are interested in \"what is the average grade at GCSE?\", and we have children grouped into different schools (@fig-clgroup RH plot), we are probably not interested in all the specific differences between grades in `r levels(schoolmot$schoolid)[6]` vs `r levels(schoolmot$schoolid)[13]` etc. If we were to run our study again, we might not collect data from the same set of schools. We can view these schools as 'clusters' - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).  \n\n```{r}\n#| label: fig-clgroup\n#| fig-height: 3.5\n#| fig-cap: \"Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\"  \n#| echo: false\nset.seed(3)\nbp <- tibble(drug = rep(0:2,50),\n       hr = 65 + (drug==1)*-2 + (drug==2)*-3 + rnorm(150,0,2)\n) |>  mutate(\n  drug = fct_recode(factor(drug),\n                    placebo=\"0\",\n                    aspirin=\"1\",\n                    betablocker=\"2\")\n)\n\nset.seed(753)\npltd <- schoolmot |> filter(schoolid %in% sample(unique(schoolmot$schoolid),10)) |> droplevels()\nlevels(pltd$schoolid)[9:10] <- c(\"...\",\"... \")\n\nggplot(bp,aes(x=drug,y=hr))+\n  geom_boxplot()+\n  labs(x=\"- Drug -\", y=\"Resting Heart Rate (bpm)\") +\n\nggplot(pltd,aes(x=schoolid,y=grade))+\n  geom_boxplot()+\n  scale_x_discrete(labels=abbreviate)+\n  labs(x=\"- School -\", y=\"Grade (%)\")+\n  theme(axis.text.x = element_text(angle=45))\n\n```\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the level of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as \"mixed effects modelling\"), which is a regression modelling technique that allows us to explore questions such as these (and many more).^[Depending on the research question and design of the study, we may be only interested in things that occur at \"level 1\" (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply \"account for the nuisance clustering\".]  \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Optional \"univariate\"and \"multivariate\"\n\nIn \"univariate\" statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.  \n\nIn \"multivariate\" statistics, there are more sources of variation. For the \"children in schools\" example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another. \n\n:::\n::: {.callout-note collapse=\"true\"}\n#### Optional: \"Panel data\"\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as 'panel data'. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:    \n\n```{r}\n#| echo: false\n#| label: fig-panel1\n#| fig-cap: \"Panels of data\"\nknitr::include_graphics(\"images/panel1.png\")\n```\n```{r}\n#| echo: false\n#| label: fig-panel2\n#| fig-cap: \"Panels of panels of data\"\nknitr::include_graphics(\"images/panel2.png\")\n```\n\n:::\n\n# Working with Clustered Data\n\n:::frame\n__SchoolMot Data__  \n\nThis dataset contains information on 900 children from 30 different schools across Scotland. The data was collected as part of a study looking at whether education-related motivation is associated with school grades.  \nAll children completed an 'education motivation' questionnaire, and their end-of-year grade average has been recorded.    \n\nIt is available at [https://uoepsy.github.io/data/schoolmot.csv](https://uoepsy.github.io/data/schoolmot.csv).  \n\n```{r}\n#| echo: false\n#| label: tbl-schoolmotdict\n#| tbl-cap: \"SchoolMot Data Dictionary. Children are clustered in Schools, with this group-identifier visible in the schoolid column\"\ntibble(variable=names(schoolmot),\n       description = c(\n         \"Child's Education Motivation Score (Z-scored)\",\n         \"Annual School Funding (in 1000s of £)\",\n         \"Name of School that the child attends\",\n         \"Child's end-of-year grade average (0-100)\")\n) %>% gt::gt()\n```\n\n:::\n\nIn terms of the dataframes we are going to be working with, clusters are simply included in another column that contains a unique identifier for each cluster:^[Note, this is not true for a set of analytical methods called \"cluster analysis\", which attempts to identify clusters that haven't been measured/observed (or may not even 'exist' in any real sense of the word).]  \n\n```{r}\n#| echo: false\nschoolmot |> slice(1,46,105,2,4,50,80) |> \n  rbind(\"...\") |> mutate(schoolid=ifelse(is.na(schoolid),\"...\",\n                                         as.character(schoolid)))\n```\n\n## Determining Sample Sizes\n\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different _levels._  \nIn the description of the __SchoolMot__ data above we are told the relevant numbers:  \n\n```{r}\n#| echo: false\ntribble(\n  ~` `,~`Unit`,~`Sample Size`,\n  \"Level 2\",\"School\",30,\n  \"Level 1 (Observations)\",\"Children\",900,\n) |> gt::gt()\n```\n\nWe can check this in our data: \n```{r}\n# TODO\n# schoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n```\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from `r unique(schoolmot$schoolid)[5]` and only 10 from `r unique(schoolmot$schoolid)[10]`, or do we have the same number in each?  \n\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the `summary()` function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):  \n\n```{r}\nschoolmot %>%\n  count(schoolid) %>%\n  summary()\n```\n\n## ICC\n\nThe __IntraClass Correlation Coefficient (ICC)__ is a descriptive measure of how much variation in a variable is explained by the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol $\\rho$:^[although this symbol get used for lots of other correlation-y things too!]  \n\n$$\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n$$\n\nThis is illustrated in the @fig-schooliccplot below, in which our continuous outcome variable (children's grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the \"between-group variance\" as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the \"within-group variance\" as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):  \n\n```{r}\n#| code-fold: true\n#| fig-cap: \"Variance in grades between schools. Data from [https://uoepsy.github.io/data/schoolmot.csv](https://uoepsy.github.io/data/schoolmot.csv)\"\n#| label: fig-schooliccplot\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n```\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can estimate from a fitted model.  \nIn the data (visualised above), it's estimated that `r round(suppressWarnings(ICC::ICCbare(schoolid, grade, data = schoolmot)),2)*100`% of the variance in grades is due to school-related differences:  \n```{r}\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Optional: Calculating ICC manually  \n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it's still a bit tricky.  \n\nLet's take a look at the formula for ICC: \n\n$$\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\ \nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n = & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}} \n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n$$\n\nSo we're going to need to calculate the grand mean of $y$, the group means of $y$, and then the various squared differences between group means and grand mean, and between observations and their respective group means.  \n\nThe code below will give us a couple of new columns. The first is the overall mean of $y$, and the second is the mean of $y$ for each group. Note that we calculate this by first using `group_by` to make the subsequent operation (the `mutate`) be applied to each group. To ensure that the grouping does not persist after this, we've passed it to `ungroup` at the end.  \n\n```{r}\nschoolmot <- \n  schoolmot %>% \n  mutate(\n    grand_mean = mean(grade)\n  ) %>%\n  group_by(schoolid) %>%\n  mutate(\n    group_mean = mean(grade)\n  ) %>%\n  ungroup()\n```\n\nNow we need to create a column which is the squared differences between the observations $y_{ij}$ and the group means $\\bar{y_i}$.  \nWe also want a column which is the squared differences between the group means $\\bar{y_i}$ and the overall mean $\\bar{y}$.  \n```{r}\nschoolmot <- schoolmot %>% \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n```\n\nAnd then we want to sum them:\n```{r}\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n```\n\nFinally, we divide them by the degrees of freedom. \nOur degrees of freedom for our between group variance $30 \\text{ groups} - 1 \\text{ grand mean}=29$  \nOur degrees of freedom for our within group variance is $900 \\text{ observations} - 30 \\text{ groups}=870$\n```{r}\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n```\n\nAnd calculate the ICC!!!  \nThe 29 here is the $k-1$ in the formula above, where $k$ is the number of observations within each group.  \n```{r}\n# ICC\n(msb-mse) /(msb + (29*mse))\n```\n\n:::\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of \"correlation\" that you are familiar with. Pearson's correlation (e.g think about a typical scatterplot) operates on *pairs of observations* (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on *data which is structured in groups*.   \n\nSuppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (@fig-icccorplot). The ICC is the correlation between such pairs. \n\n```{r}\n#| echo: false\n#| label: fig-icccorplot\n#| fig-cap: \"ICC is the correlation of randomly drawn pairs from the same group\"\n#| fig-height: 3\n#| out-width: \"60%\"\nset.seed(23456)\nget_random_pair <- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\nsims <- replicate(15, get_random_pair())\nsims <- t(sims)\nplot(sims, xlab=\"Child 1\",ylab=\"Child 2\",main=\"15 randomly drawn pairs of children\")\n```\n\n\n::: {.callout-note collapse=\"true\"}\n### Optional: A Little Simulation\n\nWe can actually do the \"randomly drawn pair of observations from the same group\" via simulation.  \nThe code below creates a function for us to use. Can you figure out how it works?\n```{r}\nget_random_pair <- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n```\nTry it out, by running it several times.\n```{r}\nget_random_pair()\n```\n\nNow let's make our computer do it loads and loads of times:\n```{r}\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims <- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims <- t(sims)\ncor(sims[,1], sims[,2])\n```\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Optional: correlations from group-structured data\n\nLet's suppose we had only 2 observations in each group.  \n```{r echo=FALSE}\ntempdat <- read.csv(\"https://uoepsy.github.io/data/iccexplainer.csv\")\nhead(tempdat) %>% rbind(.,rep(\"...\", 3))\n```\n\n```{r include=F}\nlibrary(nlme)\nres <- lme(y ~ 1, random = ~ 1 | cluster, data=tempdat, method=\"ML\")\nic <- getVarCov(res)[1] / (getVarCov(res)[1] + res$sigma^2)\n```\n\n__The ICC for this data is `r round(ic,2)`.__    \n\nNow suppose we *reshape* our data so that we have one row per group, and one column for each observation to look like this:\n```{r echo=F}\ntempdat_wide <- tempdat %>% \n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") \ntempdat_wide %>% head %>% rbind(.,rep(\"...\", 3))\n```\nCalculating Pearson's correlation on those two columns yields `r cor(tempdat_wide$obs1, tempdat_wide$obs2) %>% round(.,2)`, which isn't quite right. It's close, but not quite.. \n\n:::imp \nThe crucial thing here is that it is completely arbitrary which observations get called \"obs1\" and which get called \"obs2\".  \nThe data aren't paired, they're just random draws from a __group.__\n:::\n\nEssentially, there are lots of different combinations of \"pairs\" here. \nThere are the ones we have shown above:\n```{r echo=F}\nhead(tempdat_wide) %>% rbind(., rep(\"...\",3))\n```\nBut we might have equally chosen any of these:  \n\n:::panelset\n:::panel\n#### ...\n```{r echo=F}\nset.seed(12)\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::panel\n#### ...\n```{r echo=F}\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::panel\n#### ...\n```{r echo=F}\nsample_n(tempdat, n()) %>% arrange(cluster) %>% group_by(cluster) %>% \n  mutate(observation = 1:n()) %>% ungroup %>%\n  pivot_wider(names_from=observation, values_from=y, names_prefix = \"obs\") %>% head() %>% rbind(., rep(\"...\",3))\n```\n:::\n:::\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of `r round(ic, 2)`!\n\n__ICC = the expected correlation of a *randomly drawn pair* of observations from the same group.__\n\n:::\n\n## Visualisations\n\nWhen we're visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. \nFor instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens _within_ a given school (see e.g. @fig-egschool from earlier).  \n\nSome useful __ggplot__ tools here are:  \n\n- `facet_wrap()` - make a separate little plot for each level of a grouping variable\n- the `group` aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n:::panelset\n\n:::panel\n##### facets\n```{r}\nggplot(schoolmot, aes(x=motiv,y=grade,col))+\n  geom_point() +\n  facet_wrap(~schoolid)\n```\n:::\n:::panel\n##### group\n```{r}\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)\n```\n:::\n:::\n\n\n# Information, pooled\n\nWith our current toolset (linear regression), there are two avenues for us with respect to how we analyse clustered data. We can either ignore the clustering completely (and violate our assumptions), we can add the cluster-level differences in as another predictor. These reflect different ways in which we can \"pool\" information from across the different clusters.  \n\n:::sticky\n__Complete Pooling__  \n\nAll information from different clusters is pooled together to estimate the relevant association.  \n\n```{r}\n#| eval: false\nlm(grade ~ motiv, data = schoolmot)\n```\nTake all the children, fit a regression line for `grade ~ motivation`.  \n\n:::\n\n:::sticky\n__No Pooling__  \n\nInformation from each cluster contributes **only** to that specific cluster's estimate.  \n\n```{r}\n#| eval: false\nlm(grade ~ motiv*schoolid, data = schoolmot)\n```\nEstimate school-specific differences in grades, and school-specific differences in the effect of motivation on grades. Get _loads_ of coefficients that aren't really of interest.   \n\n\n\n:::\n\n\nEstimating the school-specific differences in our model (the no-pooling approach) is clearly better than simply ignoring them, but it does mean that we are treating the schools as if they are completely independent entities.  \nSuppose we had another school - School X - for which we had only _three_ children's data (@fig-borrowstrength). Intuitively, we don't want to trust the line for School X as much as we trust the others (where we have 30 children's data). \n\n```{r}\n#| echo: false\n#| label: fig-borrowstrength\n#| fig-cap: \"Blue lines show the estimated fitted values from the no-pooling approach lm(grade ~ motiv * schoolid). School X's estimate is based on just the 3 datapoints from that school, and does not take into account any of the other schools in anyway.\"\nset.seed(123)\nbind_rows(\n  schoolmot |> filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4,6)]),\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4),\n    grade = 50 + 10*motiv + rnorm(3,0,10)\n  )\n) -> tdf \n\ntdf |> \n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  geom_line(data=broom::augment(lm(grade~motiv*schoolid,tdf)),aes(y=.fitted),\n            col=\"blue\",lwd=.5)+\n  facet_wrap(~schoolid)\n \n```\n\nIn the no-pooling approach, it is only those three children from School X that contribute to the School X line. What would be great is if we could _somehow_ use the information from the other schools to inform our estimation of what is going on in School X. This is what multi-level modelling achieves, _partially_ pooling information across the groups, and this is where we'll turn to next.    \n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"01a_clustered.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"1A: Clustered Data | LMM","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}