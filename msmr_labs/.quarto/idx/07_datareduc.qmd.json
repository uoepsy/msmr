{"title":"7A: Wide Data, PCA and EFA","markdown":{"yaml":{"title":"7A: Wide Data, PCA and EFA","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Working with wide data","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- working with wide data\n- data reduction techniques: PCA and EFA\n\n:::\n\n\n\ncopy in stuff from standalone pages\n\n\n# PCA\n\n- the goal of PCA\n  - pca cov vs cor\n- the math (either rewrite or remove)\n- pca in R\n- doing PCA\n- the output  \n- methods for evaluating number of components\n- extracting and using component scores\n\n# EFA\n\n- the goal of EFA\n  - often multiple times and comparing solutions\n  - rotation vs the orthogonality of PCA\n  - this brings indeterminacy.\n  - factor scores must be estimated, rather than extracted\n- methods of evaluating number of components\n- doing EFA\n  - the output\n- estimating factor scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### The goal of PCA  \n\nThe goal of principal component analysis (PCA) is to find a _smaller_ number of uncorrelated variables which are linear combinations of the original ( _many_ ) variables and explain most of the variation in the data.\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. \nHow we measure such constructs is a very important consideration for research. The things we're interested in are very rarely the things we are *directly* measuring. \n\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety? ^[Even if we cut open someone's brain, it's unclear what we would be looking for in order to 'measure' it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings]. More often than not, we measure these things using questionnaire based methods, to capture the multiple dimensions of the thing we are trying to assess. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of \"anxiety\"). But they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess \"effects on anxiety\", rather than \"effects on anxiety q1 + effects on anxiety q2 + ...\", etc.  \n\nThis leads us to the idea of *reducing the dimensionality of our data*. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? \n\n:::\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: (some of) the math behind it  \n\nDoing data reduction can feel a bit like magic, and in part that's just because it's quite complicated. \n\n**The intuition**  \n\nOne way we might construct a square matrix that is symmetric along the diagonal is to compute the product of a vector $\\mathbf{f}$ with its transpose $\\mathbf{f'}$:  \n$$\n\\begin{equation*}\n\\mathbf{f} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\qquad \n\\mathbf{f} \\mathbf{f'} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n0.60, 0.77, 0.69, 0.83, 0.60, 0.88 \\\\\n\\end{bmatrix} \n\\qquad = \\qquad\n\\begin{bmatrix}\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 0.54, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 0.41, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 0.66, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 0.79 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nThe difference between this an a correlation matrix is that in the correlation matrix the diagonal has values of 1 (the correlation of a variable with itself is 1).  and lets call it **R**.\n$$\n\\begin{equation*}\n\\mathbf{R} = \n\\begin{bmatrix}\n1.00, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 1.00, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 1.00, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 1.00, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 1.00, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 1.00 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nPCA is about trying to determine the vector **f** which gets close to generating the correlation matrix **R**. It's a bit like unscrambling eggs!  \n\nWe start by expressing the correlation matrix $R$ as the product of a matrix $C$ and it's inverse $C'$, where $\\mathbf{C}$ are our \"principal components\" - a set of orthogonal vectors that together can reproduce the correlation matrix.  \n$\\mathbf{R = CC'}$.  \n\n<!-- If $n$ is number of variables in $R$, then $i^{th}$ component $C_i$ is the linear sum of each variable multiplied by some weighting:   -->\n<!-- $$ -->\n<!-- C_i = \\sum_{j=1}^{n}w_{ij}x_{j} -->\n<!-- $$ -->\n\n**How do we find $C$?**\n\nThis is where \"eigen decomposition\" comes in.  \nFor the $n \\times n$ correlation matrix $\\mathbf{R}$, there is an **eigenvector** $x_i$ that solves the equation \n$$\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n$$\nWhere the vector multiplied by the correlation matrix is equal to some **eigenvalue** $\\lambda_i$ multiplied by that vector.  \nWe can write this without subscript $i$ as: \n$$\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n$$\nthe vectors which make up $\\mathbf{X}$ must be orthogonal [($\\mathbf{XX' = I}$)](https://miro.medium.com/max/700/1*kyg5XbrY1AOB946IE5nWWg.png), which means that $\\mathbf{R = X \\lambda X'}$\n \nWe can actually do this in R manually. \nCreating a correlation matrix:  \n```{r}\n# lets create a correlation matrix, as the product of ff'\nf <- c(.5,.7,.6,.8,.5,.9)\nR <- f %*% t(f)\n#give rownames and colnames\nrownames(R)<-colnames(R)<-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)<-1\nR\n```\n\nEigen Decomposition\n```{r}\n# do eigen decomposition\ne <- eigen(R)\nprint(e, digits=2)\n```\n\nThe eigenvectors are orthogonal ($\\mathbf{XX' = I}$):\n```{r}\nround(e$vectors %*% t(e$vectors),2)\n```\n\nThe Principal Components $\\mathbf{C}$ are the eigenvectors scaled by the square root of the eigenvalues:\n```{r}\n#eigenvectors\ne$vectors\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\nC <- e$vectors %*% diag(sqrt(e$values))\nC\n```\n\nAnd we can reproduce our correlation matrix, because $\\mathbf{R = CC'}$. \n```{r}\nC %*% t(C)\n```\n\nNow lets imagine we only consider 1 principal component.  \nWe can do this with the `principal()` function: \n```{r}\nlibrary(psych)\npc1<-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n```\n\nLook familiar? It looks like the first component we computed manually. The first column of $\\mathbf{C}$:\n```{r}\ncbind(pc1$loadings, C=C[,1])\n```\nWe can now ask \"how well does this component (on its own) recreate our correlation matrix?\" \n```{r}\nC[,1] %*% t(C[,1])\n```\nIt looks close, but not quite. How much not quite? Measurably so!\n```{r}\nR - (C[,1] %*% t(C[,1]))\n```\n\nNotice the values on the diagonals of $\\mathbf{c_1}\\mathbf{c_1}'$.\n```{r}\ndiag(C[,1] %*% t(C[,1]))\n```\nThese aren't 1, like they are in $R$. But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar? \n```{r}\npc1$communality\n```\nAnd likewise the 1 minus these is the unexplained variance:  \n```{r}\n1 - diag(C[,1] %*% t(C[,1]))\npc1$uniquenesses\n```\n\n:::\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- working with wide data\n- data reduction techniques: PCA and EFA\n\n:::\n\n\n# Working with wide data\n\ncopy in stuff from standalone pages\n\n\n# PCA\n\n- the goal of PCA\n  - pca cov vs cor\n- the math (either rewrite or remove)\n- pca in R\n- doing PCA\n- the output  \n- methods for evaluating number of components\n- extracting and using component scores\n\n# EFA\n\n- the goal of EFA\n  - often multiple times and comparing solutions\n  - rotation vs the orthogonality of PCA\n  - this brings indeterminacy.\n  - factor scores must be estimated, rather than extracted\n- methods of evaluating number of components\n- doing EFA\n  - the output\n- estimating factor scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### The goal of PCA  \n\nThe goal of principal component analysis (PCA) is to find a _smaller_ number of uncorrelated variables which are linear combinations of the original ( _many_ ) variables and explain most of the variation in the data.\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. \nHow we measure such constructs is a very important consideration for research. The things we're interested in are very rarely the things we are *directly* measuring. \n\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety? ^[Even if we cut open someone's brain, it's unclear what we would be looking for in order to 'measure' it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings]. More often than not, we measure these things using questionnaire based methods, to capture the multiple dimensions of the thing we are trying to assess. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of \"anxiety\"). But they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess \"effects on anxiety\", rather than \"effects on anxiety q1 + effects on anxiety q2 + ...\", etc.  \n\nThis leads us to the idea of *reducing the dimensionality of our data*. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? \n\n:::\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: (some of) the math behind it  \n\nDoing data reduction can feel a bit like magic, and in part that's just because it's quite complicated. \n\n**The intuition**  \n\nOne way we might construct a square matrix that is symmetric along the diagonal is to compute the product of a vector $\\mathbf{f}$ with its transpose $\\mathbf{f'}$:  \n$$\n\\begin{equation*}\n\\mathbf{f} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\qquad \n\\mathbf{f} \\mathbf{f'} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n0.60, 0.77, 0.69, 0.83, 0.60, 0.88 \\\\\n\\end{bmatrix} \n\\qquad = \\qquad\n\\begin{bmatrix}\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 0.54, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 0.41, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 0.66, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 0.79 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nThe difference between this an a correlation matrix is that in the correlation matrix the diagonal has values of 1 (the correlation of a variable with itself is 1).  and lets call it **R**.\n$$\n\\begin{equation*}\n\\mathbf{R} = \n\\begin{bmatrix}\n1.00, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 1.00, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 1.00, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 1.00, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 1.00, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 1.00 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nPCA is about trying to determine the vector **f** which gets close to generating the correlation matrix **R**. It's a bit like unscrambling eggs!  \n\nWe start by expressing the correlation matrix $R$ as the product of a matrix $C$ and it's inverse $C'$, where $\\mathbf{C}$ are our \"principal components\" - a set of orthogonal vectors that together can reproduce the correlation matrix.  \n$\\mathbf{R = CC'}$.  \n\n<!-- If $n$ is number of variables in $R$, then $i^{th}$ component $C_i$ is the linear sum of each variable multiplied by some weighting:   -->\n<!-- $$ -->\n<!-- C_i = \\sum_{j=1}^{n}w_{ij}x_{j} -->\n<!-- $$ -->\n\n**How do we find $C$?**\n\nThis is where \"eigen decomposition\" comes in.  \nFor the $n \\times n$ correlation matrix $\\mathbf{R}$, there is an **eigenvector** $x_i$ that solves the equation \n$$\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n$$\nWhere the vector multiplied by the correlation matrix is equal to some **eigenvalue** $\\lambda_i$ multiplied by that vector.  \nWe can write this without subscript $i$ as: \n$$\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n$$\nthe vectors which make up $\\mathbf{X}$ must be orthogonal [($\\mathbf{XX' = I}$)](https://miro.medium.com/max/700/1*kyg5XbrY1AOB946IE5nWWg.png), which means that $\\mathbf{R = X \\lambda X'}$\n \nWe can actually do this in R manually. \nCreating a correlation matrix:  \n```{r}\n# lets create a correlation matrix, as the product of ff'\nf <- c(.5,.7,.6,.8,.5,.9)\nR <- f %*% t(f)\n#give rownames and colnames\nrownames(R)<-colnames(R)<-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)<-1\nR\n```\n\nEigen Decomposition\n```{r}\n# do eigen decomposition\ne <- eigen(R)\nprint(e, digits=2)\n```\n\nThe eigenvectors are orthogonal ($\\mathbf{XX' = I}$):\n```{r}\nround(e$vectors %*% t(e$vectors),2)\n```\n\nThe Principal Components $\\mathbf{C}$ are the eigenvectors scaled by the square root of the eigenvalues:\n```{r}\n#eigenvectors\ne$vectors\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\nC <- e$vectors %*% diag(sqrt(e$values))\nC\n```\n\nAnd we can reproduce our correlation matrix, because $\\mathbf{R = CC'}$. \n```{r}\nC %*% t(C)\n```\n\nNow lets imagine we only consider 1 principal component.  \nWe can do this with the `principal()` function: \n```{r}\nlibrary(psych)\npc1<-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n```\n\nLook familiar? It looks like the first component we computed manually. The first column of $\\mathbf{C}$:\n```{r}\ncbind(pc1$loadings, C=C[,1])\n```\nWe can now ask \"how well does this component (on its own) recreate our correlation matrix?\" \n```{r}\nC[,1] %*% t(C[,1])\n```\nIt looks close, but not quite. How much not quite? Measurably so!\n```{r}\nR - (C[,1] %*% t(C[,1]))\n```\n\nNotice the values on the diagonals of $\\mathbf{c_1}\\mathbf{c_1}'$.\n```{r}\ndiag(C[,1] %*% t(C[,1]))\n```\nThese aren't 1, like they are in $R$. But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar? \n```{r}\npc1$communality\n```\nAnd likewise the 1 minus these is the unexplained variance:  \n```{r}\n1 - diag(C[,1] %*% t(C[,1]))\npc1$uniquenesses\n```\n\n:::\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"07_datareduc.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"7A: Wide Data, PCA and EFA","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}