{"title":"Week 4 Exercises: Nested and Crossed","markdown":{"yaml":{"title":"Week 4 Exercises: Nested and Crossed","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Psychoeducation Treatment Effects","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(lme4)\n```\n\n\n\n\n```{r}\n#| eval: false\n#| echo: false\nsimm2<-function(seed=NULL,b0=0,b1=1,b2=1,z0=1,z1=1,e=1){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  n_groups = round(runif(1,1,15))*2\n  npg = 5\n  g = rep(1:n_groups, e = 5)      # the group identifier\n  x = rep(0:4,n_groups)\n  b = rep(0:1,e=n_groups/2)\n  b = b[g]\n  re0 = rnorm(n_groups, sd = z0)  # random intercepts\n  re  = re0[g]\n  rex = rnorm(n_groups, sd = z1)  # random effects\n  re_x  = rex[g]\n  lp = (b0 + re) + (b1 + re_x)*x + b2*x*b \n  y = rnorm(length(g), mean = lp, sd = e) # create a continuous target variable\n  # y_bin = rbinom(N, size = 1, prob = plogis(lp)) # create a binary target variable\n  data.frame(x, b=factor(b), g=factor(g), y)\n}\neseed = round(runif(1,1e3,1e6))\nset.seed(929918)\nbig = tibble(\n    school = 1:30,\n    int = rnorm(30,20,1),\n    sl = rnorm(30,-.3,.5),\n    intr = rnorm(30,-1,.5),\n    z0 = runif(30,.5,1.5),\n    z1 = runif(30,.5,1.5),\n    e = runif(30,.5,1)\n  )\n  big = big |> mutate(\n    data = pmap(list(int,sl,intr,z0,z1,e), ~simm2(b0=..1,b1=..2,b2=..3,z0=..4,z1=..5,e=..6))\n  ) |> unnest(data)\n\n  # m = lmer(round(y)~x*b+(1+x*b|school)+(1+x|school:g),big)\n  # broom.mixed::augment(m) |>\n  #   ggplot(aes(x=x,y=.fitted,col=factor(b)))+\n  #   geom_point(aes(y=`round(y)`))+\n  #   geom_line(aes(group=interaction(school,g)))\n\ntnames = unique(replicate(100,paste0(sample(LETTERS,2),collapse=\"\")))\n  \nbig <- big |> transmute(\n    therapist = tnames[school],\n    group = ifelse(b==0,\"Control\",\"Treatment\"),\n    patient = pmap_chr(list(therapist,group,g),~paste(..1,..2,..3,sep=\"_\")),\n    visit = x,\n    GAD = pmin(35,pmax(7,round(y)+5))\n  ) \n\nbig |> select(patient,visit,GAD) |>\n  pivot_wider(names_from=visit,values_from=GAD, names_prefix=\"visit_\") |>\n  write_csv(file=\"../../data/lmm_gadeduc.csv\")\n\n\n\n\n```\n\n\n\n:::frame\n__Data: gadeduc.csv__\n\n```{r}\n#| include: false\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\ngeduc1 = geduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |>\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n# m = lmer(GAD~visit*group+(1+visit*group|therapist)+(1+visit|therapist:patient),geduc1)\n# summary(m)\ntn = geduc1 |> group_by(therapist) |> summarise(np = n_distinct(patient))\n```\n\nThis is synthetic data from a randomised controlled trial, in which `r nrow(tn)` therapists randomly assigned patients (each therapist saw between `r min(tn[,'np'])` and `r max(tn[,'np'])` patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).  \n\nThe control group of patients received standard sessions offered by the therapists. \nFor the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).  \n\nThe data are available at [https://uoepsy.github.io/data/lmm_gadeduc.csv](https://uoepsy.github.io/data/lmm_gadeduc.csv){target=\"_blank\"}\n\nYou can find a data dictionary below:\n```{r}\n#| echo: false\n#| label: tbl-lmm_gadeduc.csv\n#| tbl-cap: \"Data Dictionary: lmm_gadeduc.csv\"\ntibble(\n    variable = names(geduc),\n    description = c(\"A patient code in which the labels take the form <Therapist initials>_<group>_<patient number>.\",\"Score on the GAD7 at baseline\", \n                    \"GAD7 at 1 month assessment\",\n                    \"GAD7 at 2 month assessment\",\n                    \"GAD7 at 3 month assessment\",\n                    \"GAD7 at 4 month assessment\"\n                    )\n)  |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nUh-oh... these data aren't in the same shape as the other datasets we've been giving you..  \n\nCan you get it into a format that is ready for modelling?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- It's wide, and we want it long.  \n- Once it's long. \"visit_0\", \"visit_1\",.. needs to become the numbers 0, 1, ...\n- One variable (`patient`) contains lots of information that we want to separate out. There's a handy function in the __tidyverse__ called `separate()`, check out the help docs!  \n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"1 - reshaping\", slabel=F,show=T, toggle=params$TOGGLE)`\nHere's the data. We have one row per patient, but we have multiple observations for each patient across the columns..  \n```{r}\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n```\n\nWe can make it long by taking the all the columns from `visit_0` to `visit_4` and shoving their values into one variable, and keeping the name of the column they come from as another variable:  \n```{r}\ngeduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n```\n\n\n`r solend()`\n`r solbegin(label=\"2 - time is numeric\", slabel=F,show=T, toggle=params$TOGGLE)`\nNow we know how to get our data long, we need to sort out our time variable (`visit`) and make it into numbers.  \nWe can replace all occurrences of the string `\"visit_\"` with nothingness `\"\"`, and then convert them to numeric.  \n\n```{r}\ngeduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n```\n\n\n`r solend()`\n`r solbegin(label=\"3 - splitting up the patient variable\", slabel=F,show=T, toggle=params$TOGGLE)`\nFinally, we need to sort out the `patient` variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore \"_\".  \n\nThe `separate()` function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:  \n```{r}\ngeduc_long <- geduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |>\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n```\n\nAnd we're ready to go!  \n```{r}\ngeduc_long\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nVisualise the data. Does it look like the treatment had an effect?  \nDoes it look like it worked for every therapist?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- remember, `stat_summary()` is very useful for aggregating data inside a plot.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere's the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective.. \n```{r}\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n```\n\nLet's split this up by therapist, so we can see the averages across each therapist's set of patients.  \nThere's clear variability between therapists in how well the treatment worked. For instance, the therapists `EU` and `OD` don't seem to have much difference between their groups of patients.\n```{r}\n#| out-width: \"100%\"\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.  \n`r qend()`\n`r solbegin(label=\"1 - fixed effects\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe want to know if how anxiety (`GAD`) changes over time (`visit`) is different between treatment and control (`group`).  \n\nHopefully this should hopefully come as no surprise^[if it does, head back to where we learned about interactions in the single level regressions `lm()`. It's just the same here.] - it's an interaction!  \n\n\n```{r}\n#| eval: false\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n```\n\n`r solend()`\n`r solbegin(label=\"2 - grouping structure\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe have multiple observations for each of the `r sum(tn$np)` patients, and those patients are nested within `r nrow(tn)` therapists.  \n\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient \"1\" from therapist \"AO\" is a different person from patient \"1\" from therapist \"BJ\". To correctly group the observations into different patients (and not 'patient numbers'), we need to have `therapist:patient`.  \n\nSo we capture therapist-level differences in `( ... | therapist)` and the patients-within-therapist-level differences in `( ... | therapist:patient)`:  \n\n```{r}\n#| eval: false\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n```\n\n\n`r solend()`\n`r solbegin(label=\"3 - random effects\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nNote that each patient can change differently in their anxiety levels over time - i.e. the slope of `visit` could vary by participant.  \n\nLikewise, some therapists could have patients who change differently from patients from another therapist, so `visit|therapist` can be included.  \n\nEach patient is in one of the two groups - they're _either_ treatment _or_ control. So we can't say that \"differences in anxiety due to treatment varies between patients\", because for any one patient the \"difference in anxiety due to treatment\" is not defined in our study design.  \n\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect _could_ be different for different therapists!  \n\n```{r}\nmod1 <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFor each of the models below, what is wrong with the random effect structure?  \n\n```{r}\n#| eval: false\nmodelA <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n```\n\n```{r}\n#| eval: false\nmodelB <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n```\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nmodelA <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n```\n\nThe `patient` variable doesn't capture the different patients _within_ therapists, so this actually fits crossed random effects and treats all data where `patient==1` as from the same group (even if this includes several different patients' worth of data from different therapists!)\n\n```{r}\n#| eval: false\nmodelB <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n```\n\nUsing the `/` here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can't vary by patient, so this doesn't work. hence why we need to split them up into `(...|therapist)+(...|therapist:patient)`.  \n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nLet's suppose that I don't want the psychoeducation treatment, I just want the standard therapy sessions that the 'Control' group received. Which therapist should I go to?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`dotplot.ranef.mer()` might help here!  \nYou can read about `ranef` in [Chapter 2 #making-model-predictions](https://uoepsy.github.io/lmm/02_lmm.html#making-model-predictions){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nIt would be best to go to one of the therapists `SZ`, `YS`, or `IT`...  \n\nWhy? These therapists all have the most negative slope of visit:  \n\n```{r}\n#| fig-height: 6\ndotplot.ranef.mer(ranef(mod1))$therapist\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nRecreate this plot.  \n\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.  \n\n```{r} \n#| echo: false\neffplot <- effects::effect(\"visit*group\",mod1) |>\n  as.data.frame()\n\nbroom.mixed::augment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient)\n  ) |>\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")\n\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- you can get the patient-specific lines using `augment()` from the __broom.mixed__ package, and the fixed effects estimates using the __effects__ package. \n- remember that the \"patient\" column doesn't group observations into unique patients. \n- remember you can pull multiple datasets into ggplot:  \n```{r}\n#| eval: false\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n```\n- see more in [Chapter 2 #visualising-models](https://uoepsy.github.io/lmm/02_lmm.html#visualising-models){target=\"_blank\"}\n\n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"1 - the relevant parts\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nThe __effects__ package will give us the fixed effect estimates: \n```{r}\nlibrary(effects)\nlibrary(broom.mixed)\neffplot <- effect(\"visit*group\",mod1) |>\n  as.data.frame()\n```\n\nWe want to get the fitted values for each patient. We can get fitted values using `augment()`. But the `patient` variable doesn't capture the _unique_ patients, it just captures their numbers (which aren't unique to each therapist).  \nSo we can create a new column called `upatient` which pastes together the therapists initials and the patient numbers\n\n```{r}\naugment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n```\n`r solend()`\n`r solbegin(label=\"2 - constructing the plot\", slabel=F,show=T, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\nlibrary(broom.mixed)\neffplot <- effect(\"visit*group\",mod1) |>\n  as.data.frame()\n\naugment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |>\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")\n```\n\n\n`r solend()`\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Jokes\n\n```{r}\n#| eval: false\n#| echo: false\n#set.seed()\njokeid = read_csv(\"data/laughlab_jokes.csv\")\n\nset.seed(123)\nn_groups = 90\nN = n_groups * nrow(jokeid)\ng = rep(1:n_groups, e = nrow(jokeid))\nb = rbinom(n_groups,1,.5)[g]\nb = unlist(lapply(1:n_groups, \\(x) sample(rep(0:1,e=15))))\nj = rep(1:nrow(jokeid), n_groups)\n\ng.re0 = rnorm(n_groups,0,8)[g]\ng.re = MASS::mvrnorm(n_groups, mu = c(0,0), Sigma = matrix(c(8,1,1,3),nrow=2))\ng.re0 = g.re[,1][g]\ng.reb = g.re[,2][g]\n\nj.re = MASS::mvrnorm(nrow(jokeid), mu = c(0,0), Sigma = matrix(c(5,2,2,5),nrow=2))\nj.re0 = j.re[,1][j]\nj.reb = j.re[,2][j]\nb0 = runif(1,30,55)\nb1 = runif(1,1,3)\nlp = b0 + g.re0 + j.re0 +\n  (b1 + j.reb + g.reb)*b\ny = rnorm(N, mean = lp, sd = 6)\n#y_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf=data.frame(g = factor(g),j = factor(j),b,y)\n\ndf = left_join(df,\n          left_join(jokeid, \n                    tibble(\n                      rank = order(j.re[,1],decreasing = TRUE),\n                      j = factor(1:30)\n                      )\n          ) |> dplyr::select(joke, j)\n)\n\nggplot(df,aes(x=b,y=y,group=g))+\n  geom_point(alpha=.1,size=.5)+\n  stat_summary(geom=\"pointrange\",position=position_dodge(width=.2))\n  \n\nlm(y~b,df)\nm = lmer(y~1+b+(1|g)+(1+b|j),df)\nsummary(m)\n\n\nnames(df)\n\nlaughs <- df |> transmute(\n  ppt = paste0(\"PPTID\",g),\n  joke_label = joke,\n  joke_id = j,\n  delivery = case_when(\n    b == 0 ~ \"audio\",\n    b == 1 ~ \"video\"\n  ),\n  rating = round(y)\n)\n\nwrite_csv(laughs, \"../../data/lmm_laughs.csv\")\n```\n\n:::frame\n__Data: lmm_laughs.csv__  \n\n```{r}\n#| include: false\nlaughs <- read_csv(\"https://uoepsy.github.io/data/lmm_laughs.csv\")\n```\n\nThese data are simulated to imitate an experiment that investigates the effect of visual non-verbal communication (i.e. gestures, facial expressions) on joke appreciation. \n`r n_distinct(laughs$ppt)` Participants took part in the experiment, in which they each rated how funny they found a set of `r n_distinct(laughs$joke_id)` jokes. For each participant, the order of these `r n_distinct(laughs$joke_id)` jokes was randomly set for each run of the experiment. For each participant, the set of jokes was randomly split into two halves, with the first half being presented in audio-only, and the second half being presented in audio and video. This meant that each participant saw `r n_distinct(laughs$joke_id)/2` jokes with video and `r n_distinct(laughs$joke_id)/2` without, and each joke would be presented in with video roughly half of the times it was seen.  \n\nThe researchers want to investigate whether the delivery (audio/audiovideo) of jokes is associated with differences in humour-ratings.  \n\nData are available at [https://uoepsy.github.io/data/lmm_laughs.csv](https://uoepsy.github.io/data/lmm_laughs.csv){target=\"_blank\"}\n\n```{r}\n#| echo: false\n#| label: tbl-laughdict\n#| tbl-cap: \"Data Dictionary: lmm_laughs.csv\"\ntibble(\n  variable = names(laughs),\n  description = c(\"Participant Identification Number\",\n                  \"Joke presented\",\n                  \"Joke Identification Number\",\n                  \"Experimental manipulation: whether joke was presented in audio-only ('audio') or in audiovideo ('video')\",\n                  \"Humour rating chosen on a slider from 0 to 100\")\n) |> gt::gt()\n```\n\n\n\n:::\n\n\n\n`r qbegin(qcounter())`\nPrior to getting hold of any data, we should be able to write out the structure of our ideal \"maximal\" model given that we have a description of the design of the study.  \n\nCan you do so?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nDon't know where to start? Try following the steps in [Chapter 8 #maximal-model](https://uoepsy.github.io/lmm/08_modelbuilding.html#maximal-model){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe want to estimate the effect of delivery on humour rating of jokes:  \n`rating ~ delivery`  \n\nWe have 30 observations for each participant. Participants are just another sampling unit here.  \n`rating ~ delivery + (1 | ppt)`  \n\nWe have 90 observations for each joke. We're not interested in specific jokes here, so we can think of these as a random set of experimental items that we might choose differently next time we conduct an experiment to assess delivery~rating:\n`rating ~ delivery + (1 | ppt) + (1 | joke_id)`  \n\nParticipants each see 15 jokes without video, and 15 with. The `delivery` variable is *\"within\"* participant. Some participants might respond differently when there is video (vs without) whereas some might not rate jokes any differently. The effect of delivery on rating might be *vary by participant*:  \n`rating ~ delivery + (1 + delivery | ppt) + (1 | joke_id)`  \n\nEach joke is presented both with and without the video. Some jokes might really benefit from gestures and facial expressions, whereas some might not. The effect of delivery on rating might be *vary by joke*:  \n`rating ~ delivery + (1 + delivery | ppt) + (1 + delivery | joke_id)`  \n\n`r solend()`\n\n`r qbegin(qcounter())`\nRead in and clean the data (if necessary).  \n\nCreate some plots showing: \n\n1. the average rating for audio vs audio+video for each joke\n2. the average rating for audio vs audio+video for each participant\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- you could use `facet_wrap`, or even `stat_summary`!  \n- you might want to use `joke_id`, rather than `joke_label` (the labels are very long!)\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere is one using `facet_wrap`:\n```{r}\nggplot(laughs, aes(x = delivery, y = rating)) +\n  geom_boxplot()+\n  facet_wrap(~joke_id)\n```\n\nAnd one using `stat_summary()` for participants: \n```{r}\nggplot(laughs, aes(x = delivery, y = rating)) +\n  stat_summary(geom=\"pointrange\", aes(group = ppt),\n               position = position_dodge(width=.2))+\n  stat_summary(geom=\"line\", aes(group = ppt),\n               position = position_dodge(width=.2))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nFit an appropriate model to address the research aims of the study. \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis should be the one you came up with a couple of questions ago!\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nmod <- lmer(rating ~ delivery + \n              (1 + delivery | joke_id) +\n              (1 + delivery| ppt), data = laughs)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nWhich joke is funniest when presented just in audio? For which joke does the video make the most difference to ratings?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThese can all be answered by examining the random effects with `ranef()`.  \nSee [Chapter 2 #making-model-predictions](https://uoepsy.github.io/lmm/02_lmm.html#making-model-predictions){target=\"_blank\"}.  \n\nIf you're using `joke_id`, can you find out the actual joke that these correspond to?\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\ndotplot.ranef.mer(ranef(mod))$joke_id\n```\n\nJoke 19 is the funniest apparently! (not sure I agree)\n\nLots of ways to find what the joke actually is. Here is one way:\n```{r}\nlaughs |> count(joke_id, joke_label) |>\n  filter(joke_id==19) |>\n  pull(joke_label)\n```\n\n\nAnd from the plot above, Joke 28 has the most benefit of video. \nWe can quickly check this with something like:  \n```{r}\nranef(mod)$joke_id |>\n  filter(deliveryvideo == max(deliveryvideo))\n```\n\nThe joke itself is a bit weird.. maybe the video really helped!  \n\n```{r}\nlaughs |> count(joke_id, joke_label) |>\n  filter(joke_id==28) |>\n  pull(joke_label)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDo jokes that are rated funnier when presented in audio-only tend to also benefit more from the addition of video?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThink careful about this question. The random effects show us that jokes vary in their intercepts (ratings in audio-only) and in their effects of delivery (the random slopes). We want to know if these are related..  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nVarCorr(mod)\n```\nIt's the correlation here that tell us - jokes rated higher in the audio-only tend to have a bigger effect of the video. \n\nWe can see this in a plot if we like. Here every dot is a joke, and the x-axis shows whether it is above or below the average rating for audio-only (the intercept). The y-axis shows whether it is above or below the average effect of video. \n```{r}\nplot(ranef(mod)$joke)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCreate a plot of the estimated effect of video on humour-ratings. Try to plot not only the fixed effects, but the raw data too.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nSee e.g. [Chapter 2 #visualising-models](https://uoepsy.github.io/lmm/02_lmm.html#visualising-models){target=\"_blank\"}\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\n\nplotdatf <- effect(\"delivery\",mod) |>\n  as.data.frame()\n\nggplot(data = laughs, aes(x = delivery)) +\n  geom_jitter(aes(y = rating), width = .1, height = 0, alpha = .1) +\n  geom_pointrange(data = plotdatf,\n                  aes(y = fit, ymin = lower, ymax = upper),\n                  position=position_nudge(x=.2))\n```\n\n`r solend()`\n\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Optional Extra: Vocab Development\n\n:::frame\n__Data: pvt_bilingual.csv__ \n\n```{r}\n#| echo: false\npvt <- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\npvtsc = pvt |>\n  group_by(school, child) |>\n  mutate(n_obs = 1:n()) |>\n  ungroup() |>\n  mutate(child = ifelse(n_obs>7, paste0(child,\" 2\"), child)) |> count(school,child,isBilingual)\n```\n\n`r nrow(pvtsc)` children from `r length(unique(pvt$school))` schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). `r sum(pvtsc$isBilingual==0)` were monolingual English speakers, and `r sum(pvtsc$isBilingual==1)` were bilingual (english + another language). \n\nPrevious research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. The aim of the present study is to investigate differences in the development of vocabulary between monolingual and bilingual children.  \n\nThe data are available at [https://uoepsy.github.io/data/pvt_bilingual.csv](https://uoepsy.github.io/data/pvt_bilingual.csv).  \n\n```{r}\n#| echo: false\n#| label: tbl-pvtdict\n#| tbl-cap: \"Data Dictionary: pvt_bilingual.csv\"\ntibble(variable = names(pvt),\n       description = c(\n         \"Child's name\",\n         \"School Identifier\",\n         \"Binary variable indicating whether the child is monolingual (0) or bilingual (1)\",\n         \"Age (years)\",\n         \"Score on the Picture Vocab Test (PVT). Scores range 0 to 60\")\n) |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n`r qbegin(paste0(qcounter(), \" - Less Guided\"))`\nConduct an analysis to estimate the differences in trajectories of vocabulary development between children attending bilingual schools vs those attending monolingual schools.  \n\nWrite up your results.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n1. always plot your data!\n2. read the study background: \"increase steadily ... before beginning to plateau\" describes a _curve_!  \n3. plotting the data can give an initial sense of the possible need for higher order polynomials.  \n4. multiple observations for each child. multiple children in each school.. \n\n:::\n\n\n`r qend()`\n\n`r solbegin(label=\"1 - initial data checks\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLet's read in the data:  \n```{r}\npvt <- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\nhead(pvt)\n```\nFirst, we're going to want `isBilingual` to be a factor, because the 1s and 0s are categories, not numbers. \n```{r}\npvt <- pvt |> mutate(isBilingual = factor(isBilingual))\n```\n\n\nWe have 30 distinct schools:\n```{r}\nn_distinct(pvt$school)\n```\n\nAnd 418 distinct children. Is that right?  \n```{r}\nn_distinct(pvt$child)\n```\n\nGiven that the `pvt$child` variable is just the first name of the child, it's entirely likely that there will be, for instance more than one \"Martin\".  \n\nThis says that there are 487!  \n```{r}\npvt |> count(school, child) |> nrow()\n```\n\nBut wait... we could _still_ have issues. What if there were 2 \"Martin\"s at the same school??\n```{r}\npvt |> \n  # count the school-children groups\n  count(school, child) |> \n  # arrange the output so that the highest \n  # values of the 'n' column are at the top\n  arrange(desc(n))\n```\n\nAha! There are 7 cases where schools have two children of the same name. Remember that each child was measured at 7 timepoints. We shouldn't have people with 14!  \n\nIf we actually _look_ at the data, we'll see that it is very neatly organised, with each child's data together. This means that we could feasibly make an educated guess that, e.g., the \"Jackson\" from \"School 3\" in rows 155-161 is different from the \"Jackson\" from \"School 3\" at rows 190-196. \n\nBecause of the ordering of our data, we can do something like this:  \n```{r}\npvt <- \n  pvt |>\n  # group by the school and child\n  group_by(school, child) |>\n  mutate(\n    # make a new variable which counts from 1 to \n    # the number of rows for each school-child\n    n_obs = 1:n()\n  ) |>\n  # ungroup the data\n  ungroup() |>\n  mutate(\n    # change it so that if the n_obs is >7, the \n    # child becomes \"[name] 2\", to indicate they're the second\n    # child with that name\n    child = ifelse(n_obs>7, paste0(child,\" 2\"), child)\n  )\n```\n\nNow we have 494!  \n```{r}\npvt |> count(school, child) |> nrow()\n```\n\nAnd nobody has anything other than 7 observations!\n```{r}\npvt |> count(school, child) |>\n  filter(n != 7)\n```\n\nPhew!  \n`r solend()`\n`r solbegin(label=\"2 - exploratory plots and descriptives\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nHow many bilingual children?  \n```{r}\npvt |> group_by(isBilingual) |>\n  summarise(nchild = n_distinct(child))\n```\n\nHow many children (mono/bilingual) in each school? \n```{r}\npvt |> group_by(school, isBilingual) |>\n  summarise(nchild = n_distinct(child)) |>\n  ggplot(aes(y=school,x=nchild,fill=isBilingual))+\n  geom_col()\n```\n\n\n\nOkay, let's just fit an intercept-only model:  \n```{r}\npvt_null <- lmer(PVT ~ 1 + \n                   (1 | school) +\n                   (1 | school:child), data = pvt)\nsummary(pvt_null)\n```\n\n```{r}\n#| echo: false\nvcres = VarCorr(pvt_null) |> as.data.frame()\nvcres = round(vcres$vcov,2)\n```\n\n\nAs we can see from `summary(pvt_null)`, the random intercept variances are `r vcres[1]` for child-level, `r vcres[2]` for school-level, and the residual variance is `r vcres[3]`.\n\nSo child level differences account for $\\frac{`r vcres[1]`}{`r paste0(vcres,collapse=\" + \")`} = `r round(vcres[1]/sum(vcres),2)`$ of the variance in PVT scores, and child & school differences together account for $\\frac{`r paste0(vcres[1:2],collapse=\" + \")`}{`r paste0(vcres,collapse=\" + \")`} = `r round(sum(vcres[1:2])/sum(vcres),2)`$ of the variance.\n\nHere's an initial plot too:\n\n```{r}\nggplot(pvt, aes(x=age,y=PVT,col=factor(isBilingual)))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n```\n\n`r solend()`\n`r solbegin(label=\"3 - modelling\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nI feel like either raw or orthogonal polynomials would be fine here - there's nothing explicit from the study background about stuff \"at baseline\". There's the stuff about the plateau at 7 or 8, but we can get that from the model plots. Orthogonal will allow us to compare the trajectories overall (their linear trend, the 'curviness' and 'wiggliness').  \n\nAn additional benefit of orthogonal polynomials is that we are less likely to get singular fits when we include polynomial terms in our random effects. Remember, raw polynomials are correlated, so often the by-participant variances in raw poly terms are highly correlated. \n\nI've gone for 3 degrees of polynomials here because the plot above shows a bit of an S-shape for the bilinguals. \n\n```{r}\npvt <- pvt |> mutate(\n  poly1 = poly(age, 3)[,1],\n  poly2 = poly(age, 3)[,2],\n  poly3 = poly(age, 3)[,3],\n)\n```\n\nThese models do not converge.  \nI've tried to preserve the by-child random effects of time, because while I think Schools probably _do_ vary, all the schools teach the same curriculum, whereas there's _a lot_ of varied things that can influence a child's vocabulary, both in and out of school.    \n```{r}\n#| eval: false\nmod1 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + (poly1 + poly2 + poly3)*isBilingual | school) + \n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod2 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * (poly1 + poly2) + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod3 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod4 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod5 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# relative to the variance in time slopes, there's v little by-school variance in bilingual differences in vocab\n\nmod6 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly2 +  poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# looks like curvature doesn't vary between schools much as linear and wiggliness \n```\n\nthis one converges! \n```{r}\nmod7 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n```\n\n\n`r solend()`\n`r solbegin(label=\"4 - model plots\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLet's plot the predictions: \n```{r}\nlibrary(broom.mixed)\naugment(mod7) |> \n  mutate(\n    poly1 = round(poly1, 3) # because of rounding errors that make plot weird\n  ) |>\n  ggplot(aes(x=poly1,col=isBilingual))+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n```\n\nRemember that these are just the average of all model fitted values. Because everyone has complete data here, we can use these to visualise our model estimates without worrying about them being distorted by missing data. Just like the fixed effects themselves though, all this is focussed on \"the average child from the average school\". The plot hides a lot of the *variability* that we have actually modelled. \nFor instance, consider how our view changes when we add in the individual lines for each child (below). The lines for the average child are just the same as above, but we can now see just how much children vary. \n\n```{r}\n#| code-fold: true\naugment(mod7) |> \n  mutate(\n    poly1 = round(poly1, 3) \n  ) |>\n  ggplot(aes(x=poly1,col=isBilingual))+\n  # make a line for each \"school-child\" group: \n  geom_line(aes(group=interaction(school,child),\n                y=.fitted),alpha=.1)+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n```\n  \nLet's refit our model with lmerTest:  \n```{r}\nmod7 <- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nsummary(mod7)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - model interpretation\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nThe exact value of the coefficients here don't really give us anything meaningful. Note that in our plots above, the x-axis is \"poly1\", which is some weirdly re-scaled version of \"age\", so these coefficients represent things like the estimated change in PVT when \"moving up 1 in poly1\". But these units are not remotely useful to us. Even if we wanted to try and scale back to get out something we could interpret in terms of years of age, the whole point of non-linear change is that we can't just say \"PVT increases by $b$ with every extra year of age\", because we need to consider the higher order polynomial terms.  \n\nThe long and the short of it is that while the numbers don't really matter here, the sign (+ or -) and the significance _does_ tell us stuff:  \n\n```{r}\n#| echo: false\nmm <- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\ntidy(mm) |> filter(effect==\"fixed\") |> transmute(\n  term,\n  est = round(estimate,2),\n  p = ifelse(p.value<.05,\"*\",\"\"),\n  interpretation = c(\n    \"average vocab score at the mean age (for monolingual)\",\n    \"vocab increases over time (for monolingual children)\",\n    \"the increase in vocab becomes more gradual (for monolingual children)\",\n    \"no significant wiggliness to vocab trajectory of the average monolingual child\",\n    \"average vocab score at mean age is lower for bilingual vs monolingual children\",\n    \"no significant difference in linear trend of vocab for bilingual vs monolingual\",\n    \"curvature for vocab trajectory of bilingual children significantly differs from that of monolinguals\",\n    \"wiggliness for vocab trajectory of bilingual children significantly differs from that of monolinguals\"\n  )\n) |> gt::gt()\n```\n\nFrom the random effects, we get even more information! All this stuff is kind of \"added context\" for the conclusions (which will come from our fixed effects).  \n```{r}\nVarCorr(mod7)\n```\nSchool's vary in the average child vocab score at mean age with an SD of 4.5. Schools with higher vocab scores at the mean age tend to have lower linear increase. Within schools, children vary in the vocab scores at mean age with an SD of 5.9. Children with steeper linear increase in vocab tend to have less curvature. \n\n\n\n\n<!-- ::: {.callout-caution collapse=\"true\"} -->\n<!-- #### optional extra: plotting uncertainty of fixed effects -->\n\n<!-- We've seen a few times that taking all the fitted values and averaging can give us the trajectories. But we've also seen that this can cause issues if we have missing data, or unbalanced designs. We have also had to use things like the __effects__ package to show uncertainty in the fixed effects estimates, because this is not the same as the standard errors around the average fitted values.   -->\n\n<!-- Another option instead of the __effects__ package (harder to make work with polynomials), is to use lots and lots of predictions from the model.   -->\n\n<!-- In USMR, we often visualised our model in a 3 step process: -->\n\n<!-- 1. make a little dataframe across the values that we want to plot -->\n<!-- 2. use `broom::augment(model, newdata = plotdata, interval = \"confidence\")` to add some predictions and confidence intervals.   -->\n<!-- 3. plot!   -->\n\n<!-- We can use this same logic in multilevel models, but step 2 is a little different.   -->\n\n\n<!-- First we make a little dataframe.   -->\n<!-- Because the actual estimation of orthogonal polynomials can depend on the number of values we're putting in (`poly(4:10, 2)` does not give the same scaling as `poly(c(4:10,4:10),2)`), it's best to just capture the polynomials that we actually fitted our model to.   -->\n<!-- So here we get the first 7 rows of each poly1, because those correspond to our polynomials that we fitted the model to.   -->\n<!-- ```{r} -->\n<!-- plotdat <- -->\n<!--   tibble( -->\n<!--     age = 4:10, -->\n<!--     poly1 = pvt$poly1[1:7], -->\n<!--     poly2 = pvt$poly2[1:7], -->\n<!--     poly3 = pvt$poly3[1:7], -->\n<!--     isBilingual = \"0\", -->\n<!--     school = \".\", -->\n<!--     child = \".\" -->\n<!--   ) -->\n<!-- ``` -->\n\n<!-- We're going to have to give it _some_ values for `school` and for `child`, but we don't want to predict for any specific school or child that is in our dataset.  -->\n<!-- By setting these to something like \".\", that doesn't occur in our data, we can estimate based on the fixed coefficients only.   -->\n\n<!-- Note that currently `isBilingual` is set to 0, but we want to plot for the bilinguals too, so we'll need to duplicate our `plotdat` object:    -->\n<!-- ```{r} -->\n<!-- plotdat <- -->\n<!--   bind_rows( -->\n<!--     plotdat, -->\n<!--     plotdat |> mutate(isBilingual=1) -->\n<!--   ) -->\n<!-- ``` -->\n\n<!-- Finally, the __merTools__ package has the `predictInterval` function that can get us some intervals   -->\n<!-- ```{r} -->\n<!-- library(merTools) -->\n\n<!-- bind_cols( -->\n<!--   plotdat,  -->\n<!--   predictInterval(merMod = mod7, newdata = plotdat, -->\n<!--                   level = 0.95, n.sims = 2000, -->\n<!--                   include.resid.var = FALSE) -->\n<!--           ) |> -->\n<!--   ggplot(aes(x=poly1,y=fit,ymin=lwr,ymax=upr, -->\n<!--              col=isBilingual,fill=isBilingual))+ -->\n<!--   geom_line()+ -->\n<!--   geom_ribbon(alpha=.2) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n\n\n\n`r solend()`\n\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Optional Extra: Test Enhanced Learning\n\n:::frame\n__Data: Test-enhanced learning__  \n\nAn experiment was run to conceptually replicate \"test-enhanced learning\" (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (`StudyStudy`), the other group studied the material once then did a test (`StudyTest`). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (`Test_word`). \n\nThe critical (replication) prediction is that the `StudyStudy` group recall more items on the immediate test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.  \n\nThe following code loads the data into your R environment by creating a variable called `tel`:\n\n```{r}\n#| eval: false\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n```\n\n```{r} \n#| echo: false\n#| label: tbl-teldict\n#| tbl-cap: \"Data Dictionary: testenhancedlearning.Rdata\"\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\ntibble(\n  variable=names(tel),\n  description=c(\"Unique Participant Identifier\", \"Group denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\",\"Time of recall test ('min' = Immediate, 'week' = One week later)\",\"Word being recalled (175 different test words)\",\"Whether or not the word was correctly recalled\",\"Time to recall word (milliseconds)\")\n) |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nLoad and plot the data. Does it look like the effect was replicated?  \n\n> The critical (replication) prediction is that the `StudyStudy` group recall more items on the immediate test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe can actually look at this from a couple of different angles. The most obvious option is to take successful learning as \"correctly recalling\" an item. This means we take the `Correct` variable as our outcome.  \n\nNote we also have `Rtime` - the \"time-to-recall\" of an item. This could also work as an outcome, but note that it also includes the time it took participants to provide an incorrect response too. If this was your own project, you may well want to provide analyses of `Correct`, and then also of the time-taken, but on the subset of correctly recalled items.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\nggplot(tel, aes(Delay, Correct, col=Group)) + \n  stat_summary(fun.data=mean_se, geom=\"pointrange\")+\n  theme_light()\n```\n\nThat looks like test-enhanced learning to me!  \n\n\n`r solend()`\n\n`r qbegin(qcounter())`\n\nTest the critical hypothesis using a mixed-effects model.  \n\nFit the maximal random effect structure supported by the experimental design. Simplify the random effect structure until you reach a model that converges.  \n\n__Note:__ Some of the models you attempt here might take time to fit. This is normal, and you can cancel the estimation at any time by pressing the escape key.  \nI suggest that you write your initial model, set it running, and then look at the first solution to see if it converged for me. You can assume that if it didn't work for me, it also won't work for you. I've shown the random effects for each model, in case it helps in deciding your next step.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWhat we're aiming to do here is to follow [Barr et al.'s](https://doi.org/10.1016/j.jml.2012.11.001) advice of defining our maximal model and then removing only the terms to allow a non-singular fit.  \n\n+ What kind of model will you use? What is our outcome? is it binary, or continuous? \n+ We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects?\n\n:::\n\n`r qend()` \n`r solbegin(label=\"1 - maximal model\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\n# save(mod1,mod2,mod3,mod4,file=\"data/telmodels.Rdata\")\nload(\"data/telmodels.Rdata\")\n```\n\nWe have a __crossed__ random effect structure here. Each participant was tested on every word, and each word was seen by every participant.  \n\nSubjects were tested at both the `min` and `week`, so `Delay|Subject_ID` can be included (some people might retain the items better than others).  \nLikewise the words were seen at both `min` and `week`, and some words might be more easy to retain than others (`Delay|Test_word`).  \n\nThe participants are in either one group or another, so we can't have `Group|Subject_ID`. However, the words were seen by people in both groups, so we _can_ have `Group|Test_word`.  \n\nThis one took my computer about 6 minutes.  \n```{r}\n#| eval: false\nmod1 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay * Group | Test_word),\n             family=binomial, data=tel)\n```\n<p style=\"color:red;font-size:.8em\">\nWarning message:<br>\nIn checkConv(attr(opt, \"derivs\"), opt\\$par, ctrl = control\\$checkConv,  :<br>\n  Model failed to converge with max|grad| = 0.0184773 (tol = 0.002, component 1)\n</p>\n\n```{r}\nVarCorr(mod1)\n```\n\n`r solend()`\n`r solbegin(label=\"2 - Removing Delay*Group|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLets remove the interaction in the by-word random effects.  \nThis one took about 5 minutes...\n```{r}\n#| eval: false\nmod2 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay + Group | Test_word),\n             family=binomial, data=tel)\n```\n<p style=\"color:red;font-size:.8em\">\nWarning message:<br>\nIn checkConv(attr(opt, \"derivs\"), opt\\$par, ctrl = control\\$checkConv,  :<br>\n  Model failed to converge with max|grad| = 0.00887744 (tol = 0.002, component 1)\n</p>\n\n```{r}\nVarCorr(mod2)\n```\n\n`r solend()`\n`r solbegin(label=\"3 - Removing Group|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nWe still have a singular fit here. Thinking about the study, if we are going to remove __one__ of the by-testword random effects (`Delay` or `Group`), which one do we consider to be more theoretically justified? Is the effect of Delay likely to vary by test-words? More so than the effect of group is likely to vary by test-words?  \nQuite possibly - it's reasonable to think that some words will be more easily retained over a week than others, but I can't come up with a reason to think why `StudyTest` vs `StudyStudy` would have a different effect for some words than others.  \n\nLet's remove the by-testword random effect of group. \n```{r}\n#| eval: false\nmod3 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay | Test_word),\n             family=binomial, data=tel)\n```\n\nThis one converges! But we still have a correlation of -1.  \nWhy did we not get a warning message?  \n```{r}\nVarCorr(mod3)\n```\n\nThe `isSingular()` function allows us to check for when a model is close to singularity (i.e. on the boundary of the feasible parameter space):  \n```{r}\nisSingular(mod3)\n```\n\nThis suggests that we're fine! However, note that this function has a tolerance, which is by default set to 1e-4, or 0.0001. Change this to 1e-3, and it indicates a problem.  \n```{r}\nisSingular(mod3, tol=1e-3)\n```\n\nSo we're kind of in a grey area that suggests we are close to overfitting here, so it might be worth continuing to simplify.  \n\n\n`r solend()`\n`r solbegin(label=\"4 - Removing Delay|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nmod4 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 | Test_word),\n             family=binomial, data=tel)\n```\nEven when we raise the tolerance here, we're still fine:  \n```{r}\nisSingular(mod4, tol=1e-3)\n```\nAnd here's our random effects: \n```{r}\nVarCorr(mod4)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - Comparisons\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nIn the process of getting to our final model, we've just fitted quite a few models that didn't converge. We definitely don't want to do anything with these models (i.e. we shouldn't report them or use them in model comparisons etc).  \n\nHowever, it can sometimes be useful to just check how estimates of fixed effects and their standard errors differ across these models. More often than not, this simply provides us with reassurance that the removal of random effects hasn't actually had too much of an impact on anything we're going to conduct inferences on.  \n\nFor instance, in all these models, the fixed effects estimates are all pretty similar, suggesting that they've all found similar estimates of these parameters which have been largely invariant to our refinement of the random effects.  \n\n```{r}\n#| echo: false\nbind_rows(\n broom.mixed::tidy(mod1) |> filter(effect==\"fixed\") |> mutate(mod=\"mod1\"), \n broom.mixed::tidy(mod2) |> filter(effect==\"fixed\") |> mutate(mod=\"mod2\"), \n broom.mixed::tidy(mod3) |> filter(effect==\"fixed\") |> mutate(mod=\"mod3\"), \n broom.mixed::tidy(mod4) |> filter(effect==\"fixed\") |> mutate(mod=\"mod4\")\n) |> transmute(mod,term,\n               estimate = paste0(round(estimate,3), \" (\",round(std.error,3),\")\")) |>\n  pivot_wider(values_from=estimate,names_from=mod) |> gt::gt()\n```\n\n`r solend()` \n\n\n\n`r qbegin(qcounter())`\nCreate a plot of the predicted probabilities and uncertainty for each of the `Delay * Group` combinations.  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\neffplot <- effect(\"Delay:Group\", mod4) |>\n  as.data.frame()\n\nggplot(effplot, aes(Delay, fit, color=Group)) + \n  geom_pointrange(aes(ymax=upper, ymin=lower), \n                  position=position_dodge(width = 0.2))+\n  theme_classic() # just for a change :)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\nHere are odds ratios for our model:  \n```{r}\n#| code-fold: true\n# cbind combines columns\ncbind(\n  # the odds ratios:\n  OR = exp(fixef(mod4)), \n  # the CIs:\n  exp(confint(mod4, method=\"Wald\", parm=\"beta_\"))\n)\n```\n\nWhat should we do with this information? How should we apply test-enhanced learning to learning R and statistics?  \n`r qend()` \n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\nWe'll get the benefits of test-enhanced learning if we try these exercises before looking at any of the solutions that are visible! If we don't test ourselves, we're more likely to forget it in the long run.  \n`r solend()` \n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(lme4)\n```\n\n\n\n# Psychoeducation Treatment Effects\n\n```{r}\n#| eval: false\n#| echo: false\nsimm2<-function(seed=NULL,b0=0,b1=1,b2=1,z0=1,z1=1,e=1){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  n_groups = round(runif(1,1,15))*2\n  npg = 5\n  g = rep(1:n_groups, e = 5)      # the group identifier\n  x = rep(0:4,n_groups)\n  b = rep(0:1,e=n_groups/2)\n  b = b[g]\n  re0 = rnorm(n_groups, sd = z0)  # random intercepts\n  re  = re0[g]\n  rex = rnorm(n_groups, sd = z1)  # random effects\n  re_x  = rex[g]\n  lp = (b0 + re) + (b1 + re_x)*x + b2*x*b \n  y = rnorm(length(g), mean = lp, sd = e) # create a continuous target variable\n  # y_bin = rbinom(N, size = 1, prob = plogis(lp)) # create a binary target variable\n  data.frame(x, b=factor(b), g=factor(g), y)\n}\neseed = round(runif(1,1e3,1e6))\nset.seed(929918)\nbig = tibble(\n    school = 1:30,\n    int = rnorm(30,20,1),\n    sl = rnorm(30,-.3,.5),\n    intr = rnorm(30,-1,.5),\n    z0 = runif(30,.5,1.5),\n    z1 = runif(30,.5,1.5),\n    e = runif(30,.5,1)\n  )\n  big = big |> mutate(\n    data = pmap(list(int,sl,intr,z0,z1,e), ~simm2(b0=..1,b1=..2,b2=..3,z0=..4,z1=..5,e=..6))\n  ) |> unnest(data)\n\n  # m = lmer(round(y)~x*b+(1+x*b|school)+(1+x|school:g),big)\n  # broom.mixed::augment(m) |>\n  #   ggplot(aes(x=x,y=.fitted,col=factor(b)))+\n  #   geom_point(aes(y=`round(y)`))+\n  #   geom_line(aes(group=interaction(school,g)))\n\ntnames = unique(replicate(100,paste0(sample(LETTERS,2),collapse=\"\")))\n  \nbig <- big |> transmute(\n    therapist = tnames[school],\n    group = ifelse(b==0,\"Control\",\"Treatment\"),\n    patient = pmap_chr(list(therapist,group,g),~paste(..1,..2,..3,sep=\"_\")),\n    visit = x,\n    GAD = pmin(35,pmax(7,round(y)+5))\n  ) \n\nbig |> select(patient,visit,GAD) |>\n  pivot_wider(names_from=visit,values_from=GAD, names_prefix=\"visit_\") |>\n  write_csv(file=\"../../data/lmm_gadeduc.csv\")\n\n\n\n\n```\n\n\n\n:::frame\n__Data: gadeduc.csv__\n\n```{r}\n#| include: false\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\ngeduc1 = geduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |>\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n# m = lmer(GAD~visit*group+(1+visit*group|therapist)+(1+visit|therapist:patient),geduc1)\n# summary(m)\ntn = geduc1 |> group_by(therapist) |> summarise(np = n_distinct(patient))\n```\n\nThis is synthetic data from a randomised controlled trial, in which `r nrow(tn)` therapists randomly assigned patients (each therapist saw between `r min(tn[,'np'])` and `r max(tn[,'np'])` patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).  \n\nThe control group of patients received standard sessions offered by the therapists. \nFor the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).  \n\nThe data are available at [https://uoepsy.github.io/data/lmm_gadeduc.csv](https://uoepsy.github.io/data/lmm_gadeduc.csv){target=\"_blank\"}\n\nYou can find a data dictionary below:\n```{r}\n#| echo: false\n#| label: tbl-lmm_gadeduc.csv\n#| tbl-cap: \"Data Dictionary: lmm_gadeduc.csv\"\ntibble(\n    variable = names(geduc),\n    description = c(\"A patient code in which the labels take the form <Therapist initials>_<group>_<patient number>.\",\"Score on the GAD7 at baseline\", \n                    \"GAD7 at 1 month assessment\",\n                    \"GAD7 at 2 month assessment\",\n                    \"GAD7 at 3 month assessment\",\n                    \"GAD7 at 4 month assessment\"\n                    )\n)  |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nUh-oh... these data aren't in the same shape as the other datasets we've been giving you..  \n\nCan you get it into a format that is ready for modelling?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- It's wide, and we want it long.  \n- Once it's long. \"visit_0\", \"visit_1\",.. needs to become the numbers 0, 1, ...\n- One variable (`patient`) contains lots of information that we want to separate out. There's a handy function in the __tidyverse__ called `separate()`, check out the help docs!  \n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"1 - reshaping\", slabel=F,show=T, toggle=params$TOGGLE)`\nHere's the data. We have one row per patient, but we have multiple observations for each patient across the columns..  \n```{r}\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n```\n\nWe can make it long by taking the all the columns from `visit_0` to `visit_4` and shoving their values into one variable, and keeping the name of the column they come from as another variable:  \n```{r}\ngeduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n```\n\n\n`r solend()`\n`r solbegin(label=\"2 - time is numeric\", slabel=F,show=T, toggle=params$TOGGLE)`\nNow we know how to get our data long, we need to sort out our time variable (`visit`) and make it into numbers.  \nWe can replace all occurrences of the string `\"visit_\"` with nothingness `\"\"`, and then convert them to numeric.  \n\n```{r}\ngeduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n```\n\n\n`r solend()`\n`r solbegin(label=\"3 - splitting up the patient variable\", slabel=F,show=T, toggle=params$TOGGLE)`\nFinally, we need to sort out the `patient` variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore \"_\".  \n\nThe `separate()` function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:  \n```{r}\ngeduc_long <- geduc |> \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |>\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |>\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n```\n\nAnd we're ready to go!  \n```{r}\ngeduc_long\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nVisualise the data. Does it look like the treatment had an effect?  \nDoes it look like it worked for every therapist?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- remember, `stat_summary()` is very useful for aggregating data inside a plot.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere's the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective.. \n```{r}\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n```\n\nLet's split this up by therapist, so we can see the averages across each therapist's set of patients.  \nThere's clear variability between therapists in how well the treatment worked. For instance, the therapists `EU` and `OD` don't seem to have much difference between their groups of patients.\n```{r}\n#| out-width: \"100%\"\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.  \n`r qend()`\n`r solbegin(label=\"1 - fixed effects\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe want to know if how anxiety (`GAD`) changes over time (`visit`) is different between treatment and control (`group`).  \n\nHopefully this should hopefully come as no surprise^[if it does, head back to where we learned about interactions in the single level regressions `lm()`. It's just the same here.] - it's an interaction!  \n\n\n```{r}\n#| eval: false\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n```\n\n`r solend()`\n`r solbegin(label=\"2 - grouping structure\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe have multiple observations for each of the `r sum(tn$np)` patients, and those patients are nested within `r nrow(tn)` therapists.  \n\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient \"1\" from therapist \"AO\" is a different person from patient \"1\" from therapist \"BJ\". To correctly group the observations into different patients (and not 'patient numbers'), we need to have `therapist:patient`.  \n\nSo we capture therapist-level differences in `( ... | therapist)` and the patients-within-therapist-level differences in `( ... | therapist:patient)`:  \n\n```{r}\n#| eval: false\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n```\n\n\n`r solend()`\n`r solbegin(label=\"3 - random effects\", slabel=F,params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nNote that each patient can change differently in their anxiety levels over time - i.e. the slope of `visit` could vary by participant.  \n\nLikewise, some therapists could have patients who change differently from patients from another therapist, so `visit|therapist` can be included.  \n\nEach patient is in one of the two groups - they're _either_ treatment _or_ control. So we can't say that \"differences in anxiety due to treatment varies between patients\", because for any one patient the \"difference in anxiety due to treatment\" is not defined in our study design.  \n\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect _could_ be different for different therapists!  \n\n```{r}\nmod1 <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nFor each of the models below, what is wrong with the random effect structure?  \n\n```{r}\n#| eval: false\nmodelA <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n```\n\n```{r}\n#| eval: false\nmodelB <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n```\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nmodelA <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n```\n\nThe `patient` variable doesn't capture the different patients _within_ therapists, so this actually fits crossed random effects and treats all data where `patient==1` as from the same group (even if this includes several different patients' worth of data from different therapists!)\n\n```{r}\n#| eval: false\nmodelB <- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n```\n\nUsing the `/` here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can't vary by patient, so this doesn't work. hence why we need to split them up into `(...|therapist)+(...|therapist:patient)`.  \n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nLet's suppose that I don't want the psychoeducation treatment, I just want the standard therapy sessions that the 'Control' group received. Which therapist should I go to?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`dotplot.ranef.mer()` might help here!  \nYou can read about `ranef` in [Chapter 2 #making-model-predictions](https://uoepsy.github.io/lmm/02_lmm.html#making-model-predictions){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nIt would be best to go to one of the therapists `SZ`, `YS`, or `IT`...  \n\nWhy? These therapists all have the most negative slope of visit:  \n\n```{r}\n#| fig-height: 6\ndotplot.ranef.mer(ranef(mod1))$therapist\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nRecreate this plot.  \n\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.  \n\n```{r} \n#| echo: false\neffplot <- effects::effect(\"visit*group\",mod1) |>\n  as.data.frame()\n\nbroom.mixed::augment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient)\n  ) |>\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")\n\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- you can get the patient-specific lines using `augment()` from the __broom.mixed__ package, and the fixed effects estimates using the __effects__ package. \n- remember that the \"patient\" column doesn't group observations into unique patients. \n- remember you can pull multiple datasets into ggplot:  \n```{r}\n#| eval: false\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n```\n- see more in [Chapter 2 #visualising-models](https://uoepsy.github.io/lmm/02_lmm.html#visualising-models){target=\"_blank\"}\n\n\n:::\n\n\n`r qend()`\n`r solbegin(label=\"1 - the relevant parts\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nThe __effects__ package will give us the fixed effect estimates: \n```{r}\nlibrary(effects)\nlibrary(broom.mixed)\neffplot <- effect(\"visit*group\",mod1) |>\n  as.data.frame()\n```\n\nWe want to get the fitted values for each patient. We can get fitted values using `augment()`. But the `patient` variable doesn't capture the _unique_ patients, it just captures their numbers (which aren't unique to each therapist).  \nSo we can create a new column called `upatient` which pastes together the therapists initials and the patient numbers\n\n```{r}\naugment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n```\n`r solend()`\n`r solbegin(label=\"2 - constructing the plot\", slabel=F,show=T, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\nlibrary(broom.mixed)\neffplot <- effect(\"visit*group\",mod1) |>\n  as.data.frame()\n\naugment(mod1) |> \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |>\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")\n```\n\n\n`r solend()`\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Jokes\n\n```{r}\n#| eval: false\n#| echo: false\n#set.seed()\njokeid = read_csv(\"data/laughlab_jokes.csv\")\n\nset.seed(123)\nn_groups = 90\nN = n_groups * nrow(jokeid)\ng = rep(1:n_groups, e = nrow(jokeid))\nb = rbinom(n_groups,1,.5)[g]\nb = unlist(lapply(1:n_groups, \\(x) sample(rep(0:1,e=15))))\nj = rep(1:nrow(jokeid), n_groups)\n\ng.re0 = rnorm(n_groups,0,8)[g]\ng.re = MASS::mvrnorm(n_groups, mu = c(0,0), Sigma = matrix(c(8,1,1,3),nrow=2))\ng.re0 = g.re[,1][g]\ng.reb = g.re[,2][g]\n\nj.re = MASS::mvrnorm(nrow(jokeid), mu = c(0,0), Sigma = matrix(c(5,2,2,5),nrow=2))\nj.re0 = j.re[,1][j]\nj.reb = j.re[,2][j]\nb0 = runif(1,30,55)\nb1 = runif(1,1,3)\nlp = b0 + g.re0 + j.re0 +\n  (b1 + j.reb + g.reb)*b\ny = rnorm(N, mean = lp, sd = 6)\n#y_bin = rbinom(N, size = 1, prob = plogis(lp))\ndf=data.frame(g = factor(g),j = factor(j),b,y)\n\ndf = left_join(df,\n          left_join(jokeid, \n                    tibble(\n                      rank = order(j.re[,1],decreasing = TRUE),\n                      j = factor(1:30)\n                      )\n          ) |> dplyr::select(joke, j)\n)\n\nggplot(df,aes(x=b,y=y,group=g))+\n  geom_point(alpha=.1,size=.5)+\n  stat_summary(geom=\"pointrange\",position=position_dodge(width=.2))\n  \n\nlm(y~b,df)\nm = lmer(y~1+b+(1|g)+(1+b|j),df)\nsummary(m)\n\n\nnames(df)\n\nlaughs <- df |> transmute(\n  ppt = paste0(\"PPTID\",g),\n  joke_label = joke,\n  joke_id = j,\n  delivery = case_when(\n    b == 0 ~ \"audio\",\n    b == 1 ~ \"video\"\n  ),\n  rating = round(y)\n)\n\nwrite_csv(laughs, \"../../data/lmm_laughs.csv\")\n```\n\n:::frame\n__Data: lmm_laughs.csv__  \n\n```{r}\n#| include: false\nlaughs <- read_csv(\"https://uoepsy.github.io/data/lmm_laughs.csv\")\n```\n\nThese data are simulated to imitate an experiment that investigates the effect of visual non-verbal communication (i.e. gestures, facial expressions) on joke appreciation. \n`r n_distinct(laughs$ppt)` Participants took part in the experiment, in which they each rated how funny they found a set of `r n_distinct(laughs$joke_id)` jokes. For each participant, the order of these `r n_distinct(laughs$joke_id)` jokes was randomly set for each run of the experiment. For each participant, the set of jokes was randomly split into two halves, with the first half being presented in audio-only, and the second half being presented in audio and video. This meant that each participant saw `r n_distinct(laughs$joke_id)/2` jokes with video and `r n_distinct(laughs$joke_id)/2` without, and each joke would be presented in with video roughly half of the times it was seen.  \n\nThe researchers want to investigate whether the delivery (audio/audiovideo) of jokes is associated with differences in humour-ratings.  \n\nData are available at [https://uoepsy.github.io/data/lmm_laughs.csv](https://uoepsy.github.io/data/lmm_laughs.csv){target=\"_blank\"}\n\n```{r}\n#| echo: false\n#| label: tbl-laughdict\n#| tbl-cap: \"Data Dictionary: lmm_laughs.csv\"\ntibble(\n  variable = names(laughs),\n  description = c(\"Participant Identification Number\",\n                  \"Joke presented\",\n                  \"Joke Identification Number\",\n                  \"Experimental manipulation: whether joke was presented in audio-only ('audio') or in audiovideo ('video')\",\n                  \"Humour rating chosen on a slider from 0 to 100\")\n) |> gt::gt()\n```\n\n\n\n:::\n\n\n\n`r qbegin(qcounter())`\nPrior to getting hold of any data, we should be able to write out the structure of our ideal \"maximal\" model given that we have a description of the design of the study.  \n\nCan you do so?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nDon't know where to start? Try following the steps in [Chapter 8 #maximal-model](https://uoepsy.github.io/lmm/08_modelbuilding.html#maximal-model){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe want to estimate the effect of delivery on humour rating of jokes:  \n`rating ~ delivery`  \n\nWe have 30 observations for each participant. Participants are just another sampling unit here.  \n`rating ~ delivery + (1 | ppt)`  \n\nWe have 90 observations for each joke. We're not interested in specific jokes here, so we can think of these as a random set of experimental items that we might choose differently next time we conduct an experiment to assess delivery~rating:\n`rating ~ delivery + (1 | ppt) + (1 | joke_id)`  \n\nParticipants each see 15 jokes without video, and 15 with. The `delivery` variable is *\"within\"* participant. Some participants might respond differently when there is video (vs without) whereas some might not rate jokes any differently. The effect of delivery on rating might be *vary by participant*:  \n`rating ~ delivery + (1 + delivery | ppt) + (1 | joke_id)`  \n\nEach joke is presented both with and without the video. Some jokes might really benefit from gestures and facial expressions, whereas some might not. The effect of delivery on rating might be *vary by joke*:  \n`rating ~ delivery + (1 + delivery | ppt) + (1 + delivery | joke_id)`  \n\n`r solend()`\n\n`r qbegin(qcounter())`\nRead in and clean the data (if necessary).  \n\nCreate some plots showing: \n\n1. the average rating for audio vs audio+video for each joke\n2. the average rating for audio vs audio+video for each participant\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n- you could use `facet_wrap`, or even `stat_summary`!  \n- you might want to use `joke_id`, rather than `joke_label` (the labels are very long!)\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere is one using `facet_wrap`:\n```{r}\nggplot(laughs, aes(x = delivery, y = rating)) +\n  geom_boxplot()+\n  facet_wrap(~joke_id)\n```\n\nAnd one using `stat_summary()` for participants: \n```{r}\nggplot(laughs, aes(x = delivery, y = rating)) +\n  stat_summary(geom=\"pointrange\", aes(group = ppt),\n               position = position_dodge(width=.2))+\n  stat_summary(geom=\"line\", aes(group = ppt),\n               position = position_dodge(width=.2))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nFit an appropriate model to address the research aims of the study. \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThis should be the one you came up with a couple of questions ago!\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nmod <- lmer(rating ~ delivery + \n              (1 + delivery | joke_id) +\n              (1 + delivery| ppt), data = laughs)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nWhich joke is funniest when presented just in audio? For which joke does the video make the most difference to ratings?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThese can all be answered by examining the random effects with `ranef()`.  \nSee [Chapter 2 #making-model-predictions](https://uoepsy.github.io/lmm/02_lmm.html#making-model-predictions){target=\"_blank\"}.  \n\nIf you're using `joke_id`, can you find out the actual joke that these correspond to?\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\ndotplot.ranef.mer(ranef(mod))$joke_id\n```\n\nJoke 19 is the funniest apparently! (not sure I agree)\n\nLots of ways to find what the joke actually is. Here is one way:\n```{r}\nlaughs |> count(joke_id, joke_label) |>\n  filter(joke_id==19) |>\n  pull(joke_label)\n```\n\n\nAnd from the plot above, Joke 28 has the most benefit of video. \nWe can quickly check this with something like:  \n```{r}\nranef(mod)$joke_id |>\n  filter(deliveryvideo == max(deliveryvideo))\n```\n\nThe joke itself is a bit weird.. maybe the video really helped!  \n\n```{r}\nlaughs |> count(joke_id, joke_label) |>\n  filter(joke_id==28) |>\n  pull(joke_label)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDo jokes that are rated funnier when presented in audio-only tend to also benefit more from the addition of video?  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThink careful about this question. The random effects show us that jokes vary in their intercepts (ratings in audio-only) and in their effects of delivery (the random slopes). We want to know if these are related..  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nVarCorr(mod)\n```\nIt's the correlation here that tell us - jokes rated higher in the audio-only tend to have a bigger effect of the video. \n\nWe can see this in a plot if we like. Here every dot is a joke, and the x-axis shows whether it is above or below the average rating for audio-only (the intercept). The y-axis shows whether it is above or below the average effect of video. \n```{r}\nplot(ranef(mod)$joke)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nCreate a plot of the estimated effect of video on humour-ratings. Try to plot not only the fixed effects, but the raw data too.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nSee e.g. [Chapter 2 #visualising-models](https://uoepsy.github.io/lmm/02_lmm.html#visualising-models){target=\"_blank\"}\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\n\nplotdatf <- effect(\"delivery\",mod) |>\n  as.data.frame()\n\nggplot(data = laughs, aes(x = delivery)) +\n  geom_jitter(aes(y = rating), width = .1, height = 0, alpha = .1) +\n  geom_pointrange(data = plotdatf,\n                  aes(y = fit, ymin = lower, ymax = upper),\n                  position=position_nudge(x=.2))\n```\n\n`r solend()`\n\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Optional Extra: Vocab Development\n\n:::frame\n__Data: pvt_bilingual.csv__ \n\n```{r}\n#| echo: false\npvt <- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\npvtsc = pvt |>\n  group_by(school, child) |>\n  mutate(n_obs = 1:n()) |>\n  ungroup() |>\n  mutate(child = ifelse(n_obs>7, paste0(child,\" 2\"), child)) |> count(school,child,isBilingual)\n```\n\n`r nrow(pvtsc)` children from `r length(unique(pvt$school))` schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). `r sum(pvtsc$isBilingual==0)` were monolingual English speakers, and `r sum(pvtsc$isBilingual==1)` were bilingual (english + another language). \n\nPrevious research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. The aim of the present study is to investigate differences in the development of vocabulary between monolingual and bilingual children.  \n\nThe data are available at [https://uoepsy.github.io/data/pvt_bilingual.csv](https://uoepsy.github.io/data/pvt_bilingual.csv).  \n\n```{r}\n#| echo: false\n#| label: tbl-pvtdict\n#| tbl-cap: \"Data Dictionary: pvt_bilingual.csv\"\ntibble(variable = names(pvt),\n       description = c(\n         \"Child's name\",\n         \"School Identifier\",\n         \"Binary variable indicating whether the child is monolingual (0) or bilingual (1)\",\n         \"Age (years)\",\n         \"Score on the Picture Vocab Test (PVT). Scores range 0 to 60\")\n) |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n`r qbegin(paste0(qcounter(), \" - Less Guided\"))`\nConduct an analysis to estimate the differences in trajectories of vocabulary development between children attending bilingual schools vs those attending monolingual schools.  \n\nWrite up your results.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n1. always plot your data!\n2. read the study background: \"increase steadily ... before beginning to plateau\" describes a _curve_!  \n3. plotting the data can give an initial sense of the possible need for higher order polynomials.  \n4. multiple observations for each child. multiple children in each school.. \n\n:::\n\n\n`r qend()`\n\n`r solbegin(label=\"1 - initial data checks\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLet's read in the data:  \n```{r}\npvt <- read_csv(\"https://uoepsy.github.io/data/pvt_bilingual.csv\")\nhead(pvt)\n```\nFirst, we're going to want `isBilingual` to be a factor, because the 1s and 0s are categories, not numbers. \n```{r}\npvt <- pvt |> mutate(isBilingual = factor(isBilingual))\n```\n\n\nWe have 30 distinct schools:\n```{r}\nn_distinct(pvt$school)\n```\n\nAnd 418 distinct children. Is that right?  \n```{r}\nn_distinct(pvt$child)\n```\n\nGiven that the `pvt$child` variable is just the first name of the child, it's entirely likely that there will be, for instance more than one \"Martin\".  \n\nThis says that there are 487!  \n```{r}\npvt |> count(school, child) |> nrow()\n```\n\nBut wait... we could _still_ have issues. What if there were 2 \"Martin\"s at the same school??\n```{r}\npvt |> \n  # count the school-children groups\n  count(school, child) |> \n  # arrange the output so that the highest \n  # values of the 'n' column are at the top\n  arrange(desc(n))\n```\n\nAha! There are 7 cases where schools have two children of the same name. Remember that each child was measured at 7 timepoints. We shouldn't have people with 14!  \n\nIf we actually _look_ at the data, we'll see that it is very neatly organised, with each child's data together. This means that we could feasibly make an educated guess that, e.g., the \"Jackson\" from \"School 3\" in rows 155-161 is different from the \"Jackson\" from \"School 3\" at rows 190-196. \n\nBecause of the ordering of our data, we can do something like this:  \n```{r}\npvt <- \n  pvt |>\n  # group by the school and child\n  group_by(school, child) |>\n  mutate(\n    # make a new variable which counts from 1 to \n    # the number of rows for each school-child\n    n_obs = 1:n()\n  ) |>\n  # ungroup the data\n  ungroup() |>\n  mutate(\n    # change it so that if the n_obs is >7, the \n    # child becomes \"[name] 2\", to indicate they're the second\n    # child with that name\n    child = ifelse(n_obs>7, paste0(child,\" 2\"), child)\n  )\n```\n\nNow we have 494!  \n```{r}\npvt |> count(school, child) |> nrow()\n```\n\nAnd nobody has anything other than 7 observations!\n```{r}\npvt |> count(school, child) |>\n  filter(n != 7)\n```\n\nPhew!  \n`r solend()`\n`r solbegin(label=\"2 - exploratory plots and descriptives\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nHow many bilingual children?  \n```{r}\npvt |> group_by(isBilingual) |>\n  summarise(nchild = n_distinct(child))\n```\n\nHow many children (mono/bilingual) in each school? \n```{r}\npvt |> group_by(school, isBilingual) |>\n  summarise(nchild = n_distinct(child)) |>\n  ggplot(aes(y=school,x=nchild,fill=isBilingual))+\n  geom_col()\n```\n\n\n\nOkay, let's just fit an intercept-only model:  \n```{r}\npvt_null <- lmer(PVT ~ 1 + \n                   (1 | school) +\n                   (1 | school:child), data = pvt)\nsummary(pvt_null)\n```\n\n```{r}\n#| echo: false\nvcres = VarCorr(pvt_null) |> as.data.frame()\nvcres = round(vcres$vcov,2)\n```\n\n\nAs we can see from `summary(pvt_null)`, the random intercept variances are `r vcres[1]` for child-level, `r vcres[2]` for school-level, and the residual variance is `r vcres[3]`.\n\nSo child level differences account for $\\frac{`r vcres[1]`}{`r paste0(vcres,collapse=\" + \")`} = `r round(vcres[1]/sum(vcres),2)`$ of the variance in PVT scores, and child & school differences together account for $\\frac{`r paste0(vcres[1:2],collapse=\" + \")`}{`r paste0(vcres,collapse=\" + \")`} = `r round(sum(vcres[1:2])/sum(vcres),2)`$ of the variance.\n\nHere's an initial plot too:\n\n```{r}\nggplot(pvt, aes(x=age,y=PVT,col=factor(isBilingual)))+\n  stat_summary(geom=\"pointrange\")+\n  stat_summary(geom=\"line\")\n```\n\n`r solend()`\n`r solbegin(label=\"3 - modelling\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nI feel like either raw or orthogonal polynomials would be fine here - there's nothing explicit from the study background about stuff \"at baseline\". There's the stuff about the plateau at 7 or 8, but we can get that from the model plots. Orthogonal will allow us to compare the trajectories overall (their linear trend, the 'curviness' and 'wiggliness').  \n\nAn additional benefit of orthogonal polynomials is that we are less likely to get singular fits when we include polynomial terms in our random effects. Remember, raw polynomials are correlated, so often the by-participant variances in raw poly terms are highly correlated. \n\nI've gone for 3 degrees of polynomials here because the plot above shows a bit of an S-shape for the bilinguals. \n\n```{r}\npvt <- pvt |> mutate(\n  poly1 = poly(age, 3)[,1],\n  poly2 = poly(age, 3)[,2],\n  poly3 = poly(age, 3)[,3],\n)\n```\n\nThese models do not converge.  \nI've tried to preserve the by-child random effects of time, because while I think Schools probably _do_ vary, all the schools teach the same curriculum, whereas there's _a lot_ of varied things that can influence a child's vocabulary, both in and out of school.    \n```{r}\n#| eval: false\nmod1 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + (poly1 + poly2 + poly3)*isBilingual | school) + \n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod2 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * (poly1 + poly2) + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod3 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual * poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod4 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly2 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nmod5 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + isBilingual + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# relative to the variance in time slopes, there's v little by-school variance in bilingual differences in vocab\n\nmod6 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly2 +  poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n# looks like curvature doesn't vary between schools much as linear and wiggliness \n```\n\nthis one converges! \n```{r}\nmod7 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n```\n\n\n`r solend()`\n`r solbegin(label=\"4 - model plots\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLet's plot the predictions: \n```{r}\nlibrary(broom.mixed)\naugment(mod7) |> \n  mutate(\n    poly1 = round(poly1, 3) # because of rounding errors that make plot weird\n  ) |>\n  ggplot(aes(x=poly1,col=isBilingual))+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n```\n\nRemember that these are just the average of all model fitted values. Because everyone has complete data here, we can use these to visualise our model estimates without worrying about them being distorted by missing data. Just like the fixed effects themselves though, all this is focussed on \"the average child from the average school\". The plot hides a lot of the *variability* that we have actually modelled. \nFor instance, consider how our view changes when we add in the individual lines for each child (below). The lines for the average child are just the same as above, but we can now see just how much children vary. \n\n```{r}\n#| code-fold: true\naugment(mod7) |> \n  mutate(\n    poly1 = round(poly1, 3) \n  ) |>\n  ggplot(aes(x=poly1,col=isBilingual))+\n  # make a line for each \"school-child\" group: \n  geom_line(aes(group=interaction(school,child),\n                y=.fitted),alpha=.1)+\n  stat_summary(geom=\"pointrange\",aes(y=PVT))+\n  stat_summary(geom=\"line\", aes(y=.fitted))\n```\n  \nLet's refit our model with lmerTest:  \n```{r}\nmod7 <- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\nsummary(mod7)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - model interpretation\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nThe exact value of the coefficients here don't really give us anything meaningful. Note that in our plots above, the x-axis is \"poly1\", which is some weirdly re-scaled version of \"age\", so these coefficients represent things like the estimated change in PVT when \"moving up 1 in poly1\". But these units are not remotely useful to us. Even if we wanted to try and scale back to get out something we could interpret in terms of years of age, the whole point of non-linear change is that we can't just say \"PVT increases by $b$ with every extra year of age\", because we need to consider the higher order polynomial terms.  \n\nThe long and the short of it is that while the numbers don't really matter here, the sign (+ or -) and the significance _does_ tell us stuff:  \n\n```{r}\n#| echo: false\nmm <- lmerTest::lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + \n       (1 + poly1 + poly3 | school) +\n       (1 + (poly1 + poly2 + poly3) | school:child),\n     data = pvt)\n\ntidy(mm) |> filter(effect==\"fixed\") |> transmute(\n  term,\n  est = round(estimate,2),\n  p = ifelse(p.value<.05,\"*\",\"\"),\n  interpretation = c(\n    \"average vocab score at the mean age (for monolingual)\",\n    \"vocab increases over time (for monolingual children)\",\n    \"the increase in vocab becomes more gradual (for monolingual children)\",\n    \"no significant wiggliness to vocab trajectory of the average monolingual child\",\n    \"average vocab score at mean age is lower for bilingual vs monolingual children\",\n    \"no significant difference in linear trend of vocab for bilingual vs monolingual\",\n    \"curvature for vocab trajectory of bilingual children significantly differs from that of monolinguals\",\n    \"wiggliness for vocab trajectory of bilingual children significantly differs from that of monolinguals\"\n  )\n) |> gt::gt()\n```\n\nFrom the random effects, we get even more information! All this stuff is kind of \"added context\" for the conclusions (which will come from our fixed effects).  \n```{r}\nVarCorr(mod7)\n```\nSchool's vary in the average child vocab score at mean age with an SD of 4.5. Schools with higher vocab scores at the mean age tend to have lower linear increase. Within schools, children vary in the vocab scores at mean age with an SD of 5.9. Children with steeper linear increase in vocab tend to have less curvature. \n\n\n\n\n<!-- ::: {.callout-caution collapse=\"true\"} -->\n<!-- #### optional extra: plotting uncertainty of fixed effects -->\n\n<!-- We've seen a few times that taking all the fitted values and averaging can give us the trajectories. But we've also seen that this can cause issues if we have missing data, or unbalanced designs. We have also had to use things like the __effects__ package to show uncertainty in the fixed effects estimates, because this is not the same as the standard errors around the average fitted values.   -->\n\n<!-- Another option instead of the __effects__ package (harder to make work with polynomials), is to use lots and lots of predictions from the model.   -->\n\n<!-- In USMR, we often visualised our model in a 3 step process: -->\n\n<!-- 1. make a little dataframe across the values that we want to plot -->\n<!-- 2. use `broom::augment(model, newdata = plotdata, interval = \"confidence\")` to add some predictions and confidence intervals.   -->\n<!-- 3. plot!   -->\n\n<!-- We can use this same logic in multilevel models, but step 2 is a little different.   -->\n\n\n<!-- First we make a little dataframe.   -->\n<!-- Because the actual estimation of orthogonal polynomials can depend on the number of values we're putting in (`poly(4:10, 2)` does not give the same scaling as `poly(c(4:10,4:10),2)`), it's best to just capture the polynomials that we actually fitted our model to.   -->\n<!-- So here we get the first 7 rows of each poly1, because those correspond to our polynomials that we fitted the model to.   -->\n<!-- ```{r} -->\n<!-- plotdat <- -->\n<!--   tibble( -->\n<!--     age = 4:10, -->\n<!--     poly1 = pvt$poly1[1:7], -->\n<!--     poly2 = pvt$poly2[1:7], -->\n<!--     poly3 = pvt$poly3[1:7], -->\n<!--     isBilingual = \"0\", -->\n<!--     school = \".\", -->\n<!--     child = \".\" -->\n<!--   ) -->\n<!-- ``` -->\n\n<!-- We're going to have to give it _some_ values for `school` and for `child`, but we don't want to predict for any specific school or child that is in our dataset.  -->\n<!-- By setting these to something like \".\", that doesn't occur in our data, we can estimate based on the fixed coefficients only.   -->\n\n<!-- Note that currently `isBilingual` is set to 0, but we want to plot for the bilinguals too, so we'll need to duplicate our `plotdat` object:    -->\n<!-- ```{r} -->\n<!-- plotdat <- -->\n<!--   bind_rows( -->\n<!--     plotdat, -->\n<!--     plotdat |> mutate(isBilingual=1) -->\n<!--   ) -->\n<!-- ``` -->\n\n<!-- Finally, the __merTools__ package has the `predictInterval` function that can get us some intervals   -->\n<!-- ```{r} -->\n<!-- library(merTools) -->\n\n<!-- bind_cols( -->\n<!--   plotdat,  -->\n<!--   predictInterval(merMod = mod7, newdata = plotdat, -->\n<!--                   level = 0.95, n.sims = 2000, -->\n<!--                   include.resid.var = FALSE) -->\n<!--           ) |> -->\n<!--   ggplot(aes(x=poly1,y=fit,ymin=lwr,ymax=upr, -->\n<!--              col=isBilingual,fill=isBilingual))+ -->\n<!--   geom_line()+ -->\n<!--   geom_ribbon(alpha=.2) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n\n\n\n`r solend()`\n\n\n<br>\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Optional Extra: Test Enhanced Learning\n\n:::frame\n__Data: Test-enhanced learning__  \n\nAn experiment was run to conceptually replicate \"test-enhanced learning\" (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (`StudyStudy`), the other group studied the material once then did a test (`StudyTest`). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (`Test_word`). \n\nThe critical (replication) prediction is that the `StudyStudy` group recall more items on the immediate test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.  \n\nThe following code loads the data into your R environment by creating a variable called `tel`:\n\n```{r}\n#| eval: false\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n```\n\n```{r} \n#| echo: false\n#| label: tbl-teldict\n#| tbl-cap: \"Data Dictionary: testenhancedlearning.Rdata\"\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\ntibble(\n  variable=names(tel),\n  description=c(\"Unique Participant Identifier\", \"Group denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\",\"Time of recall test ('min' = Immediate, 'week' = One week later)\",\"Word being recalled (175 different test words)\",\"Whether or not the word was correctly recalled\",\"Time to recall word (milliseconds)\")\n) |>\n    kableExtra::kbl() |>\n    kableExtra::kable_styling(full_width = FALSE)\n```\n\n:::\n\n\n`r qbegin(qcounter())`\nLoad and plot the data. Does it look like the effect was replicated?  \n\n> The critical (replication) prediction is that the `StudyStudy` group recall more items on the immediate test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWe can actually look at this from a couple of different angles. The most obvious option is to take successful learning as \"correctly recalling\" an item. This means we take the `Correct` variable as our outcome.  \n\nNote we also have `Rtime` - the \"time-to-recall\" of an item. This could also work as an outcome, but note that it also includes the time it took participants to provide an incorrect response too. If this was your own project, you may well want to provide analyses of `Correct`, and then also of the time-taken, but on the subset of correctly recalled items.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\nggplot(tel, aes(Delay, Correct, col=Group)) + \n  stat_summary(fun.data=mean_se, geom=\"pointrange\")+\n  theme_light()\n```\n\nThat looks like test-enhanced learning to me!  \n\n\n`r solend()`\n\n`r qbegin(qcounter())`\n\nTest the critical hypothesis using a mixed-effects model.  \n\nFit the maximal random effect structure supported by the experimental design. Simplify the random effect structure until you reach a model that converges.  \n\n__Note:__ Some of the models you attempt here might take time to fit. This is normal, and you can cancel the estimation at any time by pressing the escape key.  \nI suggest that you write your initial model, set it running, and then look at the first solution to see if it converged for me. You can assume that if it didn't work for me, it also won't work for you. I've shown the random effects for each model, in case it helps in deciding your next step.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWhat we're aiming to do here is to follow [Barr et al.'s](https://doi.org/10.1016/j.jml.2012.11.001) advice of defining our maximal model and then removing only the terms to allow a non-singular fit.  \n\n+ What kind of model will you use? What is our outcome? is it binary, or continuous? \n+ We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects?\n\n:::\n\n`r qend()` \n`r solbegin(label=\"1 - maximal model\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\n# save(mod1,mod2,mod3,mod4,file=\"data/telmodels.Rdata\")\nload(\"data/telmodels.Rdata\")\n```\n\nWe have a __crossed__ random effect structure here. Each participant was tested on every word, and each word was seen by every participant.  \n\nSubjects were tested at both the `min` and `week`, so `Delay|Subject_ID` can be included (some people might retain the items better than others).  \nLikewise the words were seen at both `min` and `week`, and some words might be more easy to retain than others (`Delay|Test_word`).  \n\nThe participants are in either one group or another, so we can't have `Group|Subject_ID`. However, the words were seen by people in both groups, so we _can_ have `Group|Test_word`.  \n\nThis one took my computer about 6 minutes.  \n```{r}\n#| eval: false\nmod1 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay * Group | Test_word),\n             family=binomial, data=tel)\n```\n<p style=\"color:red;font-size:.8em\">\nWarning message:<br>\nIn checkConv(attr(opt, \"derivs\"), opt\\$par, ctrl = control\\$checkConv,  :<br>\n  Model failed to converge with max|grad| = 0.0184773 (tol = 0.002, component 1)\n</p>\n\n```{r}\nVarCorr(mod1)\n```\n\n`r solend()`\n`r solbegin(label=\"2 - Removing Delay*Group|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nLets remove the interaction in the by-word random effects.  \nThis one took about 5 minutes...\n```{r}\n#| eval: false\nmod2 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay + Group | Test_word),\n             family=binomial, data=tel)\n```\n<p style=\"color:red;font-size:.8em\">\nWarning message:<br>\nIn checkConv(attr(opt, \"derivs\"), opt\\$par, ctrl = control\\$checkConv,  :<br>\n  Model failed to converge with max|grad| = 0.00887744 (tol = 0.002, component 1)\n</p>\n\n```{r}\nVarCorr(mod2)\n```\n\n`r solend()`\n`r solbegin(label=\"3 - Removing Group|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nWe still have a singular fit here. Thinking about the study, if we are going to remove __one__ of the by-testword random effects (`Delay` or `Group`), which one do we consider to be more theoretically justified? Is the effect of Delay likely to vary by test-words? More so than the effect of group is likely to vary by test-words?  \nQuite possibly - it's reasonable to think that some words will be more easily retained over a week than others, but I can't come up with a reason to think why `StudyTest` vs `StudyStudy` would have a different effect for some words than others.  \n\nLet's remove the by-testword random effect of group. \n```{r}\n#| eval: false\nmod3 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 + Delay | Test_word),\n             family=binomial, data=tel)\n```\n\nThis one converges! But we still have a correlation of -1.  \nWhy did we not get a warning message?  \n```{r}\nVarCorr(mod3)\n```\n\nThe `isSingular()` function allows us to check for when a model is close to singularity (i.e. on the boundary of the feasible parameter space):  \n```{r}\nisSingular(mod3)\n```\n\nThis suggests that we're fine! However, note that this function has a tolerance, which is by default set to 1e-4, or 0.0001. Change this to 1e-3, and it indicates a problem.  \n```{r}\nisSingular(mod3, tol=1e-3)\n```\n\nSo we're kind of in a grey area that suggests we are close to overfitting here, so it might be worth continuing to simplify.  \n\n\n`r solend()`\n`r solbegin(label=\"4 - Removing Delay|Test_word\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#| eval: false\nmod4 <- glmer(Correct ~ Delay*Group +\n             (1 + Delay | Subject_ID) +\n             (1 | Test_word),\n             family=binomial, data=tel)\n```\nEven when we raise the tolerance here, we're still fine:  \n```{r}\nisSingular(mod4, tol=1e-3)\n```\nAnd here's our random effects: \n```{r}\nVarCorr(mod4)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - Comparisons\", slabel=F,show=TRUE, toggle=params$TOGGLE)`\n\nIn the process of getting to our final model, we've just fitted quite a few models that didn't converge. We definitely don't want to do anything with these models (i.e. we shouldn't report them or use them in model comparisons etc).  \n\nHowever, it can sometimes be useful to just check how estimates of fixed effects and their standard errors differ across these models. More often than not, this simply provides us with reassurance that the removal of random effects hasn't actually had too much of an impact on anything we're going to conduct inferences on.  \n\nFor instance, in all these models, the fixed effects estimates are all pretty similar, suggesting that they've all found similar estimates of these parameters which have been largely invariant to our refinement of the random effects.  \n\n```{r}\n#| echo: false\nbind_rows(\n broom.mixed::tidy(mod1) |> filter(effect==\"fixed\") |> mutate(mod=\"mod1\"), \n broom.mixed::tidy(mod2) |> filter(effect==\"fixed\") |> mutate(mod=\"mod2\"), \n broom.mixed::tidy(mod3) |> filter(effect==\"fixed\") |> mutate(mod=\"mod3\"), \n broom.mixed::tidy(mod4) |> filter(effect==\"fixed\") |> mutate(mod=\"mod4\")\n) |> transmute(mod,term,\n               estimate = paste0(round(estimate,3), \" (\",round(std.error,3),\")\")) |>\n  pivot_wider(values_from=estimate,names_from=mod) |> gt::gt()\n```\n\n`r solend()` \n\n\n\n`r qbegin(qcounter())`\nCreate a plot of the predicted probabilities and uncertainty for each of the `Delay * Group` combinations.  \n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(effects)\neffplot <- effect(\"Delay:Group\", mod4) |>\n  as.data.frame()\n\nggplot(effplot, aes(Delay, fit, color=Group)) + \n  geom_pointrange(aes(ymax=upper, ymin=lower), \n                  position=position_dodge(width = 0.2))+\n  theme_classic() # just for a change :)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\nHere are odds ratios for our model:  \n```{r}\n#| code-fold: true\n# cbind combines columns\ncbind(\n  # the odds ratios:\n  OR = exp(fixef(mod4)), \n  # the CIs:\n  exp(confint(mod4, method=\"Wald\", parm=\"beta_\"))\n)\n```\n\nWhat should we do with this information? How should we apply test-enhanced learning to learning R and statistics?  \n`r qend()` \n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\nWe'll get the benefits of test-enhanced learning if we try these exercises before looking at any of the solutions that are visible! If we don't test ourselves, we're more likely to forget it in the long run.  \n`r solend()` \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"04exDORAIN.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Week 4 Exercises: Nested and Crossed","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}