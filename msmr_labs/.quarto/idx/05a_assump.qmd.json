{"title":"5A: Model Assumptions","markdown":{"yaml":{"title":"5A: Model Assumptions","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"MLM Assumptions & Diagnostics","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- Multilevel model assumptions: random effects can be thought of as another level of residual!  \n\n- Influence in multilevel models: influential observations and influential groups.\n\n:::\n\n\nHopefully by now you are getting comfortable with the idea that all our models are simplifications, and so there is always going to be some difference between a model and real-life. This difference - the _residual_ - will ideally just be randomness, and we assess this by checking for systematic patterns in the residual term.  \n\nNot much is different in the multilevel model - we simply now have \"residuals\" on multiple levels. We are assuming that our group-level differences represent one level of randomness, and that our observations represent another level. We can see these two levels in @fig-lmmres, with the group-level deviations from the fixed effects ($\\color{orange}{\\zeta_{0i}}$ and $\\color{orange}{\\zeta_{1i}}$) along with the observation-level deviations from that groups line ($\\varepsilon_{ij}$).  \n\n```{r}\n#| label: fig-lmmres\n#| echo: false\n#| out-width: \"100%\"\n#| fig-cap: \"Multilevel model with group $i$ highlighted\"\nknitr::include_graphics(\"images/un_lmm2.png\")\n```\n\n```{r}\n#| include: false\nset.seed(7775)\nn_groups = 25\nnpg = round(runif(25,5,25))\ng = unlist(lapply(1:n_groups, function(x) rep(x, npg[x])))\nx = round(rnorm(25,6.5,2.3),1)\nx = pmax(0,pmin(10,x))\nx = x[g]\nx1 = rnorm(length(g))\nx1 = as.numeric(cut(x1,6,labels=c(5:10)))+4\nre0 = rnorm(n_groups, sd = 1.3) \nre  = re0[g]\nrex = rnorm(n_groups, sd = .3) \nre_x  = rex[g]\nlp = (14 + re) + (.3 + re_x)*x1 + .4*x\ny = round(rnorm(length(x), mean = lp, sd = 1))\nset.seed(444)\nybin = rbinom(length(g),1,plogis(scale(lp)))\ndf = data.frame(x, x1, g=factor(g), y, ybin)\n\ndepts = c(\"Language Sciences\",\"Philosophy\",\"Psychology\",\"Education\",\"Research (EDU)\",\"Sport\",\"Economics\",\"Chemistry\",\"Clinical Psychology\",\"Counselling Studies\",\"Nursing Studies\",\"African Studies\",\"International Public Health Policy\",\"Politics\",\"South Asian Studies\",\"Social Anthropology\",\"Sociology\",\"Social Policy\",\"Social Work\",\"Accounting\",\"Business Studies\",\"Asian Studies\",\"Celtic\",\"European Languages and Cultures\",\"English Literature\",\"Islamic and Middle Eastern Studies\",\"Scottish Ethnology\",\"Chemical Engineering\",\"Civil Engineering\",\"Electronics\",\"Mechanical Engineering\",\"Animal Welfare and Animal Behaviour\",\"BVMS\",\"Equine Science\",\"Veterinary Sciences\",\"BSc Hons (Royal (Dick) Sch of Veterinary Studies)\",\"Architecture and Landscape Architecture\",\"Art\",\"Design\",\"History of Art\",\"Music\")\nset.seed(653)\ndepts = c(\"Psychology\",sample(depts[depts!=\"Psychology\"]))\n\njsdat <- df |> transmute(\n  NSSrating = x,\n  dept = depts[g],\n  payscale = x1,\n  jobsat = y,\n  jobsat_binary = ybin\n) |> slice_sample(prop=1)\n# sjPlot::plot_model(m,type=\"eff\")\n# summary(m)\n# m = lmer(jobsat ~ 1 + payscale + NSSrating + (1 + payscale| dept),\n#               data = jsdat)\n# summary(m)\n# write_csv(jsdat, \"../../data/msmr_nssjobsat.csv\")\n```\n\n\nLet's suppose we are studying employee job satisfaction at the university. We have `r nrow(jsdat)` employees from `r length(unique(jsdat$dept))` different departments, and we got them to fill in a job satisfaction questionnaire, and got information on what their payscale was. We have also taken information from the national student survey on the level of student satisfaction for each department.  \n\nEach datapoint here represents an individual employee, and these employees are grouped into departments.  \n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\njsuni <- read_csv(\"https://uoepsy.github.io/data/msmr_nssjobsat.csv\")\nhead(jsuni)\n```\n\nWe had a model that included by-participant random effects, such as:  \n```{r}\njsmod <- lmer(jobsat ~ 1 + payscale + NSSrating + \n                (1 + payscale| dept), \n              data = jsuni)\n```\n\nThe equation for such a model would take the following form:  \n$$\n\\begin{align}\n\\text{For Employee }j\\text{ from Department }i& \\\\\n\\text{Level 1 (Employee):}& \\\\\n\\text{Stress}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{Payscale}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (Department):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{NSSrating}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n& \\qquad \\\\\n& \\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon)\n\\end{align}\n$$\n\nNote that this equation makes clear the distributional assumptions that our models make. It states the residuals $\\varepsilon$ are normally distributed with a mean of zero, and it says the same thing about our random effects!  \n\n\n## Level 1 residuals\n\nWe can get the level 1 (observation-level) residuals the same way we used to do for `lm()` - by just using `resid()` or `residuals()`.   Additionally, there are a few useful techniques for plotting these which we have listed below:  \n\n::::panelset\n:::panel\n#### resid vs fitted\n\nWe can plot the residuals vs fitted model (just like we used to for `lm()`), and assess the extend to which the assumption holds that the residuals are zero mean. \n_(we want the blue smoothed line to be fairly close to zero across the plot)_  \n```{r}\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n```\n\n:::\n:::panel\n#### scale-location\n\nAgain, like we can for `lm()`, we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.  \n_(we want the blue smoothed line to be close to horizontal across the plot)_  \n```{r}\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n```\n\n:::\n:::panel\n#### facetted plots\n\nWe can also plot these \"resid v fitted\" and \"scale-location\" plots for each cluster, to check that our residual mean and variance is not related to the clusters:  \n```{r}\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n```\n\n```{r}\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n```\n    \n:::\n:::panel\n#### residual normality\n    \nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:    \n_(we want the datapoints to follow close to the diagonal line)_  \n```{r}\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n```\n\n```{r}\nhist(resid(jsmod))\n```\n\n\n:::\n::::\n\n## Level 2+ residuals\n\nThe second level of residuals in the multilevel model are actually just our random effects! We've seen them already whenever we use `ranef()`!  \n\nTo get out these we often need to do a bit of indexing. `ranef(model)` will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.  \n\nBelow, we see that `ranef(jsmod)` gives us something with one entry, `$dept`, which contains 2 columns (the random intercepts and random slopes of `payscale`):  \n```{r}\n#| eval: false\nranef(jsmod)\n```\n```\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \n```\n\nSo we can extract the random intercepts using `ranef(jsmod)$dept[,1]`.  \n\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:  \n\ne.g., for the random intercepts:  \n```{r}\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n```\n\nand for the random slopes:  \n```{r}\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])\n```\n\n## model simulations\n\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each 'draw' (i.e. set of simulated values), we can look at how well it maps to the observed set of values:  \n\nOne quick way to do this is with the `check_predictions()` function from the __performance__ package:  \n\n```{r}\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n```\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: doing it yourself\n\nDoing this ourself gives us a lot more scope to query differences between our observed vs model-predicted data.  \n\nThe `simulate()` function will simulate response variable values for us.  \nThe `re.form = NULL` bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we _have_ in our data). If we said `re.form = NA` it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.  \n\n```{r}\nmodsim <- simulate(jsmod, nsim = 200, re.form=NULL)\n```\nTo get this plotted, we'll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:  \n```{r}\n# take the simulations\nmodsim |> \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |>\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n```\n\nHowever, we can also go further! We can pick a statistic, let's use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.  \n\nHere are 1000 simulations. This time I don't care about simulating for these specific clusters, I just want to compare to random draws of clusters:  \n```{r}\nsims <- simulate(jsmod, nsim=1000, re.form=NA)\n```\n\nThe `apply()` function (see also `lapply`, `sapply` ,`vapply`, `tapply`) is a really nice way to take an object, and apply a function to it. \nThe number 2 here is to say \"do it on each column\". If we had 1 it would be saying \"do it on each row\".  \nThis gives us the IQR of each simulation:\n```{r}\nsimsIQR <- apply(sims, 2, IQR)\n```\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.  \n```{r}\nmean(IQR(jsuni$jobsat)>simsIQR)\n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Influence\n\nJust like individual observations can exert influence on a single level linear model fitted with `lm()`, they can influence our multilevel models too. Just like our assumptions now apply at multiple levels, influence can happen at multiple levels too.  \n\nAn individual observation might influence our model (as in @fig-infl1), but so might an entire cluster (@fig-infl2).  \n\n:::: {.columns}\n:::{.column width=\"47.5%\"}\n\n```{r}\n#| echo: false\n#| label: fig-infl1\n#| fig-cap: \"Dashed lines show fixed effects from (black line) model with the highlighted red observation, and (red line) model without the highlighted red observation.\"\nset.seed(987)\ndf<-junk::sim_basicrs() |> filter(as.numeric(g)<7)\ndf[1,1]<-3\ndf[1,3]<- -2\n\nm=lmer(y~x+(1+x|g),df)\nm1=lmer(y~x+(1+x|g),df[-1,])\n\nggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(alpha=.3,size=3)+\n  geom_point(data=df[1,],alpha=1,col=\"red\",size=3)+\n  stat_smooth(geom=\"line\",lwd=1,method=lm,se=F,alpha=.3)+\n  geom_abline(intercept=fixef(m)[1],slope=fixef(m)[2],lwd=1,col=\"red\",\n              lty=\"dashed\")+\n  geom_abline(intercept=fixef(m1)[1],slope=fixef(m1)[2],lwd=1,col=\"black\",\n              lty=\"dashed\")\n```\n\n:::\n:::{.column width=\"5%\"}\n:::\n:::{.column width=\"47.5%\"}\n\n```{r}\n#| echo: false\n#| label: fig-infl2\n#| fig-cap: \"Dashed lines show fixed effects from (black line) model with the highlighted group, and (red line) model without the highlighted group.\"\nset.seed(333)\ndf<-junk::sim_basicrs() |> filter(as.numeric(g)<7)\n\ndf[1:10,1] <- scale(df[1:10,1])[,1]*.5\ndf[1:10,3] <- df[1:10,1]*-2+rnorm(10,0,.3)\n\nm=lmer(y~x+(1+x|g),df)\nm1=lmer(y~x+(1+x|g),df[-c(1:10),])\n\nggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(alpha=.3,size=3)+\n  geom_point(data=df[1:10,],alpha=1,size=3)+\n  stat_smooth(geom=\"line\",lwd=1,method=lm,se=F,alpha=.3) +\n  geom_abline(intercept=fixef(m)[1],slope=fixef(m)[2],lwd=1,col=\"red\",\n              lty=\"dashed\")+\n  geom_abline(intercept=fixef(m1)[1],slope=fixef(m1)[2],lwd=1,col=\"black\",\n              lty=\"dashed\")\n```\n\n:::\n::::\n\nThere are two main packages for examining influence in multilevel models, __HLMdiag__ and __influence.ME__ that work by going through and deleting each observation/cluster and examining how much things change. __HLMdiag__ works for `lmer()` but not `glmer()`, but provides a one-step approximation of the influence, which is often quicker than the full refitting process. The __influence.ME__ package will be slower, but works for both `lmer()` and `glmer()`.    \n\n::::panelset\n:::panel\n#### HLMdiag\n\nWe use this package by creating an object using `hlm_influence()`, specifying the level at which we want to examine influence.  \nIn this case, `level = 1` means we are looking at the influence of individual observations, and `level = \"dept\"` specifies that we are looking at the influence of the levels in the `dept` variable.  \nThe `approx = TRUE` part means that we are asking for the approximate calculations of influence measures, rather than asking for it to actually iteratively delete each observation and re-fit the model.  \n\n```{r}\nlibrary(HLMdiag)\ninf1 <- hlm_influence(model = jsmod, level = 1, approx = TRUE)\ninf2 <- hlm_influence(model = jsmod, level = \"dept\", approx = TRUE)\n```\n\nWe can then plot metrics such as the cooks distance for each of these. Note that the \"internal\" cutoff here adds a red line to the plots, and is simply calculated as the 3rd quartile plus 3 times the IQR, thereby capturing a relative measure of outlyingness.  \nAs with the measures of influence for `lm()`, such cutoffs are somewhat arbitrary, and so should be interpreted with caution.  \n```{r}\ndotplot_diag(inf1$cooksd, cutoff = \"internal\")\ndotplot_diag(inf2$cooksd, \n             index = inf2$dept, cutoff = \"internal\")\n```\n\n\n:::\n:::panel\n#### influence.ME\n\nWe use this package by creating an object using `influence()`, specifying the level at which we want to examine influence.  \nIn this case, `obs = TRUE` means we are looking at the influence of individual observations, and `group = \"dept\"` specifies that we are looking at the influence of the levels in the `dept` variable.  \n```{r}\nlibrary(influence.ME)\ninf1 <- influence(model = jsmod, obs = TRUE)\ninf2 <- influence(model = jsmod, group = \"dept\")\n```\n\nWe can then plot metrics such as the cooks distance for each of these:\n```{r}\nplot(inf1, which = \"cook\", sort=TRUE)\nplot(inf2, which = \"cook\", sort=TRUE)\n```\n\n:::\n::::\n\nTo conduct a sensitivity analysis of our models robustness to the inclusion of such influential observations, we would simply fit our model with and without that observation/cluster, and examine if/how our conclusions would change.  \nFor instance, to refit our model without employees of the 'Nursing Studies' department: \n\n```{r}\njsmod_NS <- lmer(jobsat ~ 1 + payscale + NSSrating + \n                (1 + payscale| dept), \n              data = jsuni |> \n                filter(dept!=\"Nursing Studies\"))\n```\nThe `tidy()` function from __broom.mixed__ is a quick way to print out parameters from each model:  \n```{r}\ntidy(jsmod)\ntidy(jsmod_NS)\n```\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Optional Extra: The DHARMa Package\n\n__DHARMa__ is a cool package that attempts to construct easy to interpret residuals for models of various families (allowing us to construct useful plots to look at for binomial and poisson models fitted with `glmer()`). \n\nIt achieves this by doing lots of simulated draws from the model, and for each observation $i$ it asks where the actual value falls in relation to the simulated values for that observation (i.e. that combination of predictors). If the observed is exactly what we would expect, it would fall right in the middle taking a value of 0.5 (0.5 of the simulated distribution for observation $i$ will be below and 0.5 will be below). If all the simulated values fall below the observed value, it would take the value 1. The DHARMa package has a good explanation of how this works in more detail ([https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html){target=\"_blank\"}), but it can be done for `lmer()`, `glmer()` and more.  \n\nHere's a binomial logistic model, where instead of a job-satisfaction _score_, we have a simple yes/no for job satisfaction:   \n```{r}\njsmod2 <- glmer(jobsat_binary ~ 1 + payscale + NSSrating + \n                  (1 + payscale | dept), \n                data = jsuni, family=binomial)\n```\n\nThe DHARMa package works by first getting the simulated residuals\n```{r}\nlibrary(DHARMa)\nmsim <- simulateResiduals(fittedModel = jsmod2)\n```\n\nIf our model is correctly specified, then the DHARMa residuals we would expect to be uniform (i.e. flat), and also uniform distributed across any predictor.  \n\nIt offers some nice ways to assess this, by plotting the observed vs expected distribution on a QQplot:  \n```{r}\nplotQQunif(msim)\n```\n\nAnd it plots the residuals against the fitted, while adding lines for quantiles of the distribution. If the middle one is flat, it equates to the \"zero mean\" assumption, and if the outer two are flat it equates to our \"constant variance\" assumption.  \n```{r}\nplotResiduals(msim)\n```\n\nWe can also plot the residuals against specific predictors:\n```{r}\nplotResiduals(msim, form = jsuni$NSSrating)\n```\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n\n\n:::lo\nThis reading:  \n\n- Multilevel model assumptions: random effects can be thought of as another level of residual!  \n\n- Influence in multilevel models: influential observations and influential groups.\n\n:::\n\n# MLM Assumptions & Diagnostics\n\nHopefully by now you are getting comfortable with the idea that all our models are simplifications, and so there is always going to be some difference between a model and real-life. This difference - the _residual_ - will ideally just be randomness, and we assess this by checking for systematic patterns in the residual term.  \n\nNot much is different in the multilevel model - we simply now have \"residuals\" on multiple levels. We are assuming that our group-level differences represent one level of randomness, and that our observations represent another level. We can see these two levels in @fig-lmmres, with the group-level deviations from the fixed effects ($\\color{orange}{\\zeta_{0i}}$ and $\\color{orange}{\\zeta_{1i}}$) along with the observation-level deviations from that groups line ($\\varepsilon_{ij}$).  \n\n```{r}\n#| label: fig-lmmres\n#| echo: false\n#| out-width: \"100%\"\n#| fig-cap: \"Multilevel model with group $i$ highlighted\"\nknitr::include_graphics(\"images/un_lmm2.png\")\n```\n\n```{r}\n#| include: false\nset.seed(7775)\nn_groups = 25\nnpg = round(runif(25,5,25))\ng = unlist(lapply(1:n_groups, function(x) rep(x, npg[x])))\nx = round(rnorm(25,6.5,2.3),1)\nx = pmax(0,pmin(10,x))\nx = x[g]\nx1 = rnorm(length(g))\nx1 = as.numeric(cut(x1,6,labels=c(5:10)))+4\nre0 = rnorm(n_groups, sd = 1.3) \nre  = re0[g]\nrex = rnorm(n_groups, sd = .3) \nre_x  = rex[g]\nlp = (14 + re) + (.3 + re_x)*x1 + .4*x\ny = round(rnorm(length(x), mean = lp, sd = 1))\nset.seed(444)\nybin = rbinom(length(g),1,plogis(scale(lp)))\ndf = data.frame(x, x1, g=factor(g), y, ybin)\n\ndepts = c(\"Language Sciences\",\"Philosophy\",\"Psychology\",\"Education\",\"Research (EDU)\",\"Sport\",\"Economics\",\"Chemistry\",\"Clinical Psychology\",\"Counselling Studies\",\"Nursing Studies\",\"African Studies\",\"International Public Health Policy\",\"Politics\",\"South Asian Studies\",\"Social Anthropology\",\"Sociology\",\"Social Policy\",\"Social Work\",\"Accounting\",\"Business Studies\",\"Asian Studies\",\"Celtic\",\"European Languages and Cultures\",\"English Literature\",\"Islamic and Middle Eastern Studies\",\"Scottish Ethnology\",\"Chemical Engineering\",\"Civil Engineering\",\"Electronics\",\"Mechanical Engineering\",\"Animal Welfare and Animal Behaviour\",\"BVMS\",\"Equine Science\",\"Veterinary Sciences\",\"BSc Hons (Royal (Dick) Sch of Veterinary Studies)\",\"Architecture and Landscape Architecture\",\"Art\",\"Design\",\"History of Art\",\"Music\")\nset.seed(653)\ndepts = c(\"Psychology\",sample(depts[depts!=\"Psychology\"]))\n\njsdat <- df |> transmute(\n  NSSrating = x,\n  dept = depts[g],\n  payscale = x1,\n  jobsat = y,\n  jobsat_binary = ybin\n) |> slice_sample(prop=1)\n# sjPlot::plot_model(m,type=\"eff\")\n# summary(m)\n# m = lmer(jobsat ~ 1 + payscale + NSSrating + (1 + payscale| dept),\n#               data = jsdat)\n# summary(m)\n# write_csv(jsdat, \"../../data/msmr_nssjobsat.csv\")\n```\n\n\nLet's suppose we are studying employee job satisfaction at the university. We have `r nrow(jsdat)` employees from `r length(unique(jsdat$dept))` different departments, and we got them to fill in a job satisfaction questionnaire, and got information on what their payscale was. We have also taken information from the national student survey on the level of student satisfaction for each department.  \n\nEach datapoint here represents an individual employee, and these employees are grouped into departments.  \n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\njsuni <- read_csv(\"https://uoepsy.github.io/data/msmr_nssjobsat.csv\")\nhead(jsuni)\n```\n\nWe had a model that included by-participant random effects, such as:  \n```{r}\njsmod <- lmer(jobsat ~ 1 + payscale + NSSrating + \n                (1 + payscale| dept), \n              data = jsuni)\n```\n\nThe equation for such a model would take the following form:  \n$$\n\\begin{align}\n\\text{For Employee }j\\text{ from Department }i& \\\\\n\\text{Level 1 (Employee):}& \\\\\n\\text{Stress}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{Payscale}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (Department):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{NSSrating}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right) \\\\\n& \\qquad \\\\\n& \\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon)\n\\end{align}\n$$\n\nNote that this equation makes clear the distributional assumptions that our models make. It states the residuals $\\varepsilon$ are normally distributed with a mean of zero, and it says the same thing about our random effects!  \n\n\n## Level 1 residuals\n\nWe can get the level 1 (observation-level) residuals the same way we used to do for `lm()` - by just using `resid()` or `residuals()`.   Additionally, there are a few useful techniques for plotting these which we have listed below:  \n\n::::panelset\n:::panel\n#### resid vs fitted\n\nWe can plot the residuals vs fitted model (just like we used to for `lm()`), and assess the extend to which the assumption holds that the residuals are zero mean. \n_(we want the blue smoothed line to be fairly close to zero across the plot)_  \n```{r}\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n```\n\n:::\n:::panel\n#### scale-location\n\nAgain, like we can for `lm()`, we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.  \n_(we want the blue smoothed line to be close to horizontal across the plot)_  \n```{r}\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n```\n\n:::\n:::panel\n#### facetted plots\n\nWe can also plot these \"resid v fitted\" and \"scale-location\" plots for each cluster, to check that our residual mean and variance is not related to the clusters:  \n```{r}\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n```\n\n```{r}\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n```\n    \n:::\n:::panel\n#### residual normality\n    \nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:    \n_(we want the datapoints to follow close to the diagonal line)_  \n```{r}\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n```\n\n```{r}\nhist(resid(jsmod))\n```\n\n\n:::\n::::\n\n## Level 2+ residuals\n\nThe second level of residuals in the multilevel model are actually just our random effects! We've seen them already whenever we use `ranef()`!  \n\nTo get out these we often need to do a bit of indexing. `ranef(model)` will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.  \n\nBelow, we see that `ranef(jsmod)` gives us something with one entry, `$dept`, which contains 2 columns (the random intercepts and random slopes of `payscale`):  \n```{r}\n#| eval: false\nranef(jsmod)\n```\n```\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \n```\n\nSo we can extract the random intercepts using `ranef(jsmod)$dept[,1]`.  \n\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:  \n\ne.g., for the random intercepts:  \n```{r}\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n```\n\nand for the random slopes:  \n```{r}\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])\n```\n\n## model simulations\n\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each 'draw' (i.e. set of simulated values), we can look at how well it maps to the observed set of values:  \n\nOne quick way to do this is with the `check_predictions()` function from the __performance__ package:  \n\n```{r}\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n```\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: doing it yourself\n\nDoing this ourself gives us a lot more scope to query differences between our observed vs model-predicted data.  \n\nThe `simulate()` function will simulate response variable values for us.  \nThe `re.form = NULL` bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we _have_ in our data). If we said `re.form = NA` it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.  \n\n```{r}\nmodsim <- simulate(jsmod, nsim = 200, re.form=NULL)\n```\nTo get this plotted, we'll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:  \n```{r}\n# take the simulations\nmodsim |> \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |>\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n```\n\nHowever, we can also go further! We can pick a statistic, let's use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.  \n\nHere are 1000 simulations. This time I don't care about simulating for these specific clusters, I just want to compare to random draws of clusters:  \n```{r}\nsims <- simulate(jsmod, nsim=1000, re.form=NA)\n```\n\nThe `apply()` function (see also `lapply`, `sapply` ,`vapply`, `tapply`) is a really nice way to take an object, and apply a function to it. \nThe number 2 here is to say \"do it on each column\". If we had 1 it would be saying \"do it on each row\".  \nThis gives us the IQR of each simulation:\n```{r}\nsimsIQR <- apply(sims, 2, IQR)\n```\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.  \n```{r}\nmean(IQR(jsuni$jobsat)>simsIQR)\n```\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Influence\n\nJust like individual observations can exert influence on a single level linear model fitted with `lm()`, they can influence our multilevel models too. Just like our assumptions now apply at multiple levels, influence can happen at multiple levels too.  \n\nAn individual observation might influence our model (as in @fig-infl1), but so might an entire cluster (@fig-infl2).  \n\n:::: {.columns}\n:::{.column width=\"47.5%\"}\n\n```{r}\n#| echo: false\n#| label: fig-infl1\n#| fig-cap: \"Dashed lines show fixed effects from (black line) model with the highlighted red observation, and (red line) model without the highlighted red observation.\"\nset.seed(987)\ndf<-junk::sim_basicrs() |> filter(as.numeric(g)<7)\ndf[1,1]<-3\ndf[1,3]<- -2\n\nm=lmer(y~x+(1+x|g),df)\nm1=lmer(y~x+(1+x|g),df[-1,])\n\nggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(alpha=.3,size=3)+\n  geom_point(data=df[1,],alpha=1,col=\"red\",size=3)+\n  stat_smooth(geom=\"line\",lwd=1,method=lm,se=F,alpha=.3)+\n  geom_abline(intercept=fixef(m)[1],slope=fixef(m)[2],lwd=1,col=\"red\",\n              lty=\"dashed\")+\n  geom_abline(intercept=fixef(m1)[1],slope=fixef(m1)[2],lwd=1,col=\"black\",\n              lty=\"dashed\")\n```\n\n:::\n:::{.column width=\"5%\"}\n:::\n:::{.column width=\"47.5%\"}\n\n```{r}\n#| echo: false\n#| label: fig-infl2\n#| fig-cap: \"Dashed lines show fixed effects from (black line) model with the highlighted group, and (red line) model without the highlighted group.\"\nset.seed(333)\ndf<-junk::sim_basicrs() |> filter(as.numeric(g)<7)\n\ndf[1:10,1] <- scale(df[1:10,1])[,1]*.5\ndf[1:10,3] <- df[1:10,1]*-2+rnorm(10,0,.3)\n\nm=lmer(y~x+(1+x|g),df)\nm1=lmer(y~x+(1+x|g),df[-c(1:10),])\n\nggplot(df,aes(x=x,y=y,col=g))+\n  geom_point(alpha=.3,size=3)+\n  geom_point(data=df[1:10,],alpha=1,size=3)+\n  stat_smooth(geom=\"line\",lwd=1,method=lm,se=F,alpha=.3) +\n  geom_abline(intercept=fixef(m)[1],slope=fixef(m)[2],lwd=1,col=\"red\",\n              lty=\"dashed\")+\n  geom_abline(intercept=fixef(m1)[1],slope=fixef(m1)[2],lwd=1,col=\"black\",\n              lty=\"dashed\")\n```\n\n:::\n::::\n\nThere are two main packages for examining influence in multilevel models, __HLMdiag__ and __influence.ME__ that work by going through and deleting each observation/cluster and examining how much things change. __HLMdiag__ works for `lmer()` but not `glmer()`, but provides a one-step approximation of the influence, which is often quicker than the full refitting process. The __influence.ME__ package will be slower, but works for both `lmer()` and `glmer()`.    \n\n::::panelset\n:::panel\n#### HLMdiag\n\nWe use this package by creating an object using `hlm_influence()`, specifying the level at which we want to examine influence.  \nIn this case, `level = 1` means we are looking at the influence of individual observations, and `level = \"dept\"` specifies that we are looking at the influence of the levels in the `dept` variable.  \nThe `approx = TRUE` part means that we are asking for the approximate calculations of influence measures, rather than asking for it to actually iteratively delete each observation and re-fit the model.  \n\n```{r}\nlibrary(HLMdiag)\ninf1 <- hlm_influence(model = jsmod, level = 1, approx = TRUE)\ninf2 <- hlm_influence(model = jsmod, level = \"dept\", approx = TRUE)\n```\n\nWe can then plot metrics such as the cooks distance for each of these. Note that the \"internal\" cutoff here adds a red line to the plots, and is simply calculated as the 3rd quartile plus 3 times the IQR, thereby capturing a relative measure of outlyingness.  \nAs with the measures of influence for `lm()`, such cutoffs are somewhat arbitrary, and so should be interpreted with caution.  \n```{r}\ndotplot_diag(inf1$cooksd, cutoff = \"internal\")\ndotplot_diag(inf2$cooksd, \n             index = inf2$dept, cutoff = \"internal\")\n```\n\n\n:::\n:::panel\n#### influence.ME\n\nWe use this package by creating an object using `influence()`, specifying the level at which we want to examine influence.  \nIn this case, `obs = TRUE` means we are looking at the influence of individual observations, and `group = \"dept\"` specifies that we are looking at the influence of the levels in the `dept` variable.  \n```{r}\nlibrary(influence.ME)\ninf1 <- influence(model = jsmod, obs = TRUE)\ninf2 <- influence(model = jsmod, group = \"dept\")\n```\n\nWe can then plot metrics such as the cooks distance for each of these:\n```{r}\nplot(inf1, which = \"cook\", sort=TRUE)\nplot(inf2, which = \"cook\", sort=TRUE)\n```\n\n:::\n::::\n\nTo conduct a sensitivity analysis of our models robustness to the inclusion of such influential observations, we would simply fit our model with and without that observation/cluster, and examine if/how our conclusions would change.  \nFor instance, to refit our model without employees of the 'Nursing Studies' department: \n\n```{r}\njsmod_NS <- lmer(jobsat ~ 1 + payscale + NSSrating + \n                (1 + payscale| dept), \n              data = jsuni |> \n                filter(dept!=\"Nursing Studies\"))\n```\nThe `tidy()` function from __broom.mixed__ is a quick way to print out parameters from each model:  \n```{r}\ntidy(jsmod)\ntidy(jsmod_NS)\n```\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Optional Extra: The DHARMa Package\n\n__DHARMa__ is a cool package that attempts to construct easy to interpret residuals for models of various families (allowing us to construct useful plots to look at for binomial and poisson models fitted with `glmer()`). \n\nIt achieves this by doing lots of simulated draws from the model, and for each observation $i$ it asks where the actual value falls in relation to the simulated values for that observation (i.e. that combination of predictors). If the observed is exactly what we would expect, it would fall right in the middle taking a value of 0.5 (0.5 of the simulated distribution for observation $i$ will be below and 0.5 will be below). If all the simulated values fall below the observed value, it would take the value 1. The DHARMa package has a good explanation of how this works in more detail ([https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html){target=\"_blank\"}), but it can be done for `lmer()`, `glmer()` and more.  \n\nHere's a binomial logistic model, where instead of a job-satisfaction _score_, we have a simple yes/no for job satisfaction:   \n```{r}\njsmod2 <- glmer(jobsat_binary ~ 1 + payscale + NSSrating + \n                  (1 + payscale | dept), \n                data = jsuni, family=binomial)\n```\n\nThe DHARMa package works by first getting the simulated residuals\n```{r}\nlibrary(DHARMa)\nmsim <- simulateResiduals(fittedModel = jsmod2)\n```\n\nIf our model is correctly specified, then the DHARMa residuals we would expect to be uniform (i.e. flat), and also uniform distributed across any predictor.  \n\nIt offers some nice ways to assess this, by plotting the observed vs expected distribution on a QQplot:  \n```{r}\nplotQQunif(msim)\n```\n\nAnd it plots the residuals against the fitted, while adding lines for quantiles of the distribution. If the middle one is flat, it equates to the \"zero mean\" assumption, and if the outer two are flat it equates to our \"constant variance\" assumption.  \n```{r}\nplotResiduals(msim)\n```\n\nWe can also plot the residuals against specific predictors:\n```{r}\nplotResiduals(msim, form = jsuni$NSSrating)\n```\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"05a_assump.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"5A: Model Assumptions","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}