{"title":"Week 3 Exercises: Non-Linear Change","markdown":{"yaml":{"title":"Week 3 Exercises: Non-Linear Change","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Cognitive Task Performance","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n:::frame\n__Dataset: Az.rda__  \n  \nThese data are available at [https://uoepsy.github.io/data/Az.rda](https://uoepsy.github.io/data/Az.rda). You can load the dataset using:  \n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n```\nand you will find the `Az` object in your environment.  \n\nThe `Az` object contains information on 30 Participants with probable Alzheimer's Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100. \n\nWe're interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.  \n\n```{r}\n#| echo: false\ntibble(\n    variable = names(Az),\n    description = c(\"Unique Subject Identifier\",\"Time point of the study (1 to 10)\",\"Task type (Memory, cADL, sADL)\",\"Score on test (range 0 to 100)\")\n) |> gt::gt()\n```\n:::\n\n`r qbegin(qcounter())`\nLoad in the data and examine it.  \nHow many participants, how many observations per participant, per task?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n```\n\n30 participants: \n```{r}\nlength(unique(Az$Subject))\n```\n\nDoes every participant have 10 datapoints for each Task type?  \n```{r}\nany( table(Az$Subject, Az$Task) != 10 )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nNo modelling just yet.  \n\nPlot the performance over time for each type of task.  \n\nTry using `stat_summary` so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.  \n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n```\n`r solend()`\n\n`r qbegin(qcounter())`\nWhy do you think *raw/natural* polynomials might be more useful than *orthogonal* polynomials for these data?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rates of change) at a specific point in time?  \n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nTODO \nBecause we're likely to be interested in whether there are task differences at the starting baseline point\n`r solend()`\n\n\n`r qbegin(qcounter())`\nChoose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nTODO \n```{r}\nlibrary(lme4)\nAz <- Az |> mutate(\n  poly1 = poly(Time,2,raw=T)[,1],\n  poly2 = poly(Time,2,raw=T)[,2]\n)\n```\n\n\n```{r}\n#| eval: false\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n```\n\n`r solend()`\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Categorical random effects on the RHS\n\nWhen we have a categorical random effect (i.e. where the `x` in `(1 + x | g)` is a categorical variable), then model estimation can often get tricky, because \"the effect of x\" for a categorical variable with $k$ levels is identified via $k-1$ parameters, meaning we have a lot of variances and covariances to estimate when we include `x|g`.  \n\n:::: {.columns}\n:::{.column width=\"45%\"}\n\nWhen `x` is numeric:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n\nWhen `x` is categorical with $k$ levels:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n::::\n\nHowever, we can use an alternative formation of the random effects by putting a categorical `x` into the right-hand side:  \nInstead of `(1 + x | g)` we can fit `(1 | g) + (1 | g:x)`.   \n\nThe symbol `:` in `g:x` is used to refer to the combination of `g` and `x`.  \n\n```{r}\n#| echo: false\ngx = tibble(\n  g = c(\"p1\",\"p1\",\"p1\",\"...\",\"p2\",\"p2\",\"...\"),\n  x = c(\"a\",\"a\",\"b\",\"...\",\"a\",\"b\",\"...\"),\n  `g:x` = as.character(interaction(g,x))\n)\ngx[c(4,7),3] <- \"...\"\ngx$g = paste0(\"  \",gx$g,\"   \")\ngx$x = paste0(\"  \",gx$x,\"   \")\nnames(gx)<-c(\"  g   \",\"  x   \",\"g:x\")\nas.data.frame(gx)\n```\n\nIt's a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:  \n\n- `(1 + x | g)`:  each group of `g` can have a different intercept and a different effect of `x`  \n- `(1 | g) + (1 | g:x)`: each group of `g` can have a different intercept, and each level of x within each `g` can have a different intercept.  \n\nBoth of these allow the outcome `y` to change across `x` differently for each group in `g` (i.e. both of them result in `y` being different for each level of `x` in each group `g`).  \nThe first does so explicitly by estimating the group level variance of the `y~x` effect.  \nThe second one estimates the variance of $y$ between groups, and also the variance of $y$ between 'levels of x within groups'. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of `x`, we don't have to worry about lots of covariances:  \n\n\n:::: {.columns}\n:::{.column width=\"45%\"}\n`(1 + x | g)`  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n`(1 | g) + (1 | g:x)`  \n\n\n```\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n```\n\n:::\n::::\n\n:::\n\n`r qbegin(qcounter())`\nOkay, so the model didn't converge.  It's trying to estimate __a lot__ of things in the random effects (even though it didn't converge, try looking at `VarCorr(model)` to see all the covariances it is trying to estimate).  \n\n\nTry adjusting your model by moving `Task` to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one) \n\n**This is our first experience of our random effect structures becoming more complex than simply `(.... | group)`. This is going to feel confusing, but don't worry, we'll see more structures like this next week.**    \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)`  \n\n:::\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nVarCorr(m2)\n```\n\n```{r}\n\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\n- run series of model comparisons investigating whether\n  - tasks differ in their linear change\n  - tasks differ in quadratic change\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we're interested in the how the average participant performs over the study).  \nSo our models want to have the same random effect structure, but different fixed effects.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r}\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nProvide an interpretation of each coefficient from the full model  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n#| echo: false\nbroom.mixed::tidy(m3) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    interpretation = \"... \"\n  ) |> gt::gt()\n```\n\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectory.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\npp1 <- bind_rows(poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble()\n          ) |>\n  mutate(Task=rep(c(\"cADL\",\"sADL\",\"Memory\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"Task\")\nbts = bootMer(m3,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=2)\n\npp1$pred = predict(m3, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=Task,fill=Task))+\n  geom_line(lwd=1)+\n  # geom_ribbon(alpha=.2)+\n  NULL\n```\n\n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nMake a plot of the average model predicted values across time.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(broom.mixed)\naugment(m3) |>\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")\n```\n\n\n`r solend()`\n\n\n<!-- # Polynomials and overfitting -->\n\n<!-- :::frame -->\n<!-- Two quotes -->\n\n<!-- \"all models are wrong. some are useful.\" [(George Box, 1976)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949).   -->\n\n<!-- \"...it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd. The construction of idealized representations that capture important stable aspects of such systems is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from other kinds of model.\" (Sir David Cox, 1995).   -->\n\n<!-- ::: -->\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n# Cognitive Task Performance\n\n:::frame\n__Dataset: Az.rda__  \n  \nThese data are available at [https://uoepsy.github.io/data/Az.rda](https://uoepsy.github.io/data/Az.rda). You can load the dataset using:  \n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n```\nand you will find the `Az` object in your environment.  \n\nThe `Az` object contains information on 30 Participants with probable Alzheimer's Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100. \n\nWe're interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.  \n\n```{r}\n#| echo: false\ntibble(\n    variable = names(Az),\n    description = c(\"Unique Subject Identifier\",\"Time point of the study (1 to 10)\",\"Task type (Memory, cADL, sADL)\",\"Score on test (range 0 to 100)\")\n) |> gt::gt()\n```\n:::\n\n`r qbegin(qcounter())`\nLoad in the data and examine it.  \nHow many participants, how many observations per participant, per task?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n```\n\n30 participants: \n```{r}\nlength(unique(Az$Subject))\n```\n\nDoes every participant have 10 datapoints for each Task type?  \n```{r}\nany( table(Az$Subject, Az$Task) != 10 )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nNo modelling just yet.  \n\nPlot the performance over time for each type of task.  \n\nTry using `stat_summary` so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.  \n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n```\n`r solend()`\n\n`r qbegin(qcounter())`\nWhy do you think *raw/natural* polynomials might be more useful than *orthogonal* polynomials for these data?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rates of change) at a specific point in time?  \n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nTODO \nBecause we're likely to be interested in whether there are task differences at the starting baseline point\n`r solend()`\n\n\n`r qbegin(qcounter())`\nChoose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nTODO \n```{r}\nlibrary(lme4)\nAz <- Az |> mutate(\n  poly1 = poly(Time,2,raw=T)[,1],\n  poly2 = poly(Time,2,raw=T)[,2]\n)\n```\n\n\n```{r}\n#| eval: false\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n```\n\n`r solend()`\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Categorical random effects on the RHS\n\nWhen we have a categorical random effect (i.e. where the `x` in `(1 + x | g)` is a categorical variable), then model estimation can often get tricky, because \"the effect of x\" for a categorical variable with $k$ levels is identified via $k-1$ parameters, meaning we have a lot of variances and covariances to estimate when we include `x|g`.  \n\n:::: {.columns}\n:::{.column width=\"45%\"}\n\nWhen `x` is numeric:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n\nWhen `x` is categorical with $k$ levels:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n::::\n\nHowever, we can use an alternative formation of the random effects by putting a categorical `x` into the right-hand side:  \nInstead of `(1 + x | g)` we can fit `(1 | g) + (1 | g:x)`.   \n\nThe symbol `:` in `g:x` is used to refer to the combination of `g` and `x`.  \n\n```{r}\n#| echo: false\ngx = tibble(\n  g = c(\"p1\",\"p1\",\"p1\",\"...\",\"p2\",\"p2\",\"...\"),\n  x = c(\"a\",\"a\",\"b\",\"...\",\"a\",\"b\",\"...\"),\n  `g:x` = as.character(interaction(g,x))\n)\ngx[c(4,7),3] <- \"...\"\ngx$g = paste0(\"  \",gx$g,\"   \")\ngx$x = paste0(\"  \",gx$x,\"   \")\nnames(gx)<-c(\"  g   \",\"  x   \",\"g:x\")\nas.data.frame(gx)\n```\n\nIt's a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:  \n\n- `(1 + x | g)`:  each group of `g` can have a different intercept and a different effect of `x`  \n- `(1 | g) + (1 | g:x)`: each group of `g` can have a different intercept, and each level of x within each `g` can have a different intercept.  \n\nBoth of these allow the outcome `y` to change across `x` differently for each group in `g` (i.e. both of them result in `y` being different for each level of `x` in each group `g`).  \nThe first does so explicitly by estimating the group level variance of the `y~x` effect.  \nThe second one estimates the variance of $y$ between groups, and also the variance of $y$ between 'levels of x within groups'. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of `x`, we don't have to worry about lots of covariances:  \n\n\n:::: {.columns}\n:::{.column width=\"45%\"}\n`(1 + x | g)`  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n`(1 | g) + (1 | g:x)`  \n\n\n```\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n```\n\n:::\n::::\n\n:::\n\n`r qbegin(qcounter())`\nOkay, so the model didn't converge.  It's trying to estimate __a lot__ of things in the random effects (even though it didn't converge, try looking at `VarCorr(model)` to see all the covariances it is trying to estimate).  \n\n\nTry adjusting your model by moving `Task` to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one) \n\n**This is our first experience of our random effect structures becoming more complex than simply `(.... | group)`. This is going to feel confusing, but don't worry, we'll see more structures like this next week.**    \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)`  \n\n:::\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nVarCorr(m2)\n```\n\n```{r}\n\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\n- run series of model comparisons investigating whether\n  - tasks differ in their linear change\n  - tasks differ in quadratic change\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we're interested in the how the average participant performs over the study).  \nSo our models want to have the same random effect structure, but different fixed effects.  \n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r}\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3)\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nProvide an interpretation of each coefficient from the full model  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n#| echo: false\nbroom.mixed::tidy(m3) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    interpretation = \"... \"\n  ) |> gt::gt()\n```\n\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectory.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\npp1 <- bind_rows(poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(1,10,length.out=50),2,raw=T) |> as_tibble()\n          ) |>\n  mutate(Task=rep(c(\"cADL\",\"sADL\",\"Memory\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"Task\")\nbts = bootMer(m3,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=2)\n\npp1$pred = predict(m3, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=Task,fill=Task))+\n  geom_line(lwd=1)+\n  # geom_ribbon(alpha=.2)+\n  NULL\n```\n\n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nMake a plot of the average model predicted values across time.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\nlibrary(broom.mixed)\naugment(m3) |>\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")\n```\n\n\n`r solend()`\n\n\n<!-- # Polynomials and overfitting -->\n\n<!-- :::frame -->\n<!-- Two quotes -->\n\n<!-- \"all models are wrong. some are useful.\" [(George Box, 1976)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949).   -->\n\n<!-- \"...it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd. The construction of idealized representations that capture important stable aspects of such systems is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from other kinds of model.\" (Sir David Cox, 1995).   -->\n\n<!-- ::: -->\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"03exROSENBLUTH.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Week 3 Exercises: Non-Linear Change","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}