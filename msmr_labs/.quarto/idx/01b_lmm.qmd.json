{"title":"1B: Linear Mixed Models/Multi-level Models","markdown":{"yaml":{"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"A Note on terminology","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\n:::{.callout-caution collapse=\"true\"}\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n:::\n\n\n# LMM\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as a fixed value.  \n\nIn the example where we model School children's grades as a function of their motivation score, we can fit a simple regression model, and the estimated parameters are two values that define the intercept and the slope of the line in @fig-schoolplot1.  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(0,10),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",size=2) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",size=2) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",size=2) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",size=2) \n  \n  \n```\nThese two values are fixed. It does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\n\n\n\nthe linear mixed model (LMM) \n\nfits a distribution of intercepts.\na center ( a single value and a spread)\n\nso for a given school, the intercept is b0 + z0i  \na fixed number plus some random deviation\n\n\n\n```{r}\n#| eval: false\n#| echo: false\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1) \n```\n\n\n```{r}\n#| echo: false\n#| eval: false\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1) \n\n```\n\n\nintercepts vary\nslopes vary\n\nno pooling vs partial pooling: \n\n```{r}\n#| echo: false\n#| eval: false\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) + \n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"orange\",lwd=1) +\n  ylim(0,100)\n\n```\n\n\nhow is it different to fixed eff?\n\"partial pooling\" (link back to above)\nshrinkage\n- socialist vs liberal analogy?  \n\nhow/why does it do this?\nby modelling a distribution of lines  \n\n\n$$\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n$$\n\n# fitting LMM in R\n\nlme4\nlmer\n\n# model parameters\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n$$\n\n$$\n\n\nwe _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. \n\n# terminology: fixed effects, random effects, variance components\n\nwe often use \"random effects\" to just mean the distribution of random deviations. i.e. \n\nsometimes you might hear \n\"random effect of group\"\n\"random effect for group\"\n\"random effect [of x] by group\"\n\ngenerally, people are referring to the `(1 + ... | cluster)` bit. \n\ngraphic on how to read it.\n\nintercept >> 1\nslope of x >> x\n| >> varies by\nthese groups >> cluster\n\na common stumbling block. \n\"effect of x varies by cluster\" is not the same as \"x varies by cluster\".  \n\n\n\n\n\n# estimation\n\n## ML and REML\n\nMLE explainer\n\n- problem for lmm\nest fix > est varcorr > est fix > est varcorr\nest of varcorr assumes fixed effects are known. \nthis biases var ests to be slightly smaller  \na bit like n-1 in formula for sd\n\nREML\n- OLS to partial out fixef > \n  est varcorr > est varcorr > est varcorr > \n  use GLS to est fixef\n- in the estimation of varcorr, the fixed effects are 0 _by definition_\n  \n\n\n\n\n\n\n\n\n\n## fitting issues\n\nconvergence warnings, singular fits \n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\n:::{.callout-caution collapse=\"true\"}\n## A Note on terminology\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n:::\n\n\n# LMM\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as a fixed value.  \n\nIn the example where we model School children's grades as a function of their motivation score, we can fit a simple regression model, and the estimated parameters are two values that define the intercept and the slope of the line in @fig-schoolplot1.  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(0,10),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",size=2) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",size=2) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",size=2) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",size=2) \n  \n  \n```\nThese two values are fixed. It does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\n\n\n\nthe linear mixed model (LMM) \n\nfits a distribution of intercepts.\na center ( a single value and a spread)\n\nso for a given school, the intercept is b0 + z0i  \na fixed number plus some random deviation\n\n\n\n```{r}\n#| eval: false\n#| echo: false\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1) \n```\n\n\n```{r}\n#| echo: false\n#| eval: false\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1) \n\n```\n\n\nintercepts vary\nslopes vary\n\nno pooling vs partial pooling: \n\n```{r}\n#| echo: false\n#| eval: false\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) + \n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"orange\",lwd=1) +\n  ylim(0,100)\n\n```\n\n\nhow is it different to fixed eff?\n\"partial pooling\" (link back to above)\nshrinkage\n- socialist vs liberal analogy?  \n\nhow/why does it do this?\nby modelling a distribution of lines  \n\n\n$$\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n$$\n\n# fitting LMM in R\n\nlme4\nlmer\n\n# model parameters\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n$$\n\n$$\n\n\nwe _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. \n\n# terminology: fixed effects, random effects, variance components\n\nwe often use \"random effects\" to just mean the distribution of random deviations. i.e. \n\nsometimes you might hear \n\"random effect of group\"\n\"random effect for group\"\n\"random effect [of x] by group\"\n\ngenerally, people are referring to the `(1 + ... | cluster)` bit. \n\ngraphic on how to read it.\n\nintercept >> 1\nslope of x >> x\n| >> varies by\nthese groups >> cluster\n\na common stumbling block. \n\"effect of x varies by cluster\" is not the same as \"x varies by cluster\".  \n\n\n\n\n\n# estimation\n\n## ML and REML\n\nMLE explainer\n\n- problem for lmm\nest fix > est varcorr > est fix > est varcorr\nest of varcorr assumes fixed effects are known. \nthis biases var ests to be slightly smaller  \na bit like n-1 in formula for sd\n\nREML\n- OLS to partial out fixef > \n  est varcorr > est varcorr > est varcorr > \n  use GLS to est fixef\n- in the estimation of varcorr, the fixed effects are 0 _by definition_\n  \n\n\n\n\n\n\n\n\n\n## fitting issues\n\nconvergence warnings, singular fits \n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"01b_lmm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}