{"title":"1B: Linear Mixed Models/Multi-level Models","markdown":{"yaml":{"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Fixed effects","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as **fixed** values - i.e. we estimate just one number for $b_0$, and one number for $b_1$, for $b_2$ and so on, and that's it.  \n\nIn the example where we model School children's grades as a function of their motivation score, when we fit a simple regression model of `lm(grade ~ motiv)`, the estimated parameters are two values that define a line - an intercept and a slope (as in @fig-schoolplot1).  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",lwd=2) \n  \n  \n```\n\nThe intercept and slope here are 'fixed' in the sense that it does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\nTo make this point really clear, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: \n\n$$\n\\begin{align}\n&\\text{For child }i \\\\\n&\\text{grade}_i = b_0 + b_1 \\cdot \\text{motiv}_i + \\epsilon_i \n\\end{align}\n$$\ni.e. For any child $i$ that we choose, that child's grade ($\\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\\text{motiv}_i$).  \n  \nThe issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children in our study are actually related to one another in that they can be grouped into the schools that we sampled them from. It's entirely possible (and likely) that there are school-level differences might actually account for quite a lot of the variation in grades (in [1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable) we actually estimated this to account for approx 40% of the variation in grades).  \n\nTODO CHECK below\n\nWe saw how we might add in `school` as a predictor to our linear model to estimate all these school-level differences (`lm(grade ~ schoolid + motiv)`). This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing that we want to account for - adding in the clustering as another predictor will completely account for *all* cluster-level variability in our outcome variable.  \n\nHowever, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), and at the school-level (funding). For these scenarios, we really need a multilevel model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### clusters as fixed effects\n\nWe have already seen that we can include fixed effects for cluster differences (we referred to this as \"no pooling\").  \n\ne.g. to fit school-level differences in grades, we could use:\n```{r}\n#| eval: false\nfemod <- lm(grade ~ motiv + schoolid, data = schoolmot)\n```\n\nThe model equation for this would look something like:\n$$\n\\begin{align}\n\\text{For child }i& \\\\\n\\text{grade}_i =\\, &b_0 + b_1 \\cdot \\text{motiv}_i + b_2 \\cdot \\text{isSchool2}_i + b_3 \\cdot \\text{isSchool3}_i\\,\\, + \\,\\, ... \\,\\, + \\\\\n& ... + \\,\\, ... \\,\\, + \\,\\, ... \\,\\, + \\\\\n& b_p \\cdot \\text{isSchoolP}_i\\,\\, + \\epsilon_i \n\\end{align}\n$$\n\nThe school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. \n\nBecause these set of coefficients accoutn for **all** of the school-level differences in grades, it means we are then unable to consider other school-level variables like `funding` (how much govt funding the school receives). If we try, we can see that a coefficient for `funding` is not able to be estimated because `schoolid` is explaining everything school-related:    \n\n```{r}\n#| eval: false\nfemod2 <- lm(grade ~ motiv + schoolid + funding, data = schoolmot)\nsummary(femod2)\n```\n```\nCoefficients: (1 not defined because of singularities)\n                                   Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)                        36.2655      2.5667    14.129   < 2e-16 ***\nmotiv                              1.5723       0.3673    4.281   2.07e-05 ***\nschoolidBalfron High School        1.0683       2.8923    0.369   0.711946 \nschoolidBanff Academy             -3.3253       2.9142   -1.141   0.254163 \n...                                ...          ...       ...     ... \n...                                ...          ...       ...     ... \nfunding                            NA           NA        NA      NA  \n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# The multi-level model\n\nThe multi-level model is an alternative model structure that accounts for cluster-level differences in a more flexible and parsimonious way. It achieves this by taking some of the estimated coefficients $b_?$ in our linear regression model and modelling these as randomly varying by clusters (i.e. each cluster gets its own value of $b_?$).  \n\n\nLet's see how this works by starting with the intercept, $b_0$.  \n\n## random intercept \n\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.^[Some books use \"cluster $j$ >> observation $i$\", others use \"cluster $i$ >> observation $j$\". We use the latter here] Then, we can take our coefficients $b_?$ and allow them to be different for each cluster $i$ by adding the suffix $b_{?i}$. Below, we have done this for our intercept $b_0$.  \n  \nHowever, we also need to _define_ these differences, and the multilevel model does this by expressing each cluster's intercept as a deviation ($\\zeta_{0i}$ for cluster $i$, below) from a fixed number ($\\gamma_{00}$, below). Because these differences are to do with the _clusters_ (and not the individual observations within them), we often write these as a \"level 2 equation\":    \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_1 \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\n\n::: {.callout-tip collapse=\"true\"}\n#### mixed-effects notation  \n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:  \n\n$$\n\\color{red}{y_{ij}} = \\underbrace{(\\gamma_{00} + \\color{orange}{\\zeta_{0i}})}_{\\color{blue}{b_{0i}}} \\cdot 1 + \\color{blue}{b_{1}} \\cdot x_{ij}  +  \\varepsilon_{ij}\n$$  \n\nThis notation typically corresponds with the \"mixed effects\" terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:  \n\n$$\n\\color{red}{y_{ij}} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\color{orange}{\\underbrace{\\zeta_{0i}}_{\\textrm{random}}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_2}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n$$\n\n:::\n\nReturning to our school children's grade example, we can fit a model with \"random intercepts for schools\", which would account for the possibility that some schools have higher grades, some have lower grades, etc.  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\nIf we consider one of our schools (e.g. \"Dyce Academy\") we can see that our model predicts that this school has higher grades than most other schools (@fig-ri_1school). We can see how this is modelled as a deviation $\\zeta_{0\\text{DA}}$ (DA for Dyce Academy) from some fixed value $\\gamma_{00}$.  \n\n```{r}\n#| label: fig-ri_1school\n#| echo: false\n#| fig-cap: \"Fitted values from a multilevel model with random intercepts for schools\"\nlibrary(ggforce)\nlibrary(ggfx)\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rimod, newdata=plotlabs)\n\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n#  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nspecg = plotlines |> filter(g==10) |>\n  mutate(f = fixef(rimod)[1])\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + \n  geom_line(data = specg,lwd=1,\n            aes(x=x,y=.fitted,group=g),alpha=1,col=\"orange\") +\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\") +\n  geom_curve(\n    data=specg[1,],\n    aes(x=0,xend=0,y=.fitted,yend=f,col=\"orange\"),\n    curvature=.2,lwd=1\n  ) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma*\"00\"),size=5,\n           hjust=1,vjust=1.3,col=\"#a41ae4\",parse=TRUE) +\n  annotate(\"text\",x=-.1,y=mean(unlist(specg[1,5:6])),\n           label=expression(zeta*\"0DA\"),size=5,\n           hjust=1.2,col=\"orange\")+\n  geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_label(data=plotlabs[grepl(\"Dyce\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,col=\"orange\")+\n  guides(col=\"none\")\n```\n\nTODO PANEL PLOT\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of `lm(grade ~ motiv + schoolid)`). This would also estimate an difference for each cluster?  \n\n:::sticky\nThe key to the multilevel model is that we are not actually estimating the cluster-specific differences themselves (although we can_ get these out). We are estimating a **distribution** of differences. \n:::\n\nSpecifically, the parameters of the multilevel model that are being estimated are the mean and _variance_ of a _normal_ distribution of clusters.  \n\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n\\end{align}\n$$\n\n\n- a fixed intercept $\\gamma_{00}$  \n- the variance with which schools deviate from the fixed intercept $\\sigma_0$  \n- a fixed slope for `motiv` $b_1$  \n- (and we also get the residual variance too, in $\\sigma_\\varepsilon$)  \n\n```{r}\n#| echo: false\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.2), sigma=2) + \n  stat_eye(side=\"left\",\n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\")  +\n\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\")+\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.1) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma*\"00\"),size=5,\n           hjust=1,vjust=1.3,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rimod)[1],\n               yend=fixef(rimod)[1]+sqrt(VarCorr(rimod)[[1]][1])-.5,\n               col=\"darkorange\",lwd=1) + \n  annotate(\"text\",x=-.1,y=47,\n           label=expression(sigma*\"0\"),size=5,\n           hjust=1.2,col=\"darkorange\")\n```\n\n## random slopes\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n\\end{align}\n$$\n\n```{r}\n#| echo: false\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", \n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\") \n\n# TODO add separate curve for slope dist, with rug (coloured)\n\n```\n\n\nintercepts vary\nslopes vary\n\nTODO PANEL PLOT\n\n\n## partial pooling\n\nIt's tempting to think that we could get the fixed intercept $\\gamma_{00}$ by calculating a simple linear model for each school and taking the average intercept. However, the multilevel model is more clever than that. The amount by which Each cluster in a multilevel model contributes to the estimate of the fixed intercept by an amount that depends on:   \n\na) how much between-cluster variation there is relative to within-cluster variation (TODO if clusters are very distinct )\nb) the number of observations in each cluster\n\nThis is a really useful feature, because it means that we \n\nless data\nand, when clusters are quite similar/less distinct, then they contribute similar amounts\n\n\n- socialist vs liberal analogy?  \n\nhow/why does it do this?\nby modelling a distribution of lines  \n\n\n```{r}\n#| echo: false\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) + \n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"orange\",lwd=1) +\n  ylim(0,100)\n\n```\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: how does it work?  \n\n$$\n\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}}\n$$\n\nThis means that the fixed center of this distribution (the $\\gamma_00$) However, \n\nTODO - what defines the amount to which a cluster contributes to the fixed estimate?   \nhow do far away/few n clusters influence the estimate?   \n\nin the multilevel modelling approach, each school gets its own intercept and slope, but these 'borrow strength' from the others.   \n\nThe borrowing of strength is more apparent for the (what would be) more extreme clusters, as well as those that have fewer datapoints. What happens to these cluster estimates is that they are shrunk towards the population average. \n\n$\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}}$\n\nmore shrinkage when:\n  - smaller n_j\n  - when within var is large relative to between var\n  - both of these are basically 'when we have less information about that group'  \n\n\nhttps://jeanettemumford.org/MixedModelSeries/v4-introduction-to-regularization-in-mixed-models.html#introduction-2\n\nhttps://jeanettemumford.org/MixedModelSeries/v5-conditional-modes-vs-means.html#the-equation-for-shrinkage\n\n:::\n\n\n\n\n# fitting multilevel models in R\n\n\nfixed estimates = average cluster  \n\nlme4 lmer\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n\nwe _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. \n\n::: {.callout-note collapse=\"true\"}\n#### terminology: fixed effects, random effects, variance components\n\nwe often use \"random effects\" to just mean the distribution of random deviations. i.e. \n\nsometimes you might hear \n\"random effect of group\"\n\"random effect for group\"\n\"random effect [of x] by group\"\n\ngenerally, people are referring to the `(1 + ... | cluster)` bit. \n\ngraphic on how to read it.\n\nintercept >> 1\nslope of x >> x\n| >> varies by\nthese groups >> cluster\n\na common stumbling block. \n\"effect of x varies by cluster\" is not the same as \"x varies by cluster\".  \n:::\n\n\n## model parameters\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n$$\n\n$$\n\n\n## cluster predictions  \n\nranef\n\nfixef + ranef = coef\n\ndotplot.ranef.mer\n\n```{r}\n#| echo: false\n#| eval: false\n\nexpand_grid(\n  schoolid=unique(schoolmot$schoolid),\n  motiv=0:10\n) |> broom::augment(\n  lm(grade~motiv+schoolid,schoolmot),\n  newdata=_, interval=\"confidence\"\n) -> plotlinesfix\n\nbasep + \n  with_blur(geom_line(data = plotlines,\n                      aes(x=x,y=.fitted,group=g),\n                      alpha=.4,col=\"orange\"),sigma=2) +\n  with_blur(geom_line(data = plotlinesfix,\n                      aes(x=motiv,y=.fitted,group=schoolid),\n                      alpha=.4,col=\"blue\"),sigma=2) +\n  geom_line(data = plotlinesfix[grepl(\"Calderglen\",plotlinesfix$schoolid), ], \n            aes(x=motiv,y=.fitted,group=schoolid),col=\"blue\")+\n  geom_line(data = plotlines[plotlines$g==6,], \n            aes(x=x,y=.fitted,group=g),col=\"orange\")\n\n\n```\n\n\n\n\n\n# model estimation\n\n## ML and REML\n\nMLE explainer\n\n- problem for lmm\nest fix > est varcorr > est fix > est varcorr\nest of varcorr assumes fixed effects are known. \nthis biases var ests to be slightly smaller  \na bit like n-1 in formula for sd\n\nREML\n- OLS to partial out fixef > \n  est varcorr > est varcorr > est varcorr > \n  use GLS to est fixef\n- in the estimation of varcorr, the fixed effects are 0 _by definition_\n  \n\n\n## convergence issues\n\nconvergence warnings, singular fits \n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Fixed effects\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as **fixed** values - i.e. we estimate just one number for $b_0$, and one number for $b_1$, for $b_2$ and so on, and that's it.  \n\nIn the example where we model School children's grades as a function of their motivation score, when we fit a simple regression model of `lm(grade ~ motiv)`, the estimated parameters are two values that define a line - an intercept and a slope (as in @fig-schoolplot1).  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",lwd=2) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",lwd=2) \n  \n  \n```\n\nThe intercept and slope here are 'fixed' in the sense that it does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\nTo make this point really clear, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: \n\n$$\n\\begin{align}\n&\\text{For child }i \\\\\n&\\text{grade}_i = b_0 + b_1 \\cdot \\text{motiv}_i + \\epsilon_i \n\\end{align}\n$$\ni.e. For any child $i$ that we choose, that child's grade ($\\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\\text{motiv}_i$).  \n  \nThe issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children in our study are actually related to one another in that they can be grouped into the schools that we sampled them from. It's entirely possible (and likely) that there are school-level differences might actually account for quite a lot of the variation in grades (in [1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable) we actually estimated this to account for approx 40% of the variation in grades).  \n\nTODO CHECK below\n\nWe saw how we might add in `school` as a predictor to our linear model to estimate all these school-level differences (`lm(grade ~ schoolid + motiv)`). This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing that we want to account for - adding in the clustering as another predictor will completely account for *all* cluster-level variability in our outcome variable.  \n\nHowever, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), and at the school-level (funding). For these scenarios, we really need a multilevel model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### clusters as fixed effects\n\nWe have already seen that we can include fixed effects for cluster differences (we referred to this as \"no pooling\").  \n\ne.g. to fit school-level differences in grades, we could use:\n```{r}\n#| eval: false\nfemod <- lm(grade ~ motiv + schoolid, data = schoolmot)\n```\n\nThe model equation for this would look something like:\n$$\n\\begin{align}\n\\text{For child }i& \\\\\n\\text{grade}_i =\\, &b_0 + b_1 \\cdot \\text{motiv}_i + b_2 \\cdot \\text{isSchool2}_i + b_3 \\cdot \\text{isSchool3}_i\\,\\, + \\,\\, ... \\,\\, + \\\\\n& ... + \\,\\, ... \\,\\, + \\,\\, ... \\,\\, + \\\\\n& b_p \\cdot \\text{isSchoolP}_i\\,\\, + \\epsilon_i \n\\end{align}\n$$\n\nThe school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. \n\nBecause these set of coefficients accoutn for **all** of the school-level differences in grades, it means we are then unable to consider other school-level variables like `funding` (how much govt funding the school receives). If we try, we can see that a coefficient for `funding` is not able to be estimated because `schoolid` is explaining everything school-related:    \n\n```{r}\n#| eval: false\nfemod2 <- lm(grade ~ motiv + schoolid + funding, data = schoolmot)\nsummary(femod2)\n```\n```\nCoefficients: (1 not defined because of singularities)\n                                   Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)                        36.2655      2.5667    14.129   < 2e-16 ***\nmotiv                              1.5723       0.3673    4.281   2.07e-05 ***\nschoolidBalfron High School        1.0683       2.8923    0.369   0.711946 \nschoolidBanff Academy             -3.3253       2.9142   -1.141   0.254163 \n...                                ...          ...       ...     ... \n...                                ...          ...       ...     ... \nfunding                            NA           NA        NA      NA  \n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# The multi-level model\n\nThe multi-level model is an alternative model structure that accounts for cluster-level differences in a more flexible and parsimonious way. It achieves this by taking some of the estimated coefficients $b_?$ in our linear regression model and modelling these as randomly varying by clusters (i.e. each cluster gets its own value of $b_?$).  \n\n\nLet's see how this works by starting with the intercept, $b_0$.  \n\n## random intercept \n\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.^[Some books use \"cluster $j$ >> observation $i$\", others use \"cluster $i$ >> observation $j$\". We use the latter here] Then, we can take our coefficients $b_?$ and allow them to be different for each cluster $i$ by adding the suffix $b_{?i}$. Below, we have done this for our intercept $b_0$.  \n  \nHowever, we also need to _define_ these differences, and the multilevel model does this by expressing each cluster's intercept as a deviation ($\\zeta_{0i}$ for cluster $i$, below) from a fixed number ($\\gamma_{00}$, below). Because these differences are to do with the _clusters_ (and not the individual observations within them), we often write these as a \"level 2 equation\":    \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_1 \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\n\n::: {.callout-tip collapse=\"true\"}\n#### mixed-effects notation  \n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:  \n\n$$\n\\color{red}{y_{ij}} = \\underbrace{(\\gamma_{00} + \\color{orange}{\\zeta_{0i}})}_{\\color{blue}{b_{0i}}} \\cdot 1 + \\color{blue}{b_{1}} \\cdot x_{ij}  +  \\varepsilon_{ij}\n$$  \n\nThis notation typically corresponds with the \"mixed effects\" terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:  \n\n$$\n\\color{red}{y_{ij}} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\color{orange}{\\underbrace{\\zeta_{0i}}_{\\textrm{random}}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_2}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n$$\n\n:::\n\nReturning to our school children's grade example, we can fit a model with \"random intercepts for schools\", which would account for the possibility that some schools have higher grades, some have lower grades, etc.  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\nIf we consider one of our schools (e.g. \"Dyce Academy\") we can see that our model predicts that this school has higher grades than most other schools (@fig-ri_1school). We can see how this is modelled as a deviation $\\zeta_{0\\text{DA}}$ (DA for Dyce Academy) from some fixed value $\\gamma_{00}$.  \n\n```{r}\n#| label: fig-ri_1school\n#| echo: false\n#| fig-cap: \"Fitted values from a multilevel model with random intercepts for schools\"\nlibrary(ggforce)\nlibrary(ggfx)\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rimod, newdata=plotlabs)\n\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n#  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nspecg = plotlines |> filter(g==10) |>\n  mutate(f = fixef(rimod)[1])\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + \n  geom_line(data = specg,lwd=1,\n            aes(x=x,y=.fitted,group=g),alpha=1,col=\"orange\") +\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\") +\n  geom_curve(\n    data=specg[1,],\n    aes(x=0,xend=0,y=.fitted,yend=f,col=\"orange\"),\n    curvature=.2,lwd=1\n  ) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma*\"00\"),size=5,\n           hjust=1,vjust=1.3,col=\"#a41ae4\",parse=TRUE) +\n  annotate(\"text\",x=-.1,y=mean(unlist(specg[1,5:6])),\n           label=expression(zeta*\"0DA\"),size=5,\n           hjust=1.2,col=\"orange\")+\n  geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_label(data=plotlabs[grepl(\"Dyce\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,col=\"orange\")+\n  guides(col=\"none\")\n```\n\nTODO PANEL PLOT\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of `lm(grade ~ motiv + schoolid)`). This would also estimate an difference for each cluster?  \n\n:::sticky\nThe key to the multilevel model is that we are not actually estimating the cluster-specific differences themselves (although we can_ get these out). We are estimating a **distribution** of differences. \n:::\n\nSpecifically, the parameters of the multilevel model that are being estimated are the mean and _variance_ of a _normal_ distribution of clusters.  \n\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n\\end{align}\n$$\n\n\n- a fixed intercept $\\gamma_{00}$  \n- the variance with which schools deviate from the fixed intercept $\\sigma_0$  \n- a fixed slope for `motiv` $b_1$  \n- (and we also get the residual variance too, in $\\sigma_\\varepsilon$)  \n\n```{r}\n#| echo: false\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.2), sigma=2) + \n  stat_eye(side=\"left\",\n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\")  +\n\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\")+\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.1) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma*\"00\"),size=5,\n           hjust=1,vjust=1.3,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rimod)[1],\n               yend=fixef(rimod)[1]+sqrt(VarCorr(rimod)[[1]][1])-.5,\n               col=\"darkorange\",lwd=1) + \n  annotate(\"text\",x=-.1,y=47,\n           label=expression(sigma*\"0\"),size=5,\n           hjust=1.2,col=\"darkorange\")\n```\n\n## random slopes\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n\\end{align}\n$$\n\n```{r}\n#| echo: false\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nbasep + \n  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + \n  stat_eye(side=\"left\", \n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\") \n\n# TODO add separate curve for slope dist, with rug (coloured)\n\n```\n\n\nintercepts vary\nslopes vary\n\nTODO PANEL PLOT\n\n\n## partial pooling\n\nIt's tempting to think that we could get the fixed intercept $\\gamma_{00}$ by calculating a simple linear model for each school and taking the average intercept. However, the multilevel model is more clever than that. The amount by which Each cluster in a multilevel model contributes to the estimate of the fixed intercept by an amount that depends on:   \n\na) how much between-cluster variation there is relative to within-cluster variation (TODO if clusters are very distinct )\nb) the number of observations in each cluster\n\nThis is a really useful feature, because it means that we \n\nless data\nand, when clusters are quite similar/less distinct, then they contribute similar amounts\n\n\n- socialist vs liberal analogy?  \n\nhow/why does it do this?\nby modelling a distribution of lines  \n\n\n```{r}\n#| echo: false\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) + \n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"orange\",lwd=1) +\n  ylim(0,100)\n\n```\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: how does it work?  \n\n$$\n\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}}\n$$\n\nThis means that the fixed center of this distribution (the $\\gamma_00$) However, \n\nTODO - what defines the amount to which a cluster contributes to the fixed estimate?   \nhow do far away/few n clusters influence the estimate?   \n\nin the multilevel modelling approach, each school gets its own intercept and slope, but these 'borrow strength' from the others.   \n\nThe borrowing of strength is more apparent for the (what would be) more extreme clusters, as well as those that have fewer datapoints. What happens to these cluster estimates is that they are shrunk towards the population average. \n\n$\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}}$\n\nmore shrinkage when:\n  - smaller n_j\n  - when within var is large relative to between var\n  - both of these are basically 'when we have less information about that group'  \n\n\nhttps://jeanettemumford.org/MixedModelSeries/v4-introduction-to-regularization-in-mixed-models.html#introduction-2\n\nhttps://jeanettemumford.org/MixedModelSeries/v5-conditional-modes-vs-means.html#the-equation-for-shrinkage\n\n:::\n\n\n\n\n# fitting multilevel models in R\n\n\nfixed estimates = average cluster  \n\nlme4 lmer\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n\nwe _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. \n\n::: {.callout-note collapse=\"true\"}\n#### terminology: fixed effects, random effects, variance components\n\nwe often use \"random effects\" to just mean the distribution of random deviations. i.e. \n\nsometimes you might hear \n\"random effect of group\"\n\"random effect for group\"\n\"random effect [of x] by group\"\n\ngenerally, people are referring to the `(1 + ... | cluster)` bit. \n\ngraphic on how to read it.\n\nintercept >> 1\nslope of x >> x\n| >> varies by\nthese groups >> cluster\n\na common stumbling block. \n\"effect of x varies by cluster\" is not the same as \"x varies by cluster\".  \n:::\n\n\n## model parameters\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n$$\n\n$$\n\n\n## cluster predictions  \n\nranef\n\nfixef + ranef = coef\n\ndotplot.ranef.mer\n\n```{r}\n#| echo: false\n#| eval: false\n\nexpand_grid(\n  schoolid=unique(schoolmot$schoolid),\n  motiv=0:10\n) |> broom::augment(\n  lm(grade~motiv+schoolid,schoolmot),\n  newdata=_, interval=\"confidence\"\n) -> plotlinesfix\n\nbasep + \n  with_blur(geom_line(data = plotlines,\n                      aes(x=x,y=.fitted,group=g),\n                      alpha=.4,col=\"orange\"),sigma=2) +\n  with_blur(geom_line(data = plotlinesfix,\n                      aes(x=motiv,y=.fitted,group=schoolid),\n                      alpha=.4,col=\"blue\"),sigma=2) +\n  geom_line(data = plotlinesfix[grepl(\"Calderglen\",plotlinesfix$schoolid), ], \n            aes(x=motiv,y=.fitted,group=schoolid),col=\"blue\")+\n  geom_line(data = plotlines[plotlines$g==6,], \n            aes(x=x,y=.fitted,group=g),col=\"orange\")\n\n\n```\n\n\n\n\n\n# model estimation\n\n## ML and REML\n\nMLE explainer\n\n- problem for lmm\nest fix > est varcorr > est fix > est varcorr\nest of varcorr assumes fixed effects are known. \nthis biases var ests to be slightly smaller  \na bit like n-1 in formula for sd\n\nREML\n- OLS to partial out fixef > \n  est varcorr > est varcorr > est varcorr > \n  use GLS to est fixef\n- in the estimation of varcorr, the fixed effects are 0 _by definition_\n  \n\n\n## convergence issues\n\nconvergence warnings, singular fits \n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"01b_lmm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}