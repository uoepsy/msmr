{"title":"1B: Linear Mixed Models/Multi-level Models","markdown":{"yaml":{"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"different names for the same thing","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\n\n:::lo\nThis reading:  \n\n- Introducing the multilevel model (MLM)\n- How the MLM achieves partial pooling\n- Fitting multilevel models in R\n- Model estimation and convergence\n\n::: {.callout-tip collapse=\"true\"}\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n:::\n\n:::\n\n\n# Fixed effects\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as **fixed** values - i.e. we estimate just one number for $b_0$, and one number for $b_1$, for $b_2$ and so on, and that's it.  \n\nIn the example where we model school children's grades as a function of their motivation score, when we fit a simple regression model of `lm(grade ~ motiv)`, the estimated parameters are two values that define a line - an intercept and a slope (as in @fig-schoolplot1).  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1,aes(col=schoolid)) +\n  guides(col=\"none\")+\n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(alpha=.2,col=\"black\") +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  annotate(\"text\",label=expression(b[\"0\"]),\n           col=\"blue\",size=5,\n           x=0,y=coef(fmod)[1],hjust=-.2,vjust=1.2)+\n  \n  annotate(\"text\",label=expression(b[\"1\"]),\n           col=\"blue\",size=5,\n           x=1.5,y=coef(fmod)[1]+1,hjust=-.2,vjust=1)+\n  geom_segment(x=1,xend=1.5,y=coef(fmod)[1]+1,yend=coef(fmod)[1],\n               col=\"blue\")+\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",lwd=1) \n  \n  \n```\n\nThe intercept and slope here are 'fixed' in the sense that it does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\nTo make this point really clear, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: \n\n$$\n\\begin{align}\n&\\text{For child }i \\\\\n&\\text{grade}_i = b_0 + b_1 \\cdot \\text{motiv}_i + \\epsilon_i \n\\end{align}\n$$\ni.e. For any child $i$ that we choose, that child's grade ($\\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\\text{motiv}_i$).  \n  \nThe issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children in our study are actually related to one another in that they can be grouped into the schools that we sampled them from. It's entirely possible (and likely) that there are school-level differences might actually account for quite a lot of the variation in grades. In the previous reading we actually estimated this to account for approximately `r round(suppressWarnings(ICC::ICCbare(schoolid, grade, data = schoolmot)),2)*100`% grade variation ([1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable)).  \n\nOne option we have hinted at is that we could consider adding in the `schoolid` as a predictor to our linear model to estimate all these school-level differences (`lm(grade ~ schoolid + motiv)`). This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing that we want to account for - adding in the clustering as another predictor will completely account for *all* cluster-level variability in our outcome variable.  \n\nHowever, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), *and* at the school-level (funding). For these scenarios, we really need a multilevel model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### clusters as fixed effects\n\nWe have already seen that we can include fixed effects for cluster differences (we referred to this as \"no pooling\").  \n\ne.g. to fit school-level differences in grades, we could use:\n```{r}\n#| eval: false\nfemod <- lm(grade ~ motiv + schoolid, data = schoolmot)\n```\n\nThe model equation for this would look something like:\n$$\n\\begin{align}\n\\text{For child }i& \\\\\n\\text{grade}_i =\\, &b_0 + b_1 \\cdot \\text{motiv}_i + b_2 \\cdot \\text{isSchool2}_i + b_3 \\cdot \\text{isSchool3}_i\\,\\, + \\,\\, ... \\,\\, + \\\\\n& ... + \\,\\, ... \\,\\, + \\,\\, ... \\,\\, + \\\\\n& b_p \\cdot \\text{isSchoolP}_i\\,\\, + \\epsilon_i \n\\end{align}\n$$\n\nThe school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. \n\nBecause these set of coefficients account for **all** of the school-level differences in grades, it means we are then unable to consider other school-level variables like `funding` (how much govt funding the school receives). If we try, we can see that a coefficient for `funding` is not able to be estimated because `schoolid` is explaining everything school-related:    \n\n```{r}\n#| eval: false\nfemod2 <- lm(grade ~ motiv + schoolid + funding, data = schoolmot)\nsummary(femod2)\n```\n```\nCoefficients: (1 not defined because of singularities)\n                                  Estimate  Std. Error t value Pr(>|t|)    \n(Intercept)                       33.1420   2.8257     11.729  < 2e-16 ***\nmotiv                             4.8107    0.4145     11.606  < 2e-16 ***\nschoolidArdnamurchan High School -9.6072    3.1035    -3.096   0.00203 ** \nschoolidBalwearie High School    -16.6493   3.0922    -5.384   9.36e-08 ***\n...                               ...       ...        ...     ... \n...                               ...       ...        ...     ... \nfunding                           NA        NA         NA      NA  \n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Introducing the Multilevel Model\n\nThe multi-level model is an alternative model structure that accounts for cluster-level differences in a more flexible and parsimonious way. It achieves this by taking some of the estimated coefficients (the $b_?$'s) in our linear regression model and modelling these as randomly varying by clusters (i.e. clusters differ in their value for $b_?$).  \n\n\nLet's see how this works by starting with the intercept, $b_0$.  \n\n## random intercepts\n\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.^[Some books use \"cluster $j$ >> observation $i$\", others use \"cluster $i$ >> observation $j$\". We use the latter here] Then, we can take a coefficient $b_?$ and allow it to be different for each cluster $i$ by adding the suffix $b_{?i}$. Below, we have done this for our intercept $b_0$, which has become $b_{0i}$.    \n  \nHowever, we also need to _define_ these differences in some way, and the multilevel model does this by expressing each cluster's intercept as a deviation ($\\zeta_{0i}$ for cluster $i$, below) from a fixed number ($\\gamma_{00}$, below). Because these differences are to do with the _clusters_ (and not the individual observations within them), we often write these as a \"level 2 equation\":    \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n$$\n\n::: {.callout-tip collapse=\"true\"}\n#### mixed-effects notation  \n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:  \n\n$$\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n$$  \n\nThis notation typically corresponds with the \"mixed effects\" terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:  \n\n$$\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n$$\n\n:::\n\nReturning to our school children's grade example, we can fit a model with \"random intercepts for schools\", which would account for some schools having higher grades, some having lower grades, etc.  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\nIf we consider one of our schools (e.g. \"Beeslack Community High School\") we can see that our model predicts that this school has higher grades than most other schools (@fig-ri_1school). We can see how this is modelled as a deviation $\\zeta_{0\\text{B}}$ (B for Beeslack) from some fixed value $\\gamma_{00}$.  \n\n```{r}\n#| label: fig-ri_1school\n#| echo: false\n#| fig-cap: \"Fitted values from a multilevel model with random intercepts for schools\"\nlibrary(ggforce)\nlibrary(ggfx)\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rimod, newdata=plotlabs)\n\n\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nspecg = plotlines |> filter(g==4) |>\n  mutate(f = fixef(rimod)[1])\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + \n  geom_line(data = specg,lwd=1,\n            aes(x=x,y=.fitted,group=g),alpha=1,col=\"darkorange3\") +\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\") +\n  geom_curve(\n    data=specg[1,],\n    aes(x=0,xend=0,y=.fitted,yend=f),col=\"darkorange3\",\n    curvature=.2,lwd=1\n  ) +\n  geom_point(x=0,y=fixef(rimod)[1],size=3,col=\"#a41ae4\")+\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.25,vjust=1.2,col=\"#a41ae4\") +\n  annotate(\"text\",x=-.1,y=mean(unlist(specg[1,6:7])),\n           label=expression(zeta[\"0B\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_label(data=plotlabs[grepl(\"Bees\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,col=\"darkorange3\")+\n  guides(col=\"none\")\n```\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of `lm(grade ~ motiv + schoolid)`), which would also estimate a difference for each cluster?  \n\n:::sticky\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we _can_ get these out). We are estimating a **distribution** of deviations. \n:::\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the _variance_ of a _normal_ distribution of clusters.  \n\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n:::: {.columns}\n:::{.column width=\"35%\"}\n\n<br>  \n\n- a fixed intercept $\\gamma_{00}$  \n- the variance with which schools deviate from the fixed intercept $\\sigma^2_0$  \n- a fixed slope for `motiv` $b_1$  \n- and we also need the residual variance too $\\sigma^2_\\varepsilon$  \n\n\n:::\n:::{.column width=\"10%\"}\n:::\n:::{.column width=\"55%\"}\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\nRemember, $\\sim N(m,s)$ is a way of writing \"are normally distributed with a mean of $m$ and a standard deviation of $s$\". So the $\\zeta_{0i} \\sim N(0,\\sigma_0)$ bit is saying that the school deviations from the fixed intercept are modelled as a _normal distribution_, with a mean of 0, and a standard deviation of $\\sigma_0$ (which gets estimated by our model).  \n\nThis can be seen in @fig-ri_param - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.  \n\n```{r}\n#| label: fig-ri_param\n#| echo: false\n#| fig-cap: \"grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components).\"\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.2), sigma=2) + \n  stat_eye(side=\"left\",\n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\")  +\n\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\")+\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.1) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.25,vjust=1.2,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rimod)[1],\n               yend=fixef(rimod)[1]+sqrt(VarCorr(rimod)[[1]][1]),\n               col=\"darkorange3\",lwd=1) + \n  annotate(\"text\",x=-.1,y=33,\n           label=expression(sigma[\"0\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_segment(x=0,xend=1,\n               y=(fixef(rimod) %*% c(1,0))[1],\n               yend=(fixef(rimod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(rimod) %*% c(1,1))[1],\n               yend=(fixef(rimod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(rimod) %*% c(1,2))[1],\n               yend=(fixef(rimod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(rimod) %*% c(1,3))[1],\n               yend=(fixef(rimod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(rimod) %*% c(1,0))[1],\n               yend=(fixef(rimod) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(rimod) %*% c(1,1))[1],\n               yend=(fixef(rimod) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(rimod) %*% c(1,2))[1],\n               yend=(fixef(rimod) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(rimod) %*% c(1,3))[1],\n               yend=(fixef(rimod) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.5,y=31,\n           label=expression(b[\"1\"]),size=4,\n           col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.4,\n               y=36.5,yend=31,\n               col=\"#a41ae4\")\n  \n```\n\n## random slopes\n\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). \nFor instance, we can allow the slope of $x$ on $y$ to be different for each cluster, by specifying in our model that $b_{1i}$ is a distribution of cluster deviations $\\zeta_{1i}$ around the fixed slope $\\gamma_{10}$.  \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n$$\n\nWhen we have random intercepts _and_ random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying \"random intercepts and slopes are normally distributed with mean of 0 and standard deviations of $\\sigma_0$ and $\\sigma_1$ respectively, and with a correlation of $\\rho \\sigma_0 \\sigma_1$\". We'll see more on this in future weeks, so don't worry too much right now.  \n\nIn @fig-rslope, we can see now that both the intercept *and* the slope of grades across motivation are varying by-school.  \n\n```{r}\n#| label: fig-rslope\n#| fig-cap: \"predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\"\n#| echo: false\n#| out-width: \"100%\"\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rsmod, newdata=plotlabs)\n\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=rowname),alpha=.5),sigma=1) +\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  guides(col=\"none\")+\n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\")\n```\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations $\\zeta_{1i}$ around a fixed estimate $\\gamma_{10}$. \n\nSo each group (school) now has, as visualised in @fig-unlmm: \n\n1) a deviation from the fixed intercept\n2) a deviation from the fixed slope  \n\n```{r}\n#| echo: false\n#| label: fig-unlmm\n#| fig-cap: \"random intercepts and random slopes\"\nknitr::include_graphics(\"images/un_lmm.png\")\n```\n\nWhile it's possible to show the distribution of intercepts on the left hand side of our `grade ~ motiv` plot, it's hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in @fig-rslopes2. We can see, for instance, that \"Hutcheson's Grammar School\" has a higher intercept, but a lower slope.  \n\n```{r}\n#| label: fig-rslopes2\n#| fig-cap: \"grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\"\n#| echo: false\n#| out-width: \"100%\"\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rsmod, newdata=plotlabs)\n\n\np1 <- basep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=rowname),alpha=.5),sigma=1) +\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_line(data = plotlines[grepl(\"Hutche\",plotlines$rowname),], \n            aes(x=x,y=.fitted,group=g,col=rowname),\n            alpha=1,col=\"green4\",lwd=.75) +\n  geom_label(data=plotlabs[grepl(\"Hutche\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.7,col=\"green4\")+\n  guides(col=\"none\")+\n  stat_eye(side=\"left\", \n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\") +\n  \n  annotate(\"text\",x=-.1,y=fixef(rsmod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.2,vjust=1.2,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rsmod)[1],\n               yend=fixef(rsmod)[1]+sqrt(VarCorr(rsmod)[[1]][1])-1,col=\"darkorange3\",lwd=1) + \n  annotate(\"text\",x=-.1,y=fixef(rsmod)[1]+5,\n           label=expression(sigma[\"0\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_segment(x=0,xend=1,\n               y=(fixef(rsmod) %*% c(1,0))[1],\n               yend=(fixef(rsmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(rsmod) %*% c(1,1))[1],\n               yend=(fixef(rsmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(rsmod) %*% c(1,2))[1],\n               yend=(fixef(rsmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(rsmod) %*% c(1,3))[1],\n               yend=(fixef(rsmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(rsmod) %*% c(1,0))[1],\n               yend=(fixef(rsmod) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(rsmod) %*% c(1,1))[1],\n               yend=(fixef(rsmod) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(rsmod) %*% c(1,2))[1],\n               yend=(fixef(rsmod) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(rsmod) %*% c(1,3))[1],\n               yend=(fixef(rsmod) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.9,y=33,hjust=0,\n           label=expression(gamma[\"10\"]),\n           size=5,col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.7,\n               y=36,yend=33,\n               col=\"#a41ae4\")\n\n\np2 <- as.data.frame(ranef(rsmod)$schoolid) |>\n  rownames_to_column() |>\n  ggplot(aes(x=motiv))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(rsmod)[[1]][2,2])),fill=\"#a41ae4\",xlim=c(-6,6),alpha=.3)+\n  with_blur(geom_rug(alpha=.7,aes(col=rowname),lwd=1,\n                     length = unit(0.05, \"npc\")),sigma=1) +\n  geom_segment(x=0,\n               xend=sqrt(VarCorr(rsmod)[[1]][2,2]),\n               y=0.009,yend=0.009,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",label=expression(sigma[\"1\"]),size=5,\n           x=1,y=0.021,col=\"darkorange3\")+\n  geom_vline(xintercept=0,col=\"#a41ae4\")+\n  annotate(\"text\",label=expression(gamma[\"11\"]),size=5,\n           x=0,y=0.08,col=\"#a41ae4\",hjust=-.2)+\n  scale_y_continuous(NULL,breaks=NULL)+\n  scale_x_continuous(expression(zeta[\"1i\"]))+\n  guides(col=\"none\")+\n  annotate(\"text\",label=\"Hutchesons'\\nGrammar School\",\n           x=-3.5,y=.03,col=\"green4\",vjust=0)+\n  geom_segment(x=-3.02,xend=-3.5,y=0,yend=0.03,\n               col=\"green4\")\n\n(p1 / p2) + plot_layout(heights=c(2,1))\n```\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: joint distribution of intercept and slopes\n\nWhen we have random intercepts __and__ slopes in our model, we don't just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. \nThis comes back to the part of the equation we mentioned briefly above, where we used:  \n\n- $\\sigma_0$ to represent the standard deviation of intercept deviations\n- $\\sigma_1$ to represent the standard deviation of slope deviations\n- $\\rho \\sigma_0 \\sigma_1$ to represent the correlation between intercept deviations and slope deviations  \n\n$$\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n$$\nFor a visual intuition about this, see @fig-randcor, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).  \n\n```{r}\n#| label: fig-randcor\n#| fig-cap: \"Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values\"  \n#| echo: false\n\nlibrary(ggside)\npdist = MASS::mvrnorm(1e6, mu=c(0,0),Sigma=VarCorr(rsmod)[[1]]) |>\n  as_tibble() |>\n  mutate(int=`(Intercept)`)\nas.data.frame(ranef(rsmod)$schoolid) |>\n  rownames_to_column() |>\n  mutate(int=`(Intercept)`) |>\n  ggplot(aes(x=int,y=motiv)) +\n  guides(col=\"none\")+\n  with_blur(geom_point(aes(col=rowname),size=3,alpha=.5),sigma=1) + \n  geom_density2d() +\n  scale_x_continuous(expression(zeta[\"0i\"]))+\n  scale_y_continuous(expression(zeta[\"1i\"]))+\n  geom_point(x=6.80639854,y=-3.03100767,col=\"green4\",size=3)+\n  annotate(\"text\",label=\"Hutchesons'\\nGrammar School\",\n           x=9,y=-5,col=\"green4\",vjust=1)+\n  geom_segment(x=6.8,xend=9,y=-3.03,yend=-5,\n               col=\"green4\") + \n  #with_blur(geom_rug(alpha=.7,aes(col=rowname),lwd=1,\n  #                   length = unit(0.05, \"npc\")),sigma=1) + \n  geom_xsidedensity(data=pdist,fill=\"#a41ae4\", alpha=.4,col=NA)+\n  geom_ysidedensity(data=pdist,fill=\"#a41ae4\", alpha=.4, col=NA)+\n  theme_ggside_void()\n```\n\n\n:::\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Partial pooling\n\nAs the multilevel model treats our clusters as a random distribution of deviations around some fixed center, we can think of that fixed center as the 'average cluster'. It's tempting to think that we could get the fixed intercept $\\gamma_{00}$ by calculating a simple linear model for each school and taking the average of all the intercepts. However, the multilevel model is much more clever than that. \n\nThe amount by which each cluster contributes to the fixed estimate depends on:    \n\na) how much between-cluster variation there is relative to within-cluster variation\nb) the number of observations in the cluster\n\nThis is a really useful feature, because it means that a) the model is more skeptical of clusters with few datapoints than of those with many datapoints, and b) this skepticism is weaker when clusters are in general more distinct from one another than when they are quite similar.  \n\nAnother way of thinking about this is by looking at the model predicted lines for each cluster, which are _also_ adjusted in the same way. In @fig-ppool, the blue lines show a simple linear model fitted to the data from each school (no pooling), and the orange lines show our predictions from the model with random intercepts and slopes for each school (partial pooling). \n\nNote two useful features:  \n\n1. For \"Hypothetical School X\", which has far fewer datapoints, the multilevel model line is 'shrunk' back towards the average school line. This is good because we intuitively don't want to give as much weight to that school as to the others.\n2. It is possible for the multilevel model to estimate a line for \"Hypothetical School Y\" _even though it only has one datapoint_. This is because these predicted lines \"borrow strength\" from the other schools.  \n\n\n```{r}\n#| echo: false\n#| label: fig-ppool\n#| fig-cap: \"A set of 6 schools. The blue lines show simple regression models fitted to each schools' data separately (no pooling). The orange lines show the predictions from the multilevel model in which both intercepts and slopes of motiv vary by school (partial pooling)\"\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf,\n              control=lmerControl(optimizer=\"bobyqa\"))\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1,4,5,9)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1,4,5,9)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1,4,5,9)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) +\n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"darkorange3\",lwd=1)+\n  ylim(0,100)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### A (not very good) analogy\n\nThe no-pooling approach (i.e. something like `lm(grade~motiv*schoolid)`, or fitting a simple `lm()` separately to each school) is like the libertarian ideology (valuing autonomy and personal freedom). Each school gets the freedom to define its own line without interference from others.  \n\nThe partial-pooling approach (the multilevel model) is more akin to social democracy, where there is a recognition of the value of personal freedoms and diverse perspectives, but this is *within* the framework of a broader societal structure. \n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: how does it work?  \n\nThe amount to which a cluster contributes to a fixed estimate (and the amount by which any predictions for that cluster are shrunk towards the average) is proportional to:^[this exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added]\n\n$$\n\\begin{align}\n&\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}} \\\\\n\\qquad \\\\\n\\text{Where:} \\\\\n& \\sigma^2_b = \\text{variance between clusters} \\\\\n& \\sigma^2_e = \\text{variance within clusters} \\\\\n& n_i = \\text{number of observations within cluster }i \\\\\n\\end{align}\n$$\n\nThis means that there is less contribution from (and more shrinkage of estimates for) clusters when:\n\n- smaller $n_j$ (we have less information about a cluster)\n- when within-cluster variance is large relative to between-cluster variance (clustering is not very informative)\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Fitting Multilevel Models in R\n\nWhile there is a big conceptual shift from the single level regression model to the multilevel model, the shift in R code is considerably less.  \n\nWe're going to use the `lme4` package, and specifically the functions `lmer()` and `glmer()`. \"(g)lmer\" here stands for \"(generalised) linear mixed effects regression\", and the two functions are designed to be logical extensions of `lm()` and `glm()`.  \n\n```{r}\n#| echo: false\n#| label: fig-lmercode\n#| out-width: \"100%\"\n#| fig-cap: \"Syntax of the lmer() function from the lme4 package. For now, focus on the formula section, and how this extends from the lm() function.\"\nknitr::include_graphics(\"images/lmercode.png\")\n```\n\nWe write the first bit of our **formula** just the same as our old friend the normal linear model `y ~ 1 + x1 + x2 + ...`, where `y` is the name of our outcome variable, `1` is the intercept (which we don't have to explicitly state as it will be included anyway) and `x1`, `x2` etc are the names of our explanatory variables (our predictors).  \n\nWith `lmer()`, we have the addition of __random effect terms__, specified in parenthesis with the `|` operator (the vertical line | is often found to the left of the z key on QWERTY keyboards).  \nWe use the `|` operator to separate the parameters (intercept, slope etc.) on the left-hand side, from the grouping variable(s) on the right-hand side, by which we would like to model these parameters as varying.  \n\nFor instance, the two models we have been looking at can be fitted with:  \n\n```{r}\nlibrary(lme4)\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\n# a model with random intercepts by school\n# (estimate how schools vary in their intercept)\nsmod1 <- lmer(grade ~ 1 + motiv + (1 | schoolid), \n              data = schoolmot)\n\n# a model with random intercepts and random slopes by school\n# (estimate how schools vary in their intercept, and in their slope of motiv)\nsmod2 <- lmer(grade ~ 1 + motiv + (1 + motiv | schoolid), \n              data = schoolmot)\n```\n\n\nMuch like the simple `lm()` models, we can get a nice summary output: \n```{r}\nsummary(smod2)\n```\n\nThe output contains two major components - the \"fixed effects\" and the \"random effects\". The fixed effects part contains the estimated intercept and slope(s) for the average school. The random effects part contains the estimated variance (and standard deviation^[remember, variance = standard deviation squared]) of school deviations around those fixed estimates.  \n\nSo from our ouput we can build up a picture in our heads - we know from the fixed effects that in the average school, children with zero motivation have an estimated grade of `r round(fixef(smod2)[1])`, and for every 1 more motivated a child is, their grades are estimated to increase by `r round(fixef(smod2)[2],2)`.  \n\nThe random effects part tells us that we would expect schools to vary in their intercepts with a standard deviation of 12.6, and in their slopes with a standard deviation of 2.1.  \n\nIf we recall our rough heuristic about normal distributions - that 95% of the distribution falls within 2 standard deviations, then we can start to build up a picture - we would expect most schools to have intercepts about 25 either side of the fixed intercept, and we would expect most schools to have slopes that are about 4 either side of the fixed slope (so most slopes will be between 0.5 and 8.5 - i.e. most will be positive).  \n\nSo what we're getting back to is something a bit like @fig-rslope that we saw earlier - the model provides a description of the population of schools that we have sampled from. To illustrate this more, we could imagine simulating (from our model) 1000 new schools, and plotting their lines, and we would get something like @fig-sim.  \n\n```{r}\n#| echo: false\n#| label: fig-sim\n#| fig-cap: \"Model estimated fixed effects, with 1000 simulated hypothetical school lines\"\nplotlines = MASS::mvrnorm(1000, mu=fixef(smod2),Sigma=VarCorr(smod2)[[1]]) |>\n  as.data.frame()\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=0)+\n  geom_abline(data=plotlines, aes(intercept=`(Intercept)`, slope=motiv),alpha=.05) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)+\n  geom_abline(intercept=fixef(smod2)[1],slope=fixef(smod2)[2], lwd=1, col=\"#a41ae4\") +\n  geom_point(x=0,y=fixef(smod2)[1],size=3,col=\"#a41ae4\")+\n  annotate(\"text\",x=-.1,y=fixef(smod2)[1],\n           label=\"29.23\",size=5,\n           hjust=-.2,vjust=1.3,col=\"#a41ae4\",parse=TRUE) +\n  geom_segment(x=0,xend=1,\n               y=(fixef(smod2) %*% c(1,0))[1],\n               yend=(fixef(smod2) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(smod2) %*% c(1,1))[1],\n               yend=(fixef(smod2) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(smod2) %*% c(1,2))[1],\n               yend=(fixef(smod2) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(smod2) %*% c(1,3))[1],\n               yend=(fixef(smod2) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(smod2) %*% c(1,0))[1],\n               yend=(fixef(smod2) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(smod2) %*% c(1,1))[1],\n               yend=(fixef(smod2) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(smod2) %*% c(1,2))[1],\n               yend=(fixef(smod2) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(smod2) %*% c(1,3))[1],\n               yend=(fixef(smod2) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.9,y=33,hjust=0,\n           label=\"4.48\",size=5,\n           col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.7,\n               y=36,yend=33,\n               col=\"#a41ae4\")\n```\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### mapping summary output to parts of the equation\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\nknitr::include_graphics(\"images/lmeroutput.png\")\n```\n\n:::\n\n## Extracting model parameters\n\nAlongside `summary()`, there are some useful functions in R that allow us to extract the parameters estimated by the model: \n\n:::rtip\n__fixed effects__  \n\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\n```{r}\nfixef(smod2)\n```\n\n:::\n\n\n:::rtip\n__random effect variances__  \n\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\n```{r}\nVarCorr(smod2)\n```\n\n:::\n\n## Making model predictions  \n\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models   \n\n:::rtip\n__random effects__  \n\nOften referred to as the \"random effects\", the deviations for each cluster from the fixed effects can be obtained using `ranef()`.  \nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that \"Anderson High School\" has an estimated intercept that is 7.07 _higher_ than average, and an estimate slope of motivation that is 0.47 _lower_ than average.  \n\n```{r}\n#| eval: false\nranef(smod2)\n```\n```\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\n```\n\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.  \n```{r}\ndotplot.ranef.mer(ranef(smod2))\n```\n\n:::\n\n:::rtip\n__cluster coefficients__  \n\nRather than looking at _deviations_ from fixed effects, we can calculate the intercept and slope for each cluster.  \nFor example, if we are estimating that \"Anderson High School\" has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.  \n\nWe can get these out using `coef()`\n```{r}\n#| eval: false\ncoef(smod2)\n```\n```\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n```\n:::sticky\n<center>\nfixef() + ranef() = coef()\n</center>\n:::\n\n:::\n\n\n## A more complex model\n\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child's education motivation, `motiv`), and our observations (children) happen to be clustered into groups (schools). \n\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).  \n\nFor instance, we might have questions that take the form:  \n\n- \"does [Level-2 variable] predict [Level-1 outcome]?\"  \n- \"does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?  \n\n_(in our example, Level-1 = children, Level-2 = Schools)._  \n\nConsider, for example, if we want to investigate whether the relationship between children's motivation levels and their grades is different depending upon the source of school funding (private vs state).  \n\nAddressing such questions simply requires a more complex fixed effect structure (specifically the interaction between `motiv` and `funding` (private vs state).    \n\n```{r}\nsmod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n```\n\nNote, we _cannot_ include `funding` in the random effects part of our model, because \"the effect of funding on school grades\" is something we assess by comparing _between_ schools. We cannot think of that effect varying by-school because every school is _either_ \"private\" _or_ \"state\" funded. We never observe \"Ardnamurchan High School\" as anything other than \"state\" funded, so \"the effect on grades of being state/private funded\" does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).  \n\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the `funding` coefficient and the `motiv:funding` interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in @fig-rslopes2, to having two lines (one for \"private\" schools and one for \"state\" schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Model equation\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome. \n\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the `lmer()` syntax in R, rather than in terms of the mathematical expressions. \n\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!  \n\nBecause the `funding` variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n$$\n\nIt is sometimes easier to think of this in the \"mixed effects notation\" we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:  \n$$\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n$$\n  \n::: {.callout-caution collapse=\"true\"}\n#### optional: an attempted visual explanation\n\n@fig-crosslev1 shows an attempted visual intuition of how the different parts of the model work:  \n\n```{r}\n#| echo: false\n#| label: fig-crosslev1\n#| fig-cap: \"visual explanation of a model with a cross-level interaction\"\npschools = c(\"Kilsyth Academy\",\"St Andrew's Academy\",\"Calderglen High School\",\"St Ambrose High School\", \"Beeslack Community High School\",\"The Mary Erskine School\",\"Tarbert Academy\",\"Stewarton Academy\",\"St Columba's High School\",\"Castlebrae Community High School\",\"Penicuik High School\")\n\nplotlabs = schoolmot |> count(schoolid,funding) |> select(-n) |>\n  mutate(motiv=10,x=10)\nplotlabs$y = predict(smod3, newdata=plotlabs)\n\nplotlines = as.data.frame(coef(smod3)$schoolid) |> \n  rownames_to_column() |>\n  mutate(g = 1:n(),\n         funding = ifelse(rowname%in%pschools,\"private\",\"state\"),\n         fundingstate = ifelse(funding==\"state\",fundingstate,0),\n         `motiv:fundingstate` = ifelse(funding==\"state\",`motiv:fundingstate`,0),\n         data = pmap(list(`(Intercept)`,motiv,fundingstate,`motiv:fundingstate`), \n                     ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)+..3+(..4)*(0:10)))\n  ) |> unnest(data) |>\n  select(rowname,g,funding,x,.fitted)\n\n\npf <- basep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=funding),alpha=.4),sigma=2) +\n  geom_text(data=plotlabs,\n            aes(x=x,y=y,label=schoolid,col=funding),\n            hjust=0,alpha=.3)+\n  geom_abline(intercept=fixef(smod3)[1],slope=fixef(smod3)[2], lwd=1,col=\"#00a8cc\") + \n  geom_abline(intercept=sum(fixef(smod3)[c(1,3)]),slope=sum(fixef(smod3)[c(2,4)]), \n              lwd=1,col=\"#a41ae4\")+\n  # stat_eye(side=\"left\",\n  #          data=tibble(motiv=-1,grade=50),\n  #          aes(x=0,ydist=dist_normal(fixef(smod3)[1],sqrt(VarCorr(smod3)[[1]][1]))),\n  #          alpha=.3, fill=\"#00a8cc\") +\n  # stat_eye(side=\"left\",\n  #          data=tibble(motiv=-1,grade=50),\n  #          aes(x=0,ydist=dist_normal(sum(fixef(smod3)[c(1,3)]),sqrt(VarCorr(smod3)[[1]][1]))),\n  #          alpha=.3, fill=\"#a41ae4\") +\n  geom_point(x=0,y=fixef(smod3)[1],size=3,col=\"#00a8cc\")+\n annotate(\"text\",x=-.1,y=fixef(smod3)[1],\n         label=expression(gamma[\"00\"]),size=5,\n         hjust=-.25,vjust=-.5,col=\"#00a8cc\") +\n geom_segment(x=1,xend=2,\n              y=(fixef(smod3)[1:2]%*%c(1,1))[[1]],\n              yend=(fixef(smod3)[1:2]%*%c(1,1))[[1]])+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)[1:2]%*%c(1,1))[[1]],\n              yend=(fixef(smod3)[1:2]%*%c(1,2))[[1]],\n              col=\"#00a8cc\",lwd=1)+\n  geom_segment(x=1,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,1,1,1))[[1]])+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,2))[[1]],\n              col=\"#a41ae4\",lwd=1)+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,1))[[1]],\n              col=\"#00a8cc\",lwd=1)+\n  geom_segment(x=1,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,1))[[1]],\n              col=\"#00a8cc\") +\n  geom_segment(x=0.1,xend=0.1,\n              y=(fixef(smod3)%*%c(1,0,0,0))[[1]],\n              yend=(fixef(smod3)%*%c(1,0,1,0))[[1]],\n              col=\"green4\",lwd=1) +\n  annotate(\"text\",x=0.1,y=33,\n            label=expression(gamma[\"01\"]),size=5,\n            hjust=-.3,col=\"green4\") +\n  \n  annotate(\"text\",x=2,y=(fixef(smod3)%*%c(1,1,0,0))[[1]],\n            label=expression(gamma[\"10\"]),size=5,\n            hjust=-.2,col=\"#00a8cc\") +\n  annotate(\"text\",x=2,y=(fixef(smod3)%*%c(1.1,1,1,1))[[1]],\n            label=expression(gamma[\"11\"]),size=5,\n            hjust=-.2,col=\"#a41ae4\") +\n #  \n # geom_segment(x=-.2,xend=-.2,y=fixef(smod3)[1],\n #                yend=fixef(smod3)[1]+sqrt(VarCorr(smod3)[[1]][1]),\n #                col=\"darkorange3\",lwd=1) +\n # annotate(\"text\",x=-.2,y=45,\n #            label=expression(sigma[\"0\"]),size=5,\n #            hjust=1.2,col=\"darkorange3\") +\n # geom_segment(x=-.2,xend=-.2,y=sum(fixef(smod3)[c(1,3)]),\n #               yend=sum(fixef(smod3)[c(1,3)])+sqrt(VarCorr(smod3)[[1]][1]),\n #               col=\"darkorange3\",lwd=1) +\n # annotate(\"text\",x=-.2,y=27,\n #           label=expression(sigma[\"0\"]),size=5,\n #           hjust=1.2,col=\"darkorange3\")+\n  NULL\n\npi <- as.data.frame(ranef(smod3)$schoolid) |>\n  rownames_to_column() |>\n  mutate(funding = ifelse(rowname%in%pschools,\"private\",\"state\")) |>\n  ggplot(aes(x=`(Intercept)`))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(smod3)[[1]][1,1])),fill=\"#a41ae4\",xlim=c(-30,30),alpha=.3)+\n  geom_rug(alpha=.7,aes(col=funding),lwd=1,\n                     length = unit(0.05, \"npc\")) +\n  scale_color_manual(values=c(\"#00a8cc\",\"#a41ae4\"))+\n  geom_segment(x=0,xend=sqrt(VarCorr(smod3)[[1]][1,1]),\n               y=0.005, yend=0.005,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",\n           y=0.01,x=sqrt(VarCorr(smod3)[[1]][1,1])/2,\n           label=expression(sigma[\"0\"]),\n           col=\"darkorange3\",size=5)+\n  guides(col=\"none\")+\n  scale_y_continuous(NULL,breaks=NULL)+\n  labs(title=\"intercept deviations\",x=expression(zeta[\"0i\"]))\n\nps <- as.data.frame(ranef(smod3)$schoolid) |>\n  rownames_to_column() |>\n  mutate(funding = ifelse(rowname%in%pschools,\"private\",\"state\")) |>\n  ggplot(aes(x=motiv))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(smod3)[[1]][2,2])),fill=\"#a41ae4\",xlim=c(-6,6),alpha=.3)+\n  geom_rug(alpha=.7,aes(col=funding),lwd=1,\n                     length = unit(0.05, \"npc\")) +\n  geom_segment(x=0,xend=sqrt(VarCorr(smod3)[[1]][2,2]),\n               y=0.03, yend=0.03,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",\n           y=0.06,x=sqrt(VarCorr(smod3)[[1]][2,2])/2,\n           label=expression(sigma[\"1\"]),\n           col=\"darkorange3\",size=5)+\n  guides(col=\"none\")+\n  scale_color_manual(values=c(\"#00a8cc\",\"#a41ae4\"))+\n  scale_y_continuous(NULL,breaks=NULL)+\n  labs(title=\"slope deviations\",x=expression(zeta[\"1i\"]))\n\n(pf + (pi / ps)) + plot_layout(widths=c(2,1))\n```\n\n\n:::\n\n:::\n\n\n\n\n::::panelset\n:::panel\n#### model summary\n\n```{r}\nsummary(smod3)\n```\n\n:::\n:::panel\n#### plot  \n\nFor plotting the fixed effect estimates (which are often the bit we're most interested in) from multilevel models, we can't rely on using `predict()`, `fitted()` or `augment()`, as these return to us the cluster-specific predicted values.  \n\nInstead, we need to use tools like the __effects__ package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:  \n\n```{r}\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |>\n  as.data.frame() |>\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)\n```\n\n:::\n::::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Estimation\n\nWith single level regression models fitted with `lm()`, our estimated coefficients could actually be obtained using some matrix algebra.   Fitting multilevel models is a bit more complicated, and so we have to instead rely on an iterative procedure known as \"maximum likelihood estimation\" (\"ML\" or \"MLE\").  \n\nWe actually saw this previously when fitting logistic regressions. The process is iterative in that it involves testing the fit of a set of candidate parameters (i.e. giving some initial values for our coefficients) and working out what direction we need to change them in order to get a better fit. We then change them accordingly, evalute the fit, change them, evaluate fit, ... and so on until we reach a point where we don't think we can get any better.  \n\nIn maximum likelihood estimation, the \"fit\" of the model is assessed through the _likelihood_ (the probability of seeing our data, given some hypothesis, see [here](lvp.html){target=\"_blank\"} for an explanation). \n\nIf we were estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve (@fig-mle, LH panel).  \n\nHowever, our typical models estimate a whole bunch of parameters, and with lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (@fig-mle RH panel shows it in 3D). What MLE does is try to find the maximum (the top the mountain), but avoid local maxima (false summits) and impossible values (e.g., variances $\\leq 0$), without getting stuck (in plateaus). \n```{r}\n#| label: fig-mle\n#| echo: false\n#| fig-cap: \"maximum likelihood estimation in one dimension (LEFT), and a 3D visual of a multi-dimensional likelihood surface (RIGHT)\"\nknitr::include_graphics(\"images/mle.png\")\n```\n\nWe can choose whether to estimate our model parameters with ML (maximum likelihood) or REML (restricted maximum likelihood) with the `REML` argument of `lmer()`:  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = **_logical_**, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n:::sticky\n__TL;DR__  \n\n`lmer()` models are by default fitted with REML, which tends to be better for small samples.    \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why REML?\n\nREML overcomes a problem for multilevel models fitted with standard MLE, which is that at each iteration of the maximum likelihood, the random effect variances are estimated _after_ the fixed effects (because they are the variances of clusters around the fixed effects). This means that we are essentially treating the fixed effects as a known constant when estimating the random effect variance. The downside of this is that it biases our variance estimates to be smaller than they should be^[it's a bit like n-1 being in the denominator of the formula for standard deviation], especially if $n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 < 50$. This leads to the standard errors of the fixed effects being too small, thereby inflating our type 1 error rate (i.e. greater chance of incorrectly rejecting our null hypothesis).\n\nREML avoids this by first partialling out the fixed effects (i.e. removing all of the association, so that the fixed effects are 0 _by definition_), then using maximum likelihood to iteratively estimate the random effect variances. At the end, it then uses generalised least squares to estimate the fixed effects given the known random effect structure. Because this separates the estimation of fixed and random parts of the model, it results in unbiased estimates of the variance components.   \n\n:::\n\n:::\n\n\n## convergence warnings & singular fits \n\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different 'optimisers'. \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(**_options_**) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\nTechnical problems to do with **model convergence** and **'singular fit'** come into play when the optimiser we are using either can't find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn't possible.  \n\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:  \n\n- A typical convergence warning:  \n<p style=\"color:red\">warning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)</p>\n\n- A singular fit:  \n<p style=\"color:red\">boundary (singular) fit: see ?isSingular</p>\n\n\n:::sticky\n__Do not trust the results of a model that does not converge__\n:::\n\n\nThere are lots of different ways to [deal with these](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html) (to try to rule out hypotheses about what is causing them), but for the time being, if `lmer()` gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add `control = lmerControl(optimizer = \"bobyqa\")` when you run your model.  \n\n```{r eval=F}\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n```\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\") |>\n  mutate(schoolid=factor(schoolid))\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n\n\n:::lo\nThis reading:  \n\n- Introducing the multilevel model (MLM)\n- How the MLM achieves partial pooling\n- Fitting multilevel models in R\n- Model estimation and convergence\n\n::: {.callout-tip collapse=\"true\"}\n#### different names for the same thing\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n```{r}\n#| echo: false\n#| label: fig-wordcloud\n#| fig-cap: \"size weighted by hits on google scholar search (sept 2020)\"  \ntribble(\n  ~word, ~freq,\n  \"multi-level model\", 154000 + 31300,\n  \"hierarchical linear model\", 24000,\n  \"mixed-effects model\", 56500 + 191000,\n  \"mixed model\", 1500000,\n  \"random coefficient model\", 11200+6920,\n  \"random-effects model\", 101000 + 501000,\n  \"random parameter model\", 2140 + 1460,\n  \"random-intercept model\", 17100 + 2930, \n  \"variance components model\", 6210 + 5560,\n  \"partial pooling\", 5120,\n  \"mixed error-component model\", 62,\n  \"random slope model\", 4010 + 1620,\n  \"panel data model\", 55400,\n  \"latent curve model\", 1520,\n  \"growth curve model\", 18400\n) -> mlmname\n\n\nmlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5\n\n#wordcloud2(mlmname, shape=\"diamond\", size=.4)\nlibrary(wordcloud)\nwordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,\n          min.freq=1,\n          scale=c(4,.5),\n          rot.per=0,\n          fixed.asp=T,\n          #ordered.colors=T,\n          colors=\"#a41ae4\")\n```\n\n:::\n\n:::\n\n\n# Fixed effects\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as **fixed** values - i.e. we estimate just one number for $b_0$, and one number for $b_1$, for $b_2$ and so on, and that's it.  \n\nIn the example where we model school children's grades as a function of their motivation score, when we fit a simple regression model of `lm(grade ~ motiv)`, the estimated parameters are two values that define a line - an intercept and a slope (as in @fig-schoolplot1).  \n \n```{r}\n#| echo: false\n#| label: fig-schoolplot1\n#| fig-cap: \"Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.\"\nlibrary(lme4)\nlibrary(ggdist)\nlibrary(distributional)\nfmod = lm(grade~motiv,schoolmot)\nrimod = lmer(grade~motiv+(1|schoolid),schoolmot)\nrsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)\nbasep = ggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=.1,aes(col=schoolid)) +\n  guides(col=\"none\")+\n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)\n\nbasep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +\n  geom_point(alpha=.2,col=\"black\") +\n  geom_point(x=0,y=coef(fmod)[1],size=3,col=\"blue\") +\n  annotate(\"text\",label=expression(b[\"0\"]),\n           col=\"blue\",size=5,\n           x=0,y=coef(fmod)[1],hjust=-.2,vjust=1.2)+\n  \n  annotate(\"text\",label=expression(b[\"1\"]),\n           col=\"blue\",size=5,\n           x=1.5,y=coef(fmod)[1]+1,hjust=-.2,vjust=1)+\n  geom_segment(x=1,xend=1.5,y=coef(fmod)[1]+1,yend=coef(fmod)[1],\n               col=\"blue\")+\n  geom_segment(x=0,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(coef(fmod) %*% c(1,0))[1],\n               yend=(coef(fmod) %*% c(1,1))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(coef(fmod) %*% c(1,1))[1],\n               yend=(coef(fmod) %*% c(1,2))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(coef(fmod) %*% c(1,2))[1],\n               yend=(coef(fmod) %*% c(1,3))[1],col=\"blue\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(coef(fmod) %*% c(1,3))[1],\n               yend=(coef(fmod) %*% c(1,4))[1],col=\"blue\",lwd=1) \n  \n  \n```\n\nThe intercept and slope here are 'fixed' in the sense that it does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). \n\n```{r}\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n```{r}\n#| echo: false\n.pp(summary(srmod),l=list(0,9:13))\n```\n\nTo make this point really clear, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: \n\n$$\n\\begin{align}\n&\\text{For child }i \\\\\n&\\text{grade}_i = b_0 + b_1 \\cdot \\text{motiv}_i + \\epsilon_i \n\\end{align}\n$$\ni.e. For any child $i$ that we choose, that child's grade ($\\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\\text{motiv}_i$).  \n  \nThe issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children in our study are actually related to one another in that they can be grouped into the schools that we sampled them from. It's entirely possible (and likely) that there are school-level differences might actually account for quite a lot of the variation in grades. In the previous reading we actually estimated this to account for approximately `r round(suppressWarnings(ICC::ICCbare(schoolid, grade, data = schoolmot)),2)*100`% grade variation ([1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable)).  \n\nOne option we have hinted at is that we could consider adding in the `schoolid` as a predictor to our linear model to estimate all these school-level differences (`lm(grade ~ schoolid + motiv)`). This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing that we want to account for - adding in the clustering as another predictor will completely account for *all* cluster-level variability in our outcome variable.  \n\nHowever, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), *and* at the school-level (funding). For these scenarios, we really need a multilevel model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### clusters as fixed effects\n\nWe have already seen that we can include fixed effects for cluster differences (we referred to this as \"no pooling\").  \n\ne.g. to fit school-level differences in grades, we could use:\n```{r}\n#| eval: false\nfemod <- lm(grade ~ motiv + schoolid, data = schoolmot)\n```\n\nThe model equation for this would look something like:\n$$\n\\begin{align}\n\\text{For child }i& \\\\\n\\text{grade}_i =\\, &b_0 + b_1 \\cdot \\text{motiv}_i + b_2 \\cdot \\text{isSchool2}_i + b_3 \\cdot \\text{isSchool3}_i\\,\\, + \\,\\, ... \\,\\, + \\\\\n& ... + \\,\\, ... \\,\\, + \\,\\, ... \\,\\, + \\\\\n& b_p \\cdot \\text{isSchoolP}_i\\,\\, + \\epsilon_i \n\\end{align}\n$$\n\nThe school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. \n\nBecause these set of coefficients account for **all** of the school-level differences in grades, it means we are then unable to consider other school-level variables like `funding` (how much govt funding the school receives). If we try, we can see that a coefficient for `funding` is not able to be estimated because `schoolid` is explaining everything school-related:    \n\n```{r}\n#| eval: false\nfemod2 <- lm(grade ~ motiv + schoolid + funding, data = schoolmot)\nsummary(femod2)\n```\n```\nCoefficients: (1 not defined because of singularities)\n                                  Estimate  Std. Error t value Pr(>|t|)    \n(Intercept)                       33.1420   2.8257     11.729  < 2e-16 ***\nmotiv                             4.8107    0.4145     11.606  < 2e-16 ***\nschoolidArdnamurchan High School -9.6072    3.1035    -3.096   0.00203 ** \nschoolidBalwearie High School    -16.6493   3.0922    -5.384   9.36e-08 ***\n...                               ...       ...        ...     ... \n...                               ...       ...        ...     ... \nfunding                           NA        NA         NA      NA  \n```\n\n:::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Introducing the Multilevel Model\n\nThe multi-level model is an alternative model structure that accounts for cluster-level differences in a more flexible and parsimonious way. It achieves this by taking some of the estimated coefficients (the $b_?$'s) in our linear regression model and modelling these as randomly varying by clusters (i.e. clusters differ in their value for $b_?$).  \n\n\nLet's see how this works by starting with the intercept, $b_0$.  \n\n## random intercepts\n\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.^[Some books use \"cluster $j$ >> observation $i$\", others use \"cluster $i$ >> observation $j$\". We use the latter here] Then, we can take a coefficient $b_?$ and allow it to be different for each cluster $i$ by adding the suffix $b_{?i}$. Below, we have done this for our intercept $b_0$, which has become $b_{0i}$.    \n  \nHowever, we also need to _define_ these differences in some way, and the multilevel model does this by expressing each cluster's intercept as a deviation ($\\zeta_{0i}$ for cluster $i$, below) from a fixed number ($\\gamma_{00}$, below). Because these differences are to do with the _clusters_ (and not the individual observations within them), we often write these as a \"level 2 equation\":    \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n$$\n\n::: {.callout-tip collapse=\"true\"}\n#### mixed-effects notation  \n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:  \n\n$$\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n$$  \n\nThis notation typically corresponds with the \"mixed effects\" terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:  \n\n$$\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n$$\n\n:::\n\nReturning to our school children's grade example, we can fit a model with \"random intercepts for schools\", which would account for some schools having higher grades, some having lower grades, etc.  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\nIf we consider one of our schools (e.g. \"Beeslack Community High School\") we can see that our model predicts that this school has higher grades than most other schools (@fig-ri_1school). We can see how this is modelled as a deviation $\\zeta_{0\\text{B}}$ (B for Beeslack) from some fixed value $\\gamma_{00}$.  \n\n```{r}\n#| label: fig-ri_1school\n#| echo: false\n#| fig-cap: \"Fitted values from a multilevel model with random intercepts for schools\"\nlibrary(ggforce)\nlibrary(ggfx)\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rimod, newdata=plotlabs)\n\n\nplotlines = \n  as.data.frame(coef(rimod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nspecg = plotlines |> filter(g==4) |>\n  mutate(f = fixef(rimod)[1])\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + \n  geom_line(data = specg,lwd=1,\n            aes(x=x,y=.fitted,group=g),alpha=1,col=\"darkorange3\") +\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\") +\n  geom_curve(\n    data=specg[1,],\n    aes(x=0,xend=0,y=.fitted,yend=f),col=\"darkorange3\",\n    curvature=.2,lwd=1\n  ) +\n  geom_point(x=0,y=fixef(rimod)[1],size=3,col=\"#a41ae4\")+\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.25,vjust=1.2,col=\"#a41ae4\") +\n  annotate(\"text\",x=-.1,y=mean(unlist(specg[1,6:7])),\n           label=expression(zeta[\"0B\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_label(data=plotlabs[grepl(\"Bees\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,col=\"darkorange3\")+\n  guides(col=\"none\")\n```\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of `lm(grade ~ motiv + schoolid)`), which would also estimate a difference for each cluster?  \n\n:::sticky\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we _can_ get these out). We are estimating a **distribution** of deviations. \n:::\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the _variance_ of a _normal_ distribution of clusters.  \n\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n:::: {.columns}\n:::{.column width=\"35%\"}\n\n<br>  \n\n- a fixed intercept $\\gamma_{00}$  \n- the variance with which schools deviate from the fixed intercept $\\sigma^2_0$  \n- a fixed slope for `motiv` $b_1$  \n- and we also need the residual variance too $\\sigma^2_\\varepsilon$  \n\n\n:::\n:::{.column width=\"10%\"}\n:::\n:::{.column width=\"55%\"}\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n$$\n:::\n::::\n\nRemember, $\\sim N(m,s)$ is a way of writing \"are normally distributed with a mean of $m$ and a standard deviation of $s$\". So the $\\zeta_{0i} \\sim N(0,\\sigma_0)$ bit is saying that the school deviations from the fixed intercept are modelled as a _normal distribution_, with a mean of 0, and a standard deviation of $\\sigma_0$ (which gets estimated by our model).  \n\nThis can be seen in @fig-ri_param - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.  \n\n```{r}\n#| label: fig-ri_param\n#| echo: false\n#| fig-cap: \"grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components).\"\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.2), sigma=2) + \n  stat_eye(side=\"left\",\n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\")  +\n\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\")+\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.1) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.25,vjust=1.2,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rimod)[1],\n               yend=fixef(rimod)[1]+sqrt(VarCorr(rimod)[[1]][1]),\n               col=\"darkorange3\",lwd=1) + \n  annotate(\"text\",x=-.1,y=33,\n           label=expression(sigma[\"0\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_segment(x=0,xend=1,\n               y=(fixef(rimod) %*% c(1,0))[1],\n               yend=(fixef(rimod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(rimod) %*% c(1,1))[1],\n               yend=(fixef(rimod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(rimod) %*% c(1,2))[1],\n               yend=(fixef(rimod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(rimod) %*% c(1,3))[1],\n               yend=(fixef(rimod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(rimod) %*% c(1,0))[1],\n               yend=(fixef(rimod) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(rimod) %*% c(1,1))[1],\n               yend=(fixef(rimod) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(rimod) %*% c(1,2))[1],\n               yend=(fixef(rimod) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(rimod) %*% c(1,3))[1],\n               yend=(fixef(rimod) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.5,y=31,\n           label=expression(b[\"1\"]),size=4,\n           col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.4,\n               y=36.5,yend=31,\n               col=\"#a41ae4\")\n  \n```\n\n## random slopes\n\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). \nFor instance, we can allow the slope of $x$ on $y$ to be different for each cluster, by specifying in our model that $b_{1i}$ is a distribution of cluster deviations $\\zeta_{1i}$ around the fixed slope $\\gamma_{10}$.  \n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n$$\n\nWhen we have random intercepts _and_ random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying \"random intercepts and slopes are normally distributed with mean of 0 and standard deviations of $\\sigma_0$ and $\\sigma_1$ respectively, and with a correlation of $\\rho \\sigma_0 \\sigma_1$\". We'll see more on this in future weeks, so don't worry too much right now.  \n\nIn @fig-rslope, we can see now that both the intercept *and* the slope of grades across motivation are varying by-school.  \n\n```{r}\n#| label: fig-rslope\n#| fig-cap: \"predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\"\n#| echo: false\n#| out-width: \"100%\"\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rsmod, newdata=plotlabs)\n\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=rowname),alpha=.5),sigma=1) +\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  guides(col=\"none\")+\n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\")\n```\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations $\\zeta_{1i}$ around a fixed estimate $\\gamma_{10}$. \n\nSo each group (school) now has, as visualised in @fig-unlmm: \n\n1) a deviation from the fixed intercept\n2) a deviation from the fixed slope  \n\n```{r}\n#| echo: false\n#| label: fig-unlmm\n#| fig-cap: \"random intercepts and random slopes\"\nknitr::include_graphics(\"images/un_lmm.png\")\n```\n\nWhile it's possible to show the distribution of intercepts on the left hand side of our `grade ~ motiv` plot, it's hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in @fig-rslopes2. We can see, for instance, that \"Hutcheson's Grammar School\" has a higher intercept, but a lower slope.  \n\n```{r}\n#| label: fig-rslopes2\n#| fig-cap: \"grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\"\n#| echo: false\n#| out-width: \"100%\"\nplotlines = \n  as.data.frame(coef(rsmod)$schoolid) |> \n  rownames_to_column() |>\n  mutate(\n    g = 1:n(),\n    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))\n  ) |> unnest(data)\n\nplotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)\nplotlabs$y = predict(rsmod, newdata=plotlabs)\n\n\np1 <- basep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=rowname),alpha=.5),sigma=1) +\n    geom_text(data=plotlabs,\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.3)+\n  geom_line(data = plotlines[grepl(\"Hutche\",plotlines$rowname),], \n            aes(x=x,y=.fitted,group=g,col=rowname),\n            alpha=1,col=\"green4\",lwd=.75) +\n  geom_label(data=plotlabs[grepl(\"Hutche\",plotlabs$schoolid),],\n             aes(x=x,y=y,label=schoolid),\n             hjust=0,alpha=.7,col=\"green4\")+\n  guides(col=\"none\")+\n  stat_eye(side=\"left\", \n           data=tibble(motiv=-1,grade=50),\n           aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), \n           alpha=.3, fill=\"#a41ae4\") + \n  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col=\"#a41ae4\") +\n  \n  annotate(\"text\",x=-.1,y=fixef(rsmod)[1],\n           label=expression(gamma[\"00\"]),size=5,\n           hjust=-.2,vjust=1.2,col=\"#a41ae4\",parse=TRUE) + \n  geom_segment(x=-.1,xend=-.1,y=fixef(rsmod)[1],\n               yend=fixef(rsmod)[1]+sqrt(VarCorr(rsmod)[[1]][1])-1,col=\"darkorange3\",lwd=1) + \n  annotate(\"text\",x=-.1,y=fixef(rsmod)[1]+5,\n           label=expression(sigma[\"0\"]),size=5,\n           hjust=1.2,col=\"darkorange3\")+\n  geom_segment(x=0,xend=1,\n               y=(fixef(rsmod) %*% c(1,0))[1],\n               yend=(fixef(rsmod) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(rsmod) %*% c(1,1))[1],\n               yend=(fixef(rsmod) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(rsmod) %*% c(1,2))[1],\n               yend=(fixef(rsmod) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(rsmod) %*% c(1,3))[1],\n               yend=(fixef(rsmod) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(rsmod) %*% c(1,0))[1],\n               yend=(fixef(rsmod) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(rsmod) %*% c(1,1))[1],\n               yend=(fixef(rsmod) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(rsmod) %*% c(1,2))[1],\n               yend=(fixef(rsmod) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(rsmod) %*% c(1,3))[1],\n               yend=(fixef(rsmod) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.9,y=33,hjust=0,\n           label=expression(gamma[\"10\"]),\n           size=5,col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.7,\n               y=36,yend=33,\n               col=\"#a41ae4\")\n\n\np2 <- as.data.frame(ranef(rsmod)$schoolid) |>\n  rownames_to_column() |>\n  ggplot(aes(x=motiv))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(rsmod)[[1]][2,2])),fill=\"#a41ae4\",xlim=c(-6,6),alpha=.3)+\n  with_blur(geom_rug(alpha=.7,aes(col=rowname),lwd=1,\n                     length = unit(0.05, \"npc\")),sigma=1) +\n  geom_segment(x=0,\n               xend=sqrt(VarCorr(rsmod)[[1]][2,2]),\n               y=0.009,yend=0.009,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",label=expression(sigma[\"1\"]),size=5,\n           x=1,y=0.021,col=\"darkorange3\")+\n  geom_vline(xintercept=0,col=\"#a41ae4\")+\n  annotate(\"text\",label=expression(gamma[\"11\"]),size=5,\n           x=0,y=0.08,col=\"#a41ae4\",hjust=-.2)+\n  scale_y_continuous(NULL,breaks=NULL)+\n  scale_x_continuous(expression(zeta[\"1i\"]))+\n  guides(col=\"none\")+\n  annotate(\"text\",label=\"Hutchesons'\\nGrammar School\",\n           x=-3.5,y=.03,col=\"green4\",vjust=0)+\n  geom_segment(x=-3.02,xend=-3.5,y=0,yend=0.03,\n               col=\"green4\")\n\n(p1 / p2) + plot_layout(heights=c(2,1))\n```\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: joint distribution of intercept and slopes\n\nWhen we have random intercepts __and__ slopes in our model, we don't just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. \nThis comes back to the part of the equation we mentioned briefly above, where we used:  \n\n- $\\sigma_0$ to represent the standard deviation of intercept deviations\n- $\\sigma_1$ to represent the standard deviation of slope deviations\n- $\\rho \\sigma_0 \\sigma_1$ to represent the correlation between intercept deviations and slope deviations  \n\n$$\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n$$\nFor a visual intuition about this, see @fig-randcor, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).  \n\n```{r}\n#| label: fig-randcor\n#| fig-cap: \"Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values\"  \n#| echo: false\n\nlibrary(ggside)\npdist = MASS::mvrnorm(1e6, mu=c(0,0),Sigma=VarCorr(rsmod)[[1]]) |>\n  as_tibble() |>\n  mutate(int=`(Intercept)`)\nas.data.frame(ranef(rsmod)$schoolid) |>\n  rownames_to_column() |>\n  mutate(int=`(Intercept)`) |>\n  ggplot(aes(x=int,y=motiv)) +\n  guides(col=\"none\")+\n  with_blur(geom_point(aes(col=rowname),size=3,alpha=.5),sigma=1) + \n  geom_density2d() +\n  scale_x_continuous(expression(zeta[\"0i\"]))+\n  scale_y_continuous(expression(zeta[\"1i\"]))+\n  geom_point(x=6.80639854,y=-3.03100767,col=\"green4\",size=3)+\n  annotate(\"text\",label=\"Hutchesons'\\nGrammar School\",\n           x=9,y=-5,col=\"green4\",vjust=1)+\n  geom_segment(x=6.8,xend=9,y=-3.03,yend=-5,\n               col=\"green4\") + \n  #with_blur(geom_rug(alpha=.7,aes(col=rowname),lwd=1,\n  #                   length = unit(0.05, \"npc\")),sigma=1) + \n  geom_xsidedensity(data=pdist,fill=\"#a41ae4\", alpha=.4,col=NA)+\n  geom_ysidedensity(data=pdist,fill=\"#a41ae4\", alpha=.4, col=NA)+\n  theme_ggside_void()\n```\n\n\n:::\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Partial pooling\n\nAs the multilevel model treats our clusters as a random distribution of deviations around some fixed center, we can think of that fixed center as the 'average cluster'. It's tempting to think that we could get the fixed intercept $\\gamma_{00}$ by calculating a simple linear model for each school and taking the average of all the intercepts. However, the multilevel model is much more clever than that. \n\nThe amount by which each cluster contributes to the fixed estimate depends on:    \n\na) how much between-cluster variation there is relative to within-cluster variation\nb) the number of observations in the cluster\n\nThis is a really useful feature, because it means that a) the model is more skeptical of clusters with few datapoints than of those with many datapoints, and b) this skepticism is weaker when clusters are in general more distinct from one another than when they are quite similar.  \n\nAnother way of thinking about this is by looking at the model predicted lines for each cluster, which are _also_ adjusted in the same way. In @fig-ppool, the blue lines show a simple linear model fitted to the data from each school (no pooling), and the orange lines show our predictions from the model with random intercepts and slopes for each school (partial pooling). \n\nNote two useful features:  \n\n1. For \"Hypothetical School X\", which has far fewer datapoints, the multilevel model line is 'shrunk' back towards the average school line. This is good because we intuitively don't want to give as much weight to that school as to the others.\n2. It is possible for the multilevel model to estimate a line for \"Hypothetical School Y\" _even though it only has one datapoint_. This is because these predicted lines \"borrow strength\" from the other schools.  \n\n\n```{r}\n#| echo: false\n#| label: fig-ppool\n#| fig-cap: \"A set of 6 schools. The blue lines show simple regression models fitted to each schools' data separately (no pooling). The orange lines show the predictions from the multilevel model in which both intercepts and slopes of motiv vary by school (partial pooling)\"\nset.seed(123)\n# sort(unique(schoolmot$schoolid))[c(1:4)]\nbind_rows(\n  schoolmot,\n  tibble(\n    schoolid = \"Hypothetical School X\",\n    motiv = c(-1,0.1,1.4)+5,\n    grade = 10*motiv + rnorm(3,0,10)\n  )\n) |> bind_rows(x=_, \n               tibble(schoolid=\"Hypothetical School Y\",motiv = -2+5, grade = 65)\n               ) -> tdf \n\nrsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf,\n              control=lmerControl(optimizer=\"bobyqa\"))\nfemod2 = lm(grade~motiv*schoolid,tdf)\n\nfeplot = expand_grid(\n  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1,4,5,9)]),\n               \"Hypothetical School X\"),\n  motiv = seq(0,10,.1)\n) %>% mutate(.fitted = predict(femod2, newdata = .)) \n\n\nrsplot = \n  as.data.frame(coef(rsmod2)$schoolid) |> \n  rownames_to_column(var=\"schoolid\") |>\n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1,4,5,9)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> \n  unnest(data)\n\ntdf |> \n  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1,4,5,9)] | \n           grepl(\"Hypothetical\", schoolid)) |>\n  ggplot(aes(x=motiv,y=grade))+\n  geom_point()+\n  facet_wrap(~schoolid) +\n  geom_line(data=feplot,\n            aes(y=.fitted),col=\"blue\",lwd=1)+\n  geom_line(data=rsplot,\n            aes(x=x,y=.fitted),col=\"darkorange3\",lwd=1)+\n  ylim(0,100)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### A (not very good) analogy\n\nThe no-pooling approach (i.e. something like `lm(grade~motiv*schoolid)`, or fitting a simple `lm()` separately to each school) is like the libertarian ideology (valuing autonomy and personal freedom). Each school gets the freedom to define its own line without interference from others.  \n\nThe partial-pooling approach (the multilevel model) is more akin to social democracy, where there is a recognition of the value of personal freedoms and diverse perspectives, but this is *within* the framework of a broader societal structure. \n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: how does it work?  \n\nThe amount to which a cluster contributes to a fixed estimate (and the amount by which any predictions for that cluster are shrunk towards the average) is proportional to:^[this exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added]\n\n$$\n\\begin{align}\n&\\frac{\\sigma^2_{b} }{\\sigma^2_b + \\frac{\\sigma^2_e }{n_i}} \\\\\n\\qquad \\\\\n\\text{Where:} \\\\\n& \\sigma^2_b = \\text{variance between clusters} \\\\\n& \\sigma^2_e = \\text{variance within clusters} \\\\\n& n_i = \\text{number of observations within cluster }i \\\\\n\\end{align}\n$$\n\nThis means that there is less contribution from (and more shrinkage of estimates for) clusters when:\n\n- smaller $n_j$ (we have less information about a cluster)\n- when within-cluster variance is large relative to between-cluster variance (clustering is not very informative)\n\n:::\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Fitting Multilevel Models in R\n\nWhile there is a big conceptual shift from the single level regression model to the multilevel model, the shift in R code is considerably less.  \n\nWe're going to use the `lme4` package, and specifically the functions `lmer()` and `glmer()`. \"(g)lmer\" here stands for \"(generalised) linear mixed effects regression\", and the two functions are designed to be logical extensions of `lm()` and `glm()`.  \n\n```{r}\n#| echo: false\n#| label: fig-lmercode\n#| out-width: \"100%\"\n#| fig-cap: \"Syntax of the lmer() function from the lme4 package. For now, focus on the formula section, and how this extends from the lm() function.\"\nknitr::include_graphics(\"images/lmercode.png\")\n```\n\nWe write the first bit of our **formula** just the same as our old friend the normal linear model `y ~ 1 + x1 + x2 + ...`, where `y` is the name of our outcome variable, `1` is the intercept (which we don't have to explicitly state as it will be included anyway) and `x1`, `x2` etc are the names of our explanatory variables (our predictors).  \n\nWith `lmer()`, we have the addition of __random effect terms__, specified in parenthesis with the `|` operator (the vertical line | is often found to the left of the z key on QWERTY keyboards).  \nWe use the `|` operator to separate the parameters (intercept, slope etc.) on the left-hand side, from the grouping variable(s) on the right-hand side, by which we would like to model these parameters as varying.  \n\nFor instance, the two models we have been looking at can be fitted with:  \n\n```{r}\nlibrary(lme4)\nschoolmot <- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\n# a model with random intercepts by school\n# (estimate how schools vary in their intercept)\nsmod1 <- lmer(grade ~ 1 + motiv + (1 | schoolid), \n              data = schoolmot)\n\n# a model with random intercepts and random slopes by school\n# (estimate how schools vary in their intercept, and in their slope of motiv)\nsmod2 <- lmer(grade ~ 1 + motiv + (1 + motiv | schoolid), \n              data = schoolmot)\n```\n\n\nMuch like the simple `lm()` models, we can get a nice summary output: \n```{r}\nsummary(smod2)\n```\n\nThe output contains two major components - the \"fixed effects\" and the \"random effects\". The fixed effects part contains the estimated intercept and slope(s) for the average school. The random effects part contains the estimated variance (and standard deviation^[remember, variance = standard deviation squared]) of school deviations around those fixed estimates.  \n\nSo from our ouput we can build up a picture in our heads - we know from the fixed effects that in the average school, children with zero motivation have an estimated grade of `r round(fixef(smod2)[1])`, and for every 1 more motivated a child is, their grades are estimated to increase by `r round(fixef(smod2)[2],2)`.  \n\nThe random effects part tells us that we would expect schools to vary in their intercepts with a standard deviation of 12.6, and in their slopes with a standard deviation of 2.1.  \n\nIf we recall our rough heuristic about normal distributions - that 95% of the distribution falls within 2 standard deviations, then we can start to build up a picture - we would expect most schools to have intercepts about 25 either side of the fixed intercept, and we would expect most schools to have slopes that are about 4 either side of the fixed slope (so most slopes will be between 0.5 and 8.5 - i.e. most will be positive).  \n\nSo what we're getting back to is something a bit like @fig-rslope that we saw earlier - the model provides a description of the population of schools that we have sampled from. To illustrate this more, we could imagine simulating (from our model) 1000 new schools, and plotting their lines, and we would get something like @fig-sim.  \n\n```{r}\n#| echo: false\n#| label: fig-sim\n#| fig-cap: \"Model estimated fixed effects, with 1000 simulated hypothetical school lines\"\nplotlines = MASS::mvrnorm(1000, mu=fixef(smod2),Sigma=VarCorr(smod2)[[1]]) |>\n  as.data.frame()\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point(alpha=0)+\n  geom_abline(data=plotlines, aes(intercept=`(Intercept)`, slope=motiv),alpha=.05) + \n  geom_vline(xintercept=0,lty=\"dashed\")+\n  scale_x_continuous(limits=c(-1,12),breaks=0:10)+\n  geom_abline(intercept=fixef(smod2)[1],slope=fixef(smod2)[2], lwd=1, col=\"#a41ae4\") +\n  geom_point(x=0,y=fixef(smod2)[1],size=3,col=\"#a41ae4\")+\n  annotate(\"text\",x=-.1,y=fixef(smod2)[1],\n           label=\"29.23\",size=5,\n           hjust=-.2,vjust=1.3,col=\"#a41ae4\",parse=TRUE) +\n  geom_segment(x=0,xend=1,\n               y=(fixef(smod2) %*% c(1,0))[1],\n               yend=(fixef(smod2) %*% c(1,0))[1]) +\n  geom_segment(x=1,xend=2,\n               y=(fixef(smod2) %*% c(1,1))[1],\n               yend=(fixef(smod2) %*% c(1,1))[1]) +\n  geom_segment(x=2,xend=3,\n               y=(fixef(smod2) %*% c(1,2))[1],\n               yend=(fixef(smod2) %*% c(1,2))[1]) + \n  geom_segment(x=3,xend=4,\n               y=(fixef(smod2) %*% c(1,3))[1],\n               yend=(fixef(smod2) %*% c(1,3))[1]) + \n  geom_segment(x=1,xend=1,\n               y=(fixef(smod2) %*% c(1,0))[1],\n               yend=(fixef(smod2) %*% c(1,1))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=2,xend=2,\n               y=(fixef(smod2) %*% c(1,1))[1],\n               yend=(fixef(smod2) %*% c(1,2))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=3,xend=3,\n               y=(fixef(smod2) %*% c(1,2))[1],\n               yend=(fixef(smod2) %*% c(1,3))[1],col=\"#a41ae4\",lwd=1) +\n  geom_segment(x=4,xend=4,\n               y=(fixef(smod2) %*% c(1,3))[1],\n               yend=(fixef(smod2) %*% c(1,4))[1],col=\"#a41ae4\",lwd=1) +\n  annotate(\"text\",x=2.9,y=33,hjust=0,\n           label=\"4.48\",size=5,\n           col=\"#a41ae4\")+\n  geom_segment(x=2,xend=2.7,\n               y=36,yend=33,\n               col=\"#a41ae4\")\n```\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### mapping summary output to parts of the equation\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\nknitr::include_graphics(\"images/lmeroutput.png\")\n```\n\n:::\n\n## Extracting model parameters\n\nAlongside `summary()`, there are some useful functions in R that allow us to extract the parameters estimated by the model: \n\n:::rtip\n__fixed effects__  \n\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\n```{r}\nfixef(smod2)\n```\n\n:::\n\n\n:::rtip\n__random effect variances__  \n\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\n```{r}\nVarCorr(smod2)\n```\n\n:::\n\n## Making model predictions  \n\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models   \n\n:::rtip\n__random effects__  \n\nOften referred to as the \"random effects\", the deviations for each cluster from the fixed effects can be obtained using `ranef()`.  \nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that \"Anderson High School\" has an estimated intercept that is 7.07 _higher_ than average, and an estimate slope of motivation that is 0.47 _lower_ than average.  \n\n```{r}\n#| eval: false\nranef(smod2)\n```\n```\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\n```\n\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.  \n```{r}\ndotplot.ranef.mer(ranef(smod2))\n```\n\n:::\n\n:::rtip\n__cluster coefficients__  \n\nRather than looking at _deviations_ from fixed effects, we can calculate the intercept and slope for each cluster.  \nFor example, if we are estimating that \"Anderson High School\" has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.  \n\nWe can get these out using `coef()`\n```{r}\n#| eval: false\ncoef(smod2)\n```\n```\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n```\n:::sticky\n<center>\nfixef() + ranef() = coef()\n</center>\n:::\n\n:::\n\n\n## A more complex model\n\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child's education motivation, `motiv`), and our observations (children) happen to be clustered into groups (schools). \n\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).  \n\nFor instance, we might have questions that take the form:  \n\n- \"does [Level-2 variable] predict [Level-1 outcome]?\"  \n- \"does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?  \n\n_(in our example, Level-1 = children, Level-2 = Schools)._  \n\nConsider, for example, if we want to investigate whether the relationship between children's motivation levels and their grades is different depending upon the source of school funding (private vs state).  \n\nAddressing such questions simply requires a more complex fixed effect structure (specifically the interaction between `motiv` and `funding` (private vs state).    \n\n```{r}\nsmod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n```\n\nNote, we _cannot_ include `funding` in the random effects part of our model, because \"the effect of funding on school grades\" is something we assess by comparing _between_ schools. We cannot think of that effect varying by-school because every school is _either_ \"private\" _or_ \"state\" funded. We never observe \"Ardnamurchan High School\" as anything other than \"state\" funded, so \"the effect on grades of being state/private funded\" does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).  \n\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the `funding` coefficient and the `motiv:funding` interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in @fig-rslopes2, to having two lines (one for \"private\" schools and one for \"state\" schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Model equation\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome. \n\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the `lmer()` syntax in R, rather than in terms of the mathematical expressions. \n\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!  \n\nBecause the `funding` variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:  \n\n$$\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n$$\n\nIt is sometimes easier to think of this in the \"mixed effects notation\" we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:  \n$$\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n$$\n  \n::: {.callout-caution collapse=\"true\"}\n#### optional: an attempted visual explanation\n\n@fig-crosslev1 shows an attempted visual intuition of how the different parts of the model work:  \n\n```{r}\n#| echo: false\n#| label: fig-crosslev1\n#| fig-cap: \"visual explanation of a model with a cross-level interaction\"\npschools = c(\"Kilsyth Academy\",\"St Andrew's Academy\",\"Calderglen High School\",\"St Ambrose High School\", \"Beeslack Community High School\",\"The Mary Erskine School\",\"Tarbert Academy\",\"Stewarton Academy\",\"St Columba's High School\",\"Castlebrae Community High School\",\"Penicuik High School\")\n\nplotlabs = schoolmot |> count(schoolid,funding) |> select(-n) |>\n  mutate(motiv=10,x=10)\nplotlabs$y = predict(smod3, newdata=plotlabs)\n\nplotlines = as.data.frame(coef(smod3)$schoolid) |> \n  rownames_to_column() |>\n  mutate(g = 1:n(),\n         funding = ifelse(rowname%in%pschools,\"private\",\"state\"),\n         fundingstate = ifelse(funding==\"state\",fundingstate,0),\n         `motiv:fundingstate` = ifelse(funding==\"state\",`motiv:fundingstate`,0),\n         data = pmap(list(`(Intercept)`,motiv,fundingstate,`motiv:fundingstate`), \n                     ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)+..3+(..4)*(0:10)))\n  ) |> unnest(data) |>\n  select(rowname,g,funding,x,.fitted)\n\n\npf <- basep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g,col=funding),alpha=.4),sigma=2) +\n  geom_text(data=plotlabs,\n            aes(x=x,y=y,label=schoolid,col=funding),\n            hjust=0,alpha=.3)+\n  geom_abline(intercept=fixef(smod3)[1],slope=fixef(smod3)[2], lwd=1,col=\"#00a8cc\") + \n  geom_abline(intercept=sum(fixef(smod3)[c(1,3)]),slope=sum(fixef(smod3)[c(2,4)]), \n              lwd=1,col=\"#a41ae4\")+\n  # stat_eye(side=\"left\",\n  #          data=tibble(motiv=-1,grade=50),\n  #          aes(x=0,ydist=dist_normal(fixef(smod3)[1],sqrt(VarCorr(smod3)[[1]][1]))),\n  #          alpha=.3, fill=\"#00a8cc\") +\n  # stat_eye(side=\"left\",\n  #          data=tibble(motiv=-1,grade=50),\n  #          aes(x=0,ydist=dist_normal(sum(fixef(smod3)[c(1,3)]),sqrt(VarCorr(smod3)[[1]][1]))),\n  #          alpha=.3, fill=\"#a41ae4\") +\n  geom_point(x=0,y=fixef(smod3)[1],size=3,col=\"#00a8cc\")+\n annotate(\"text\",x=-.1,y=fixef(smod3)[1],\n         label=expression(gamma[\"00\"]),size=5,\n         hjust=-.25,vjust=-.5,col=\"#00a8cc\") +\n geom_segment(x=1,xend=2,\n              y=(fixef(smod3)[1:2]%*%c(1,1))[[1]],\n              yend=(fixef(smod3)[1:2]%*%c(1,1))[[1]])+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)[1:2]%*%c(1,1))[[1]],\n              yend=(fixef(smod3)[1:2]%*%c(1,2))[[1]],\n              col=\"#00a8cc\",lwd=1)+\n  geom_segment(x=1,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,1,1,1))[[1]])+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,2))[[1]],\n              col=\"#a41ae4\",lwd=1)+\n  geom_segment(x=2,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,1))[[1]],\n              col=\"#00a8cc\",lwd=1)+\n  geom_segment(x=1,xend=2,\n              y=(fixef(smod3)%*%c(1,1,1,1))[[1]],\n              yend=(fixef(smod3)%*%c(1,2,1,1))[[1]],\n              col=\"#00a8cc\") +\n  geom_segment(x=0.1,xend=0.1,\n              y=(fixef(smod3)%*%c(1,0,0,0))[[1]],\n              yend=(fixef(smod3)%*%c(1,0,1,0))[[1]],\n              col=\"green4\",lwd=1) +\n  annotate(\"text\",x=0.1,y=33,\n            label=expression(gamma[\"01\"]),size=5,\n            hjust=-.3,col=\"green4\") +\n  \n  annotate(\"text\",x=2,y=(fixef(smod3)%*%c(1,1,0,0))[[1]],\n            label=expression(gamma[\"10\"]),size=5,\n            hjust=-.2,col=\"#00a8cc\") +\n  annotate(\"text\",x=2,y=(fixef(smod3)%*%c(1.1,1,1,1))[[1]],\n            label=expression(gamma[\"11\"]),size=5,\n            hjust=-.2,col=\"#a41ae4\") +\n #  \n # geom_segment(x=-.2,xend=-.2,y=fixef(smod3)[1],\n #                yend=fixef(smod3)[1]+sqrt(VarCorr(smod3)[[1]][1]),\n #                col=\"darkorange3\",lwd=1) +\n # annotate(\"text\",x=-.2,y=45,\n #            label=expression(sigma[\"0\"]),size=5,\n #            hjust=1.2,col=\"darkorange3\") +\n # geom_segment(x=-.2,xend=-.2,y=sum(fixef(smod3)[c(1,3)]),\n #               yend=sum(fixef(smod3)[c(1,3)])+sqrt(VarCorr(smod3)[[1]][1]),\n #               col=\"darkorange3\",lwd=1) +\n # annotate(\"text\",x=-.2,y=27,\n #           label=expression(sigma[\"0\"]),size=5,\n #           hjust=1.2,col=\"darkorange3\")+\n  NULL\n\npi <- as.data.frame(ranef(smod3)$schoolid) |>\n  rownames_to_column() |>\n  mutate(funding = ifelse(rowname%in%pschools,\"private\",\"state\")) |>\n  ggplot(aes(x=`(Intercept)`))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(smod3)[[1]][1,1])),fill=\"#a41ae4\",xlim=c(-30,30),alpha=.3)+\n  geom_rug(alpha=.7,aes(col=funding),lwd=1,\n                     length = unit(0.05, \"npc\")) +\n  scale_color_manual(values=c(\"#00a8cc\",\"#a41ae4\"))+\n  geom_segment(x=0,xend=sqrt(VarCorr(smod3)[[1]][1,1]),\n               y=0.005, yend=0.005,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",\n           y=0.01,x=sqrt(VarCorr(smod3)[[1]][1,1])/2,\n           label=expression(sigma[\"0\"]),\n           col=\"darkorange3\",size=5)+\n  guides(col=\"none\")+\n  scale_y_continuous(NULL,breaks=NULL)+\n  labs(title=\"intercept deviations\",x=expression(zeta[\"0i\"]))\n\nps <- as.data.frame(ranef(smod3)$schoolid) |>\n  rownames_to_column() |>\n  mutate(funding = ifelse(rowname%in%pschools,\"private\",\"state\")) |>\n  ggplot(aes(x=motiv))+\n  geom_area(stat = \"function\", fun = dnorm,args=list(mean=0,sd=sqrt(VarCorr(smod3)[[1]][2,2])),fill=\"#a41ae4\",xlim=c(-6,6),alpha=.3)+\n  geom_rug(alpha=.7,aes(col=funding),lwd=1,\n                     length = unit(0.05, \"npc\")) +\n  geom_segment(x=0,xend=sqrt(VarCorr(smod3)[[1]][2,2]),\n               y=0.03, yend=0.03,\n               col=\"darkorange3\",lwd=1)+\n  annotate(\"text\",\n           y=0.06,x=sqrt(VarCorr(smod3)[[1]][2,2])/2,\n           label=expression(sigma[\"1\"]),\n           col=\"darkorange3\",size=5)+\n  guides(col=\"none\")+\n  scale_color_manual(values=c(\"#00a8cc\",\"#a41ae4\"))+\n  scale_y_continuous(NULL,breaks=NULL)+\n  labs(title=\"slope deviations\",x=expression(zeta[\"1i\"]))\n\n(pf + (pi / ps)) + plot_layout(widths=c(2,1))\n```\n\n\n:::\n\n:::\n\n\n\n\n::::panelset\n:::panel\n#### model summary\n\n```{r}\nsummary(smod3)\n```\n\n:::\n:::panel\n#### plot  \n\nFor plotting the fixed effect estimates (which are often the bit we're most interested in) from multilevel models, we can't rely on using `predict()`, `fitted()` or `augment()`, as these return to us the cluster-specific predicted values.  \n\nInstead, we need to use tools like the __effects__ package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:  \n\n```{r}\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |>\n  as.data.frame() |>\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)\n```\n\n:::\n::::\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Estimation\n\nWith single level regression models fitted with `lm()`, our estimated coefficients could actually be obtained using some matrix algebra.   Fitting multilevel models is a bit more complicated, and so we have to instead rely on an iterative procedure known as \"maximum likelihood estimation\" (\"ML\" or \"MLE\").  \n\nWe actually saw this previously when fitting logistic regressions. The process is iterative in that it involves testing the fit of a set of candidate parameters (i.e. giving some initial values for our coefficients) and working out what direction we need to change them in order to get a better fit. We then change them accordingly, evalute the fit, change them, evaluate fit, ... and so on until we reach a point where we don't think we can get any better.  \n\nIn maximum likelihood estimation, the \"fit\" of the model is assessed through the _likelihood_ (the probability of seeing our data, given some hypothesis, see [here](lvp.html){target=\"_blank\"} for an explanation). \n\nIf we were estimating just one single parameter (e.g. a mean), then we can imagine the process of maximum likelihood estimation in a one-dimensional world - simply finding the top of the curve (@fig-mle, LH panel).  \n\nHowever, our typical models estimate a whole bunch of parameters, and with lots of parameters being estimated and all interacting to influence the likelihood, our nice curved line becomes a complex surface (@fig-mle RH panel shows it in 3D). What MLE does is try to find the maximum (the top the mountain), but avoid local maxima (false summits) and impossible values (e.g., variances $\\leq 0$), without getting stuck (in plateaus). \n```{r}\n#| label: fig-mle\n#| echo: false\n#| fig-cap: \"maximum likelihood estimation in one dimension (LEFT), and a 3D visual of a multi-dimensional likelihood surface (RIGHT)\"\nknitr::include_graphics(\"images/mle.png\")\n```\n\nWe can choose whether to estimate our model parameters with ML (maximum likelihood) or REML (restricted maximum likelihood) with the `REML` argument of `lmer()`:  \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = **_logical_**, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(*options*) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\n:::sticky\n__TL;DR__  \n\n`lmer()` models are by default fitted with REML, which tends to be better for small samples.    \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why REML?\n\nREML overcomes a problem for multilevel models fitted with standard MLE, which is that at each iteration of the maximum likelihood, the random effect variances are estimated _after_ the fixed effects (because they are the variances of clusters around the fixed effects). This means that we are essentially treating the fixed effects as a known constant when estimating the random effect variance. The downside of this is that it biases our variance estimates to be smaller than they should be^[it's a bit like n-1 being in the denominator of the formula for standard deviation], especially if $n_\\textrm{clusters} - n_\\textrm{level 2 predictors} - 1 < 50$. This leads to the standard errors of the fixed effects being too small, thereby inflating our type 1 error rate (i.e. greater chance of incorrectly rejecting our null hypothesis).\n\nREML avoids this by first partialling out the fixed effects (i.e. removing all of the association, so that the fixed effects are 0 _by definition_), then using maximum likelihood to iteratively estimate the random effect variances. At the end, it then uses generalised least squares to estimate the fixed effects given the known random effect structure. Because this separates the estimation of fixed and random parts of the model, it results in unbiased estimates of the variance components.   \n\n:::\n\n:::\n\n\n## convergence warnings & singular fits \n\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different 'optimisers'. \n\n<br>\n<div style=\"margin-left:50px;\">\nlmer(*formula*,<br>\n&nbsp; &nbsp; &nbsp; &nbsp; data = *dataframe*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; REML = *logical*, <br>\n&nbsp; &nbsp; &nbsp; &nbsp; control = lmerControl(**_options_**) <br>\n&nbsp; &nbsp; &nbsp; &nbsp; )</div>    \n<br>\n\nTechnical problems to do with **model convergence** and **'singular fit'** come into play when the optimiser we are using either can't find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn't possible.  \n\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:  \n\n- A typical convergence warning:  \n<p style=\"color:red\">warning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)</p>\n\n- A singular fit:  \n<p style=\"color:red\">boundary (singular) fit: see ?isSingular</p>\n\n\n:::sticky\n__Do not trust the results of a model that does not converge__\n:::\n\n\nThere are lots of different ways to [deal with these](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html) (to try to rule out hypotheses about what is causing them), but for the time being, if `lmer()` gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add `control = lmerControl(optimizer = \"bobyqa\")` when you run your model.  \n\n```{r eval=F}\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))\n```\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"01b_lmm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"1B: Linear Mixed Models/Multi-level Models","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}