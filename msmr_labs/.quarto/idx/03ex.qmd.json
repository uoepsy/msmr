{"title":"Week 3 Exercises: Non-Linear Change","markdown":{"yaml":{"title":"Week 3 Exercises: Non-Linear Change","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Cognitive Task Performance","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n\n:::frame\n__Dataset: Az.rda__  \n  \nThese data are available at [https://uoepsy.github.io/data/Az.rda](https://uoepsy.github.io/data/Az.rda). You can load the dataset using:  \n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n```\nand you will find the `Az` object in your environment.  \n\nThe `Az` object contains information on 30 Participants with probable Alzheimer's Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100. \n\nWe're interested in *whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance*.  \n\n```{r}\n#| echo: false\ntibble(\n    variable = names(Az),\n    description = c(\"Unique Subject Identifier\",\"Time point of the study (1 to 10)\",\"Task type (Memory, cADL, sADL)\",\"Score on test (range 0 to 100)\")\n) |> gt::gt()\n```\n:::\n\n`r qbegin(qcounter())`\nLoad in the data and examine it.  \nHow many participants, how many observations per participant, per task?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n```\n\n30 participants: \n```{r}\nlength(unique(Az$Subject))\n```\n\nDoes every participant have 10 datapoints for each Task type?  Yes!  \n```{r}\nany( table(Az$Subject, Az$Task) != 10 )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nNo modelling just yet.  \n\nPlot the performance over time for each type of task.  \n\nTry using `stat_summary` so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nFor an example plot, see [Chapter 6: polynomial growth #example-in-mlm](https://uoepsy.github.io/lmm/06_poly.html#example-in-mlm){target=\"_blank\"}.\n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nYou can use \"pointranges\", or \"line\" and \"ribbon\".  \n`stat_summary` will take the data and for each value of `x` calculate some function (in this case the mean, or the mean and SE):  \n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n```\n`r solend()`\n\n`r qbegin(qcounter())`\nWhy do you think *raw/natural* polynomials might be more useful than *orthogonal* polynomials for these data?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rate of change) at *a specific point* in time?  \n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nBecause we're interested in whether there are task differences at the starting point, raw polynomials are probably what we want here.  \n`r solend()`\n\n\n`r qbegin(qcounter())`\nRe-center the Time variable so that the intercept is the first timepoint.  \n  \nThen choose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nNote there is no part of the research question that specifically asks about how \"gradual\" or \"quick\" the change is (which would suggest we are interested in the quadratic term).  \n\nHowever, plots (summarised at the task level, and of individual participant's data separately) can help to give us a sense of what degree of polynomial terms might be suitable to succinctly describe the trends.  \n\nIn many cases, fitting higher and higher order polynomials will likely result in a 'better fit' to our sample data, but these will be worse and worse at generalising to new data - i.e. we run the risk of *overfitting*.  \n\n:::\n\n`r qend()`\n`r solbegin(label=\"1 - how many polynomials?\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nIn our plot, there were 2 straight line and one slightly curvy one. It wasn't S-shaped or 'wiggly' or anything, there was just a bit of a bend in it, which suggests that the quadratic term could be a good approximation here.   \n\nIt's worth also looking at the individual participant trajectories. In these we can also see the curvi-ness of the blue line - it's more pronounced for some people than others, but it does seem like individual's trajectories on the Memory task may be curvilinear. \n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")+\n  facet_wrap(~Subject)\n```\n\n\n```{r}\nlibrary(lme4)\nAz <- Az |> mutate(\n  Time1 = Time-1,\n  poly1 = poly(Time1,2,raw=T)[,1],\n  poly2 = poly(Time1,2,raw=T)[,2]\n)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Or using Dan's code:  \n\n```{r}\n#| eval: false\n# import Dan's code:\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\n# this also produces a nice little plot to show the polynomials\nAz$Time1 <- Az$Time-1\nAz <- code_poly(df = Az, predictor = 'Time1',\n                poly.order = 2, orthogonal = FALSE)\n```\n\n:::\n\n\n\n`r solend()`\n`r solbegin(label=\"2 - fixed effects\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nWe're interested in how performance changes over time, but we have `poly1` and `poly2` for time, so we're at:  \n```\nlmer(Performance ~ poly1 + poly2 ... \n```\n\nOur research aims are to investigate differences between task performance (both at baseline and change over time). So we want to interact time with task:  \n```\nlmer(Performance ~ (poly1 + poly2)*Task ... \n```\n\n`r solend()`\n`r solbegin(label=\"3 - grouping structure\", slabel=F,show=T, toggle=params$TOGGLE)`\n```{r}\nhead(Az)\n```\n\nWe have `r nrow(Az)` observations, and `r table(Az$Subject)[1]` for each participant.  \n\n```\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + .... | Subject)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Fixed vs Random\n\nWe can account for group differences in models either by estimating group differences, or by estimating variance between groups:  \n\n- group as a fixed effect (`y ~ 1 + group`) = groups differ in $y$ by $b_1, b_2, ..., b_k$  \n- group as a random effect (`y ~ 1 + (1|group)`) = groups vary in $y$ with a standard deviation of $\\sigma_0$   \n\nOne way to think about whether a group is best in the random effects part or in the fixed part of our model is to think about \"what would happen if I repeated the experiment?\"  \n\n**Should variable `g` be fixed or random?**\n\n|  | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_ | \n|----------------|--------------------------------------------------|----------------------------------------------------|\n| Fixed<br>$y\\,\\sim\\,~\\,...\\, +\\, g$  | Same groups would be used   |    The groups used  |\n| Random<br>$y\\,\\sim\\,...\\,+\\,(\\,... |\\,g)$ | Different groups would be used   | A population from which the groups used are just a (random) sample |\n\n\nPractical points:  \n- Sometimes there isn't enough variability between groups to model as random effects (i.e. the variance gets estimated as too close to zero).\n- Sometimes you might not have sufficient number of groups to model as random effects (e.g. for groups of fewer than c8 things, estimates of the variance are often not a reliable reflection of the population).\n\n\n:::\n\n\n`r solend()`\n`r solbegin(label=\"4 - random effects\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nWhat slopes could vary by participant? \n\n__Q:__ Could participants vary in their performance over time?  \n__A:__ Yes, `(poly1 + poly2 | Subject)`  \n__Q:__ Could participants vary in how performance differs between Tasks?  \n__A:__ Yes, `(poly1 + poly2 + Task | Subject)`. E.g., Some participants might be much better at the memory task than other tasks, some might be better at the other tasks.  \n__Q:__ Could participants vary in how tasks differ in their performance over time?  \n__A:__ Yes, `((poly1 + poly2)*Task | Subject)`. E.g., For some participants, memory could decline more than cADL, for other participants it could decline less.  \n\n```\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + (poly1 + poly2)*Task | Subject)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - the model\", slabel=F,show=T, toggle=params$TOGGLE)`\n\n_small note: I'm using the bobyqa optimizer here because it tends to be slightly quicker than the default._  \n```{r}\n#| eval: false\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n<p style=\"color:red;font-size:.8em\">\nboundary (singular) fit: see help('isSingular')<br>\nWarning messages:<br>\n1: In commonArgs(par, fn, control, environment()) :<br>\n  maxfun < 10 * length(par)^2 is not recommended.<br>\n2: In optwrap(optimizer, devfun, getStart(start, rho\\$pp), lower = rho\\$lower,  :<br>\n  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded\n</p>\n\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nOkay, so the model didn't converge.  It's trying to estimate __a lot__ of things in the random effects (even though it didn't converge, try looking at `VarCorr(model)` to see all the covariances it is trying to estimate).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Categorical random effects on the RHS\n\nWhen we have a categorical random effect (i.e. where the `x` in `(1 + x | g)` is a categorical variable), then model estimation can often get tricky, because \"the effect of x\" for a categorical variable with $k$ levels is identified via $k-1$ parameters, meaning we have a lot of variances and covariances to estimate when we include `x|g`.  \n\n:::: {.columns}\n:::{.column width=\"45%\"}\n\nWhen `x` is numeric:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n\nWhen `x` is categorical with $k$ levels:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n::::\n\nHowever, we can use an alternative formation of the random effects by putting a categorical `x` into the right-hand side:  \nInstead of `(1 + x | g)` we can fit `(1 | g) + (1 | g:x)`.   \n\nThe symbol `:` in `g:x` is used to refer to the combination of `g` and `x`.  \n\n```{r}\n#| echo: false\ngx = tibble(\n  g = c(\"p1\",\"p1\",\"p1\",\"...\",\"p2\",\"p2\",\"...\"),\n  x = c(\"a\",\"a\",\"b\",\"...\",\"a\",\"b\",\"...\"),\n  `g:x` = as.character(interaction(g,x))\n)\ngx[c(4,7),3] <- \"...\"\ngx$g = paste0(\"  \",gx$g,\"   \")\ngx$x = paste0(\"  \",gx$x,\"   \")\nnames(gx)<-c(\"  g   \",\"  x   \",\"g:x\")\nas.data.frame(gx)\n```\n\nIt's a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:  \n\n- `(1 + x | g)`:  each group of `g` can have a different intercept and a different effect of `x`  \n- `(1 | g) + (1 | g:x)`: each group of `g` can have a different intercept, and each level of x within each `g` can have a different intercept.  \n\nBoth of these allow the outcome `y` to change across `x` differently for each group in `g` (i.e. both of them result in `y` being different for each level of `x` in each group `g`).  \nThe first does so explicitly by estimating the group level variance of the `y~x` effect.  \nThe second one estimates the variance of $y$ between groups, and also the variance of $y$ between 'levels of x within groups'. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of `x`, we don't have to worry about lots of covariances:  \n\n\n:::: {.columns}\n:::{.column width=\"45%\"}\n`(1 + x | g)`  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n`(1 | g) + (1 | g:x)`  \n\n\n```\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n```\n\n:::\n::::\n\n:::\n\n\nTry adjusting your model by first moving `Task` to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one) \n\n**This is our first experience of our random effect structures becoming more complex than simply `(.... | group)`. This is going to feel confusing, but don't worry, we'll see more structures like this next week.**    \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)`  \n\nTo then start simplifying (if this model doesn't converge), it can be helpful to look at the `VarCorr()` of the non-converging model to see if anything looks awry. Look for small variances, perfect (or near perfect) correlations. These might be sensible things to remove.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere's our model with subject-task effects on the right hand side.  \nAgain we have problems, as we have a singular fit:  \n```{r}\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n<p style=\"color:red;font-size:.8em\">boundary (singular) fit: see help('isSingular')</p>\n\nLooking at the random effects of our model, note that the `poly2|Subject` random effect has very little variance (and high correlations).  \nNote that it makes sense that by including the random effects for `Subject:Task`, there might not be much above that leftover in `Subject` random effects. \n```{r}\nVarCorr(m2)\n```\n\n\nWhen we remove it, our model converges!!  \n```{r}\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\nConduct a series of model comparisons investigating whether \n\n1. Tasks differ only in their linear change  \n2. Tasks differ in their quadratic change  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we're interested in the how the average participant performs over the study). So our models want to have the same random effect structure, but different fixed effects.  \n\nSee the end of [Chapter 6: polynomial growth #example-in-mlm](https://uoepsy.github.io/lmm/06_poly.html#example-in-mlm){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs I'm comparing these with a likelihood ratio test, I'll fit them with `REML=FALSE`\n```{r}\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3full = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3full)\n```\n\n:::int\n```{r}\n#| echo: false\nres = anova(m3int, m3lin, m3)\n```\n\nThe linear change over time differs between Tasks ($\\chi^2(`r round(res[2,7],2)`) = `r round(res[2,6],2)`, p `r format.pval(res[2,8],eps=.001)`$).  \nThe quadratic change over time differs between Tasks ($\\chi^2(`r round(res[3,7],2)`) = `r round(res[3,6],2)`, p `r format.pval(res[3,8],eps=.001)`$).  \n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### completely optional - comparisons and raw/orthogonal \n\n```{r}\n#| include: false\nAggaz <- Az |> group_by(Time1,poly1,poly2,Task) |>\n  summarise(Performance = mean(Performance)) |>\n  ungroup() |>\n  mutate(opoly1 = poly(Time1,degree=2)[,1],\n         opoly2 = poly(Time1,degree=2)[,2])\n\nmodA <- lm(Performance ~ poly1 + poly2 + Task,Aggaz)\nmodB <- lm(Performance ~ poly1*Task + poly2,Aggaz)\nmodC <- lm(Performance ~ poly1 + poly2*Task,Aggaz)\nmodD <- lm(Performance ~ (poly1 + poly2)*Task,Aggaz)\n\n\n# modAo <- lm(Performance ~ opoly1 + opoly2 + Task,Aggaz)\n# modBo <- lm(Performance ~ opoly1*Task + opoly2,Aggaz)\n# modCo <- lm(Performance ~ opoly1 + opoly2*Task,Aggaz)\n# modDo <- lm(Performance ~ (opoly1 + opoly2)*Task,Aggaz)\n# \n# \n# # anova(modA,modB)\n# # anova(modAo,modBo)\n# # \n# # anova(modC,modD)\n# # anova(modCo,modDo)\n# \np1 <- broom::augment(modA) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"A\",subtitle=\"Perf ~ poly1 + poly2 + Task\",\n       x=\"time\")+\n  guides(col=\"none\")\n\np2 <- broom::augment(modB) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"B\",subtitle=\"Perf ~ poly1*Task + poly2\",x=\"time\")+\n  guides(col=\"none\")\n\np3 <- broom::augment(modC) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"C\",subtitle=\"Perf ~ poly1 + poly2*Task\",x=\"time\")+\n  guides(col=\"none\")\n\np4 <- broom::augment(modD) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"D\",subtitle=\"Perf ~ (poly1 + poly2)*Task\",x=\"time\")+\n  guides(col=\"none\")\n\n\n\n\n\n```\n\n\nThere's an argument to be made that to ask \"do tasks differ in Linear change?\" we could test either (as above): \n\n**A:** `Performance ~ poly1 + poly2 + Task`  \nvs   \n**B:**  `Performance ~ poly1*Task + poly2`  \n  \nOr   \n  \n**C:**  `Performance ~ poly1 + poly2*Task`  \nvs  \n**D:**  `Performance ~ (poly1 + poly2)*Task`   \n\nThe comparison between models A and B assumes that all the Tasks have the same curvature. Visually, this is the comparison of which of the two patterns in plots A and B in @fig-rawcomp best fits the data. In A, the lines have the same linear slope and the same curvature. In B, the lines have the same curvature but different linear slopes.  \n\nThe comparison between models C and D is similar, but not quite the same. C has lines that have different curvatures but the same linear slope, and plot D has lines that have different curvatures and different linear slopes.  \n\n```{r}\n#| echo: false\n#| fig-cap: \"Model comparisons with raw polynomials\"\n#| label: fig-rawcomp\n(p1 + p2) / (p3 + p4)\n```\n\n\nBUT... in all these descriptions, I have used the term \"linear slope\" a little ambiguously. Because we are using raw polynomials, this is actually meaning specifically the tangent **at the start** (i.e., at the left hand side of the plot).  \n\nNote that plots for A and B both have the same amount of curvature - i.e., the lines in B are equally curvy as those in A. But this isn't true for C and D. This is because in model C, by *not* having the `poly1:Task` interaction in there, all we are saying is that the tangent at 0 is the same for all tasks. The quadratic term interaction `poly2:Task` then tries to \"make up for\" this by making the green line curvy (when actually it's just a shallower straight line).  \n\nSo the comparison between C and D (when using raw polynomials) doesn't ask \"do tasks differ in their overall linear trend?\". Instead it asks \"do tasks differ in linear trend *at the outset*? \nWith orthogonal polynomials, we would have something more like @fig-orthcomp.  \n\n\n```{r}\n#| echo: false\n#| fig-cap: \"Model comparisons with orthogonal polynomials\"\n#| label: fig-orthcomp\nmodAo <- lm(Performance ~ opoly1 + opoly2 + Task,Aggaz)\nmodBo <- lm(Performance ~ opoly1*Task + opoly2,Aggaz)\nmodCo <- lm(Performance ~ opoly1 + opoly2*Task,Aggaz)\nmodDo <- lm(Performance ~ (opoly1 + opoly2)*Task,Aggaz)\n\np1 <- broom::augment(modAo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"A\",subtitle=\"Perf ~ poly1 + poly2 + Task\",\n       x=\"time\")+\n  guides(col=\"none\")\n\np2 <- broom::augment(modBo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"B\",subtitle=\"Perf ~ poly1*Task + poly2\",x=\"time\")+\n  guides(col=\"none\")\n\np3 <- broom::augment(modCo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"C\",subtitle=\"Perf ~ poly1 + poly2*Task\",x=\"time\")+\n  guides(col=\"none\")\n\np4 <- broom::augment(modDo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"D\",subtitle=\"Perf ~ (poly1 + poly2)*Task\",x=\"time\")+\n  guides(col=\"none\")\n\n(p1 + p2) / (p3 + p4)\n```\n\nThis whole thing is because essentially in raw polynomials, `poly1` and `poly2` are correlated. So excluding one of the terms (or an interaction with one of the terms) will change the estimates for the others. In orthogonal polynomials, we don't have that issue.  \n\n:::\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nGet some confidence intervals and provide an interpretation of each coefficient from the full model.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs we've used likelihood ratio tests above, we'll get some profile likelihood confidence intervals for our parameters.  \n__note this took me about 4 mins to run__  \nIf you want to get some quicker (slightly less robust) confidence intervals, then switch `\"profile\"` to `\"Wald\"`.  \n```{r}\n#| eval: false\nconfint(m3full, method=\"profile\", parm=\"beta_\")\n```\n```{r}\n#| echo: false\n# m3.tpr <- profile(m3full, which=\"beta_\")\n# save(m3.tpr, file=\"data/m3tpr.rdata\")\nload(\"data/m3tpr.rdata\")\nm3.ci <- confint(m3.tpr) |> round(2)\n\n\nbroom.mixed::tidy(m3) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    CI = paste0(\"[\",m3.ci[,1],\", \",m3.ci[,2],\"]\"),\n    interpretation = c(\n      \"estimated score on the cADL task at baseline\",\n      \"estimated linear change in cADL scores from baseline\",\n      \"no significant curvature to the cADL trajectory\",\n      \"no significant difference between sADL and cADL tasks at baseline\",\n      paste0(\"at baseline, scores on memory task are \",abs(round(fixef(m3full)[5],2)),\" lower than cADL\"),\n      \"performance on sADL task is not decreasing from baseline as much as performance on cADL\",\n      \"performance on Memory task is decreasing (linearly) more than performance on cADL\",\n      \"no significant difference between quadratic change of sADL from that of cADL\",\n      \"significant difference in quadratic change between performance on Memory vs performance on cADL\"\n    )\n  ) |> gt::gt()\n```\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### some tables of predictions (in case they help)\n\nTo get a sense of the quadratic term 'in action', think about the predictions across time for each task: \n\nFor cADL, this is just the linear change. Every timepoint, performance decreases by 3.27.  \n\n| timepoint | cADL |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time)$ |\n| 0 | $63.88 + (-3.27 \\times 0) = 63.88$ |\n| 1 | $63.88 + (-3.27 \\times 1) = 60.61$ |\n| 2 | $63.88 + (-3.27 \\times 2) = 57.34$ |\n| 3 | $63.88 + (-3.27 \\times 3) = 54.07$ |\n\n\nFor sADL, the additional change is +1.34, so at every timepoint performance decreases by -1.93 (this is -3.27+1.34)    \n\n| timepoint | sADL |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +$ $(1.44) + (1.34 \\times time) + (-0.01 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time) + (1.34 \\times time)$ |\n| 0 | $63.88 + (-3.27 \\times 0) + (1.34 \\times 0) = 63.88$ |\n| 1 | $63.88 + (-3.27 \\times 1) + (1.34 \\times 1) = 61.95$ |\n| 2 | $63.88 + (-3.27 \\times 2)+ (1.34 \\times 2)  = 60.02$ |\n| 3 | $63.88 + (-3.27 \\times 3)+ (1.34 \\times 3) = 58.09$ |\n\n\nFor the Memory task, the quadratic term is in play. at every timepoint performance decreases by $-3.27-3.30 +(0.34 \\times time^2)$. So for low timepoints, the quadratic term doesn't make much of a difference as it does for bigger time points.  \n\n| timepoint | Memory |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +$ $(-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time) +$ $(-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)$ |\n| 0 | $63.88 + (-3.27 \\times 0) +$ $(-2.40) + (-3.30 \\times 0) + (0.34 \\times 0^2) = 61.48$ |\n| 1 | $63.88 + (-3.27 \\times 1) +$ $(-2.40) + (-3.30 \\times 1) + (0.34 \\times 1^2) = 55.25$ |\n| 2 | $63.88 + (-3.27 \\times 2) +$ $(-2.40) + (-3.30 \\times 2) + (0.34 \\times 2^2) = 49.70$ |\n| 3 | $63.88 + (-3.27 \\times 3) +$ $(-2.40) + (-3.30 \\times 3) + (0.34 \\times 3^2) = 44.83$ |\n| ... | ... |\n| 9 | $63.88 + (-3.27 \\times 9) +$ $(-2.40) + (-3.30 \\times 9) + (0.34 \\times 9^2) = 29.89$ |\n| 10 | $63.88 + (-3.27 \\times 10) +$ $(-2.40) + (-3.30 \\times 10) + (0.34 \\times 10^2) = 29.78$ |\n\n:::\n\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectories for each task.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\npp1 <- bind_rows(poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble()\n          ) |>\n  mutate(Task=rep(c(\"cADL\",\"sADL\",\"Memory\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"Task\")\nbts = bootMer(m3,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=2)\n\npp1$pred = predict(m3, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=Task,fill=Task))+\n  geom_line(lwd=1)+\n  scale_x_continuous(breaks=0:9)+\n  # geom_ribbon(alpha=.2)+\n  NULL\n```\n\n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nMake a plot showing both the average performance and the average model predicted performance across time.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nlibrary(broom.mixed)\naugment(m3) |>\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")\n```\n`r solend()`\n\n\n<!-- # Polynomials and overfitting -->\n\n<!-- :::frame -->\n<!-- Two quotes -->\n\n<!-- \"all models are wrong. some are useful.\" [(George Box, 1976)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949).   -->\n\n<!-- \"...it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd. The construction of idealized representations that capture important stable aspects of such systems is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from other kinds of model.\" (Sir David Cox, 1995).   -->\n\n<!-- ::: -->\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggdist)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\n```\n\n# Cognitive Task Performance\n\n:::frame\n__Dataset: Az.rda__  \n  \nThese data are available at [https://uoepsy.github.io/data/Az.rda](https://uoepsy.github.io/data/Az.rda). You can load the dataset using:  \n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n```\nand you will find the `Az` object in your environment.  \n\nThe `Az` object contains information on 30 Participants with probable Alzheimer's Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100. \n\nWe're interested in *whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance*.  \n\n```{r}\n#| echo: false\ntibble(\n    variable = names(Az),\n    description = c(\"Unique Subject Identifier\",\"Time point of the study (1 to 10)\",\"Task type (Memory, cADL, sADL)\",\"Score on test (range 0 to 100)\")\n) |> gt::gt()\n```\n:::\n\n`r qbegin(qcounter())`\nLoad in the data and examine it.  \nHow many participants, how many observations per participant, per task?  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n```\n\n30 participants: \n```{r}\nlength(unique(Az$Subject))\n```\n\nDoes every participant have 10 datapoints for each Task type?  Yes!  \n```{r}\nany( table(Az$Subject, Az$Task) != 10 )\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nNo modelling just yet.  \n\nPlot the performance over time for each type of task.  \n\nTry using `stat_summary` so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nFor an example plot, see [Chapter 6: polynomial growth #example-in-mlm](https://uoepsy.github.io/lmm/06_poly.html#example-in-mlm){target=\"_blank\"}.\n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nYou can use \"pointranges\", or \"line\" and \"ribbon\".  \n`stat_summary` will take the data and for each value of `x` calculate some function (in this case the mean, or the mean and SE):  \n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n```\n`r solend()`\n\n`r qbegin(qcounter())`\nWhy do you think *raw/natural* polynomials might be more useful than *orthogonal* polynomials for these data?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rate of change) at *a specific point* in time?  \n\n:::\n\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nBecause we're interested in whether there are task differences at the starting point, raw polynomials are probably what we want here.  \n`r solend()`\n\n\n`r qbegin(qcounter())`\nRe-center the Time variable so that the intercept is the first timepoint.  \n  \nThen choose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nNote there is no part of the research question that specifically asks about how \"gradual\" or \"quick\" the change is (which would suggest we are interested in the quadratic term).  \n\nHowever, plots (summarised at the task level, and of individual participant's data separately) can help to give us a sense of what degree of polynomial terms might be suitable to succinctly describe the trends.  \n\nIn many cases, fitting higher and higher order polynomials will likely result in a 'better fit' to our sample data, but these will be worse and worse at generalising to new data - i.e. we run the risk of *overfitting*.  \n\n:::\n\n`r qend()`\n`r solbegin(label=\"1 - how many polynomials?\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nIn our plot, there were 2 straight line and one slightly curvy one. It wasn't S-shaped or 'wiggly' or anything, there was just a bit of a bend in it, which suggests that the quadratic term could be a good approximation here.   \n\nIt's worth also looking at the individual participant trajectories. In these we can also see the curvi-ness of the blue line - it's more pronounced for some people than others, but it does seem like individual's trajectories on the Memory task may be curvilinear. \n```{r}\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")+\n  facet_wrap(~Subject)\n```\n\n\n```{r}\nlibrary(lme4)\nAz <- Az |> mutate(\n  Time1 = Time-1,\n  poly1 = poly(Time1,2,raw=T)[,1],\n  poly2 = poly(Time1,2,raw=T)[,2]\n)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Or using Dan's code:  \n\n```{r}\n#| eval: false\n# import Dan's code:\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\n# this also produces a nice little plot to show the polynomials\nAz$Time1 <- Az$Time-1\nAz <- code_poly(df = Az, predictor = 'Time1',\n                poly.order = 2, orthogonal = FALSE)\n```\n\n:::\n\n\n\n`r solend()`\n`r solbegin(label=\"2 - fixed effects\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nWe're interested in how performance changes over time, but we have `poly1` and `poly2` for time, so we're at:  \n```\nlmer(Performance ~ poly1 + poly2 ... \n```\n\nOur research aims are to investigate differences between task performance (both at baseline and change over time). So we want to interact time with task:  \n```\nlmer(Performance ~ (poly1 + poly2)*Task ... \n```\n\n`r solend()`\n`r solbegin(label=\"3 - grouping structure\", slabel=F,show=T, toggle=params$TOGGLE)`\n```{r}\nhead(Az)\n```\n\nWe have `r nrow(Az)` observations, and `r table(Az$Subject)[1]` for each participant.  \n\n```\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + .... | Subject)\n```\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Fixed vs Random\n\nWe can account for group differences in models either by estimating group differences, or by estimating variance between groups:  \n\n- group as a fixed effect (`y ~ 1 + group`) = groups differ in $y$ by $b_1, b_2, ..., b_k$  \n- group as a random effect (`y ~ 1 + (1|group)`) = groups vary in $y$ with a standard deviation of $\\sigma_0$   \n\nOne way to think about whether a group is best in the random effects part or in the fixed part of our model is to think about \"what would happen if I repeated the experiment?\"  \n\n**Should variable `g` be fixed or random?**\n\n|  | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_ | \n|----------------|--------------------------------------------------|----------------------------------------------------|\n| Fixed<br>$y\\,\\sim\\,~\\,...\\, +\\, g$  | Same groups would be used   |    The groups used  |\n| Random<br>$y\\,\\sim\\,...\\,+\\,(\\,... |\\,g)$ | Different groups would be used   | A population from which the groups used are just a (random) sample |\n\n\nPractical points:  \n- Sometimes there isn't enough variability between groups to model as random effects (i.e. the variance gets estimated as too close to zero).\n- Sometimes you might not have sufficient number of groups to model as random effects (e.g. for groups of fewer than c8 things, estimates of the variance are often not a reliable reflection of the population).\n\n\n:::\n\n\n`r solend()`\n`r solbegin(label=\"4 - random effects\", slabel=F,show=T, toggle=params$TOGGLE)`\n\nWhat slopes could vary by participant? \n\n__Q:__ Could participants vary in their performance over time?  \n__A:__ Yes, `(poly1 + poly2 | Subject)`  \n__Q:__ Could participants vary in how performance differs between Tasks?  \n__A:__ Yes, `(poly1 + poly2 + Task | Subject)`. E.g., Some participants might be much better at the memory task than other tasks, some might be better at the other tasks.  \n__Q:__ Could participants vary in how tasks differ in their performance over time?  \n__A:__ Yes, `((poly1 + poly2)*Task | Subject)`. E.g., For some participants, memory could decline more than cADL, for other participants it could decline less.  \n\n```\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + (poly1 + poly2)*Task | Subject)\n```\n\n`r solend()`\n`r solbegin(label=\"5 - the model\", slabel=F,show=T, toggle=params$TOGGLE)`\n\n_small note: I'm using the bobyqa optimizer here because it tends to be slightly quicker than the default._  \n```{r}\n#| eval: false\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n<p style=\"color:red;font-size:.8em\">\nboundary (singular) fit: see help('isSingular')<br>\nWarning messages:<br>\n1: In commonArgs(par, fn, control, environment()) :<br>\n  maxfun < 10 * length(par)^2 is not recommended.<br>\n2: In optwrap(optimizer, devfun, getStart(start, rho\\$pp), lower = rho\\$lower,  :<br>\n  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded\n</p>\n\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nOkay, so the model didn't converge.  It's trying to estimate __a lot__ of things in the random effects (even though it didn't converge, try looking at `VarCorr(model)` to see all the covariances it is trying to estimate).  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Categorical random effects on the RHS\n\nWhen we have a categorical random effect (i.e. where the `x` in `(1 + x | g)` is a categorical variable), then model estimation can often get tricky, because \"the effect of x\" for a categorical variable with $k$ levels is identified via $k-1$ parameters, meaning we have a lot of variances and covariances to estimate when we include `x|g`.  \n\n:::: {.columns}\n:::{.column width=\"45%\"}\n\nWhen `x` is numeric:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n\nWhen `x` is categorical with $k$ levels:  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n::::\n\nHowever, we can use an alternative formation of the random effects by putting a categorical `x` into the right-hand side:  \nInstead of `(1 + x | g)` we can fit `(1 | g) + (1 | g:x)`.   \n\nThe symbol `:` in `g:x` is used to refer to the combination of `g` and `x`.  \n\n```{r}\n#| echo: false\ngx = tibble(\n  g = c(\"p1\",\"p1\",\"p1\",\"...\",\"p2\",\"p2\",\"...\"),\n  x = c(\"a\",\"a\",\"b\",\"...\",\"a\",\"b\",\"...\"),\n  `g:x` = as.character(interaction(g,x))\n)\ngx[c(4,7),3] <- \"...\"\ngx$g = paste0(\"  \",gx$g,\"   \")\ngx$x = paste0(\"  \",gx$x,\"   \")\nnames(gx)<-c(\"  g   \",\"  x   \",\"g:x\")\nas.data.frame(gx)\n```\n\nIt's a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:  \n\n- `(1 + x | g)`:  each group of `g` can have a different intercept and a different effect of `x`  \n- `(1 | g) + (1 | g:x)`: each group of `g` can have a different intercept, and each level of x within each `g` can have a different intercept.  \n\nBoth of these allow the outcome `y` to change across `x` differently for each group in `g` (i.e. both of them result in `y` being different for each level of `x` in each group `g`).  \nThe first does so explicitly by estimating the group level variance of the `y~x` effect.  \nThe second one estimates the variance of $y$ between groups, and also the variance of $y$ between 'levels of x within groups'. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of `x`, we don't have to worry about lots of covariances:  \n\n\n:::: {.columns}\n:::{.column width=\"45%\"}\n`(1 + x | g)`  \n\n```\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n```\n\n:::\n:::{.column width=\"10%\"}\n\n:::\n:::{.column width=\"45%\"}\n`(1 | g) + (1 | g:x)`  \n\n\n```\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n```\n\n:::\n::::\n\n:::\n\n\nTry adjusting your model by first moving `Task` to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one) \n\n**This is our first experience of our random effect structures becoming more complex than simply `(.... | group)`. This is going to feel confusing, but don't worry, we'll see more structures like this next week.**    \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)`  \n\nTo then start simplifying (if this model doesn't converge), it can be helpful to look at the `VarCorr()` of the non-converging model to see if anything looks awry. Look for small variances, perfect (or near perfect) correlations. These might be sensible things to remove.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nHere's our model with subject-task effects on the right hand side.  \nAgain we have problems, as we have a singular fit:  \n```{r}\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n<p style=\"color:red;font-size:.8em\">boundary (singular) fit: see help('isSingular')</p>\n\nLooking at the random effects of our model, note that the `poly2|Subject` random effect has very little variance (and high correlations).  \nNote that it makes sense that by including the random effects for `Subject:Task`, there might not be much above that leftover in `Subject` random effects. \n```{r}\nVarCorr(m2)\n```\n\n\nWhen we remove it, our model converges!!  \n```{r}\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n\nConduct a series of model comparisons investigating whether \n\n1. Tasks differ only in their linear change  \n2. Tasks differ in their quadratic change  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we're interested in the how the average participant performs over the study). So our models want to have the same random effect structure, but different fixed effects.  \n\nSee the end of [Chapter 6: polynomial growth #example-in-mlm](https://uoepsy.github.io/lmm/06_poly.html#example-in-mlm){target=\"_blank\"}.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs I'm comparing these with a likelihood ratio test, I'll fit them with `REML=FALSE`\n```{r}\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3full = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3full)\n```\n\n:::int\n```{r}\n#| echo: false\nres = anova(m3int, m3lin, m3)\n```\n\nThe linear change over time differs between Tasks ($\\chi^2(`r round(res[2,7],2)`) = `r round(res[2,6],2)`, p `r format.pval(res[2,8],eps=.001)`$).  \nThe quadratic change over time differs between Tasks ($\\chi^2(`r round(res[3,7],2)`) = `r round(res[3,6],2)`, p `r format.pval(res[3,8],eps=.001)`$).  \n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n#### completely optional - comparisons and raw/orthogonal \n\n```{r}\n#| include: false\nAggaz <- Az |> group_by(Time1,poly1,poly2,Task) |>\n  summarise(Performance = mean(Performance)) |>\n  ungroup() |>\n  mutate(opoly1 = poly(Time1,degree=2)[,1],\n         opoly2 = poly(Time1,degree=2)[,2])\n\nmodA <- lm(Performance ~ poly1 + poly2 + Task,Aggaz)\nmodB <- lm(Performance ~ poly1*Task + poly2,Aggaz)\nmodC <- lm(Performance ~ poly1 + poly2*Task,Aggaz)\nmodD <- lm(Performance ~ (poly1 + poly2)*Task,Aggaz)\n\n\n# modAo <- lm(Performance ~ opoly1 + opoly2 + Task,Aggaz)\n# modBo <- lm(Performance ~ opoly1*Task + opoly2,Aggaz)\n# modCo <- lm(Performance ~ opoly1 + opoly2*Task,Aggaz)\n# modDo <- lm(Performance ~ (opoly1 + opoly2)*Task,Aggaz)\n# \n# \n# # anova(modA,modB)\n# # anova(modAo,modBo)\n# # \n# # anova(modC,modD)\n# # anova(modCo,modDo)\n# \np1 <- broom::augment(modA) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"A\",subtitle=\"Perf ~ poly1 + poly2 + Task\",\n       x=\"time\")+\n  guides(col=\"none\")\n\np2 <- broom::augment(modB) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"B\",subtitle=\"Perf ~ poly1*Task + poly2\",x=\"time\")+\n  guides(col=\"none\")\n\np3 <- broom::augment(modC) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"C\",subtitle=\"Perf ~ poly1 + poly2*Task\",x=\"time\")+\n  guides(col=\"none\")\n\np4 <- broom::augment(modD) |>\n  ggplot(aes(x=poly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"D\",subtitle=\"Perf ~ (poly1 + poly2)*Task\",x=\"time\")+\n  guides(col=\"none\")\n\n\n\n\n\n```\n\n\nThere's an argument to be made that to ask \"do tasks differ in Linear change?\" we could test either (as above): \n\n**A:** `Performance ~ poly1 + poly2 + Task`  \nvs   \n**B:**  `Performance ~ poly1*Task + poly2`  \n  \nOr   \n  \n**C:**  `Performance ~ poly1 + poly2*Task`  \nvs  \n**D:**  `Performance ~ (poly1 + poly2)*Task`   \n\nThe comparison between models A and B assumes that all the Tasks have the same curvature. Visually, this is the comparison of which of the two patterns in plots A and B in @fig-rawcomp best fits the data. In A, the lines have the same linear slope and the same curvature. In B, the lines have the same curvature but different linear slopes.  \n\nThe comparison between models C and D is similar, but not quite the same. C has lines that have different curvatures but the same linear slope, and plot D has lines that have different curvatures and different linear slopes.  \n\n```{r}\n#| echo: false\n#| fig-cap: \"Model comparisons with raw polynomials\"\n#| label: fig-rawcomp\n(p1 + p2) / (p3 + p4)\n```\n\n\nBUT... in all these descriptions, I have used the term \"linear slope\" a little ambiguously. Because we are using raw polynomials, this is actually meaning specifically the tangent **at the start** (i.e., at the left hand side of the plot).  \n\nNote that plots for A and B both have the same amount of curvature - i.e., the lines in B are equally curvy as those in A. But this isn't true for C and D. This is because in model C, by *not* having the `poly1:Task` interaction in there, all we are saying is that the tangent at 0 is the same for all tasks. The quadratic term interaction `poly2:Task` then tries to \"make up for\" this by making the green line curvy (when actually it's just a shallower straight line).  \n\nSo the comparison between C and D (when using raw polynomials) doesn't ask \"do tasks differ in their overall linear trend?\". Instead it asks \"do tasks differ in linear trend *at the outset*? \nWith orthogonal polynomials, we would have something more like @fig-orthcomp.  \n\n\n```{r}\n#| echo: false\n#| fig-cap: \"Model comparisons with orthogonal polynomials\"\n#| label: fig-orthcomp\nmodAo <- lm(Performance ~ opoly1 + opoly2 + Task,Aggaz)\nmodBo <- lm(Performance ~ opoly1*Task + opoly2,Aggaz)\nmodCo <- lm(Performance ~ opoly1 + opoly2*Task,Aggaz)\nmodDo <- lm(Performance ~ (opoly1 + opoly2)*Task,Aggaz)\n\np1 <- broom::augment(modAo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"A\",subtitle=\"Perf ~ poly1 + poly2 + Task\",\n       x=\"time\")+\n  guides(col=\"none\")\n\np2 <- broom::augment(modBo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"B\",subtitle=\"Perf ~ poly1*Task + poly2\",x=\"time\")+\n  guides(col=\"none\")\n\np3 <- broom::augment(modCo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"C\",subtitle=\"Perf ~ poly1 + poly2*Task\",x=\"time\")+\n  guides(col=\"none\")\n\np4 <- broom::augment(modDo) |>\n  ggplot(aes(x=opoly1,y=.fitted,col=Task))+\n  geom_line()+\n  labs(title=\"D\",subtitle=\"Perf ~ (poly1 + poly2)*Task\",x=\"time\")+\n  guides(col=\"none\")\n\n(p1 + p2) / (p3 + p4)\n```\n\nThis whole thing is because essentially in raw polynomials, `poly1` and `poly2` are correlated. So excluding one of the terms (or an interaction with one of the terms) will change the estimates for the others. In orthogonal polynomials, we don't have that issue.  \n\n:::\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nGet some confidence intervals and provide an interpretation of each coefficient from the full model.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nAs we've used likelihood ratio tests above, we'll get some profile likelihood confidence intervals for our parameters.  \n__note this took me about 4 mins to run__  \nIf you want to get some quicker (slightly less robust) confidence intervals, then switch `\"profile\"` to `\"Wald\"`.  \n```{r}\n#| eval: false\nconfint(m3full, method=\"profile\", parm=\"beta_\")\n```\n```{r}\n#| echo: false\n# m3.tpr <- profile(m3full, which=\"beta_\")\n# save(m3.tpr, file=\"data/m3tpr.rdata\")\nload(\"data/m3tpr.rdata\")\nm3.ci <- confint(m3.tpr) |> round(2)\n\n\nbroom.mixed::tidy(m3) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term, est=round(estimate,2),\n    CI = paste0(\"[\",m3.ci[,1],\", \",m3.ci[,2],\"]\"),\n    interpretation = c(\n      \"estimated score on the cADL task at baseline\",\n      \"estimated linear change in cADL scores from baseline\",\n      \"no significant curvature to the cADL trajectory\",\n      \"no significant difference between sADL and cADL tasks at baseline\",\n      paste0(\"at baseline, scores on memory task are \",abs(round(fixef(m3full)[5],2)),\" lower than cADL\"),\n      \"performance on sADL task is not decreasing from baseline as much as performance on cADL\",\n      \"performance on Memory task is decreasing (linearly) more than performance on cADL\",\n      \"no significant difference between quadratic change of sADL from that of cADL\",\n      \"significant difference in quadratic change between performance on Memory vs performance on cADL\"\n    )\n  ) |> gt::gt()\n```\n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### some tables of predictions (in case they help)\n\nTo get a sense of the quadratic term 'in action', think about the predictions across time for each task: \n\nFor cADL, this is just the linear change. Every timepoint, performance decreases by 3.27.  \n\n| timepoint | cADL |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time)$ |\n| 0 | $63.88 + (-3.27 \\times 0) = 63.88$ |\n| 1 | $63.88 + (-3.27 \\times 1) = 60.61$ |\n| 2 | $63.88 + (-3.27 \\times 2) = 57.34$ |\n| 3 | $63.88 + (-3.27 \\times 3) = 54.07$ |\n\n\nFor sADL, the additional change is +1.34, so at every timepoint performance decreases by -1.93 (this is -3.27+1.34)    \n\n| timepoint | sADL |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +$ $(1.44) + (1.34 \\times time) + (-0.01 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time) + (1.34 \\times time)$ |\n| 0 | $63.88 + (-3.27 \\times 0) + (1.34 \\times 0) = 63.88$ |\n| 1 | $63.88 + (-3.27 \\times 1) + (1.34 \\times 1) = 61.95$ |\n| 2 | $63.88 + (-3.27 \\times 2)+ (1.34 \\times 2)  = 60.02$ |\n| 3 | $63.88 + (-3.27 \\times 3)+ (1.34 \\times 3) = 58.09$ |\n\n\nFor the Memory task, the quadratic term is in play. at every timepoint performance decreases by $-3.27-3.30 +(0.34 \\times time^2)$. So for low timepoints, the quadratic term doesn't make much of a difference as it does for bigger time points.  \n\n| timepoint | Memory |\n| ---- | ---- |\n| prediction formula | $63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +$ $(-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)$ |\n| prediction formula<br><small>(with non-sig terms removed)</small> | $63.88 + (-3.27 \\times time) +$ $(-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)$ |\n| 0 | $63.88 + (-3.27 \\times 0) +$ $(-2.40) + (-3.30 \\times 0) + (0.34 \\times 0^2) = 61.48$ |\n| 1 | $63.88 + (-3.27 \\times 1) +$ $(-2.40) + (-3.30 \\times 1) + (0.34 \\times 1^2) = 55.25$ |\n| 2 | $63.88 + (-3.27 \\times 2) +$ $(-2.40) + (-3.30 \\times 2) + (0.34 \\times 2^2) = 49.70$ |\n| 3 | $63.88 + (-3.27 \\times 3) +$ $(-2.40) + (-3.30 \\times 3) + (0.34 \\times 3^2) = 44.83$ |\n| ... | ... |\n| 9 | $63.88 + (-3.27 \\times 9) +$ $(-2.40) + (-3.30 \\times 9) + (0.34 \\times 9^2) = 29.89$ |\n| 10 | $63.88 + (-3.27 \\times 10) +$ $(-2.40) + (-3.30 \\times 10) + (0.34 \\times 10^2) = 29.78$ |\n\n:::\n\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectories for each task.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n```{r}\n#| echo: false\npp1 <- bind_rows(poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble(),\n                 poly(seq(0,9,length.out=50),2,raw=T) |> as_tibble()\n          ) |>\n  mutate(Task=rep(c(\"cADL\",\"sADL\",\"Memory\"),e=50))\nnames(pp1) <-c(\"poly1\",\"poly2\",\"Task\")\nbts = bootMer(m3,FUN=function(x) predict(x,newdata=pp1,re.form=NA),nsim=2)\n\npp1$pred = predict(m3, newdata=pp1,re.form=NA)\npp1$lwr = apply(bts$t, 2, quantile, .025)\npp1$upr = apply(bts$t, 2, quantile, .975)\nggplot(pp1, aes(x=poly1,y=pred,ymin=lwr,ymax=upr,\n                col=Task,fill=Task))+\n  geom_line(lwd=1)+\n  scale_x_continuous(breaks=0:9)+\n  # geom_ribbon(alpha=.2)+\n  NULL\n```\n\n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nMake a plot showing both the average performance and the average model predicted performance across time.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\nlibrary(broom.mixed)\naugment(m3) |>\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")\n```\n`r solend()`\n\n\n<!-- # Polynomials and overfitting -->\n\n<!-- :::frame -->\n<!-- Two quotes -->\n\n<!-- \"all models are wrong. some are useful.\" [(George Box, 1976)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949).   -->\n\n<!-- \"...it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd. The construction of idealized representations that capture important stable aspects of such systems is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from other kinds of model.\" (Sir David Cox, 1995).   -->\n\n<!-- ::: -->\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"03ex.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Week 3 Exercises: Non-Linear Change","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}