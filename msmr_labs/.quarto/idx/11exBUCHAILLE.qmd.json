{"title":"Week 11 Exercises: More SEM!","markdown":{"yaml":{"title":"Week 11 Exercises: More SEM!","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Hints","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(psych)\nlibrary(semPlot)\nlibrary(lavaan)\n```\n\n\n\n\n:::frame\n__Prenatal Stress & IQ data__  \n\nA researcher is interested in the effects of prenatal stress on child cognitive outcomes.  \nShe has a 5-item measure of prenatal stress and a 5 subtest measure of child cognitive ability, collected for 500 mother-infant dyads. \n\n+ The data is available as a .csv file here: [https://uoepsy.github.io/data/stressIQ.csv](https://uoepsy.github.io/data/stressIQ.csv)\n\n```{r}\n#| echo: false\ntibble(\n  variable = names(read_csv(\"https://uoepsy.github.io/data/stressIQ.csv\")),\n  description = c(\"Participant ID\",\n                  \"acute stress\",\n                  \"chronic stress\",\n                  \"environmental stress\",\n                  \"psychological stress\",\n                  \"physiological stress\",\n                  \"verbal ability\",\n                  \"verbal memory\",\n                  \"inductive reasoning\",\n                  \"spatial orientiation\",\n                  \"perceptual speed\")\n) |> gt::gt()\n```\n\n\n:::\n\n`r qbegin(qcounter())`\nBefore we do anything with the data, grab some paper and sketch out the full model that you plan to fit to address the researcher's question.   \n\nTip: everything you need is in the description of the data. Start by drawing the specific path(s) of interest. Are these between latent variables? If so, add in the paths to the indicators for each latent variable.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe main parameter of interest here is \"the effects of prenatal stress on child cognitive outcomes\". So we have an arrow going from Stress to IQ.  \nEach of these are latent variables, for which we have observed 5 indicator variables, so we have an arrow going from \"IQ\" to each of the 5 IQ items, and from \"Stress\" to the 5 stress items. \n```{r echo=FALSE}\ndiagmod <- '\n#IQ measurement model\nIQ=~IQ1+IQ2+IQ3+IQ4+IQ5 \n#stress measurement model \nStress=~stress1+stress2+stress3+stress4+stress5 \n#structural part of model\nIQ~Stress'\nsemPlot::semPaths(lavaanify(diagmod),rotation=1)\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nRead in the data and explore it. Look at the individual distributions of each variable to get a sense of univariate normality, as well as the number of response options each item has.  \n\n\n::: {.callout-tip collapse=\"true\"}\n\nThe `multi.hist()` function from the __psych__ package is pretty useful for getting quick histograms of multiple variables. If necessary, you can set `multi.hist(data, global = FALSE)` to let each histogram's x-axis be on a different scale.\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r message=FALSE}\nlibrary(tidyverse)\nlibrary(psych)\n\nstress_IQ_data <- read_csv(\"https://uoepsy.github.io/data/stressIQ.csv\")\n```\n\nThis gives us a whole load of descriptives, means, standard deviations, and other things like skew and kurtosis.  \n```{r}\ndescribe(stress_IQ_data) # from psych package\n```\n\nHere we can see the distribution of each variable. Some immediate things of note - the stress items are only measured on 3 response options, and the IQ items are quite positively skewed (we can see this matches with the skew metrics above).  \n```{r}\nmulti.hist(stress_IQ_data, global = FALSE) # from psych package\n```\n`r solend()`\n\n## Non-normality\n\n\n`r qbegin(qcounter())`\nFit a one factor CFA for the IQ items using an appropriate estimation method.   \n  \nBe sure to first check their distributions (both numerically and visually). The `describe()` function from the __psych__ package will give you measures of skew and kurtosis.  \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Multivariate Normality (MVN)\n\n\nOne of the key assumptions of the models we have been fitting so far is that our variables are all normally distributed, and are continuous. Why?  \n\nRecall how we first introduced the idea of estimating a SEM: comparing the **observed covariance matrix** with our **model implied covariance matrix**. This heavily relies on the assumption that our observed covariance matrix provides a good representation of our data - i.e., that $var(x_i)$ and $cov(x_i,x_j)$ are adequate representations of the relations between variables. While this is true if we have a \"multivariate normal distribution\", things become more difficult when our variables are either not continuous or not normally distributed. \n\nThe multivariate normal distribution is fundamentally just the extension of our univariate normal (the bell-shaped curve we are used to seeing) to more dimensions. The condition which needs to be met is that every linear combination of the $k$ variables has a univariate normal distribution. It is denoted by $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ (note that the bold font indicates that $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are, respectively, a vector of means and a matrix of covariances).^[We've actually seen this previously in multilevel modelling, when we had random intercepts, random slopes, and the covariance between the two!]  \nThe idea of the multivariate normal can be a bit difficult to get to grips with in part because there's not an intuitive way to visualise it once we move beyond 2 or [3 variables](https://demonstrations.wolfram.com/JointDensityOfTrivariateGaussianRandomVariables/).  \n\nWith 2 variables, you can think of it just like a density plot but in 3 dimensions, as in @fig-bivnorm. You can also represent this as a \"contour\" plot, which is like looking down on @fig-bivnorm from above (just like a map!)  \n\n```{r}\n#| echo: false\n#| label: fig-bivnorm\n#| fig-cap: \"Bivariate Normal\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nx <- mvtnorm::rmvnorm(n = 1e3, mean = c(0,0), sigma = matrix(c(4,2,2,3), ncol = 2))\nd <- as.data.frame(x)\nd$density <- mvtnorm::dmvnorm(x = d)\nlibrary(plotly)\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n\nNow let's look at a situation where we have some skewed variables. @fig-mvnn shows such a distribution, and @fig-bivnon shows the contour plot (i.e. it is like @fig-mvnn viewed from above). \n\n::::panelset\n:::panel\n#### 3D\n\n```{r}\n#| echo: false\n#| label: fig-mvnn\n#| fig-cap: \"Joint distribution of some skewed variables\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nop <- list(xi=c(0,1), Psi=matrix(c(2,2,2,3), 2, 2), lambda=c(6, -1.5))\nrnd <- sn::rmsn(1e3, dp=sn::op2dp(op,\"SN\"))\nd<-as.data.frame(rnd)\nd$density <- mvtnorm::dmvnorm(x = d)\n\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n:::\n:::panel\n#### Contour\n\n```{r}\n#| label: fig-bivnon\n#| fig-cap: \"Contour plot for a non-normal bivariate distribution. Marginal distributions of each variable are presented along each axis\"\n#| echo: false\np1 <- ggplot(d,aes(x=V1,y=V2))+\n  geom_point(alpha=.2)+\n  geom_density_2d()\nggExtra::ggMarginal(p1, type=\"histogram\")\n```\n\n:::\n::::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### simulations from a covariance matrix\n\nIf we compute our covariance matrix from the 2 skewed variables, we get:\n```{r}\n#| echo: false\ncov(d[,1:2])\n```\nBut this fails to capture information about important properties of the data relating to features such as skew (lop-sided-ness) and kurtosis (pointiness). As an example, if we were to simulate data based on this covariance matrix, we get something like @fig-bivsim, which is clearly limited in its reflection of our actual data.  \n\n::::panelset\n:::panel\n#### 3D\n```{r }\n#| echo: false\n#| label: fig-bivsim\n#| fig-cap: \"Probability distribution of simulations based on a covariance matrix (of some skewed variables). The covariance matrix contains only 2 'moments' of the variables: their scale (spread) and location (center)\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nd <- as.data.frame(mvtnorm::rmvnorm(n = 1e3, mean = apply(d,2,mean)[1:2], sigma = cov(d[,1:2])))\nd$density <- mvtnorm::dmvnorm(x = d)\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n:::\n:::panel\n#### Contour\n```{r}\n#| label: fig-bivnon2\n#| fig-cap: \"Contour plot for simulated data from a covariance matrix\"\n#| echo: false\np1 <- ggplot(d,aes(x=V1,y=V2))+\n  geom_point(alpha=.2)+\n  geom_density_2d()\nggExtra::ggMarginal(p1, type=\"histogram\")\n```\n:::\n::::\n\n:::\n\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Non-normality & Robust Estimation\n\nFor determining what constitutes deviations from normality, there are various ways to calculate the skewness and kurtosis of univariate distributions (see [Joanes, D.N. and Gill, C.A (1998). Comparing measures of sample skewness and kurtosis](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_1111_1467_9884_00122) if you're interested). In addition, there are various suggested rules of thumb we can follow. Below are the most common:  \n\nSkewness rules of thumb:  \n\n- $|skew| < 0.5$: fairly symmetrical\n- $0.5 < |skew| < 1$: moderately skewed\n- $1 < |skew|$: highly skewed\n\nKurtosis rule of thumb:\n\n- $Kurtosis > 3$: Heavier tails than the normal distribution. Possibly problematic\n\n\n:::blue\n**What we can do**\n\nIn the event of non-normality, we can still use maximum likelihood to find our estimated factor loadings, coefficients and covariances. However, our standard errors (and our $\\chi^2$ model fit) will be biased. We can use a robust maximum likelihood estimator that essentially applies corrections to the standard errors^[These are similar corrections to ones that we apply in a regression world, see [LM Troubleshooting](00_lm_assumpt.html#heteroscedastic-robust-standard-errors-huber-white){target=\"_blank\"}] and $\\chi^2$ statistic.  \n\nThere are a few robust estimators in **lavaan**, but one of the more frequently used ones is \"MLR\" (maximum likelihood with robust SEs). You can find all the other options at [https://lavaan.ugent.be/tutorial/est.html](https://lavaan.ugent.be/tutorial/est.html).  \nWe can make use of these with: `sem(model, estimator=\"MLR\")` or `cfa(model, estimator=\"MLR\")`.  \n\n:::\n\n:::\n\n`r qend()` \n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe should check the item distributions for evidence of non-normality (skewness and kurtosis). We can use the `describe()` function from psych and plot the data using histograms or density curves.\n\n```{r}\n#| code-fold: true\nlibrary(kableExtra)\n\n# this is just describe() but dressed up to make a nice table output (the one we see below)\nstress_IQ_data |> \n    select(contains(\"IQ\")) |> \n    describe() |> \n    as.data.frame() |>\n    rownames_to_column(var = \"variable\") |> \n    select(variable,mean,sd,skew,kurtosis) |>\n    kable(digits = 2) |>\n    kable_styling(full_width = FALSE)\n```\n\nIt looks like some of our IQ items are pretty skewed. Let's plot them. \n\n::::panelset\n:::panel\n#### multi.hist\n\n```{r}\nstress_IQ_data |> \n  select(contains(\"IQ\")) |> \n  multi.hist()\n```\n\n:::\n:::panel\n#### ggplot\n\n```{r}\n## GGPLOT\n# temporarily reshape the data to long format to make it quicker to plot\nstress_IQ_data |> \n  pivot_longer(IQ1:IQ5, names_to=\"variable\",values_to=\"score\") |>\n  ggplot(aes(x=score))+\n  geom_density()+\n  facet_wrap(~variable)+\n  theme_light()\n```\n\n:::\n::::\n\nBecause our variables seem to be non-normal, therefore, we should use a robust estimator such as MLR for our CFA\n\n```{r robust estimator}\nmodel_IQ <- 'IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5'\n\nmodel_IQ.est <- cfa(model_IQ, data=stress_IQ_data, estimator='MLR')\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nExamine the fit of the CFA model of IQ.  \nIf it doesn't fit very well, consider checking for areas of local misfit (i.e., check your `modindices()`), and adjust your model accordingly.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe also get out robust fit measures, so we should ask for them:\n```{r}\nfitmeasures(model_IQ.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\n```\n\nThe model doesn't fit very well so we could check the modification indices for local mis-specifications\n\n```{r check mods}\nmodindices(model_IQ.est, sort=T) |> head()\n```\n\nIt looks like we might need to include residual covariances between the items IQ1 and IQ2 and maybe also between items IQ4 and IQ5. \nAs always, we need to double check this makes substantive sense. Items IQ1 and IQ2 measure verbal comprehension and verbal memory - people might be likely to score low/high on both of these due to their verbal ability. IQ4 and IQ5 might be both related due to both being tests requiring visual perception, but it's less obvious. We'd probably want to know more about the specific tests undertaken. \n\n```{r make modifications}\nmodel2_IQ <- '\n    IQ=~IQ1+IQ2+IQ3+IQ4+IQ5\n    IQ1~~IQ2\n'\nmodel2_IQ.est <- cfa(model2_IQ, data=stress_IQ_data, estimator='MLR')\n```\n\nThe fit of the model is now much improved! and our loadings are all significant and $>|0.3|$.   \nThe RMSEA is still in that grey area between 0.05 and 0.08, so we would probably want to flag this when writing up. We could keep trying to add stuff in order to get it below 0.05, but that means a high risk of overfitting.  \n```{r}\nfitmeasures(model2_IQ.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\n\nsummary(model2_IQ.est, standardized=T)\n```\n\n`r solend()`\n\n## Ordered Categoricals\n\n\n`r qbegin(qcounter())`\nFit a one-factor confirmatory factor analysis for the latent factor of Stress. Note that the items are measured on a 3-point scale!  \n\n::: {.callout-note collapse=\"true\"}\n#### Ordered-Categorical Endogenous Variables\n\nSometimes we can treat ordinal data as if it is continuous. When exactly this is appropriate is a contentious issue - some statisticians might maintain that it is *never* appropriate to treat ordinal data as continuous. In psychology, much research using SEM centers around questionnaire data, which lends itself to *likert* data (for instance, \"strongly agree\",\"agree\",\"neither agree nor disagree\",\"disagree\",\"strongly disagree\"). An often used rule of thumb is that likert data with $\\geq 5$ levels can be treated as if they are continuous without unduly influencing results (see [Johnson, D.R., & Creech, J.C. (1983). Ordinal measures in multiple indicator models: A simulation study of categorization error](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_2307_2095231)).  \n\nBut what if we don't have 5+ levels? An important note is that even if your questionnaire included 5 response options for people to choose from, if participants only _used_ 3 of them, then you actually have a variable with just 3 levels. \n\nEstimation techniques exist that allow us to model such variables by considering the set of ordered responses to correspond to portions of an underlying continuum. This involves estimating the 'thresholds' of that underlying distribution at which people switch from giving one response to the next. \n\n::::{.columns}\n:::{.column width=\"40%\"}\nRather than working with correlations, this method involves using \"polychoric correlations\", which are estimates of the correlation between two theorized normally distributed continuous variables, based on their observed ordinal manifestations. \n\n:::\n:::{.column width=\"60%\"}\n```{r}\n#| echo: false\n#| label: fig-polychor\n#| fig-cap: \"the idea of polychoric correlation is to estimate the correlation between two unobserved continuous variables by examining the distributions across the set of ordered categories that we *do* observe\"\nset.seed(444)\ndf <- tibble(\n  item1 = rnorm(1e3),\n  item2 = .6*item1 + rnorm(1e3)\n)\ndf <- as.data.frame(apply(df,2,scale))\ndf2 <- apply(df,2,function(x) cut(x,3,labels=c(\"disagree\",\"neither\",\"agree\")))\nnames(df) <- paste0(\"l\",names(df))\ndf <- cbind(df,df2)\n\n\nthresh1 <- df |> group_by(item1) |>\n  summarise(\n    min = min(litem1),\n    max = max(litem1),\n    lab = max-((max-min)/2)\n  )\n\nthresh2 <- df |> group_by(item2) |>\n  summarise(\n    min = min(litem2),\n    max = max(litem2),\n    lab = max-((max-min)/2)\n  )\n\nggplot(df,aes(x=litem1,y=litem2,col=interaction(item1,item2)))+\n  geom_point(size=3,alpha=.5)+\n  guides(col=\"none\") +\n  geom_vline(data=thresh1[-2,], aes(xintercept=min))+\n  geom_hline(data=thresh2[-2,], aes(yintercept=min))+\n  geom_text(data=thresh1,inherit.aes=F,aes(x=lab,y=-3,label=item1))+\n  geom_text(data=thresh2,inherit.aes=F,aes(y=lab,x=-3.3,label=item2),angle=90)+\n  labs(x=\"underlying agreement with item 1\",\n       y=\"underlying agreement with item 2\")\n```\n:::\n\n::::\n\n\n:::blue\n**What we can do**  \n\nIn R, **lavaan** will automatically switch to a categorical estimator (called \"DWLS\"^[Diagonally Weighted Least Squares]) if we tell it that we have some ordered-categorical variables. We can use the `ordered = c(\"item1name\",\"item2name\",\"item3name\", ...)` argument.  \nThis is true for both the `cfa()` and `sem()` functions.  \n\n\n\n\n:::\n\n:::\n\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r categorical estimation}\nlibrary(lavaan)\nlibrary(semPlot)\n# specify the model\nmodel_stress <- 'Stress =~ stress1 + stress2 + stress3 + stress4 + stress5'\n\n# estimate the model - cfa will automatically switch to a categorical estimator if we mention that our five variables are ordered-categorical, using the 'ordered' function\nmodel_stress.est <- \n  cfa(model_stress, data=stress_IQ_data,\n      ordered=c('stress1','stress2','stress3','stress4','stress5'))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nInspect your summary model output. Notice that we have a couple of additional things - we have 'scaled' and 'robust' values for the fit statistics (we have a second column for all the fit indices but using a scaled version of the $\\chi^2$ statistic, and then we also have some extra rows of 'robust' measures), and we have the estimated 'thresholds' in our output (there are two thresholds per item in this example because we have a three-point response scale). The estimates themselves are not of great interest to us.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n# inspect the output\nfitmeasures(model_stress.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\nsummary(model_stress.est, standardized=TRUE)\n```\n`r solend()`\n\n\n`r qbegin(qcounter())`\nNow its time to build the full SEM.   \nEstimate the effect of prenatal stress on IQ.  \n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n**Remember:** We know that IQ is non-normal, so we would like to use a robust estimator (e.g. MLR). However, as lavaan will tell you if you try using `estimator=\"MLR\"`, this is not supported for ordered data (i.e., the Stress items). It suggests instead using the WLSMV (weighted least square mean and variance adjusted) estimator.  \n\nAs it happens, the WLSMV estimator is just DWLS with a correction to return robust standard errors. If you specify `estimator=\"WLSMV\"` then your standard errors *will* be corrected, but don't be misled by the fact that the summary here will still say that the estimator is DWLS.   \n\n:::\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r full SEM}\nSEM_model <- '\n    #IQ measurement model\n    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 \n    IQ1 ~~ IQ2\n    IQ4 ~~ IQ5\n\n    #stress measurement model \n    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 \n\n    #structural part of model\n    IQ ~ Stress\n'\n```\n\n\n```{r}\nSEM_model.est <- sem(SEM_model, data=stress_IQ_data,\n                     ordered=c('stress1','stress2','stress3','stress4','stress5'),\n                     estimator=\"WLSMV\")\n```\n\nLet's print out the full summary.  \nNote that when we have a mix of ordered categoricals _and_ continuous variables, then we can't get the robust estimates of the fit indices. These are now NA. We do still get the scaled versions though:  \n```{r}\nsummary(SEM_model.est, fit.measures=T, standardized=T)\n```\n```{r}\n#| echo: false\nd <- partable(SEM_model.est)\n```\n\n\nWe can see that the effect of prenatal stress on offspring IQ is $\\beta$ = `r round(d |> filter(lhs==\"IQ\",rhs==\"Stress\") |> pull(est), 3)` and is statistically significant at $p<.05$.\n\n`r solend()`\n\n## Missingness\n\n\n`r qbegin(qcounter())`\nIn order to try and replicate the IQ CFA, our researcher collects a **new** sample of size $n=500$. However, she has some missing data (specifically, those who scored poorly on earlier tests tended to feel discouraged and chose not to complete further tests).  \n  \nRead in the new dataset, plot and numerically summarise the univariate distributions of the measured variables, and then conduct a CFA using the new data, taking account of the missingness (don't forget to also use an appropriate estimator to account for any non-normality). Does the model fit well?    \n  \n+ The data can be found at [https://uoepsy.github.io/data/IQdatam.csv](https://uoepsy.github.io/data/IQdatam.csv), and is in .csv format.  \n \n \n::: {.callout-note collapse=\"true\"}\n#### Missingness\n\nIt is very common to have missing data. Participants may stop halfway through the study, may decline to be followed up (if it is longitudinal) or may simply decline to answer certain sections. In addition, missing data can occur for all sorts of technical reasons (e.g, website crash and auto-submit a questionnaire, etc.). \n\nIt is important to understand the possible reasons for missing data in order to appropriately consider what data you *do* have. If missing data are missing completely random, then the data you do have should still be representative of the population. But suppose you are studying cognitive decline in an aging population, and people who are suffering from cognitive impairment are less likely to attend the follow-up assessments. Is this missingness random? No. Does it affect how you should consider your available data to represent the population of interest? Yes. \n\nThere are three main reasons that we certain data may be missing, and the acronyms are a nightmare:^[To me, these are some of the most confusing terms in statistics, because \"at random\" is used to mean \"not completely random\"!?? It might be easier to think of \"missing at random\" as \"missing conditionally at random\", but then it gives the same acronym as \"completely at random\"!]  \n\n- **MCAR: Missing Completely At Random.** Data which are MCAR are missing data for which the propensity for missingness is completely independent of any observed or unobserved variables. It is truly random if the data are missing or not.  \n\n- **MAR: Missing At Random.** Data which are MAR are missing data for which the propensity for missingness is not random, but it can be fully explained by some variables for which there is complete data. In other words, there is a systematic relationship between missing values and observed values. For example, people who are unemployed at time of assessment will likely not respond to questions on work-life satisfaction. Missing values on work-life satisfaction is unrelated to the levels of work-life satisfaction, but related to their employment status. \n\n- **MNAR: Missing Not At Random.** Data which are MNAR are missing data for which the propensity for missingness is related to the value which is missing. For example, suppose an employer tells employees that there are a limited number of bonuses to hand out, and then employees are asked to fill out a questionnaire. Thos who are less satisfied with their job may not respond to questions on job satisfaction (if they believe it will negatively impact their chances of receiving a bonus). \n\n:::blue\n**What we can do**\n\nIn SEM, there is an extremely useful method known as **full information maximum likelihood (FIML)** which, allows us to use all our available data (*including* those that have only _some_ values present).  \n\nFIML separates data into the different patterns of missingness (i.e. those that are missing on `IQ1`, those missing on `IQ1` and `IQ2`, those missing on just `IQ2`, and so on). The likelihood of each group can be estimated from the parameters that are available for that pattern. The estimation process then finds the set of parameters that maximise the likelihood of the full dataset (we can sum up the loglikelihoods of the datsets with each missingness pattern). This way it utilises *all* observed variables (hence \"full information\") to estimate the model parameters.   \n\n```{r}\n#| include: false\n#| echo: false\nset.seed(345)\nd <- data.frame(x=rnorm(1e5),y=rnorm(1e5)) |>\n  filter(abs(x)<3,abs(y)<3)\nkd <- with(d, MASS::kde2d(x,y,n=100))\nkd$y[sample(1:length(kd$y),5)] <- NA\nkd$x[sample(1:length(kd$x),5)] <- NA\n\nlibrary(plotly)\nplot_ly(x=kd$x, y = kd$y, z = kd$z) |> \n  add_surface()\n```\n\nA downside, one could argue, is that you may have specific theoretical considerations about which observed variables *should* weigh in on estimating missingness in variable $x$ (rather than *all* variables), in which case imputation techniques may be preferable. FIML only provides unbiased estimates if the data are MAR (i.e. if the missingness depends on other variables present in the data).  \n  \nIn lavaan, we can make use of full information maximum likelihood by using either `missing = \"FIML\"` or `missing = \"ML\"` (they're the same) in the functions `cfa()` and `sem()`.  \n\n::: \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: other approaches to missingness\n\nThus far in our analyses, we have been simply excluding any row for which there is missingness. This is typically known as \"listwise deletion\", and is the default for many modelling approaches. Any person who is missing _some_ data on a variable in our model will simply not be included. This method _assumes_ that the data are \"Missing Completely At Random\", because we make the assumption that studying only the people who have complete data does not bias our results in any way.  \n\nOther traditional approaches are to use basic imputation methods such as \"mean/median imputation\" (replace missing values with the average of that variable). These can be okay, but they assume that the imputed values are **exactly** what we _would_ have observed had it not been missing, rather than estimates. The lack of variability around this imputed value will shrink the standard errors because it is assumed that no deviations exist among the substituted values. \nAnother option is \"regression imputation\", where we replace missing values with the predicted values from a regression model of the variable with missingness onto some relevant predictors. This is better because our estimates of the values of missing observations could be much better, but we still make an assumption that these estimates are perfect, when we know they are just estimates.  \n\nA more sophisticated approach to this is \"Multiple Imputation\" (MI). Multiple imputation is similar to the regression imputation approach described above in that it predicts the value of missing observations based on the covariates, but instead of predicting just one value, it predicts many. Rather than saying \"the predicted value for this missing observation is 5\", we say \"the predicted value for this missing observation is from the distribution $\\sim N(5,2)$\". So we simulate $m$ draws from that distribution, and we end up with $m$ predicted values. We make $m$ copies of our data and replace the missing observations with each of our $m$ predicted values. We then do our analysis on each $m$ imputed dataset, and pool the results!  \n\n:::\n\n:::\n \n \n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe can fit the model setting `missing='FIML'`. If data are missing at random (MAR) - i.e., missingness is related to the measured variables but not the unobserved missing values - then this gives us unbiased parameter estimates. Unfortunately we can never know whether data are MAR for sure as this would require knowledge of the missing values. \n\n```{r message=FALSE}\nIQ_data_new <- read_csv(\"https://uoepsy.github.io/data/IQdatam.csv\")\nmulti.hist(IQ_data_new, global = FALSE)\n\nIQ_data_new |> select(contains(\"IQ\")) |> \n    describe() |> \n    as.data.frame() |>\n    rownames_to_column(var = \"variable\") |> \n    select(variable,mean,sd,skew,kurtosis) |>\n    kable(digits = 2) |>\n    kable_styling(full_width = FALSE)\n```\n\n```{r missingness}\nIQ_model_missing <- '\n  IQ=~IQ1+IQ2+IQ3+IQ4+IQ5\n  IQ1~~IQ2\n  IQ4~~IQ5\n'\n\nIQ_model_missing.est <- cfa(IQ_model_missing, \n                            data=IQ_data_new, \n                            missing='FIML', estimator=\"MLR\")\n\nsummary(IQ_model_missing.est, fit.measures=T, standardized=T)\n```\n\nOur fit indices all look very good!  \n`r solend()`\n\n\n`r qbegin(qcounter())`\nNote that the summary of the model output when we used FIML also told us that there are 3 patterns of missingness.  \n\nCan you find out a) what the patterns are, and b) how many people are in each pattern?  \n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`is.na()` will help a lot here, as will `distinct()` and `count()`.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\nThis will turn all the NAs into TRUE, and everything else into FALSE:  \n```{r}\n#| eval: false\nis.na(IQ_data_new)\n```\n\nIf we turn this back to a dataframe, and then pass it to `distinct()`, we get out the different patterns!  \n```{r}\nis.na(IQ_data_new) |>\n  as.data.frame() |>\n  distinct()\n```\n\nGetting the counts is more difficult, but one way would be to do it with `count()`, but we have to give it all of the variables, to get all the different combinations:  \n```{r}\nis.na(IQ_data_new) |>\n  as.data.frame() |>\n  count(ID1,IQ1,IQ2,IQ3,IQ4,IQ5)\n```\n\n\n`r solend()`\n\n\n# Extras: Equivalent models\n\n`r qbegin(paste0(\"Extra \", qcounter()))`\nReturning to our full SEM, adjust your model so that instead of `IQ ~ Stress` we are fitting `Stress ~ IQ` (i.e. child cognitive ability $\\rightarrow$ prenatal stress).  \nDoes this model fit better or worse than our first one?  \n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\nHere's our first model:  \n```{r}\nfitmeasures(SEM_model.est)[c(\"rmsea.scaled\",\"srmr\",\"tli.scaled\",\"cfi.scaled\")]\n```\n\nAnd here's the model with the direction reversed:  \n```{r}\nSEM_model2 <- '\n    #IQ measurement model\n    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 \n    IQ1 ~~ IQ2\n    IQ4 ~~ IQ5\n    \n    #stress measurement model \n    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 \n    \n    #structural part of model\n    Stress ~ IQ\n'\n\nSEM_model.est2 <- sem(SEM_model2, data=stress_IQ_data,\n                    ordered = c('stress1', 'stress2', 'stress3', 'stress4', 'stress5'),\n                    estimator=\"WLSMV\")\n\nfitmeasures(SEM_model.est2)[c(\"rmsea.scaled\",\"srmr\",\"tli.scaled\",\"cfi.scaled\")]\n```\n\n\nThe fit is exactly the same!!  \n\nOne of the strengths of SEM is that we are fitting quantitative models that very closely correspond to theoretical models. In fact, researchers may often start with two slightly different theoretical models, and SEM will allow them to make a formal comparison of the two, because they will lead to differences in model fit. \n\nHowever, it is important to be aware that for every structural equation model, there are always equivalent representations of the model structure that leads to _precisely_ the same model fit. These are essentially re-parameterisations of our model - they contain the same information, and lead to the exact same model-implied covariance matrix, but they involve different theoretical assumptions. Many of these we can rule out on conceptual grounds (i.e. a child's cognitive ability cannot influence their mothers' pre-natal stress levels), but others we cannot.  \n\n`r solend()`\n\n\n# Extras: Simulating SEM data  \n\n\n`r qbegin(paste0(\"Extra\", qcounter()))`\n\n::: {.callout-note collapse=\"true\"}\n#### Simulating data as an tool for learning\n\nAn *extremely* useful approach to learning both R and statistics is to create yourself some fake data on which you can try things out. Because you create the data, you can control any relationships, group differences etc. In doing so, you essentially make yourself a target to aim for.  \n\nMany of you will currently be in the process of collecting/acquiring data for your thesis. If you are yet to obtain your data, we **strongly** recommend that you start to simulate some data with the expected distributions in order to play around and test how your analyses works, and how to interpret the results.  \n\nI estimate that I generate fake data at least a couple of times every week just to help me work out things I don't understand. It also enables for easy sharing of reproducible chunks of code which can perfectly replicate issues and help discussions.  \n\nWhile structural equation models are a fairly complex method, simulating data from a pre-defined SEM is actually pretty straightforward. We just specify a population model (i.e. giving it some values of each parameter) and then sample from it.  \nFor example:  \n\n```{r}\npopmod <- \"\nbiscuits =~ .7*oreo + .6*digestive + .7*custardcream + \n            .6*jammydodger + .8*bourbon + .3*jaffacake\n\"\n\nnewdata <- simulateData(popmod, sample.nobs = 300)\n\nhead(newdata)\n```\n\n:::\n\n1. Do a little bit of research and find a paper that fits a structural equation model. Simulate some data from it.  \n2. Save the dataset (`write_csv()` will help), and share it with someone else, along with a description of the theoretical model. Can they estimate a model that fits well? \n\n`r qend()`\n\n\n\n# Extras: Extensions of SEM\n\nWe have really only scraped the surface of the different things we can do with SEM. If you are interested in taking you learning further, then some of the next things to start looking into:   \n\n  - Multigroup analysis (testing the model across two or more populations)  \n    - JÃ¶reskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36(4), 409-426.  \n    - Sorbom, D. (1974). A general method for studying differences in factor means and factor structures between groups. British Journal of Mathematical and Statistical Psychology, 27, 229-239.  \n\n  - Latent Growth Curves (actually just the same as a multilevel model!! &#129327; )  \n    - [Michael Clark has a great lot of resources on this](https://m-clark.github.io/mixed-models-with-R/supplemental.html), and it makes the link between random effects and latent variables super clear.  \n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(psych)\nlibrary(semPlot)\nlibrary(lavaan)\n```\n\n\n\n\n:::frame\n__Prenatal Stress & IQ data__  \n\nA researcher is interested in the effects of prenatal stress on child cognitive outcomes.  \nShe has a 5-item measure of prenatal stress and a 5 subtest measure of child cognitive ability, collected for 500 mother-infant dyads. \n\n+ The data is available as a .csv file here: [https://uoepsy.github.io/data/stressIQ.csv](https://uoepsy.github.io/data/stressIQ.csv)\n\n```{r}\n#| echo: false\ntibble(\n  variable = names(read_csv(\"https://uoepsy.github.io/data/stressIQ.csv\")),\n  description = c(\"Participant ID\",\n                  \"acute stress\",\n                  \"chronic stress\",\n                  \"environmental stress\",\n                  \"psychological stress\",\n                  \"physiological stress\",\n                  \"verbal ability\",\n                  \"verbal memory\",\n                  \"inductive reasoning\",\n                  \"spatial orientiation\",\n                  \"perceptual speed\")\n) |> gt::gt()\n```\n\n\n:::\n\n`r qbegin(qcounter())`\nBefore we do anything with the data, grab some paper and sketch out the full model that you plan to fit to address the researcher's question.   \n\nTip: everything you need is in the description of the data. Start by drawing the specific path(s) of interest. Are these between latent variables? If so, add in the paths to the indicators for each latent variable.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nThe main parameter of interest here is \"the effects of prenatal stress on child cognitive outcomes\". So we have an arrow going from Stress to IQ.  \nEach of these are latent variables, for which we have observed 5 indicator variables, so we have an arrow going from \"IQ\" to each of the 5 IQ items, and from \"Stress\" to the 5 stress items. \n```{r echo=FALSE}\ndiagmod <- '\n#IQ measurement model\nIQ=~IQ1+IQ2+IQ3+IQ4+IQ5 \n#stress measurement model \nStress=~stress1+stress2+stress3+stress4+stress5 \n#structural part of model\nIQ~Stress'\nsemPlot::semPaths(lavaanify(diagmod),rotation=1)\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nRead in the data and explore it. Look at the individual distributions of each variable to get a sense of univariate normality, as well as the number of response options each item has.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThe `multi.hist()` function from the __psych__ package is pretty useful for getting quick histograms of multiple variables. If necessary, you can set `multi.hist(data, global = FALSE)` to let each histogram's x-axis be on a different scale.\n\n:::\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r message=FALSE}\nlibrary(tidyverse)\nlibrary(psych)\n\nstress_IQ_data <- read_csv(\"https://uoepsy.github.io/data/stressIQ.csv\")\n```\n\nThis gives us a whole load of descriptives, means, standard deviations, and other things like skew and kurtosis.  \n```{r}\ndescribe(stress_IQ_data) # from psych package\n```\n\nHere we can see the distribution of each variable. Some immediate things of note - the stress items are only measured on 3 response options, and the IQ items are quite positively skewed (we can see this matches with the skew metrics above).  \n```{r}\nmulti.hist(stress_IQ_data, global = FALSE) # from psych package\n```\n`r solend()`\n\n## Non-normality\n\n\n`r qbegin(qcounter())`\nFit a one factor CFA for the IQ items using an appropriate estimation method.   \n  \nBe sure to first check their distributions (both numerically and visually). The `describe()` function from the __psych__ package will give you measures of skew and kurtosis.  \n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Multivariate Normality (MVN)\n\n\nOne of the key assumptions of the models we have been fitting so far is that our variables are all normally distributed, and are continuous. Why?  \n\nRecall how we first introduced the idea of estimating a SEM: comparing the **observed covariance matrix** with our **model implied covariance matrix**. This heavily relies on the assumption that our observed covariance matrix provides a good representation of our data - i.e., that $var(x_i)$ and $cov(x_i,x_j)$ are adequate representations of the relations between variables. While this is true if we have a \"multivariate normal distribution\", things become more difficult when our variables are either not continuous or not normally distributed. \n\nThe multivariate normal distribution is fundamentally just the extension of our univariate normal (the bell-shaped curve we are used to seeing) to more dimensions. The condition which needs to be met is that every linear combination of the $k$ variables has a univariate normal distribution. It is denoted by $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ (note that the bold font indicates that $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are, respectively, a vector of means and a matrix of covariances).^[We've actually seen this previously in multilevel modelling, when we had random intercepts, random slopes, and the covariance between the two!]  \nThe idea of the multivariate normal can be a bit difficult to get to grips with in part because there's not an intuitive way to visualise it once we move beyond 2 or [3 variables](https://demonstrations.wolfram.com/JointDensityOfTrivariateGaussianRandomVariables/).  \n\nWith 2 variables, you can think of it just like a density plot but in 3 dimensions, as in @fig-bivnorm. You can also represent this as a \"contour\" plot, which is like looking down on @fig-bivnorm from above (just like a map!)  \n\n```{r}\n#| echo: false\n#| label: fig-bivnorm\n#| fig-cap: \"Bivariate Normal\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nx <- mvtnorm::rmvnorm(n = 1e3, mean = c(0,0), sigma = matrix(c(4,2,2,3), ncol = 2))\nd <- as.data.frame(x)\nd$density <- mvtnorm::dmvnorm(x = d)\nlibrary(plotly)\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n\nNow let's look at a situation where we have some skewed variables. @fig-mvnn shows such a distribution, and @fig-bivnon shows the contour plot (i.e. it is like @fig-mvnn viewed from above). \n\n::::panelset\n:::panel\n#### 3D\n\n```{r}\n#| echo: false\n#| label: fig-mvnn\n#| fig-cap: \"Joint distribution of some skewed variables\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nop <- list(xi=c(0,1), Psi=matrix(c(2,2,2,3), 2, 2), lambda=c(6, -1.5))\nrnd <- sn::rmsn(1e3, dp=sn::op2dp(op,\"SN\"))\nd<-as.data.frame(rnd)\nd$density <- mvtnorm::dmvnorm(x = d)\n\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n:::\n:::panel\n#### Contour\n\n```{r}\n#| label: fig-bivnon\n#| fig-cap: \"Contour plot for a non-normal bivariate distribution. Marginal distributions of each variable are presented along each axis\"\n#| echo: false\np1 <- ggplot(d,aes(x=V1,y=V2))+\n  geom_point(alpha=.2)+\n  geom_density_2d()\nggExtra::ggMarginal(p1, type=\"histogram\")\n```\n\n:::\n::::\n\n\n::: {.callout-caution collapse=\"true\"}\n#### simulations from a covariance matrix\n\nIf we compute our covariance matrix from the 2 skewed variables, we get:\n```{r}\n#| echo: false\ncov(d[,1:2])\n```\nBut this fails to capture information about important properties of the data relating to features such as skew (lop-sided-ness) and kurtosis (pointiness). As an example, if we were to simulate data based on this covariance matrix, we get something like @fig-bivsim, which is clearly limited in its reflection of our actual data.  \n\n::::panelset\n:::panel\n#### 3D\n```{r }\n#| echo: false\n#| label: fig-bivsim\n#| fig-cap: \"Probability distribution of simulations based on a covariance matrix (of some skewed variables). The covariance matrix contains only 2 'moments' of the variables: their scale (spread) and location (center)\"\n#| out-width: \"100%\"\n#| out-height: \"300px\"\nd <- as.data.frame(mvtnorm::rmvnorm(n = 1e3, mean = apply(d,2,mean)[1:2], sigma = cov(d[,1:2])))\nd$density <- mvtnorm::dmvnorm(x = d)\nplot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,\n              marker = list(color = ~ density,\n                            showscale = TRUE)) |> \n  add_markers()\n```\n\n:::\n:::panel\n#### Contour\n```{r}\n#| label: fig-bivnon2\n#| fig-cap: \"Contour plot for simulated data from a covariance matrix\"\n#| echo: false\np1 <- ggplot(d,aes(x=V1,y=V2))+\n  geom_point(alpha=.2)+\n  geom_density_2d()\nggExtra::ggMarginal(p1, type=\"histogram\")\n```\n:::\n::::\n\n:::\n\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n#### Non-normality & Robust Estimation\n\nFor determining what constitutes deviations from normality, there are various ways to calculate the skewness and kurtosis of univariate distributions (see [Joanes, D.N. and Gill, C.A (1998). Comparing measures of sample skewness and kurtosis](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_1111_1467_9884_00122) if you're interested). In addition, there are various suggested rules of thumb we can follow. Below are the most common:  \n\nSkewness rules of thumb:  \n\n- $|skew| < 0.5$: fairly symmetrical\n- $0.5 < |skew| < 1$: moderately skewed\n- $1 < |skew|$: highly skewed\n\nKurtosis rule of thumb:\n\n- $Kurtosis > 3$: Heavier tails than the normal distribution. Possibly problematic\n\n\n:::blue\n**What we can do**\n\nIn the event of non-normality, we can still use maximum likelihood to find our estimated factor loadings, coefficients and covariances. However, our standard errors (and our $\\chi^2$ model fit) will be biased. We can use a robust maximum likelihood estimator that essentially applies corrections to the standard errors^[These are similar corrections to ones that we apply in a regression world, see [LM Troubleshooting](00_lm_assumpt.html#heteroscedastic-robust-standard-errors-huber-white){target=\"_blank\"}] and $\\chi^2$ statistic.  \n\nThere are a few robust estimators in **lavaan**, but one of the more frequently used ones is \"MLR\" (maximum likelihood with robust SEs). You can find all the other options at [https://lavaan.ugent.be/tutorial/est.html](https://lavaan.ugent.be/tutorial/est.html).  \nWe can make use of these with: `sem(model, estimator=\"MLR\")` or `cfa(model, estimator=\"MLR\")`.  \n\n:::\n\n:::\n\n`r qend()` \n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe should check the item distributions for evidence of non-normality (skewness and kurtosis). We can use the `describe()` function from psych and plot the data using histograms or density curves.\n\n```{r}\n#| code-fold: true\nlibrary(kableExtra)\n\n# this is just describe() but dressed up to make a nice table output (the one we see below)\nstress_IQ_data |> \n    select(contains(\"IQ\")) |> \n    describe() |> \n    as.data.frame() |>\n    rownames_to_column(var = \"variable\") |> \n    select(variable,mean,sd,skew,kurtosis) |>\n    kable(digits = 2) |>\n    kable_styling(full_width = FALSE)\n```\n\nIt looks like some of our IQ items are pretty skewed. Let's plot them. \n\n::::panelset\n:::panel\n#### multi.hist\n\n```{r}\nstress_IQ_data |> \n  select(contains(\"IQ\")) |> \n  multi.hist()\n```\n\n:::\n:::panel\n#### ggplot\n\n```{r}\n## GGPLOT\n# temporarily reshape the data to long format to make it quicker to plot\nstress_IQ_data |> \n  pivot_longer(IQ1:IQ5, names_to=\"variable\",values_to=\"score\") |>\n  ggplot(aes(x=score))+\n  geom_density()+\n  facet_wrap(~variable)+\n  theme_light()\n```\n\n:::\n::::\n\nBecause our variables seem to be non-normal, therefore, we should use a robust estimator such as MLR for our CFA\n\n```{r robust estimator}\nmodel_IQ <- 'IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5'\n\nmodel_IQ.est <- cfa(model_IQ, data=stress_IQ_data, estimator='MLR')\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nExamine the fit of the CFA model of IQ.  \nIf it doesn't fit very well, consider checking for areas of local misfit (i.e., check your `modindices()`), and adjust your model accordingly.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nWe also get out robust fit measures, so we should ask for them:\n```{r}\nfitmeasures(model_IQ.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\n```\n\nThe model doesn't fit very well so we could check the modification indices for local mis-specifications\n\n```{r check mods}\nmodindices(model_IQ.est, sort=T) |> head()\n```\n\nIt looks like we might need to include residual covariances between the items IQ1 and IQ2 and maybe also between items IQ4 and IQ5. \nAs always, we need to double check this makes substantive sense. Items IQ1 and IQ2 measure verbal comprehension and verbal memory - people might be likely to score low/high on both of these due to their verbal ability. IQ4 and IQ5 might be both related due to both being tests requiring visual perception, but it's less obvious. We'd probably want to know more about the specific tests undertaken. \n\n```{r make modifications}\nmodel2_IQ <- '\n    IQ=~IQ1+IQ2+IQ3+IQ4+IQ5\n    IQ1~~IQ2\n'\nmodel2_IQ.est <- cfa(model2_IQ, data=stress_IQ_data, estimator='MLR')\n```\n\nThe fit of the model is now much improved! and our loadings are all significant and $>|0.3|$.   \nThe RMSEA is still in that grey area between 0.05 and 0.08, so we would probably want to flag this when writing up. We could keep trying to add stuff in order to get it below 0.05, but that means a high risk of overfitting.  \n```{r}\nfitmeasures(model2_IQ.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\n\nsummary(model2_IQ.est, standardized=T)\n```\n\n`r solend()`\n\n## Ordered Categoricals\n\n\n`r qbegin(qcounter())`\nFit a one-factor confirmatory factor analysis for the latent factor of Stress. Note that the items are measured on a 3-point scale!  \n\n::: {.callout-note collapse=\"true\"}\n#### Ordered-Categorical Endogenous Variables\n\nSometimes we can treat ordinal data as if it is continuous. When exactly this is appropriate is a contentious issue - some statisticians might maintain that it is *never* appropriate to treat ordinal data as continuous. In psychology, much research using SEM centers around questionnaire data, which lends itself to *likert* data (for instance, \"strongly agree\",\"agree\",\"neither agree nor disagree\",\"disagree\",\"strongly disagree\"). An often used rule of thumb is that likert data with $\\geq 5$ levels can be treated as if they are continuous without unduly influencing results (see [Johnson, D.R., & Creech, J.C. (1983). Ordinal measures in multiple indicator models: A simulation study of categorization error](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_2307_2095231)).  \n\nBut what if we don't have 5+ levels? An important note is that even if your questionnaire included 5 response options for people to choose from, if participants only _used_ 3 of them, then you actually have a variable with just 3 levels. \n\nEstimation techniques exist that allow us to model such variables by considering the set of ordered responses to correspond to portions of an underlying continuum. This involves estimating the 'thresholds' of that underlying distribution at which people switch from giving one response to the next. \n\n::::{.columns}\n:::{.column width=\"40%\"}\nRather than working with correlations, this method involves using \"polychoric correlations\", which are estimates of the correlation between two theorized normally distributed continuous variables, based on their observed ordinal manifestations. \n\n:::\n:::{.column width=\"60%\"}\n```{r}\n#| echo: false\n#| label: fig-polychor\n#| fig-cap: \"the idea of polychoric correlation is to estimate the correlation between two unobserved continuous variables by examining the distributions across the set of ordered categories that we *do* observe\"\nset.seed(444)\ndf <- tibble(\n  item1 = rnorm(1e3),\n  item2 = .6*item1 + rnorm(1e3)\n)\ndf <- as.data.frame(apply(df,2,scale))\ndf2 <- apply(df,2,function(x) cut(x,3,labels=c(\"disagree\",\"neither\",\"agree\")))\nnames(df) <- paste0(\"l\",names(df))\ndf <- cbind(df,df2)\n\n\nthresh1 <- df |> group_by(item1) |>\n  summarise(\n    min = min(litem1),\n    max = max(litem1),\n    lab = max-((max-min)/2)\n  )\n\nthresh2 <- df |> group_by(item2) |>\n  summarise(\n    min = min(litem2),\n    max = max(litem2),\n    lab = max-((max-min)/2)\n  )\n\nggplot(df,aes(x=litem1,y=litem2,col=interaction(item1,item2)))+\n  geom_point(size=3,alpha=.5)+\n  guides(col=\"none\") +\n  geom_vline(data=thresh1[-2,], aes(xintercept=min))+\n  geom_hline(data=thresh2[-2,], aes(yintercept=min))+\n  geom_text(data=thresh1,inherit.aes=F,aes(x=lab,y=-3,label=item1))+\n  geom_text(data=thresh2,inherit.aes=F,aes(y=lab,x=-3.3,label=item2),angle=90)+\n  labs(x=\"underlying agreement with item 1\",\n       y=\"underlying agreement with item 2\")\n```\n:::\n\n::::\n\n\n:::blue\n**What we can do**  \n\nIn R, **lavaan** will automatically switch to a categorical estimator (called \"DWLS\"^[Diagonally Weighted Least Squares]) if we tell it that we have some ordered-categorical variables. We can use the `ordered = c(\"item1name\",\"item2name\",\"item3name\", ...)` argument.  \nThis is true for both the `cfa()` and `sem()` functions.  \n\n\n\n\n:::\n\n:::\n\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\n```{r categorical estimation}\nlibrary(lavaan)\nlibrary(semPlot)\n# specify the model\nmodel_stress <- 'Stress =~ stress1 + stress2 + stress3 + stress4 + stress5'\n\n# estimate the model - cfa will automatically switch to a categorical estimator if we mention that our five variables are ordered-categorical, using the 'ordered' function\nmodel_stress.est <- \n  cfa(model_stress, data=stress_IQ_data,\n      ordered=c('stress1','stress2','stress3','stress4','stress5'))\n\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nInspect your summary model output. Notice that we have a couple of additional things - we have 'scaled' and 'robust' values for the fit statistics (we have a second column for all the fit indices but using a scaled version of the $\\chi^2$ statistic, and then we also have some extra rows of 'robust' measures), and we have the estimated 'thresholds' in our output (there are two thresholds per item in this example because we have a three-point response scale). The estimates themselves are not of great interest to us.  \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r}\n# inspect the output\nfitmeasures(model_stress.est)[c(\"rmsea.robust\",\"srmr\",\"cfi.robust\",\"tli.robust\")]\nsummary(model_stress.est, standardized=TRUE)\n```\n`r solend()`\n\n\n`r qbegin(qcounter())`\nNow its time to build the full SEM.   \nEstimate the effect of prenatal stress on IQ.  \n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n**Remember:** We know that IQ is non-normal, so we would like to use a robust estimator (e.g. MLR). However, as lavaan will tell you if you try using `estimator=\"MLR\"`, this is not supported for ordered data (i.e., the Stress items). It suggests instead using the WLSMV (weighted least square mean and variance adjusted) estimator.  \n\nAs it happens, the WLSMV estimator is just DWLS with a correction to return robust standard errors. If you specify `estimator=\"WLSMV\"` then your standard errors *will* be corrected, but don't be misled by the fact that the summary here will still say that the estimator is DWLS.   \n\n:::\n\n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n```{r full SEM}\nSEM_model <- '\n    #IQ measurement model\n    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 \n    IQ1 ~~ IQ2\n    IQ4 ~~ IQ5\n\n    #stress measurement model \n    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 \n\n    #structural part of model\n    IQ ~ Stress\n'\n```\n\n\n```{r}\nSEM_model.est <- sem(SEM_model, data=stress_IQ_data,\n                     ordered=c('stress1','stress2','stress3','stress4','stress5'),\n                     estimator=\"WLSMV\")\n```\n\nLet's print out the full summary.  \nNote that when we have a mix of ordered categoricals _and_ continuous variables, then we can't get the robust estimates of the fit indices. These are now NA. We do still get the scaled versions though:  \n```{r}\nsummary(SEM_model.est, fit.measures=T, standardized=T)\n```\n```{r}\n#| echo: false\nd <- partable(SEM_model.est)\n```\n\n\nWe can see that the effect of prenatal stress on offspring IQ is $\\beta$ = `r round(d |> filter(lhs==\"IQ\",rhs==\"Stress\") |> pull(est), 3)` and is statistically significant at $p<.05$.\n\n`r solend()`\n\n## Missingness\n\n\n`r qbegin(qcounter())`\nIn order to try and replicate the IQ CFA, our researcher collects a **new** sample of size $n=500$. However, she has some missing data (specifically, those who scored poorly on earlier tests tended to feel discouraged and chose not to complete further tests).  \n  \nRead in the new dataset, plot and numerically summarise the univariate distributions of the measured variables, and then conduct a CFA using the new data, taking account of the missingness (don't forget to also use an appropriate estimator to account for any non-normality). Does the model fit well?    \n  \n+ The data can be found at [https://uoepsy.github.io/data/IQdatam.csv](https://uoepsy.github.io/data/IQdatam.csv), and is in .csv format.  \n \n \n::: {.callout-note collapse=\"true\"}\n#### Missingness\n\nIt is very common to have missing data. Participants may stop halfway through the study, may decline to be followed up (if it is longitudinal) or may simply decline to answer certain sections. In addition, missing data can occur for all sorts of technical reasons (e.g, website crash and auto-submit a questionnaire, etc.). \n\nIt is important to understand the possible reasons for missing data in order to appropriately consider what data you *do* have. If missing data are missing completely random, then the data you do have should still be representative of the population. But suppose you are studying cognitive decline in an aging population, and people who are suffering from cognitive impairment are less likely to attend the follow-up assessments. Is this missingness random? No. Does it affect how you should consider your available data to represent the population of interest? Yes. \n\nThere are three main reasons that we certain data may be missing, and the acronyms are a nightmare:^[To me, these are some of the most confusing terms in statistics, because \"at random\" is used to mean \"not completely random\"!?? It might be easier to think of \"missing at random\" as \"missing conditionally at random\", but then it gives the same acronym as \"completely at random\"!]  \n\n- **MCAR: Missing Completely At Random.** Data which are MCAR are missing data for which the propensity for missingness is completely independent of any observed or unobserved variables. It is truly random if the data are missing or not.  \n\n- **MAR: Missing At Random.** Data which are MAR are missing data for which the propensity for missingness is not random, but it can be fully explained by some variables for which there is complete data. In other words, there is a systematic relationship between missing values and observed values. For example, people who are unemployed at time of assessment will likely not respond to questions on work-life satisfaction. Missing values on work-life satisfaction is unrelated to the levels of work-life satisfaction, but related to their employment status. \n\n- **MNAR: Missing Not At Random.** Data which are MNAR are missing data for which the propensity for missingness is related to the value which is missing. For example, suppose an employer tells employees that there are a limited number of bonuses to hand out, and then employees are asked to fill out a questionnaire. Thos who are less satisfied with their job may not respond to questions on job satisfaction (if they believe it will negatively impact their chances of receiving a bonus). \n\n:::blue\n**What we can do**\n\nIn SEM, there is an extremely useful method known as **full information maximum likelihood (FIML)** which, allows us to use all our available data (*including* those that have only _some_ values present).  \n\nFIML separates data into the different patterns of missingness (i.e. those that are missing on `IQ1`, those missing on `IQ1` and `IQ2`, those missing on just `IQ2`, and so on). The likelihood of each group can be estimated from the parameters that are available for that pattern. The estimation process then finds the set of parameters that maximise the likelihood of the full dataset (we can sum up the loglikelihoods of the datsets with each missingness pattern). This way it utilises *all* observed variables (hence \"full information\") to estimate the model parameters.   \n\n```{r}\n#| include: false\n#| echo: false\nset.seed(345)\nd <- data.frame(x=rnorm(1e5),y=rnorm(1e5)) |>\n  filter(abs(x)<3,abs(y)<3)\nkd <- with(d, MASS::kde2d(x,y,n=100))\nkd$y[sample(1:length(kd$y),5)] <- NA\nkd$x[sample(1:length(kd$x),5)] <- NA\n\nlibrary(plotly)\nplot_ly(x=kd$x, y = kd$y, z = kd$z) |> \n  add_surface()\n```\n\nA downside, one could argue, is that you may have specific theoretical considerations about which observed variables *should* weigh in on estimating missingness in variable $x$ (rather than *all* variables), in which case imputation techniques may be preferable. FIML only provides unbiased estimates if the data are MAR (i.e. if the missingness depends on other variables present in the data).  \n  \nIn lavaan, we can make use of full information maximum likelihood by using either `missing = \"FIML\"` or `missing = \"ML\"` (they're the same) in the functions `cfa()` and `sem()`.  \n\n::: \n\n\n::: {.callout-caution collapse=\"true\"}\n#### optional: other approaches to missingness\n\nThus far in our analyses, we have been simply excluding any row for which there is missingness. This is typically known as \"listwise deletion\", and is the default for many modelling approaches. Any person who is missing _some_ data on a variable in our model will simply not be included. This method _assumes_ that the data are \"Missing Completely At Random\", because we make the assumption that studying only the people who have complete data does not bias our results in any way.  \n\nOther traditional approaches are to use basic imputation methods such as \"mean/median imputation\" (replace missing values with the average of that variable). These can be okay, but they assume that the imputed values are **exactly** what we _would_ have observed had it not been missing, rather than estimates. The lack of variability around this imputed value will shrink the standard errors because it is assumed that no deviations exist among the substituted values. \nAnother option is \"regression imputation\", where we replace missing values with the predicted values from a regression model of the variable with missingness onto some relevant predictors. This is better because our estimates of the values of missing observations could be much better, but we still make an assumption that these estimates are perfect, when we know they are just estimates.  \n\nA more sophisticated approach to this is \"Multiple Imputation\" (MI). Multiple imputation is similar to the regression imputation approach described above in that it predicts the value of missing observations based on the covariates, but instead of predicting just one value, it predicts many. Rather than saying \"the predicted value for this missing observation is 5\", we say \"the predicted value for this missing observation is from the distribution $\\sim N(5,2)$\". So we simulate $m$ draws from that distribution, and we end up with $m$ predicted values. We make $m$ copies of our data and replace the missing observations with each of our $m$ predicted values. We then do our analysis on each $m$ imputed dataset, and pool the results!  \n\n:::\n\n:::\n \n \n`r qend()` \n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\nWe can fit the model setting `missing='FIML'`. If data are missing at random (MAR) - i.e., missingness is related to the measured variables but not the unobserved missing values - then this gives us unbiased parameter estimates. Unfortunately we can never know whether data are MAR for sure as this would require knowledge of the missing values. \n\n```{r message=FALSE}\nIQ_data_new <- read_csv(\"https://uoepsy.github.io/data/IQdatam.csv\")\nmulti.hist(IQ_data_new, global = FALSE)\n\nIQ_data_new |> select(contains(\"IQ\")) |> \n    describe() |> \n    as.data.frame() |>\n    rownames_to_column(var = \"variable\") |> \n    select(variable,mean,sd,skew,kurtosis) |>\n    kable(digits = 2) |>\n    kable_styling(full_width = FALSE)\n```\n\n```{r missingness}\nIQ_model_missing <- '\n  IQ=~IQ1+IQ2+IQ3+IQ4+IQ5\n  IQ1~~IQ2\n  IQ4~~IQ5\n'\n\nIQ_model_missing.est <- cfa(IQ_model_missing, \n                            data=IQ_data_new, \n                            missing='FIML', estimator=\"MLR\")\n\nsummary(IQ_model_missing.est, fit.measures=T, standardized=T)\n```\n\nOur fit indices all look very good!  \n`r solend()`\n\n\n`r qbegin(qcounter())`\nNote that the summary of the model output when we used FIML also told us that there are 3 patterns of missingness.  \n\nCan you find out a) what the patterns are, and b) how many people are in each pattern?  \n\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\n`is.na()` will help a lot here, as will `distinct()` and `count()`.  \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n\nThis will turn all the NAs into TRUE, and everything else into FALSE:  \n```{r}\n#| eval: false\nis.na(IQ_data_new)\n```\n\nIf we turn this back to a dataframe, and then pass it to `distinct()`, we get out the different patterns!  \n```{r}\nis.na(IQ_data_new) |>\n  as.data.frame() |>\n  distinct()\n```\n\nGetting the counts is more difficult, but one way would be to do it with `count()`, but we have to give it all of the variables, to get all the different combinations:  \n```{r}\nis.na(IQ_data_new) |>\n  as.data.frame() |>\n  count(ID1,IQ1,IQ2,IQ3,IQ4,IQ5)\n```\n\n\n`r solend()`\n\n\n# Extras: Equivalent models\n\n`r qbegin(paste0(\"Extra \", qcounter()))`\nReturning to our full SEM, adjust your model so that instead of `IQ ~ Stress` we are fitting `Stress ~ IQ` (i.e. child cognitive ability $\\rightarrow$ prenatal stress).  \nDoes this model fit better or worse than our first one?  \n\n`r qend()`\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\nHere's our first model:  \n```{r}\nfitmeasures(SEM_model.est)[c(\"rmsea.scaled\",\"srmr\",\"tli.scaled\",\"cfi.scaled\")]\n```\n\nAnd here's the model with the direction reversed:  \n```{r}\nSEM_model2 <- '\n    #IQ measurement model\n    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 \n    IQ1 ~~ IQ2\n    IQ4 ~~ IQ5\n    \n    #stress measurement model \n    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 \n    \n    #structural part of model\n    Stress ~ IQ\n'\n\nSEM_model.est2 <- sem(SEM_model2, data=stress_IQ_data,\n                    ordered = c('stress1', 'stress2', 'stress3', 'stress4', 'stress5'),\n                    estimator=\"WLSMV\")\n\nfitmeasures(SEM_model.est2)[c(\"rmsea.scaled\",\"srmr\",\"tli.scaled\",\"cfi.scaled\")]\n```\n\n\nThe fit is exactly the same!!  \n\nOne of the strengths of SEM is that we are fitting quantitative models that very closely correspond to theoretical models. In fact, researchers may often start with two slightly different theoretical models, and SEM will allow them to make a formal comparison of the two, because they will lead to differences in model fit. \n\nHowever, it is important to be aware that for every structural equation model, there are always equivalent representations of the model structure that leads to _precisely_ the same model fit. These are essentially re-parameterisations of our model - they contain the same information, and lead to the exact same model-implied covariance matrix, but they involve different theoretical assumptions. Many of these we can rule out on conceptual grounds (i.e. a child's cognitive ability cannot influence their mothers' pre-natal stress levels), but others we cannot.  \n\n`r solend()`\n\n\n# Extras: Simulating SEM data  \n\n\n`r qbegin(paste0(\"Extra\", qcounter()))`\n\n::: {.callout-note collapse=\"true\"}\n#### Simulating data as an tool for learning\n\nAn *extremely* useful approach to learning both R and statistics is to create yourself some fake data on which you can try things out. Because you create the data, you can control any relationships, group differences etc. In doing so, you essentially make yourself a target to aim for.  \n\nMany of you will currently be in the process of collecting/acquiring data for your thesis. If you are yet to obtain your data, we **strongly** recommend that you start to simulate some data with the expected distributions in order to play around and test how your analyses works, and how to interpret the results.  \n\nI estimate that I generate fake data at least a couple of times every week just to help me work out things I don't understand. It also enables for easy sharing of reproducible chunks of code which can perfectly replicate issues and help discussions.  \n\nWhile structural equation models are a fairly complex method, simulating data from a pre-defined SEM is actually pretty straightforward. We just specify a population model (i.e. giving it some values of each parameter) and then sample from it.  \nFor example:  \n\n```{r}\npopmod <- \"\nbiscuits =~ .7*oreo + .6*digestive + .7*custardcream + \n            .6*jammydodger + .8*bourbon + .3*jaffacake\n\"\n\nnewdata <- simulateData(popmod, sample.nobs = 300)\n\nhead(newdata)\n```\n\n:::\n\n1. Do a little bit of research and find a paper that fits a structural equation model. Simulate some data from it.  \n2. Save the dataset (`write_csv()` will help), and share it with someone else, along with a description of the theoretical model. Can they estimate a model that fits well? \n\n`r qend()`\n\n\n\n# Extras: Extensions of SEM\n\nWe have really only scraped the surface of the different things we can do with SEM. If you are interested in taking you learning further, then some of the next things to start looking into:   \n\n  - Multigroup analysis (testing the model across two or more populations)  \n    - JÃ¶reskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36(4), 409-426.  \n    - Sorbom, D. (1974). A general method for studying differences in factor means and factor structures between groups. British Journal of Mathematical and Statistical Psychology, 27, 229-239.  \n\n  - Latent Growth Curves (actually just the same as a multilevel model!! &#129327; )  \n    - [Michael Clark has a great lot of resources on this](https://m-clark.github.io/mixed-models-with-R/supplemental.html), and it makes the link between random effects and latent variables super clear.  \n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"11exBUCHAILLE.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Week 11 Exercises: More SEM!","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}