{"title":"Week 8 Exercises: CFA","markdown":{"yaml":{"title":"Week 8 Exercises: CFA","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Exercises for the Enthusiastic","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(psych)\nlibrary(semPlot)\n```\n\n:::frame\n__New packages__  \n\nMake sure you have these packages installed:  \n\n+ lavaan\n+ semPlot\n\n:::\n\n\n\n:::frame\n__Dataset: radakovic_das.csv__  \n\nApathy is lack of motivation towards goal-directed behaviours. It is pervasive in a majority of psychiatric and neurological diseases, and impacts everyday life. Traditionally, apathy has been measured as a one-dimensional construct but is in fact composed of different types of demotivation.\n\n**The Dimensional Apathy Scale (DAS)** is a multidimensional assessment for demotivation, in which 3 subtypes of apathy are assessed:  \n\n- **Executive:** lack of motivation for planning, attention or organisation\n- **Emotional:** lack of emotional motivation (indifference, affective or emotional neutrality, flatness or blunting)\n- **Initiation:** lack of motivation for self-generation of thoughts and/or actions\n\nThe DAS measures these subtypes of apathy and allows for quick and easy assessment, through self-assessment, observations by informants/carers or administration by researchers or healthcare professionals.  \n\nYou can find data for the DAS when administered to 250 healthy adults at [https://uoepsy.github.io/data/radakovic_das.csv](https://uoepsy.github.io/data/radakovic_das.csv){target=\"_blank\"}, and information on the items is below.  \n\n::: {.callout-note collapse=\"true\"}\n#### DAS Dictionary\n\nAll items are measured on a 6-point Likert scale of Always (0), Almost Always (1), Often (2), Occasionally (3), Hardly Ever (4), and Never (5). Certain items (indicated in the table below with a `-` direction) are reverse scored to ensure that higher scores indicate greater levels of apathy. \n\n```{r}\n#| echo: false\nqnames = c(\"I need a bit of encouragement to get things started\",\"I contact my friends\",\"I express my emotions\",\"I think of new things to do during the day\",\"I am concerned about how my family feel\",\"I find myself staring in to space\",\"Before I do something I think about how others would feel about it\",\"I plan my days activities in advance\",\"When I receive bad news I feel bad about it\",\"I am unable to focus on a task until it is finished\",\"I lack motivation\",\"I struggle to empathise with other people\",\"I set goals for myself\",\"I try new things\",\"I am unconcerned about how others feel about my behaviour\",\"I act on things I have thought about during the day\",\"When doing a demanding task, I have difficulty working out what I have to do\",\"I keep myself busy\",\"I get easily confused when doing several things at once\",\"I become emotional easily when watching something happy or sad on TV\",\"I find it difficult to keep my mind on things\",\"I am spontaneous\",\"I am easily distracted\",\"I feel indifferent to what is going on around me\")\nrevitems = c(10,3,5,7,9,20,2,4,8,13,14,16,18,22)\nExitems = c(1,6,10,11,17,19,21,23)\nEmitems = c(3,5,7,9,12,15,20,24)\nBCIitems = c(2,4,8,13,14,16,18,22)\n\ntibble(\n  item = 1:24,\n  direction = case_when(\n    item %in% revitems ~ \"-\",\n    TRUE ~ \"+\"\n  ),\n  dimension = case_when(\n    item %in% Exitems ~ \"Executive\",\n    item %in% Emitems ~ \"Emotional\",\n    item %in% BCIitems ~ \"Initiation\"\n  ),\n  question = qnames\n) |> gt::gt()\n```\n\nHere are the item numbers that correspond to each dimension.\n\n- Executive: 1, 6, 10, 11, 17, 19, 21, 23\n- Emotional: 3, 5, 7, 9, 12, 15, 20, 24\n- Initiation: 2, 4, 8, 13, 14, 16, 18, 22\n\n:::\n\n:::\n\n`r qbegin(qcounter())`\nRead in the data. \nIt will need a little bit of tidying before we can get to fitting a CFA.  \n\nRemember that most of the actions needed for working with those sort of data are described in the [Chapter on Data Wrangling for Questionnaires](https://uoepsy.github.io/lv/00_datawrangle.html){target=\"_blank\"}.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nBy the looks of things, this is what I would consider doing:\n\n- Rename the variables to easy-to-read strings like `q1`, `q2`, `q3`, etc.\n- Set up a data dictionary that records the text of the item `q1` corresponds to, the text that `q2` corresponds to, etc.\n- Recode the Likert scale labels to numbers.\n- Reverse-code the questions with a negative direction. Note, you don't **need** to this, as they'll just end up with loadings in the opposite direction, but I would strongly recommend it for interpretation purposes.  \n- Check if there is missing data and if there is, removing those observations.\n\n\n\n:::\n\n\n`r qend()`\n`r solbegin(show=TRUE, label=\"1 - Read and check\", slabel=FALSE, toggle=params$TOGGLE)`\n\nFirst let's just read in the dataset:\n\n```{r}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nrdas <- read_csv(\"https://uoepsy.github.io/data/radakovic_das.csv\")\nhead(rdas)\n```\n\nThe names we're getting are useful in that they show the items, but they're horrible to have to use in R, so we will ideally replace them with easy to use names. \nNote also that the data is being read in as the actual response option - e.g., \"Almost Always\" - and we want to treat these as a numeric scale. So those will have to change too.  \n\n`r solend()`\n`r solbegin(show=TRUE, label=\"2 - Renaming variables\", slabel=FALSE, toggle=params$TOGGLE)`\n\nI like to make a \"data dictionary\" whenever I get data like this. While I want to rename the variables to make it easier for me to use, I also want to keep track of what the questions were.  \nHere I make a \"tibble\" (the function `data.frame()` would work too, tibble is just tidyverse version). I indicate what I am going to rename things as (\"q1\",\"q2\", ..., \"q24\"), and then I have the current names of the variables\n```{r}\nrdas_dict <- tibble(\n  variable = paste0(\"q\",1:24),\n  item = names(rdas)\n)\n```\n\nDoing this is really useful because I can't keep track in my head of what \"q5\" was.  \nIf I want to know, then I can just do: \n```{r}\nrdas_dict[5,]\n```\n\nNow let's actually change the names in our data to what we said we would:  \n```{r}\nnames(rdas) <- paste0(\"q\", 1:24)\n```\n\n`r solend()`\n`r solbegin(show=TRUE, label=\"3 - Recoding responses\", slabel=FALSE, toggle=params$TOGGLE)`\n\nOkay, so we have all our data in words, not numbers. Views on how to treat Likert data are mixed, but it's very common to treat it as continuous in Psychology.  \n\nLet's check the response values we have. Just in question 1 for now:  \n```{r}\nunique(rdas$q1)\n```\n\nA little trick that we can use to find the unique values in an entire dataset is to quickly convert the dataframe into one big long vector. Technically, a dataframe is a \"list of vectors\", and the function `unlist()` will remove this structure.  \nSo we can find all the unique values in all the questions with:  \n```{r}\nunique(unlist(rdas))\n```\n\nPerfect. So we know we have uniformity of spelling. It happens less often these days as questionnaire software is improving, but you might occasionally encounter typos in _some_ of the questions, or things with and without capital letters (R is a bit thick, and doesn't recognise that \"Often\" and \"often\" are the same thing).  \nNote that we have the 6 responses that we would expect given the description of the scale, but we also have some `NA` values, and some `[NO ENTRY]` values. Not sure how those got there.  \nWe want to turn each \"Always\" in to 0, each \"Almost Always\" in to 1, \"Often\" in to 2, and so on. If we simply leave out the \"[NO ENTRY]\", then this will be turned into a missing value `NA`, which is handy.  \n\n```{r}\nrdas <- rdas |> \n  mutate(across(q1:q24, ~case_match(.,\n    \"Always\" ~ 0,\n    \"Almost Always\" ~ 1,\n    \"Often\" ~ 2,\n    \"Occasionally\" ~ 3,\n    \"Hardly Ever\" ~ 4,\n    \"Never\" ~ 5\n  )))\nhead(rdas)\n```\n\n`r solend()`\n`r solbegin(show=TRUE, label=\"4 - Reverse Coding\", slabel=FALSE, toggle=params$TOGGLE)`\nAccording to the table of items, the ones which need to be reverse scored are:  \n```{r}\nreversed <- c(2,3,4,5,7,8,9,10,13,14,16,18,20,22)\n```\n\nFor these items, we want 5s to become 0s, 4s become 1s, and so on. \n\nThe tidyverse solution shown in the readings and solutions to previous labs will work just fine, but if you're curious, here's a different way to accomplish the same thing using functions from base R:\n\n```{r}\nrdas[, reversed] <- apply(\n  rdas[, reversed], MARGIN = 2, function(x) 5-x)\n```\n\n__Note:__ The above code works nicely because our dataset is currently ordered such that the first column is item 1, 2nd column is item 2, and so on. This means we can use _numbers_ to index the appropriate variables, rather than _names_. It would need adjusting if, for instance, our first column contained \"participant ID\", and our items only began later.  \n\n`r solend()`\n`r solbegin(show=TRUE, label=\"5 - Removing missingness\", slabel=FALSE, toggle=params$TOGGLE)`\n\nWe haven't learned about more sophisticated methods of handling missing data, so for now we will just remove any rows in which there is missingness - i.e., we'll do \"listwise deletion\":  \n\n```{r}\ncompl_rdas <- na.omit(rdas)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nSpecify the theoretical model proposed by Radakovic et al.   \n\nFor reference, check out [the example in the readings](https://uoepsy.github.io/lv/04_cfa.html#cfa-in-r---the-lavaan-package).\n\n```{r}\n#| eval: false\ndasmod <- \"\n\n\n\n\n\n\"\n```\n\nChallenge: Before you estimate the model, how many degrees of freedom do you think the model will have?\n([The readings](https://uoepsy.github.io/lv/04_cfa.html#degrees-of-freedom) will help here!)\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nYou'll have to use the data dictionary to see which items are associated with which dimensions. \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, label=\"1 - Specifying the model\", slabel=FALSE, toggle=params$TOGGLE)`\nHere is the model structure, according to the list of items in the dictionary.  \nI'm calling my factors \"Em\", \"Ex\", and \"BCI\" for \"emotional\", \"executive\" and \"behavioural/cognitive initiation\" respectively. The factor correlations will be estimated by default, but I like to write things explicitly. \n```{r}\ndasmod = \"\nEx =~ q1 + q6 + q10 + q11 + q17 + q19 + q21 + q23\nEm =~ q3 + q5 + q7 + q9 + q12 + q15 + q20 + q24\nBCI =~ q2 + q4 + q8 + q13 + q14 + q16 + q18 + q22\nEm ~~ Ex\nEm ~~ BCI\nEx ~~ BCI\n\"\n```\n\n\n`r solend()`\n`r solbegin(show=params$SHOW, label=\"2 - Computing degrees of freedom\", slabel=FALSE, toggle=params$TOGGLE)`\n\nDegrees of freedom is computed as the number of \"knowns\" minus the number of \"unknowns\".\n\nLet's start with figuring out the number of \"knowns\": the number of values in the dataset.\nThis number comes from the observed covariance matrix.\nLet's imagine a smaller dataset with only five items.\nIt'll create a covariance matrix like this:\n\n```\nvar\ncovar   var\ncovar   covar   var\ncovar   covar   covar   var\ncovar   covar   covar   covar   var\n```\n\n\nHow many values are in this matrix?\nIn the first row, there's 1, plus the second row with 2, plus the third row with 3, plus the fourth row with 4, plus the fifth row with 5.\nIn other words, there are\n\n```{r}\nsum(1:5)\n```\n\nvalues in this covariance matrix.  \n\nFor the present scenario with 24 items, we will have\n\n```{r}\nsum(1:24)\n```\n\nvalues in the covariance matrix.\n(Twenty-four of these will be each item's own variance, and the other 276 will be covariances between items.)  \n\nThe alternative formula to calculate this is $\\frac{k \\cdot (k+1)}{2}$, and we get the same number by plugging in the number of variables for $k$: $\\frac{24 \\cdot (24+1)}{2} = \\frac{600}{2} = 300$\n\nNow let's look at the number of \"unknowns\": the number of parameters the model has to estimate.\nThis number comes from the number of latent variables and how they relate to each item.\n\n- Each latent variable has its own variance, and there are three latent variables, so the model will have three latent factor variances.\n- Each item will load onto one latent variable, and there are 24 items, so the model will have 24 factor loadings.\n- Each item will have residual factor variances, and there are 24 items, so the model will have 24 residual factor variances.\n\nAdding these up, we get\n\n```{r}\n3 + 24 + 24\n```\n\nunknown parameters.\n\nFinally, let's subtract the knowns from the unknowns to get the degrees of freedom:\n\n```{r}\n300 - 51\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nEstimate the model using `cfa()`.  \nYou can choose whether you want to standardise the latent factors or fix the first loading of each factor to be 1 (it's the same model, just scaled differently).  \n\nExamine the model fit - does it fit well?\n\nWhat modifications do the modification indices suggest? Are the top three suggestions theoretically reasonable, in your opinion?  \n\nRemember, we don't really _want_ to have to make modifications to our models. If you don't need to (if the model fits well) then don't bother! (It's still worth _looking_ at the modification indices though).  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThere's a whole section on \"model fit\" in the [CFA chapter](https://uoepsy.github.io/lv/04_cfa.html#model-fit){target=\"_blank\"}!  \n\nAnd there's also a whole section on [model modifications](https://uoepsy.github.io/lv/04_cfa.html#model-modifications).\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nLet's fit the model!  \n```{r}\ndasmod.est = cfa(dasmod, compl_rdas)\n```\n\n\nThe standard test of model fit is a chi-squared test which compares the observed covariance matrix to the model-implied covariance matrix.\nIdeally, these two matrices will be fairly similar, so we want a non-significant result.\n\nWe can get the test statistic and p-value from the model's chi-squared test as follows:\n\n```{r}\nsummary(dasmod.est)$test$standard\n```\n\nSo a chi-squared test with 249 degrees of freedom results in a test statistic of 274.8, associated with a p-value of 0.125.\nThe take-away is that our observed covariance matrix is not significantly different from the model-implied covariance matrix—yay!\n\nNext, let's check the additional measures of global fit: \n\n```{r}\nfitmeasures(dasmod.est)[c(\"srmr\",\"rmsea\",\"tli\",\"cfi\")]\n```\n\nAll looks pretty good! Cut-offs for SRMR tend to vary, with some using <0.08, or <0.09, and some being stricter with <0.05. Remember, these criteria are somewhat arbitrary.  \n\nModification indices suggest a whole bunch of items that could have some associations beyond that modelled in the factors, but these are all weak correlations at around 0.2.  \n\n```{r}\nmodindices(dasmod.est, sort=TRUE) |> head()\n```\n\nThese are the top 3 being suggested. I can't see any obvious link between any of these that would make me think they are related beyond their measuring of 'apathy'.  \n```{r}\nrdas_dict[c(3,6),]\nrdas_dict[c(1,9),]\nrdas_dict[c(19,20),]\n```\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nAre the (*standardised*) loadings all \"big enough\"?  \nThere's no clear threshold that people use here - it depends a lot on the field, and on the wordings of specific items. Ideally, the same value we used in EFA ($\\geq|0.3|$) would be nice, but not crucial.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nTo get out standardised loadings, we can do:  \n```{r}\n#| eval: false\nmod.est <- cfa(model_syntax, data = ...)\nsummary(mod.est, std=TRUE)\n```\n\nAnd you'll get out an extra 2 columns in the summary output.  \n\nPay attention to the `Estimate`, `Std.lv` and `Std.all` columns in your output. The way I think of these columns is just to think of how we scale things in regression models:  \n\n- `Estimate` column : `item ~ Factor`  \n- `Std.lv` column : `item ~ scale(Factor)`  \n- `Std.all` column: `scale(item) ~ scale(Factor)`  \n\nSo if, when we fitted the model, we had specified `cfa(model, data, std.lv = TRUE)`, then the factor _already has a variance of 1_, so `Scale(Factor)` doesn't do anything.  \n\nSee [Chapter 4#interpretation](https://uoepsy.github.io/lv/04_cfa.html#interpretation){target=\"_blank\"}.  \n\n:::\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nI'm not going to print all of this right now because there's so much output, but here's how we would find standardised loadings. We can find them in the `Std.all` column.  \n```{r}\n#| eval: false\nsummary(dasmod.est, std = TRUE)\n```\n```\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Ex =~                                                                 \n    q1                1.000                               0.679    0.694\n    q6                0.679    0.121    5.607    0.000    0.461    0.433\n    ...\n    ...\n```\n\nThe standardised loadings are all (*just*) greater than $|0.3|$. Questions 13 and 15 are very close... \n\n```{r}\nrdas_dict[c(13,15),]\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDo the factors correlate in the way you would expect?  \n\nIs more emotional apathy associated with more executive apathy? and with more initiation apathy?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nIf you *didn't* reverse code the appropriate items, then this might get confusing, because we'd have to look at factor loadings to know in which direction the factor is going (i.e., are higher numbers \"more apathy\" or \"less apathy\"?).  \n\nIf you *did* reverse code the appropriate items, then you're golden, because you made them all point towards \"more\" apathy.   \n\n:::\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\nHere are the correlations we're interested in. Note that what we are seeing is that the three factors are all positively correlated, but for `Em` and `Ex` this is only weak (and not significant).   \n\nThis isn't necessary a problem, it just means that these two factors are fairly distinct/orthogonal. We might want to check back in the original paper to see what they proposed! \n```{r}\n#| eval: false\nsummary(dasmod.est, std = TRUE)\n```\n```\n...\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Ex ~~                                                                 \n    Em                0.051    0.028    1.807    0.071    0.159    0.159\n  Em ~~                                                                 \n    BCI               0.043    0.018    2.369    0.018    0.263    0.263\n  Ex ~~                                                                 \n    BCI               0.151    0.040    3.746    0.000    0.642    0.642\n```\n\n\n<!-- If we look at the loadings for the Executive apathy factor, they are all positive other than `q10`. According to our data dictionary, this `q10` is negatively worded, and the others are all positively worded. So being higher on the `Ex` factor means more Executive Apathy, which is what we want.   -->\n\n<!-- By contrast, both `Em` and `BCI` have positive loadings for the questions that are negatively worded according to the data dictionary. Note that **all** of the initiation apathy questions in the data dictionary are in the other direction. So what we are getting is that being higher the `BCI` factor means having *less* Initiation Apathy. So it's back to front. The same applies to the `Em` factor.   -->\n\n<!-- So actually, all forms of apathy are *positively* correlated (because more of each type is associated with more of the other types).   -->\n`r solend()`\n\n\n`r qbegin(qcounter())`\nMake a diagram of the model.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nFor a quick look at the structure of the model, try the `semPaths()` function from the **semPlot** package [Chapter 4 CFA#making diagrams](https://uoepsy.github.io/lv/04_cfa.html#making-diagrams){target=\"_blank\"}.\n\nIf you were going to use this sort of diagram in a proper write-up, though, it'd be better to make a nicer graphic manually (e.g., in Powerpoint, your favourite graphics software, or [semdiag](https://semdiag.psychstat.org)).\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nHere's one example:\n\n```{r}\nlibrary(semPlot)\nsemPaths(dasmod.est, whatLabels = \"std\", rotation=2)\n```\n\nThere are lots of options in `semPaths()`, so if you can make your graphic more elaborate than this one, then be our guest!\n\n`r solend()`\n\n`r qbegin(paste0(\"Optional Question \", qcounter()), qlabel=FALSE)`\n<!-- Much like for EFA, we can estimate individuals' scores on the latent factors. In lavaan, the function `lavPredict()` will get us some estimates.   -->\n\n<!-- However, for a clinician administering the DAS to a patient, this option is not available. Instead, it is common that scores on the individual items associated with a given factor are used to create a sum score or a mean score which can be used in a practical setting (e.g., scores above a given threshold might indicate cause for concern).   -->\n\nImagine that you're a clinician administering the DAS to a patient.\nIn clinical settings, it's common practice to skip the complex factor analysis we've been doing here and just create a sum score or a mean score that describe a patient's responses.\nThen clinicians can check whether the score is above some threshold to see whether there's cause for concern.\n\nFor each of the dimensions of apathy in the data, calculate sum scores for each of the 250 participants.\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nGood ol' `rowSums()` to the rescue!  \n\n:::\n`r qend()`\n`r solbegin(show=TRUE, toggle=TRUE)`\nHere are the sets of items associated with each dimension: \n```{r}\nExitems <- c(1,6,10,11,17,19,21,23)\nEmitems <- c(3,5,7,9,12,15,20,24)\nBCIitems <- c(2,4,8,13,14,16,18,22)\n```\n\nAgain, because the item numbers correspond to the column positions in our data, we can just do rowSums indexing on those column numbers to get our scores:  \n```{r}\ncompl_rdas$ExSCORE <- rowSums(compl_rdas[,Exitems])\ncompl_rdas$EmSCORE <- rowSums(compl_rdas[,Emitems])\ncompl_rdas$BCIScore <- rowSums(compl_rdas[,BCIitems])\n```\n\n`r solend()`\n\n`r qbegin(paste0(\"Optional Question \", qcounter()), qlabel = FALSE)`\nHow might you think about a sum/mean score in terms of a diagram?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWhat does a sum or mean score imply about how each item is weighted compared to the others?\nHow is this different from what a more sophisticated method like EFA or CFA can do?\n\n:::\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=TRUE)`\n\nComputing sum scores can feel like a 'model free' calculation, but actually it **does** pre-suppose a factor structure, and a much more constrained one than those we have been estimating.\nSpecifically, we're assuming that all items contribute equally to an underlying factor, rather than being weighted differently, and that all items also have the same variance as one another.\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\nknitr::include_graphics(\"images/diag_sumscore.png\")\n```\n\nFor a full explanation of this idea, see [\"Thinking twice about sum scores\", McNeish & Wolf 2020](https://doi.org/10.3758/s13428-020-01398-0){target=\"_blank\"}. \n\n`r solend()`\n\n<br>\n\n\n\n# \"DOOM\" Scrolling \n\n\n:::frame\n__Dataset: doom.csv__  \n\nThe \"Domains of Online Obsession Measure\" (DOOM) is a fictitious scale that aims to assess the sub types of addictions to online content. It was developed to measure 2 separate domains of online obsession: items 1 to 9 are representative of the \"emotional\" relationships people have with their internet usage (i.e. how it makes them feel), and items 10 to 15 reflect \"practical\" relationship (i.e., how it connects or interferes with their day-to-day life). Each item is measured on a 7-point likert scale from \"strongly disagree\" to \"strongly agree\".  \n\nWe administered this scale to 476 participants in order to assess the validity of the 2 domain structure of the online obsession measure that we obtained during scale development.  \n\nThe data are available at [https://uoepsy.github.io/data/doom.csv](https://uoepsy.github.io/data/doom.csv){target=\"_blank\"}, and the table below shows the individual item wordings.  \n\n```{r}\n#| echo: false\ndoom <- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\ntibble(\n  variable = names(doom),\n  question = c(\"i just can't stop watching videos of animals\",\n\"i spend hours scrolling through tutorials but never actually attempt any projects.\",\n\"cats are my main source of entertainment.\",\n\"life without the internet would be boring, empty, and joyless\",\n\"i try to hide how long i’ve been online\",\n\"i avoid thinking about things by scrolling on the internet\",\n\"everything i see online is either sad or terrifying\",\n\"all the negative stuff online makes me feel better about my own life\",\n\"i feel better the more 'likes' i receive\",\n\"most of my time online is spent communicating with others\",\n\"my work suffers because of the amount of time i spend online\",\n\"i spend a lot of time online for work\",\n\"i check my emails very regularly\",\n\"others in my life complain about the amount of time i spend online\",\n\"i neglect household chores to spend more time online\")\n) |> gt::gt()\n```\n\n:::\n\n`r qbegin(qcounter())`\nAssess whether the 2 domain model of online obsession provides a good fit to the validation sample of 476 participants.   \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nFrom the visual of the correlation matrix, you can see the vague outline of two groups of items correlations. Note there's a little overlap.. \n```{r}\ndoom <- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\nheatmap(cor(doom))\n```\n\nfirst we write our model: \n```{r}\nmoddoom <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n```\nThen we fit it to the data:  \n```{r}\nmoddoom.est <- cfa(moddoom, data = doom)\n```\nThen we inspect that fitted model object.   \nI'm just going to extract the fit indices here first.  \n```{r}\nfitmeasures(moddoom.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\nUh-oh.. they don't look great. \n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nAre there any areas of local misfit (certain parameters that are not in the model (and are therefore fixed to zero) but that could improve model fit _if they were_ estimated?).  \n\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nI'm printing out just the `head()`, so that I can look at the few parameters with the greatest modification indices.  \nThe top three parameters jump out immediately to me.  \n`item_1` and `item_3` have a suggested correlation of c0.5, as do `item_7` and `item_8`. In addition, it's suggested that including a loading (estimated to be about 0.6) from `item_10` to the `emot` factor would improve the model fit.\n```{r}\nmodindices(moddoom.est, sort=TRUE) |>\n  head()\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n__Beware:__ there's a slightly blurred line here that we're about to step over, and move from confirmatory back to 'exploratory'.  \n\nLook carefully at the item wordings,do any of the suggested modifications make theoretical sense? Add them to the model and re-fit it. Does this new model fit well?  \n\n\n\nIn this case, the likely reason for the poor fit of the \"DOOM\" scale, is that the person who made the items (ahem, me) doesn't really know anything about the construct they are talking about, and didn't put much care into constructing the items!  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nThere are three main proposed adjustments from our initial model:  \n\n1. `item_1 ~~ item_3`. These questions are both about animals. It would make sense that these are related over and above the underlying \"emotional internet usage\" factor. \n2. `item_7 ~~ item_8`. These are both about viewing negative content online, so it makes sense here that they would be related beyond the 'emotional' factor. \n\n3. `emot =~ item_10`. This item is about communicating with others. It currently loads highly on the `pract` factor too. It maybe makes sense here that \"communicating with others\" will capture both a practical element of internet useage _and_ an emotional one. \n\n\nPutting them all in at once could be a mistake - if we added in `emot =~ item_10`, then we change slightly the underlying construct of the `emot` factor, meaning it might make other suggested modifications (`item_7 ~~ item_8`) less important. It's a bit like [Whac-A-Mole](https://en.wikipedia.org/wiki/Whac-A-Mole){target=\"_blank\"} - you make one modification and then a whole new area of misfits appears!  \n\nLet's adjust our model:  \n```{r}\nmoddoom2 <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n```\nThen fit it to the data:  \n```{r}\nmoddoom2.est <- cfa(moddoom2, data = doom)\n\nfitmeasures(moddoom2.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\n\nThe fit is still not great, and the same suggested correlations are present in modification indices:  \n```{r}\nmodindices(moddoom2.est, sort=TRUE) |>\n  head()\n```\n\nLet's go ahead and put the covariance between `item_1` and `item_3` in. I personally went for this first because they seem more similar to me than `item_7` and `item_8` do. \n\n```{r}\nmoddoom3 <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\nitem_1 ~~ item_3\n\"\n\nmoddoom3.est <- cfa(moddoom3, data = doom)\n\nfitmeasures(moddoom3.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\n\nWhoop! It fits well! It may well be that if we inspect modification indices again, we still see that `item_7 ~~ item_8` would improve our model fit. The thing to remember however, is that we could simply keep adding parameters until we run out of degrees of freedom, and our model would \"fit better\". But such a model would not be useful. It would not generalise well, because it runs the risk of being overfitted to the nuances of this specific sample.\n\n`r solend()`\n\n`r qbegin(qcounter())`\n\nmcq style\n\nwhich do you think reflects where we're at now with our analysis\n\n- we have confirmed our theoretical measurement model of DOOM scrolling\n- we have confirmed our theoretical measurement model, but with some caveats\n- the measure of doom scrolling is crap\n- \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n`r solend()`\n\n\n\n# More Conduct Problems \n\n:::frame\n__Data: conduct_problems_2.csv__  \n\nLast week we conducted an exploratory factor analysis of a dataset to try and identify an optimal factor structure for a new measure of conduct (i.e., antisocial behavioural) problems. \n\nThis week, we'll conduct some confirmatory factor analyses (CFA) of the same inventory to assess the extent to which this 2-factor structure fits an independent sample. To do this, we have administered our measure to a new sample of n=600 adolescents. \n\nWe have re-ordered the questionnaire items to be grouped into the two types of behaviours:\n\n::::{.columns}\n:::{.column width=\"47.5%\"}\n__Non-Aggressive Behaviours__  \n```{r}\n#| echo: false\ntibble(\n  type = rep(c(\"non-aggressive\",\"aggressive\"),e=5),\n  item = paste0(\"item \",1:10),\n  behaviour = c(\"Stealing\",\"Lying\",\"Skipping school\",\"Vandalism\",\"Breaking curfew\",\"Threatening others\",\"Bullying\",\"Spreading malicious rumours\",\"Using a weapon \",\"Fighting\")\n) |> filter(grepl(\"non\",type)) |> select(-type) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(full_width = TRUE)\n```\n:::\n:::{.column width=\"5%\"}\n:::\n:::{.column width=\"47.5%\"}\n__Aggressive Behaviours__  \n```{r}\n#| echo: false\ntibble(\n  type = rep(c(\"non-aggressive\",\"aggressive\"),e=5),\n  item = paste0(\"item \",1:10),\n  behaviour = c(\"Stealing\",\"Lying\",\"Skipping school\",\"Vandalism\",\"Breaking curfew\",\"Threatening others\",\"Bullying\",\"Spreading malicious rumours\",\"Using a weapon \",\"Fighting\")\n) |> filter(!grepl(\"non\",type)) |> select(-type) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(full_width = TRUE)\n```\n:::\n::::\n\nThe data are available as a **.csv** at [https://uoepsy.github.io/data/conduct_problems_2.csv](https://uoepsy.github.io/data/conduct_problems_2.csv) \n\n:::\n\n\n`r qbegin(qcounter())`\n\n- Read in the data, and take a quick look around (e.g., cor matrix, quick pairs.panels plots etc). \n- Fit the proposed 2 factor model\n- Examine the fit of the 2-factor model of conduct problems to this new sample of 600 adolescents.  \n- Evaluate the fit, and make any model modifications if necessary (and only if you feel that there is substantive support for the modification given the items).  \n- Make a diagram of your model, using the standardised factor loadings as labels.  \n- Make a bullet point list of everything you have done so far, and the resulting conclusions. Then, if you feel like it, turn the bulleted list into written paragraphs, and you'll have a write-up of your analyses!  \n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nHere's the data:  \n```{r}\nlibrary(tidyverse)\ncp2 <- read_csv(\"https://uoepsy.github.io/data/conduct_problems_2.csv\")\n\ncor(cp2)\n```\n\nJust from the visual, it looks like the same factor structure is present in this sample.  \n```{r}\nheatmap(cor(cp2), scale = \"none\")\n```\n\nThis is our proposed model:  \n```{r}\nlibrary(lavaan)\ncpmod <- \"\n  # the non-aggressive problems factor\n  nonagg =~ item1 + item2 + item3 + item4 + item5\n\n  # the aggressive problems factor\n  agg =~ item6 + item7 + item8 + item9 + item10\n\n  # covariance between the two factors\n  # (this is included by default in cfa)\n  agg ~~ nonagg\n\"\n\ncpmod.est <- cfa(cpmod, data = cp2)\n```\n\nAnd it appears to fit pretty well! \n```{r}\nfitmeasures(cpmod.est)[c(\"srmr\",\"rmsea\",\"cfi\",\"tli\")]\n```\n\nWe can check modification indices anyway, but I don't plan on making any adjustments given that it already fits well:  \n```{r}\nmodindices(cpmod.est, sort = TRUE) |> head()\n```\n\nIt maybe makes sense that there is some residual covariance between item6 (\"threatening others\") and item10 (\"fighting\"), but it's only a weak correlation (0.14). Not worth adding.  \n\nSo let's get on with making a diagram.\nWe can rotate this however you like. Convention is typically to have it downwards but I like it left to right (not sure why!)\n```{r}\nlibrary(semPlot)\nsemPaths(cpmod.est, \n        whatLabels = \"std\", \n        rotation = 2)\n```\n\nThe lines from `agg =~ item6` and `nonagg =~ item1` are dotted to indicate that the model was initially fitted with the loading fixed to 1. \n\nBecause we're showing standardised loadings, we could just use the model when fitted with `std.lv=TRUE` just to stop these dotted lines from appearing: \n```{r}\ncpmod.est2 <- cfa(cpmod, data = cp2, std.lv = TRUE)\n\nsemPaths(cpmod.est2, \n        whatLabels = \"std\", \n        rotation = 2)\n```\n\nAnd let's give a brief write-up:  \n\n\n*A two-factor model was tested. Items 1-5 loaded on a 'non-aggressive conduct problems' factor and items 6-10 loaded on an 'aggression' factor and these factors were allowed to correlate. Scaling and identification were achieved by fixing the loading of item 1 on the non-aggressive conduct problems factor and item 6 on the aggression factor to 1. The model was estimated using maximum likelihood estimation. The model fit well with CFI=.99, TLI=0.99,  RMSEA=.04, and SRMR=.04 (Hu & Bentler, 1999).  All loadings were statistically significant and >|.3| on the standardised scale. Overall, therefore, a two-factor oblique model was supported for the conduct problems items. The correlation between the factors was $r=.38\\,\\, (p<.001)$.*\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\ninvisible(summary(cpmod.est, std=TRUE)$pe) |>\n  as_tibble() |>\n  mutate(parameter = paste0(lhs, op, rhs)) |>\n  mutate_if(is.numeric, ~round(.,3)) |>\n  dplyr::transmute(\n    parameter,est,\n    std.est = std.all, se, \n    z = ifelse(is.na(z),\"\",z),\n    pvalue = ifelse(is.na(pvalue),\"\",format.pval(pvalue,eps=.001))) |>\n  knitr::kable()\n```\n\n<!-- Modification indices and expected parameter changes were inspected but no modifications were made because no expected parameter changes were judged large enough to merit the inclusion of additional parameters given that there was little theoretical rationale for their inclusion. -->\n\n\n`r solend()`\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\nqcounter <- function(){\n  if(!exists(\"qcounter_i\")){\n    qcounter_i <<- 1\n  }else{\n    qcounter_i <<- qcounter_i + 1\n  }\n  qcounter_i\n}\nlibrary(psych)\nlibrary(semPlot)\n```\n\n:::frame\n__New packages__  \n\nMake sure you have these packages installed:  \n\n+ lavaan\n+ semPlot\n\n:::\n\n\n# Exercises for the Enthusiastic\n\n:::frame\n__Dataset: radakovic_das.csv__  \n\nApathy is lack of motivation towards goal-directed behaviours. It is pervasive in a majority of psychiatric and neurological diseases, and impacts everyday life. Traditionally, apathy has been measured as a one-dimensional construct but is in fact composed of different types of demotivation.\n\n**The Dimensional Apathy Scale (DAS)** is a multidimensional assessment for demotivation, in which 3 subtypes of apathy are assessed:  \n\n- **Executive:** lack of motivation for planning, attention or organisation\n- **Emotional:** lack of emotional motivation (indifference, affective or emotional neutrality, flatness or blunting)\n- **Initiation:** lack of motivation for self-generation of thoughts and/or actions\n\nThe DAS measures these subtypes of apathy and allows for quick and easy assessment, through self-assessment, observations by informants/carers or administration by researchers or healthcare professionals.  \n\nYou can find data for the DAS when administered to 250 healthy adults at [https://uoepsy.github.io/data/radakovic_das.csv](https://uoepsy.github.io/data/radakovic_das.csv){target=\"_blank\"}, and information on the items is below.  \n\n::: {.callout-note collapse=\"true\"}\n#### DAS Dictionary\n\nAll items are measured on a 6-point Likert scale of Always (0), Almost Always (1), Often (2), Occasionally (3), Hardly Ever (4), and Never (5). Certain items (indicated in the table below with a `-` direction) are reverse scored to ensure that higher scores indicate greater levels of apathy. \n\n```{r}\n#| echo: false\nqnames = c(\"I need a bit of encouragement to get things started\",\"I contact my friends\",\"I express my emotions\",\"I think of new things to do during the day\",\"I am concerned about how my family feel\",\"I find myself staring in to space\",\"Before I do something I think about how others would feel about it\",\"I plan my days activities in advance\",\"When I receive bad news I feel bad about it\",\"I am unable to focus on a task until it is finished\",\"I lack motivation\",\"I struggle to empathise with other people\",\"I set goals for myself\",\"I try new things\",\"I am unconcerned about how others feel about my behaviour\",\"I act on things I have thought about during the day\",\"When doing a demanding task, I have difficulty working out what I have to do\",\"I keep myself busy\",\"I get easily confused when doing several things at once\",\"I become emotional easily when watching something happy or sad on TV\",\"I find it difficult to keep my mind on things\",\"I am spontaneous\",\"I am easily distracted\",\"I feel indifferent to what is going on around me\")\nrevitems = c(10,3,5,7,9,20,2,4,8,13,14,16,18,22)\nExitems = c(1,6,10,11,17,19,21,23)\nEmitems = c(3,5,7,9,12,15,20,24)\nBCIitems = c(2,4,8,13,14,16,18,22)\n\ntibble(\n  item = 1:24,\n  direction = case_when(\n    item %in% revitems ~ \"-\",\n    TRUE ~ \"+\"\n  ),\n  dimension = case_when(\n    item %in% Exitems ~ \"Executive\",\n    item %in% Emitems ~ \"Emotional\",\n    item %in% BCIitems ~ \"Initiation\"\n  ),\n  question = qnames\n) |> gt::gt()\n```\n\nHere are the item numbers that correspond to each dimension.\n\n- Executive: 1, 6, 10, 11, 17, 19, 21, 23\n- Emotional: 3, 5, 7, 9, 12, 15, 20, 24\n- Initiation: 2, 4, 8, 13, 14, 16, 18, 22\n\n:::\n\n:::\n\n`r qbegin(qcounter())`\nRead in the data. \nIt will need a little bit of tidying before we can get to fitting a CFA.  \n\nRemember that most of the actions needed for working with those sort of data are described in the [Chapter on Data Wrangling for Questionnaires](https://uoepsy.github.io/lv/00_datawrangle.html){target=\"_blank\"}.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nBy the looks of things, this is what I would consider doing:\n\n- Rename the variables to easy-to-read strings like `q1`, `q2`, `q3`, etc.\n- Set up a data dictionary that records the text of the item `q1` corresponds to, the text that `q2` corresponds to, etc.\n- Recode the Likert scale labels to numbers.\n- Reverse-code the questions with a negative direction. Note, you don't **need** to this, as they'll just end up with loadings in the opposite direction, but I would strongly recommend it for interpretation purposes.  \n- Check if there is missing data and if there is, removing those observations.\n\n\n\n:::\n\n\n`r qend()`\n`r solbegin(show=TRUE, label=\"1 - Read and check\", slabel=FALSE, toggle=params$TOGGLE)`\n\nFirst let's just read in the dataset:\n\n```{r}\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nrdas <- read_csv(\"https://uoepsy.github.io/data/radakovic_das.csv\")\nhead(rdas)\n```\n\nThe names we're getting are useful in that they show the items, but they're horrible to have to use in R, so we will ideally replace them with easy to use names. \nNote also that the data is being read in as the actual response option - e.g., \"Almost Always\" - and we want to treat these as a numeric scale. So those will have to change too.  \n\n`r solend()`\n`r solbegin(show=TRUE, label=\"2 - Renaming variables\", slabel=FALSE, toggle=params$TOGGLE)`\n\nI like to make a \"data dictionary\" whenever I get data like this. While I want to rename the variables to make it easier for me to use, I also want to keep track of what the questions were.  \nHere I make a \"tibble\" (the function `data.frame()` would work too, tibble is just tidyverse version). I indicate what I am going to rename things as (\"q1\",\"q2\", ..., \"q24\"), and then I have the current names of the variables\n```{r}\nrdas_dict <- tibble(\n  variable = paste0(\"q\",1:24),\n  item = names(rdas)\n)\n```\n\nDoing this is really useful because I can't keep track in my head of what \"q5\" was.  \nIf I want to know, then I can just do: \n```{r}\nrdas_dict[5,]\n```\n\nNow let's actually change the names in our data to what we said we would:  \n```{r}\nnames(rdas) <- paste0(\"q\", 1:24)\n```\n\n`r solend()`\n`r solbegin(show=TRUE, label=\"3 - Recoding responses\", slabel=FALSE, toggle=params$TOGGLE)`\n\nOkay, so we have all our data in words, not numbers. Views on how to treat Likert data are mixed, but it's very common to treat it as continuous in Psychology.  \n\nLet's check the response values we have. Just in question 1 for now:  \n```{r}\nunique(rdas$q1)\n```\n\nA little trick that we can use to find the unique values in an entire dataset is to quickly convert the dataframe into one big long vector. Technically, a dataframe is a \"list of vectors\", and the function `unlist()` will remove this structure.  \nSo we can find all the unique values in all the questions with:  \n```{r}\nunique(unlist(rdas))\n```\n\nPerfect. So we know we have uniformity of spelling. It happens less often these days as questionnaire software is improving, but you might occasionally encounter typos in _some_ of the questions, or things with and without capital letters (R is a bit thick, and doesn't recognise that \"Often\" and \"often\" are the same thing).  \nNote that we have the 6 responses that we would expect given the description of the scale, but we also have some `NA` values, and some `[NO ENTRY]` values. Not sure how those got there.  \nWe want to turn each \"Always\" in to 0, each \"Almost Always\" in to 1, \"Often\" in to 2, and so on. If we simply leave out the \"[NO ENTRY]\", then this will be turned into a missing value `NA`, which is handy.  \n\n```{r}\nrdas <- rdas |> \n  mutate(across(q1:q24, ~case_match(.,\n    \"Always\" ~ 0,\n    \"Almost Always\" ~ 1,\n    \"Often\" ~ 2,\n    \"Occasionally\" ~ 3,\n    \"Hardly Ever\" ~ 4,\n    \"Never\" ~ 5\n  )))\nhead(rdas)\n```\n\n`r solend()`\n`r solbegin(show=TRUE, label=\"4 - Reverse Coding\", slabel=FALSE, toggle=params$TOGGLE)`\nAccording to the table of items, the ones which need to be reverse scored are:  \n```{r}\nreversed <- c(2,3,4,5,7,8,9,10,13,14,16,18,20,22)\n```\n\nFor these items, we want 5s to become 0s, 4s become 1s, and so on. \n\nThe tidyverse solution shown in the readings and solutions to previous labs will work just fine, but if you're curious, here's a different way to accomplish the same thing using functions from base R:\n\n```{r}\nrdas[, reversed] <- apply(\n  rdas[, reversed], MARGIN = 2, function(x) 5-x)\n```\n\n__Note:__ The above code works nicely because our dataset is currently ordered such that the first column is item 1, 2nd column is item 2, and so on. This means we can use _numbers_ to index the appropriate variables, rather than _names_. It would need adjusting if, for instance, our first column contained \"participant ID\", and our items only began later.  \n\n`r solend()`\n`r solbegin(show=TRUE, label=\"5 - Removing missingness\", slabel=FALSE, toggle=params$TOGGLE)`\n\nWe haven't learned about more sophisticated methods of handling missing data, so for now we will just remove any rows in which there is missingness - i.e., we'll do \"listwise deletion\":  \n\n```{r}\ncompl_rdas <- na.omit(rdas)\n```\n\n`r solend()`\n\n`r qbegin(qcounter())`\nSpecify the theoretical model proposed by Radakovic et al.   \n\nFor reference, check out [the example in the readings](https://uoepsy.github.io/lv/04_cfa.html#cfa-in-r---the-lavaan-package).\n\n```{r}\n#| eval: false\ndasmod <- \"\n\n\n\n\n\n\"\n```\n\nChallenge: Before you estimate the model, how many degrees of freedom do you think the model will have?\n([The readings](https://uoepsy.github.io/lv/04_cfa.html#degrees-of-freedom) will help here!)\n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nYou'll have to use the data dictionary to see which items are associated with which dimensions. \n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, label=\"1 - Specifying the model\", slabel=FALSE, toggle=params$TOGGLE)`\nHere is the model structure, according to the list of items in the dictionary.  \nI'm calling my factors \"Em\", \"Ex\", and \"BCI\" for \"emotional\", \"executive\" and \"behavioural/cognitive initiation\" respectively. The factor correlations will be estimated by default, but I like to write things explicitly. \n```{r}\ndasmod = \"\nEx =~ q1 + q6 + q10 + q11 + q17 + q19 + q21 + q23\nEm =~ q3 + q5 + q7 + q9 + q12 + q15 + q20 + q24\nBCI =~ q2 + q4 + q8 + q13 + q14 + q16 + q18 + q22\nEm ~~ Ex\nEm ~~ BCI\nEx ~~ BCI\n\"\n```\n\n\n`r solend()`\n`r solbegin(show=params$SHOW, label=\"2 - Computing degrees of freedom\", slabel=FALSE, toggle=params$TOGGLE)`\n\nDegrees of freedom is computed as the number of \"knowns\" minus the number of \"unknowns\".\n\nLet's start with figuring out the number of \"knowns\": the number of values in the dataset.\nThis number comes from the observed covariance matrix.\nLet's imagine a smaller dataset with only five items.\nIt'll create a covariance matrix like this:\n\n```\nvar\ncovar   var\ncovar   covar   var\ncovar   covar   covar   var\ncovar   covar   covar   covar   var\n```\n\n\nHow many values are in this matrix?\nIn the first row, there's 1, plus the second row with 2, plus the third row with 3, plus the fourth row with 4, plus the fifth row with 5.\nIn other words, there are\n\n```{r}\nsum(1:5)\n```\n\nvalues in this covariance matrix.  \n\nFor the present scenario with 24 items, we will have\n\n```{r}\nsum(1:24)\n```\n\nvalues in the covariance matrix.\n(Twenty-four of these will be each item's own variance, and the other 276 will be covariances between items.)  \n\nThe alternative formula to calculate this is $\\frac{k \\cdot (k+1)}{2}$, and we get the same number by plugging in the number of variables for $k$: $\\frac{24 \\cdot (24+1)}{2} = \\frac{600}{2} = 300$\n\nNow let's look at the number of \"unknowns\": the number of parameters the model has to estimate.\nThis number comes from the number of latent variables and how they relate to each item.\n\n- Each latent variable has its own variance, and there are three latent variables, so the model will have three latent factor variances.\n- Each item will load onto one latent variable, and there are 24 items, so the model will have 24 factor loadings.\n- Each item will have residual factor variances, and there are 24 items, so the model will have 24 residual factor variances.\n\nAdding these up, we get\n\n```{r}\n3 + 24 + 24\n```\n\nunknown parameters.\n\nFinally, let's subtract the knowns from the unknowns to get the degrees of freedom:\n\n```{r}\n300 - 51\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nEstimate the model using `cfa()`.  \nYou can choose whether you want to standardise the latent factors or fix the first loading of each factor to be 1 (it's the same model, just scaled differently).  \n\nExamine the model fit - does it fit well?\n\nWhat modifications do the modification indices suggest? Are the top three suggestions theoretically reasonable, in your opinion?  \n\nRemember, we don't really _want_ to have to make modifications to our models. If you don't need to (if the model fits well) then don't bother! (It's still worth _looking_ at the modification indices though).  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nThere's a whole section on \"model fit\" in the [CFA chapter](https://uoepsy.github.io/lv/04_cfa.html#model-fit){target=\"_blank\"}!  \n\nAnd there's also a whole section on [model modifications](https://uoepsy.github.io/lv/04_cfa.html#model-modifications).\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nLet's fit the model!  \n```{r}\ndasmod.est = cfa(dasmod, compl_rdas)\n```\n\n\nThe standard test of model fit is a chi-squared test which compares the observed covariance matrix to the model-implied covariance matrix.\nIdeally, these two matrices will be fairly similar, so we want a non-significant result.\n\nWe can get the test statistic and p-value from the model's chi-squared test as follows:\n\n```{r}\nsummary(dasmod.est)$test$standard\n```\n\nSo a chi-squared test with 249 degrees of freedom results in a test statistic of 274.8, associated with a p-value of 0.125.\nThe take-away is that our observed covariance matrix is not significantly different from the model-implied covariance matrix—yay!\n\nNext, let's check the additional measures of global fit: \n\n```{r}\nfitmeasures(dasmod.est)[c(\"srmr\",\"rmsea\",\"tli\",\"cfi\")]\n```\n\nAll looks pretty good! Cut-offs for SRMR tend to vary, with some using <0.08, or <0.09, and some being stricter with <0.05. Remember, these criteria are somewhat arbitrary.  \n\nModification indices suggest a whole bunch of items that could have some associations beyond that modelled in the factors, but these are all weak correlations at around 0.2.  \n\n```{r}\nmodindices(dasmod.est, sort=TRUE) |> head()\n```\n\nThese are the top 3 being suggested. I can't see any obvious link between any of these that would make me think they are related beyond their measuring of 'apathy'.  \n```{r}\nrdas_dict[c(3,6),]\nrdas_dict[c(1,9),]\nrdas_dict[c(19,20),]\n```\n\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\nAre the (*standardised*) loadings all \"big enough\"?  \nThere's no clear threshold that people use here - it depends a lot on the field, and on the wordings of specific items. Ideally, the same value we used in EFA ($\\geq|0.3|$) would be nice, but not crucial.  \n\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nTo get out standardised loadings, we can do:  \n```{r}\n#| eval: false\nmod.est <- cfa(model_syntax, data = ...)\nsummary(mod.est, std=TRUE)\n```\n\nAnd you'll get out an extra 2 columns in the summary output.  \n\nPay attention to the `Estimate`, `Std.lv` and `Std.all` columns in your output. The way I think of these columns is just to think of how we scale things in regression models:  \n\n- `Estimate` column : `item ~ Factor`  \n- `Std.lv` column : `item ~ scale(Factor)`  \n- `Std.all` column: `scale(item) ~ scale(Factor)`  \n\nSo if, when we fitted the model, we had specified `cfa(model, data, std.lv = TRUE)`, then the factor _already has a variance of 1_, so `Scale(Factor)` doesn't do anything.  \n\nSee [Chapter 4#interpretation](https://uoepsy.github.io/lv/04_cfa.html#interpretation){target=\"_blank\"}.  \n\n:::\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nI'm not going to print all of this right now because there's so much output, but here's how we would find standardised loadings. We can find them in the `Std.all` column.  \n```{r}\n#| eval: false\nsummary(dasmod.est, std = TRUE)\n```\n```\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Ex =~                                                                 \n    q1                1.000                               0.679    0.694\n    q6                0.679    0.121    5.607    0.000    0.461    0.433\n    ...\n    ...\n```\n\nThe standardised loadings are all (*just*) greater than $|0.3|$. Questions 13 and 15 are very close... \n\n```{r}\nrdas_dict[c(13,15),]\n```\n\n\n`r solend()`\n\n`r qbegin(qcounter())`\nDo the factors correlate in the way you would expect?  \n\nIs more emotional apathy associated with more executive apathy? and with more initiation apathy?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nIf you *didn't* reverse code the appropriate items, then this might get confusing, because we'd have to look at factor loadings to know in which direction the factor is going (i.e., are higher numbers \"more apathy\" or \"less apathy\"?).  \n\nIf you *did* reverse code the appropriate items, then you're golden, because you made them all point towards \"more\" apathy.   \n\n:::\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\nHere are the correlations we're interested in. Note that what we are seeing is that the three factors are all positively correlated, but for `Em` and `Ex` this is only weak (and not significant).   \n\nThis isn't necessary a problem, it just means that these two factors are fairly distinct/orthogonal. We might want to check back in the original paper to see what they proposed! \n```{r}\n#| eval: false\nsummary(dasmod.est, std = TRUE)\n```\n```\n...\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Ex ~~                                                                 \n    Em                0.051    0.028    1.807    0.071    0.159    0.159\n  Em ~~                                                                 \n    BCI               0.043    0.018    2.369    0.018    0.263    0.263\n  Ex ~~                                                                 \n    BCI               0.151    0.040    3.746    0.000    0.642    0.642\n```\n\n\n<!-- If we look at the loadings for the Executive apathy factor, they are all positive other than `q10`. According to our data dictionary, this `q10` is negatively worded, and the others are all positively worded. So being higher on the `Ex` factor means more Executive Apathy, which is what we want.   -->\n\n<!-- By contrast, both `Em` and `BCI` have positive loadings for the questions that are negatively worded according to the data dictionary. Note that **all** of the initiation apathy questions in the data dictionary are in the other direction. So what we are getting is that being higher the `BCI` factor means having *less* Initiation Apathy. So it's back to front. The same applies to the `Em` factor.   -->\n\n<!-- So actually, all forms of apathy are *positively* correlated (because more of each type is associated with more of the other types).   -->\n`r solend()`\n\n\n`r qbegin(qcounter())`\nMake a diagram of the model.  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nFor a quick look at the structure of the model, try the `semPaths()` function from the **semPlot** package [Chapter 4 CFA#making diagrams](https://uoepsy.github.io/lv/04_cfa.html#making-diagrams){target=\"_blank\"}.\n\nIf you were going to use this sort of diagram in a proper write-up, though, it'd be better to make a nicer graphic manually (e.g., in Powerpoint, your favourite graphics software, or [semdiag](https://semdiag.psychstat.org)).\n\n:::\n\n`r qend()`\n`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`\n\nHere's one example:\n\n```{r}\nlibrary(semPlot)\nsemPaths(dasmod.est, whatLabels = \"std\", rotation=2)\n```\n\nThere are lots of options in `semPaths()`, so if you can make your graphic more elaborate than this one, then be our guest!\n\n`r solend()`\n\n`r qbegin(paste0(\"Optional Question \", qcounter()), qlabel=FALSE)`\n<!-- Much like for EFA, we can estimate individuals' scores on the latent factors. In lavaan, the function `lavPredict()` will get us some estimates.   -->\n\n<!-- However, for a clinician administering the DAS to a patient, this option is not available. Instead, it is common that scores on the individual items associated with a given factor are used to create a sum score or a mean score which can be used in a practical setting (e.g., scores above a given threshold might indicate cause for concern).   -->\n\nImagine that you're a clinician administering the DAS to a patient.\nIn clinical settings, it's common practice to skip the complex factor analysis we've been doing here and just create a sum score or a mean score that describe a patient's responses.\nThen clinicians can check whether the score is above some threshold to see whether there's cause for concern.\n\nFor each of the dimensions of apathy in the data, calculate sum scores for each of the 250 participants.\n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nGood ol' `rowSums()` to the rescue!  \n\n:::\n`r qend()`\n`r solbegin(show=TRUE, toggle=TRUE)`\nHere are the sets of items associated with each dimension: \n```{r}\nExitems <- c(1,6,10,11,17,19,21,23)\nEmitems <- c(3,5,7,9,12,15,20,24)\nBCIitems <- c(2,4,8,13,14,16,18,22)\n```\n\nAgain, because the item numbers correspond to the column positions in our data, we can just do rowSums indexing on those column numbers to get our scores:  \n```{r}\ncompl_rdas$ExSCORE <- rowSums(compl_rdas[,Exitems])\ncompl_rdas$EmSCORE <- rowSums(compl_rdas[,Emitems])\ncompl_rdas$BCIScore <- rowSums(compl_rdas[,BCIitems])\n```\n\n`r solend()`\n\n`r qbegin(paste0(\"Optional Question \", qcounter()), qlabel = FALSE)`\nHow might you think about a sum/mean score in terms of a diagram?  \n\n::: {.callout-tip collapse=\"true\"}\n#### Hints\n\nWhat does a sum or mean score imply about how each item is weighted compared to the others?\nHow is this different from what a more sophisticated method like EFA or CFA can do?\n\n:::\n\n`r qend()`\n`r solbegin(show=TRUE, toggle=TRUE)`\n\nComputing sum scores can feel like a 'model free' calculation, but actually it **does** pre-suppose a factor structure, and a much more constrained one than those we have been estimating.\nSpecifically, we're assuming that all items contribute equally to an underlying factor, rather than being weighted differently, and that all items also have the same variance as one another.\n\n```{r}\n#| echo: false\n#| out-width: \"100%\"\nknitr::include_graphics(\"images/diag_sumscore.png\")\n```\n\nFor a full explanation of this idea, see [\"Thinking twice about sum scores\", McNeish & Wolf 2020](https://doi.org/10.3758/s13428-020-01398-0){target=\"_blank\"}. \n\n`r solend()`\n\n<br>\n\n\n\n# \"DOOM\" Scrolling \n\n\n:::frame\n__Dataset: doom.csv__  \n\nThe \"Domains of Online Obsession Measure\" (DOOM) is a fictitious scale that aims to assess the sub types of addictions to online content. It was developed to measure 2 separate domains of online obsession: items 1 to 9 are representative of the \"emotional\" relationships people have with their internet usage (i.e. how it makes them feel), and items 10 to 15 reflect \"practical\" relationship (i.e., how it connects or interferes with their day-to-day life). Each item is measured on a 7-point likert scale from \"strongly disagree\" to \"strongly agree\".  \n\nWe administered this scale to 476 participants in order to assess the validity of the 2 domain structure of the online obsession measure that we obtained during scale development.  \n\nThe data are available at [https://uoepsy.github.io/data/doom.csv](https://uoepsy.github.io/data/doom.csv){target=\"_blank\"}, and the table below shows the individual item wordings.  \n\n```{r}\n#| echo: false\ndoom <- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\ntibble(\n  variable = names(doom),\n  question = c(\"i just can't stop watching videos of animals\",\n\"i spend hours scrolling through tutorials but never actually attempt any projects.\",\n\"cats are my main source of entertainment.\",\n\"life without the internet would be boring, empty, and joyless\",\n\"i try to hide how long i’ve been online\",\n\"i avoid thinking about things by scrolling on the internet\",\n\"everything i see online is either sad or terrifying\",\n\"all the negative stuff online makes me feel better about my own life\",\n\"i feel better the more 'likes' i receive\",\n\"most of my time online is spent communicating with others\",\n\"my work suffers because of the amount of time i spend online\",\n\"i spend a lot of time online for work\",\n\"i check my emails very regularly\",\n\"others in my life complain about the amount of time i spend online\",\n\"i neglect household chores to spend more time online\")\n) |> gt::gt()\n```\n\n:::\n\n`r qbegin(qcounter())`\nAssess whether the 2 domain model of online obsession provides a good fit to the validation sample of 476 participants.   \n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nFrom the visual of the correlation matrix, you can see the vague outline of two groups of items correlations. Note there's a little overlap.. \n```{r}\ndoom <- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\nheatmap(cor(doom))\n```\n\nfirst we write our model: \n```{r}\nmoddoom <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n```\nThen we fit it to the data:  \n```{r}\nmoddoom.est <- cfa(moddoom, data = doom)\n```\nThen we inspect that fitted model object.   \nI'm just going to extract the fit indices here first.  \n```{r}\nfitmeasures(moddoom.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\nUh-oh.. they don't look great. \n\n`r solend()`\n\n\n\n`r qbegin(qcounter())`\nAre there any areas of local misfit (certain parameters that are not in the model (and are therefore fixed to zero) but that could improve model fit _if they were_ estimated?).  \n\n\n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nI'm printing out just the `head()`, so that I can look at the few parameters with the greatest modification indices.  \nThe top three parameters jump out immediately to me.  \n`item_1` and `item_3` have a suggested correlation of c0.5, as do `item_7` and `item_8`. In addition, it's suggested that including a loading (estimated to be about 0.6) from `item_10` to the `emot` factor would improve the model fit.\n```{r}\nmodindices(moddoom.est, sort=TRUE) |>\n  head()\n```\n\n`r solend()`\n\n\n`r qbegin(qcounter())`\n__Beware:__ there's a slightly blurred line here that we're about to step over, and move from confirmatory back to 'exploratory'.  \n\nLook carefully at the item wordings,do any of the suggested modifications make theoretical sense? Add them to the model and re-fit it. Does this new model fit well?  \n\n\n\nIn this case, the likely reason for the poor fit of the \"DOOM\" scale, is that the person who made the items (ahem, me) doesn't really know anything about the construct they are talking about, and didn't put much care into constructing the items!  \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nThere are three main proposed adjustments from our initial model:  \n\n1. `item_1 ~~ item_3`. These questions are both about animals. It would make sense that these are related over and above the underlying \"emotional internet usage\" factor. \n2. `item_7 ~~ item_8`. These are both about viewing negative content online, so it makes sense here that they would be related beyond the 'emotional' factor. \n\n3. `emot =~ item_10`. This item is about communicating with others. It currently loads highly on the `pract` factor too. It maybe makes sense here that \"communicating with others\" will capture both a practical element of internet useage _and_ an emotional one. \n\n\nPutting them all in at once could be a mistake - if we added in `emot =~ item_10`, then we change slightly the underlying construct of the `emot` factor, meaning it might make other suggested modifications (`item_7 ~~ item_8`) less important. It's a bit like [Whac-A-Mole](https://en.wikipedia.org/wiki/Whac-A-Mole){target=\"_blank\"} - you make one modification and then a whole new area of misfits appears!  \n\nLet's adjust our model:  \n```{r}\nmoddoom2 <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n```\nThen fit it to the data:  \n```{r}\nmoddoom2.est <- cfa(moddoom2, data = doom)\n\nfitmeasures(moddoom2.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\n\nThe fit is still not great, and the same suggested correlations are present in modification indices:  \n```{r}\nmodindices(moddoom2.est, sort=TRUE) |>\n  head()\n```\n\nLet's go ahead and put the covariance between `item_1` and `item_3` in. I personally went for this first because they seem more similar to me than `item_7` and `item_8` do. \n\n```{r}\nmoddoom3 <- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\nitem_1 ~~ item_3\n\"\n\nmoddoom3.est <- cfa(moddoom3, data = doom)\n\nfitmeasures(moddoom3.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n```\n\nWhoop! It fits well! It may well be that if we inspect modification indices again, we still see that `item_7 ~~ item_8` would improve our model fit. The thing to remember however, is that we could simply keep adding parameters until we run out of degrees of freedom, and our model would \"fit better\". But such a model would not be useful. It would not generalise well, because it runs the risk of being overfitted to the nuances of this specific sample.\n\n`r solend()`\n\n`r qbegin(qcounter())`\n\nmcq style\n\nwhich do you think reflects where we're at now with our analysis\n\n- we have confirmed our theoretical measurement model of DOOM scrolling\n- we have confirmed our theoretical measurement model, but with some caveats\n- the measure of doom scrolling is crap\n- \n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n`r solend()`\n\n\n\n# More Conduct Problems \n\n:::frame\n__Data: conduct_problems_2.csv__  \n\nLast week we conducted an exploratory factor analysis of a dataset to try and identify an optimal factor structure for a new measure of conduct (i.e., antisocial behavioural) problems. \n\nThis week, we'll conduct some confirmatory factor analyses (CFA) of the same inventory to assess the extent to which this 2-factor structure fits an independent sample. To do this, we have administered our measure to a new sample of n=600 adolescents. \n\nWe have re-ordered the questionnaire items to be grouped into the two types of behaviours:\n\n::::{.columns}\n:::{.column width=\"47.5%\"}\n__Non-Aggressive Behaviours__  \n```{r}\n#| echo: false\ntibble(\n  type = rep(c(\"non-aggressive\",\"aggressive\"),e=5),\n  item = paste0(\"item \",1:10),\n  behaviour = c(\"Stealing\",\"Lying\",\"Skipping school\",\"Vandalism\",\"Breaking curfew\",\"Threatening others\",\"Bullying\",\"Spreading malicious rumours\",\"Using a weapon \",\"Fighting\")\n) |> filter(grepl(\"non\",type)) |> select(-type) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(full_width = TRUE)\n```\n:::\n:::{.column width=\"5%\"}\n:::\n:::{.column width=\"47.5%\"}\n__Aggressive Behaviours__  \n```{r}\n#| echo: false\ntibble(\n  type = rep(c(\"non-aggressive\",\"aggressive\"),e=5),\n  item = paste0(\"item \",1:10),\n  behaviour = c(\"Stealing\",\"Lying\",\"Skipping school\",\"Vandalism\",\"Breaking curfew\",\"Threatening others\",\"Bullying\",\"Spreading malicious rumours\",\"Using a weapon \",\"Fighting\")\n) |> filter(!grepl(\"non\",type)) |> select(-type) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(full_width = TRUE)\n```\n:::\n::::\n\nThe data are available as a **.csv** at [https://uoepsy.github.io/data/conduct_problems_2.csv](https://uoepsy.github.io/data/conduct_problems_2.csv) \n\n:::\n\n\n`r qbegin(qcounter())`\n\n- Read in the data, and take a quick look around (e.g., cor matrix, quick pairs.panels plots etc). \n- Fit the proposed 2 factor model\n- Examine the fit of the 2-factor model of conduct problems to this new sample of 600 adolescents.  \n- Evaluate the fit, and make any model modifications if necessary (and only if you feel that there is substantive support for the modification given the items).  \n- Make a diagram of your model, using the standardised factor loadings as labels.  \n- Make a bullet point list of everything you have done so far, and the resulting conclusions. Then, if you feel like it, turn the bulleted list into written paragraphs, and you'll have a write-up of your analyses!  \n\n\n`r qend()`\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nHere's the data:  \n```{r}\nlibrary(tidyverse)\ncp2 <- read_csv(\"https://uoepsy.github.io/data/conduct_problems_2.csv\")\n\ncor(cp2)\n```\n\nJust from the visual, it looks like the same factor structure is present in this sample.  \n```{r}\nheatmap(cor(cp2), scale = \"none\")\n```\n\nThis is our proposed model:  \n```{r}\nlibrary(lavaan)\ncpmod <- \"\n  # the non-aggressive problems factor\n  nonagg =~ item1 + item2 + item3 + item4 + item5\n\n  # the aggressive problems factor\n  agg =~ item6 + item7 + item8 + item9 + item10\n\n  # covariance between the two factors\n  # (this is included by default in cfa)\n  agg ~~ nonagg\n\"\n\ncpmod.est <- cfa(cpmod, data = cp2)\n```\n\nAnd it appears to fit pretty well! \n```{r}\nfitmeasures(cpmod.est)[c(\"srmr\",\"rmsea\",\"cfi\",\"tli\")]\n```\n\nWe can check modification indices anyway, but I don't plan on making any adjustments given that it already fits well:  \n```{r}\nmodindices(cpmod.est, sort = TRUE) |> head()\n```\n\nIt maybe makes sense that there is some residual covariance between item6 (\"threatening others\") and item10 (\"fighting\"), but it's only a weak correlation (0.14). Not worth adding.  \n\nSo let's get on with making a diagram.\nWe can rotate this however you like. Convention is typically to have it downwards but I like it left to right (not sure why!)\n```{r}\nlibrary(semPlot)\nsemPaths(cpmod.est, \n        whatLabels = \"std\", \n        rotation = 2)\n```\n\nThe lines from `agg =~ item6` and `nonagg =~ item1` are dotted to indicate that the model was initially fitted with the loading fixed to 1. \n\nBecause we're showing standardised loadings, we could just use the model when fitted with `std.lv=TRUE` just to stop these dotted lines from appearing: \n```{r}\ncpmod.est2 <- cfa(cpmod, data = cp2, std.lv = TRUE)\n\nsemPaths(cpmod.est2, \n        whatLabels = \"std\", \n        rotation = 2)\n```\n\nAnd let's give a brief write-up:  \n\n\n*A two-factor model was tested. Items 1-5 loaded on a 'non-aggressive conduct problems' factor and items 6-10 loaded on an 'aggression' factor and these factors were allowed to correlate. Scaling and identification were achieved by fixing the loading of item 1 on the non-aggressive conduct problems factor and item 6 on the aggression factor to 1. The model was estimated using maximum likelihood estimation. The model fit well with CFI=.99, TLI=0.99,  RMSEA=.04, and SRMR=.04 (Hu & Bentler, 1999).  All loadings were statistically significant and >|.3| on the standardised scale. Overall, therefore, a two-factor oblique model was supported for the conduct problems items. The correlation between the factors was $r=.38\\,\\, (p<.001)$.*\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\ninvisible(summary(cpmod.est, std=TRUE)$pe) |>\n  as_tibble() |>\n  mutate(parameter = paste0(lhs, op, rhs)) |>\n  mutate_if(is.numeric, ~round(.,3)) |>\n  dplyr::transmute(\n    parameter,est,\n    std.est = std.all, se, \n    z = ifelse(is.na(z),\"\",z),\n    pvalue = ifelse(is.na(pvalue),\"\",format.pval(pvalue,eps=.001))) |>\n  knitr::kable()\n```\n\n<!-- Modification indices and expected parameter changes were inspected but no modifications were made because no expected parameter changes were judged large enough to merit the inclusion of additional parameters given that there was little theoretical rationale for their inclusion. -->\n\n\n`r solend()`\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html",{"text":"<link rel=\"stylesheet\" href=\"https://uoepsy.github.io/assets/css/ccfooter.css\" />\n"}],"number-sections":false,"output-file":"08ex.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"Week 8 Exercises: CFA","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}