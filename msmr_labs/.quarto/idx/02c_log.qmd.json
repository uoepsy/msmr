{"title":"2B: Logistic MLM | Longitudinal MLM","markdown":{"yaml":{"title":"2B: Logistic MLM | Longitudinal MLM","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"Logistic Multilevel Models","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n```\n\n:::lo\nThis reading:  \n\nTwo examples! \n\n1. Logistic multilevel models (`lm()` is to `glm()` as `lmer()` is to `glmer()`)  \n    - *Much of the heavy lifting in understanding the transition from linear >> logistic models is just the same as [USMR Week 10](https://uoepsy.github.io/usmr/2324/labs/10a_glm.html){target=\"_blank\"}, so it might be worth looking back over that for a refresher.*\n2. \"Change over time\" - Fitting multilevel models to longitudinal data.  \n    - *The application of multilevel models to longitudinal data is very much just that - we are taking the same sort of models we learned last week and simply applying them to a different context in which \"time\" is a predictor.* \n\n:::\n\n\n\n\nThe vast majority of the transition across from linear multilevel models to logistic multilevel models is identical to what we talked about in USMR for single level regression models. Remember how we simply used `glm()` and could specify the `family = \"binomial\"` in order to fit a logistic regression? Well it's much the same thing for multi-level models! \n\n+ Gaussian (normal) model: \n  - `lmer(y ~ x1 + x2 + (1 | g), data = data)`  \n+ Binomial model:^[Remember that binary outcomes are just a special case of the binomial]  \n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))`<br>or\n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")`<br>or\n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)`  \n  \n\n    \nThe same logic applies as it did for `glm()` where we are not modeling the outcome $y$ directly, but via a mapping, or \"link function\", which in this case is the logistic function. Beyond the material covered in , there are a few aspects of the logistic multilevel model that are worth commenting on. \n\nFirstly, GLMMs ('generalised linear mixed models') can take more time to run (for bigger datasets we're talking minutes, and sometimes even hours!). \n\nSecondly, our choices of methods of inference (see [2A](02a_inference.html)) are slightly different from what they were for `lmer()`. We continue to have the likelihood based methods as well as parametric bootstrapping (although this will more often result in computational issues with `glmer`). We also have the traditional $z$-statistics that have carried over from `glm()`. We can get confidence intervals that follow this same method by using `confint(model, method=\"Wald\")`, which may often be preferred purely for the fact that they are quick!   \n\nFinally, one small but noteworthy feature of the logistic multilevel model is that our fixed effects coefficients, when translated back to odds ratios, represent \"cluster specific\" effects of a predictor: \n\n- For a linear multilevel model: `lmer(y ~ x + (1 + x | cluster))`, the fixed effect of $x$ gives the change in the $y$ when $x$ is increased by one unit, averaged across clusters \n- For a logistic multilevel model: `glmer(ybin ~ x + (1 + x | cluster), family=binomial)`, the odds ratio for $x$ - `exp(fixef(model))` - gives the multiplicative change in odds of $y$ when $x$ is increased by one unit __for a particular cluster.__  \n\nThis becomes important when deciding if we want to estimate outcomes for individual clusters, or estimate group effects (in which case a mixed model might not be best). \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why are OR from glmer cluster-specific?  \n\n::::panelset\n:::panel\n#### Linear  \n\nconsider a __linear__ multilevel model:  \n`lmer(respiratory_rate ~ treatment + (1|hospital))`\n\nImagine two patients from different hospitals. One has a treatment, one does not.  \n\n  - patient $j$ from hospital $i$ is \"control\"   \n  - patient $j'$ from hospital $i'$ is \"treatment\"   \n\nThe difference in estimated outcome between patient $j$ and patient $j'$ is the \"the effect of having treatment\" plus the distance in random deviations between hospitals $i$ and $i'$  \n\nmodel for patient $j$ from hospital $i$  \n$\\hat{y}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + b_1 (Treatment_{ij} = 0)$\n\nmodel for patient $j'$ from hospital $i'$  \n$\\hat{y}_{i'j'} = (\\gamma_{00} + \\zeta_{0i'}) + b_1 (Treatment_{i'j'} = 1)$\n\ndifference:  \n$\\hat{y}_{i'j'} - \\hat{y}_{ij} = b_1 + (\\zeta_{0i'} - \\zeta_{0i}) = b_1$\n\nBecause $\\zeta \\sim N(0,\\sigma_\\zeta)$, the differences between all different $\\zeta_{0i'} - \\zeta_{0i}$ average out to be 0. \n\n:::\n:::panel\n#### Logistic\n\nNow consider a __logistic__ multilevel model:  \n`glmer(needs_operation ~ treatment + (1|hospital), family=\"binomial\")`\n\nImagine two patients from different hospitals. One has a treatment, one does not.  \n\n  - patient $j$ from hospital $i$ is \"control\"   \n  - patient $j'$ from hospital $i'$ is \"treatment\"  \n  \nThe difference in __probability of outcome__ between patient $j$ and patient $j'$ is the \"the effect of having treatment\" plus the distance in random deviations between hospitals $i$ and $i'$  \n\nmodel for patient $j$ from hospital $i$  \n$log \\left( \\frac{p_{ij}}{1 - p_{ij}} \\right)  = (\\gamma_{00} + \\zeta_{0i}) + b_1 (Treatment_{ij} = 0)$\n\nmodel for patient $j'$ from hospital $i'$  \n$log \\left( \\frac{p_{i'j'}}{1 - p_{i'j'}} \\right) = (\\gamma_{00} + \\zeta_{0i'}) + b_1 (Treatment_{i'j'} = 1)$\n\ndifference (log odds):  \n$log \\left( \\frac{p_{i'j'}}{1 - p_{i'j'}} \\right) - log \\left( \\frac{p_{ij}}{1 - p_{ij}} \\right) = b_1 + (\\zeta_{0i'} - \\zeta_{0i})$\n\ndifference (odds ratio):  \n$\\frac{p_{i'j'}/(1 - p_{i'j'})}{p_{ij}/(1 - p_{ij})} = e^{b_1 + (\\zeta_{0i'} - \\zeta_{0i})} \\neq e^{b_1}$\n\nHence, the interpretation of $e^{b_1}$ is not the odds ratio for the effect of treatment \"averaged over hospitals\", but rather for patients _from the same hospital_. \n\n:::\n::::\n\n:::\n\n## Example\n\n:::frame\n__Data: monkeystatus.csv__  \n\n```{r}\n#| echo: false\nss = 709233#round(runif(1,1e3,1e6))\nset.seed(ss)\nn_groups = 50\n# npgroup = round(runif(30,2,25))\nnpgroup = round(runif(50, 5,11))\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nx = sample(1:2,N,T)\nb = sample(letters[1:3],n_groups,T,prob=c(.3,.4,.3))\nb = b[g]\nres = MASS::mvrnorm(n=n_groups,\n                    mu=c(0,0),Sigma=matrix(c(2,0,0,.6),nrow=2))\nre0 = res[,1]\nre  = re0[g]\nrex = res[,2]\nre_x  = rex[g]\nlp = (-.5 + re) - (.4 + re_x)*scale(x)[,1] + 1.4*(b==\"a\")\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ny = lp + rnorm(N,0,1.3)\ndf = data.frame(x = x, g=factor(g), b=b,y_bin)\nmnames = unique(randomNames::randomNames(n_groups*4,which.names=\"first\"))\nmonkeystatus = df |>\n  transmute(\n    status = ifelse(b==\"a\",\"subordinate\",ifelse(b==\"b\",\"dominant\",\"adolescent\")),#cut(x,3,labels=letters[1:3]),\n    difficulty = factor(x, labels=c(\"easy\",\"difficult\")),\n    monkeyID = mnames[as.numeric(g)],\n    solved = y_bin,\n    score = round(scale(y)[,1]*10+50,1)\n  ) |> filter(!(monkeyID==\"Richard\" & difficulty==\"easy\"),\n              !(monkeyID==\"Nadheera\" & difficulty==\"difficult\"))\n\n\n\n# library(lme4)\n# m = glmer(solved ~ difficulty + status + (1+difficulty|monkeyID), monkeystatus, family=binomial)\n# summary(m)\n# \n# m1=glmer(solved ~ difficulty + (1+difficulty|monkeyID), monkeystatus, family=binomial)\n# anova(m1,m)\n# \n# lmer(probscore ~ 1 + difficulty + status +\n#                (1 + difficulty | monkeyID),\n#       data=monkeystatus) |> summary()\n\n# write_csv(monkeystatus |> select(-score),\n#           file=\"../../data/msmr_monkeystatus.csv\")\n```\n\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a \"dominant\" or \"subordinate\" status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either \"easy\" or \"difficult\", and the researchers recorded whether or not the monkey solved each problem. \n\nWe're interested in how the social status of monkeys is associated with the ability to solve problems.  \n\nThe data is available at [https://uoepsy.github.io/data/msmr_monkeystatus.csv](https://uoepsy.github.io/data/msmr_monkeystatus.csv){target=\"_blank\"}.  \n\n:::\n\n### getting to know my monkeys\n\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in _long_ format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable `monkeyID` which indicates what monkey each problem has been attempted by. We can see the `status` of each monkey, and the `difficulty` of each task, along with whether it was `solved`:   \n\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nmstat <- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n```\n\nWe can do some quick exploring to see how many monkeys we have (`r length(unique(mstat$monkeyID))`), and how many problems each one attempted (min = `r min(table(mstat$monkeyID))`, max = `r max(table(mstat$monkeyID))`:  \n```{r}\nmstat |> \n  count(monkeyID) |> # count the monkeys!  \n  summary()\n```\n\nLet's also see how many monkeys of different statuses we have in our sample:  \n```{r}\nmstat |> \n  group_by(status) |> # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n```\n\nIt's often worth plotting as much as you can to get to a sense of what we're working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.  \n```{r}\n#| out-width: \"100%\"\n#| fig-height: 6\n# which monkeys did what type of problems? \nmstat |> count(status, monkeyID, difficulty) |>\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n```\n\nWhen working with binary outcomes, it's often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like \"subordinate\" monkeys solve more problems, and adolescents solve fewer (makes sense - they're still learning!).  \n```{r}\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n```\n  \n  \n### models of monkeys  \n  \nNow we come to fitting our model.  \n\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It's very likely that difficulty of a problem is going to influence that it is solved, so we'll control for difficulty. \n```\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\n```\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan). \n```\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\n```\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he's likely to really really struggle with the difficult ones!).  \n\n```{r}\nmmod <- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n```\n\n### test and visualisations of monkey status\n\nTo examine if monkey status has an effect, we can compare with the model without `status`:   \n```{r}\n#| code-fold: true\nmmod0 <- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n```\n\n```{r}\n#| echo: false\nres = anova(mmod0,mmod)\n```\n\n\nAnd we can see that the status of monkeys _is_ associated with differences in the probability of successful problem solving ($\\chi^2(2)$ = `r round(res[2,\"Chisq\"],2)`, p `r format.pval(res[2,\"Pr(>Chisq)\"],eps=.05)`).  \n\n\nAnd if we want to visualise the relevant effect, we can (as we did with `glm()`) plot on the predicted probability scale, which is much easier to interpret:\n\n```{r}\n#| code-fold: true\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |>\n  as.data.frame() |>\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n```\n\n### interpretation\n\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:  \n```{r}\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |>\n  exp()\n```\n\n```{r}\n#| echo: false\nbroom.mixed::tidy(mmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term,\n    est = round(estimate,2),\n    OR = round(exp(estimate),2),\n    `OR interpretation` = c(\n      \"estimated odds of an adolescent monkey solving an easy problem\",\n      \"odds of successful problem solving are more than halved (0.42 times the odds) when a given monkey moves from an easy to a difficult problem\",\n      \"odds of success would be almost doubled (1.95 times the odds) if a monkey were to change from adolescent to dominant status (NB this is non-significant)\",\n      \"odds of success would quadruple (4.3 times the odds) if a monkey were to change from adolescent to subordinate status\"\n    )\n  ) |> gt::gt()\n```\n\n::: {.column-margin}\n__Side note__  \n\nContrast this with what we would get from a _linear_ multilevel model. If we were instead modelling a \"problem score\" with `lmer()`, rather than \"solved yes/no\" with `glmer()`, our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.  \n\nNote that estimating differences between groups is not quite the same idea as estimating the effect \"if a particular monkey changed from adolescent to subordinate\". In the linear world, these two things are the same, but our odds ratios give us only the latter. \n\n\n:::\n\n\n\n\n\n\n\n\n```{r}\n#| include: false\n# ss = 236154#round(runif(1,1e3,1e6))\n# set.seed(ss)\n# n_groups = 16\n# # npgroup = round(runif(30,2,25))\n# npgroup = round(runif(16,15,45))\n# g = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\n# N = length(g)\n# x = sample(letters[1:2],N,T,prob=c(.3,.7))\n# b = rnorm(n_groups)\n# b = b[g]\n# res = MASS::mvrnorm(n=n_groups,\n#                     mu=c(0,0),Sigma=matrix(c(2,0,0,.6),nrow=2))\n# re0 = res[,1]\n# re  = re0[g]\n# rex = res[,2]\n# re_x  = rex[g]\n# lp = (-1 + re) + (.9 + re_x)*(x==\"a\") + .4*b\n# y_bin = rbinom(N, size = 1, prob = plogis(lp))\n# y = lp + rnorm(N,0,1.3)\n# df = data.frame(x = x, g=factor(g), b=b,y_bin,y)\n# #set.seed(764)\n# monkeystatus = df |>\n#   transmute(\n#     status = ifelse(x==\"a\",\"sub\",\"alpha\"),\n#     troopsize = round(scale(b)[,1]*13+56),\n#     troop = paste0(\"T\",sample(1e3:1e4,n_groups))[as.numeric(g)],\n#     solved = y_bin,\n#     probscore = round(scale(y)[,1]*10+50,2)\n#   )\n\n# library(lme4)\n# subordinate monkeys (as opposed to alpha monkeys in the same troop) have increased odds of success. \n# adding one more monkey to the troop is associated with increased odds of a monkey succeeding\n# glmer(solved ~ troopsize + status + (1+status|troop), \n#       monkeystatus, family=binomial) |> summary()\n# \n# # subordinate monkeys (as opposed to alpha monkeys) tend to score 6.4 higher on the problem solving scale\n# # every additional 1 in troop size is associated with 0.3 higher scores \n# lmer(probscore ~ troopsize + status + (1+status|troop), \n#       monkeystatus) |> summary()\n\n\n```\n\n\n\n\n# MLMs for Longitudinal Data\n\nMultilevel models are perfectly geared towards dealing with longitudinal data, which can be framed as having essentially the same hierarchical structure as \"children in schools\", or \"observations within participant\".  \n\nThe only difference is that in longitudinal data, the repeated measurements within a participant are ordered according to time. For instance, this could apply to anything from studying people over decades as they age to studying eye movements across the 1 second presentation of a spoken word.  \n\nAs with all applications of multilevel models, we will want our data in long format, so it is usually going to look something like this:  \n\n```{r}\n#| echo: false\ntibble(\n  y = strsplit(\"? ? ... ... ? ? ...\",split=\" \")[[1]],\n  time = strsplit(\"1 2 ... ... 1 2 ...\",split=\" \")[[1]],\n  person = strsplit(\"1 1 ... ... 2 2 ...\",split=\" \")[[1]],\n  `... ` = rep(\"...\",7),\n  `...  ` = rep(\"...\",7),\n) |> as.data.frame() \n```\n\nWe are treating time as continuous here, which has some great benefits - we don't need to have seen everyone at every timepoint, nor do we even need to have everyone with regularly spaced intervals (although knowing the chronology is important for decisions about the shape of trajectory we might want to fit (more on that next week!)).  \n\nGiven the variables above, you might be able to guess the basic form that our models are going to take in order to assess \"change over time\".  \n\nRegressing the outcome variable on to our time variable gives us an estimate of the trajectory - i.e. how much the outcome changes with every 1 additional unit of time. Furthermore, we can fit random slopes of time in order to model how people^[assuming that it is people we are studying!] can vary in the trajectory. \n```{r}\n#| eval: false\nlmer(y ~ 1 + time + (1 + time | person), ...)\n```\n\nAnd we can fit interactions between predictors and time (`y ~ time * x`) to ask questions such as whether different groups have different trajectories of `y`, or whether the association between `x` and `y` changes over time (`x` can also be something that changes over time!). For instance, we can use multilevel models to address question such as does the development of reading ability differ between monolingual and bilingual children? Does the association between physical activity and positive mood change as we age? Do people look at the more unusual of two objects following a speech disfluency?  \n\n\n\n## Example\n\n\n:::frame\n__Data: mindfuldecline.csv__  \n\nA study is interested in examining **whether engaging in mindfulness can prevent cognitive decline in older adults**. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke's Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.   \n\n  \nThe data are available at: [https://uoepsy.github.io/data/msmr_mindfuldecline.csv](https://uoepsy.github.io/data/msmr_mindfuldecline.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n# read_csv(\"https://uoepsy.github.io/data/dapr3_mindfuldecline.csv\") |>\n#   transmute(ppt,condition,study_visit = visit, age, ACE, imp) |>\n#   write_csv(file=\"../../data/msmr_mindfuldecline.csv\")\nmmd <- read_csv(\"../../data/msmr_mindfuldecline.csv\")\ntibble(\n  variable=names(mmd),\n  description=c(\"Participant Identifier\",\n                \"Whether the participant engages in mindfulness or not (control/mindfulness)\",\n                \"Study Visit Number (1 - 10)\",\n                \"Age (in years) at study visit\",\n                \"Addenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\",\n                \"Clinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\")\n) |> gt::gt()\n\n```\n\n:::\n\n### exploring the data\n\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nhead(mmd)\n```\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n```{r}\nmmd |> \n  group_by(condition) |>\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n```\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. \nFrom the plot below, we can see that on the whole participants' cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:  \n```{r}\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n```\n\n### modelling change over time\n\nInitially, we'll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables `study_visit` and `age` represent exactly the same information (time), so we have a choice of which one to use.  \n\n::: {.callout-note collapse=\"true\"}\n#### Why the age variable (currently) causes problems\n\nAs it is, the `age` variable we have starts at 60 and goes up to 78 or so.  \n\nIf we try and use this in a model, we get an error!  \n```{r}\n#| message: true\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n```\n<span style=\"color:red\">Model failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)</span>\n\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is \"when all predictors are zero\". So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!  \n\nThis means that trying to fit `(1 + age | ppt)` will try to estimate the variability in people's change in cognition over time, and the variability in cognition at age zero. As we can see in @fig-nonconv, because the intercept is so far away from the data, the angle of each persons' slope has a _huge_ influence over where their intercept is. The more upwards a persons' slope is, the lower down their intercept is.  \n\n```{r} \n#| label: fig-nonconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age, where age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n  \n```\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):  \n\n```{r}\nVarCorr(mod1)\n```\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the `study_visit` variable.  \nEither will do, we just need to remember the units they are measured in!  \n\n\n:::\n\nLet's center age on 60:\n```{r}\nmmd$ageC <- mmd$age-60\n```\n\nAnd fit our model:\n```{r}\nmod1 <- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n```\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, $t$ statistics that are $>|2\\text{-ish}|$ are probably going to be significant when assessed properly).  \n```{r}\n#| eval: false\nsummary(mod1)\n```\n```\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\n```\n\nWe're now ready to add in group differences in their trajectories of cognition:\n```{r}\nmod2 <- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n```\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is `r round(fixef(mod2)[1])` (that's the `(Intercept)`). For these participants, scores are estimated to decrease by `r round(fixef(mod2)[2],2)` points every year (that's the slope of `ageC`). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the `condition [mindfulness]` coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline `r round(fixef(mod2)[4],2)` less than the control group every year.  \n_(note, there are always lots of ways to frame interactions. A \"reduction in decline\" feels most appropriate to me here)_  \n  \n  \nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.  \n```{r}\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n```\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.  \nAnd that's exactly what we see when we visualise those fixed effects:  \n```{r}\n#| code-fold: true\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |>\n  as.data.frame() |>\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n```\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. \nTo do so, we need to combine two data sources - the fixed effects estimation from `effect()`, and the data itself:  \n```{r}\n#| code-fold: true\nploteff <- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |>\n  as.data.frame()\n\nmmd |>\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n```\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person's cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons' trajectory. When we add in the observed trajectories, we see the variability in people's trajectories (one person even goes up over time!).  \n  \nOur model represents this variability in the random effects part. While the estimated average slope is `r round(fixef(mod2)[2],2)` for the control group (and `r round(fixef(mod2)[2],2)`+`r round(fixef(mod2)[4],2)`=`r round(sum(fixef(mod2)[c(2,4)]),2)` for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of `r round(sqrt(VarCorr(mod2)[[1]][2,2]),2)` (remember we can extract this info using `VarCorr()`, or just look in the output of `summary(model)`).  \n```{r}\nVarCorr(mod2)\n```\n\n::: {.column-margin}\n\n```{r}\n#| echo: false\n#| label: fig-slopesmargin\n#| fig-cap: \"Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\"\nggplot(data=data.frame(x=c(-1,1)), aes(x=x))+\n  stat_function(fun=dnorm, \n                args=list(fixef(mod2)[2],sqrt(VarCorr(mod2)[[1]][2,2])),\n                geom=\"line\", col=\"#a64bb0\",lwd=1)+\n  stat_function(fun=dnorm, \n                args=list(sum(fixef(mod2)[c(2,4)]),sqrt(VarCorr(mod2)[[1]][2,2])),\n                geom=\"line\", col=\"#82b69b\",lwd=1) + \n  scale_x_continuous(limits=c(-.7,.3),breaks=seq(-.7,.3,.2)) +\n  scale_y_continuous(NULL, breaks=NULL)+\n  labs(x=\"slopes\")\n```\n\n:::\n\nIf you think about what this means - it means that some participants we _would_ expect to actually increase in their slopes. If we have a normal distribution with a mean of `r round(fixef(mod2)[2],1)` or `r round(sum(fixef(mod2)[c(2,4)]),2)` and a standard distribution of `r round(sqrt(VarCorr(mod2)[[1]][2,2]),2)`, then we would expect some values to to positive (see e.g., @fig-slopesmargin).  \n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n#| include: false\nsource('assets/setup.R')\nlibrary(xaringanExtra)\nlibrary(tidyverse)\nlibrary(patchwork)\nxaringanExtra::use_panelset()\n```\n\n:::lo\nThis reading:  \n\nTwo examples! \n\n1. Logistic multilevel models (`lm()` is to `glm()` as `lmer()` is to `glmer()`)  \n    - *Much of the heavy lifting in understanding the transition from linear >> logistic models is just the same as [USMR Week 10](https://uoepsy.github.io/usmr/2324/labs/10a_glm.html){target=\"_blank\"}, so it might be worth looking back over that for a refresher.*\n2. \"Change over time\" - Fitting multilevel models to longitudinal data.  \n    - *The application of multilevel models to longitudinal data is very much just that - we are taking the same sort of models we learned last week and simply applying them to a different context in which \"time\" is a predictor.* \n\n:::\n\n\n\n# Logistic Multilevel Models\n\nThe vast majority of the transition across from linear multilevel models to logistic multilevel models is identical to what we talked about in USMR for single level regression models. Remember how we simply used `glm()` and could specify the `family = \"binomial\"` in order to fit a logistic regression? Well it's much the same thing for multi-level models! \n\n+ Gaussian (normal) model: \n  - `lmer(y ~ x1 + x2 + (1 | g), data = data)`  \n+ Binomial model:^[Remember that binary outcomes are just a special case of the binomial]  \n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial(link='logit'))`<br>or\n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = \"binomial\")`<br>or\n  - `glmer(y ~ x1 + x2 + (1 | g), data = data, family = binomial)`  \n  \n\n    \nThe same logic applies as it did for `glm()` where we are not modeling the outcome $y$ directly, but via a mapping, or \"link function\", which in this case is the logistic function. Beyond the material covered in , there are a few aspects of the logistic multilevel model that are worth commenting on. \n\nFirstly, GLMMs ('generalised linear mixed models') can take more time to run (for bigger datasets we're talking minutes, and sometimes even hours!). \n\nSecondly, our choices of methods of inference (see [2A](02a_inference.html)) are slightly different from what they were for `lmer()`. We continue to have the likelihood based methods as well as parametric bootstrapping (although this will more often result in computational issues with `glmer`). We also have the traditional $z$-statistics that have carried over from `glm()`. We can get confidence intervals that follow this same method by using `confint(model, method=\"Wald\")`, which may often be preferred purely for the fact that they are quick!   \n\nFinally, one small but noteworthy feature of the logistic multilevel model is that our fixed effects coefficients, when translated back to odds ratios, represent \"cluster specific\" effects of a predictor: \n\n- For a linear multilevel model: `lmer(y ~ x + (1 + x | cluster))`, the fixed effect of $x$ gives the change in the $y$ when $x$ is increased by one unit, averaged across clusters \n- For a logistic multilevel model: `glmer(ybin ~ x + (1 + x | cluster), family=binomial)`, the odds ratio for $x$ - `exp(fixef(model))` - gives the multiplicative change in odds of $y$ when $x$ is increased by one unit __for a particular cluster.__  \n\nThis becomes important when deciding if we want to estimate outcomes for individual clusters, or estimate group effects (in which case a mixed model might not be best). \n\n::: {.callout-caution collapse=\"true\"}\n#### optional: why are OR from glmer cluster-specific?  \n\n::::panelset\n:::panel\n#### Linear  \n\nconsider a __linear__ multilevel model:  \n`lmer(respiratory_rate ~ treatment + (1|hospital))`\n\nImagine two patients from different hospitals. One has a treatment, one does not.  \n\n  - patient $j$ from hospital $i$ is \"control\"   \n  - patient $j'$ from hospital $i'$ is \"treatment\"   \n\nThe difference in estimated outcome between patient $j$ and patient $j'$ is the \"the effect of having treatment\" plus the distance in random deviations between hospitals $i$ and $i'$  \n\nmodel for patient $j$ from hospital $i$  \n$\\hat{y}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + b_1 (Treatment_{ij} = 0)$\n\nmodel for patient $j'$ from hospital $i'$  \n$\\hat{y}_{i'j'} = (\\gamma_{00} + \\zeta_{0i'}) + b_1 (Treatment_{i'j'} = 1)$\n\ndifference:  \n$\\hat{y}_{i'j'} - \\hat{y}_{ij} = b_1 + (\\zeta_{0i'} - \\zeta_{0i}) = b_1$\n\nBecause $\\zeta \\sim N(0,\\sigma_\\zeta)$, the differences between all different $\\zeta_{0i'} - \\zeta_{0i}$ average out to be 0. \n\n:::\n:::panel\n#### Logistic\n\nNow consider a __logistic__ multilevel model:  \n`glmer(needs_operation ~ treatment + (1|hospital), family=\"binomial\")`\n\nImagine two patients from different hospitals. One has a treatment, one does not.  \n\n  - patient $j$ from hospital $i$ is \"control\"   \n  - patient $j'$ from hospital $i'$ is \"treatment\"  \n  \nThe difference in __probability of outcome__ between patient $j$ and patient $j'$ is the \"the effect of having treatment\" plus the distance in random deviations between hospitals $i$ and $i'$  \n\nmodel for patient $j$ from hospital $i$  \n$log \\left( \\frac{p_{ij}}{1 - p_{ij}} \\right)  = (\\gamma_{00} + \\zeta_{0i}) + b_1 (Treatment_{ij} = 0)$\n\nmodel for patient $j'$ from hospital $i'$  \n$log \\left( \\frac{p_{i'j'}}{1 - p_{i'j'}} \\right) = (\\gamma_{00} + \\zeta_{0i'}) + b_1 (Treatment_{i'j'} = 1)$\n\ndifference (log odds):  \n$log \\left( \\frac{p_{i'j'}}{1 - p_{i'j'}} \\right) - log \\left( \\frac{p_{ij}}{1 - p_{ij}} \\right) = b_1 + (\\zeta_{0i'} - \\zeta_{0i})$\n\ndifference (odds ratio):  \n$\\frac{p_{i'j'}/(1 - p_{i'j'})}{p_{ij}/(1 - p_{ij})} = e^{b_1 + (\\zeta_{0i'} - \\zeta_{0i})} \\neq e^{b_1}$\n\nHence, the interpretation of $e^{b_1}$ is not the odds ratio for the effect of treatment \"averaged over hospitals\", but rather for patients _from the same hospital_. \n\n:::\n::::\n\n:::\n\n## Example\n\n:::frame\n__Data: monkeystatus.csv__  \n\n```{r}\n#| echo: false\nss = 709233#round(runif(1,1e3,1e6))\nset.seed(ss)\nn_groups = 50\n# npgroup = round(runif(30,2,25))\nnpgroup = round(runif(50, 5,11))\ng = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\nN = length(g)\nx = sample(1:2,N,T)\nb = sample(letters[1:3],n_groups,T,prob=c(.3,.4,.3))\nb = b[g]\nres = MASS::mvrnorm(n=n_groups,\n                    mu=c(0,0),Sigma=matrix(c(2,0,0,.6),nrow=2))\nre0 = res[,1]\nre  = re0[g]\nrex = res[,2]\nre_x  = rex[g]\nlp = (-.5 + re) - (.4 + re_x)*scale(x)[,1] + 1.4*(b==\"a\")\ny_bin = rbinom(N, size = 1, prob = plogis(lp))\ny = lp + rnorm(N,0,1.3)\ndf = data.frame(x = x, g=factor(g), b=b,y_bin)\nmnames = unique(randomNames::randomNames(n_groups*4,which.names=\"first\"))\nmonkeystatus = df |>\n  transmute(\n    status = ifelse(b==\"a\",\"subordinate\",ifelse(b==\"b\",\"dominant\",\"adolescent\")),#cut(x,3,labels=letters[1:3]),\n    difficulty = factor(x, labels=c(\"easy\",\"difficult\")),\n    monkeyID = mnames[as.numeric(g)],\n    solved = y_bin,\n    score = round(scale(y)[,1]*10+50,1)\n  ) |> filter(!(monkeyID==\"Richard\" & difficulty==\"easy\"),\n              !(monkeyID==\"Nadheera\" & difficulty==\"difficult\"))\n\n\n\n# library(lme4)\n# m = glmer(solved ~ difficulty + status + (1+difficulty|monkeyID), monkeystatus, family=binomial)\n# summary(m)\n# \n# m1=glmer(solved ~ difficulty + (1+difficulty|monkeyID), monkeystatus, family=binomial)\n# anova(m1,m)\n# \n# lmer(probscore ~ 1 + difficulty + status +\n#                (1 + difficulty | monkeyID),\n#       data=monkeystatus) |> summary()\n\n# write_csv(monkeystatus |> select(-score),\n#           file=\"../../data/msmr_monkeystatus.csv\")\n```\n\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a \"dominant\" or \"subordinate\" status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either \"easy\" or \"difficult\", and the researchers recorded whether or not the monkey solved each problem. \n\nWe're interested in how the social status of monkeys is associated with the ability to solve problems.  \n\nThe data is available at [https://uoepsy.github.io/data/msmr_monkeystatus.csv](https://uoepsy.github.io/data/msmr_monkeystatus.csv){target=\"_blank\"}.  \n\n:::\n\n### getting to know my monkeys\n\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in _long_ format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable `monkeyID` which indicates what monkey each problem has been attempted by. We can see the `status` of each monkey, and the `difficulty` of each task, along with whether it was `solved`:   \n\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nmstat <- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n```\n\nWe can do some quick exploring to see how many monkeys we have (`r length(unique(mstat$monkeyID))`), and how many problems each one attempted (min = `r min(table(mstat$monkeyID))`, max = `r max(table(mstat$monkeyID))`:  \n```{r}\nmstat |> \n  count(monkeyID) |> # count the monkeys!  \n  summary()\n```\n\nLet's also see how many monkeys of different statuses we have in our sample:  \n```{r}\nmstat |> \n  group_by(status) |> # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n```\n\nIt's often worth plotting as much as you can to get to a sense of what we're working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.  \n```{r}\n#| out-width: \"100%\"\n#| fig-height: 6\n# which monkeys did what type of problems? \nmstat |> count(status, monkeyID, difficulty) |>\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n```\n\nWhen working with binary outcomes, it's often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like \"subordinate\" monkeys solve more problems, and adolescents solve fewer (makes sense - they're still learning!).  \n```{r}\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n```\n  \n  \n### models of monkeys  \n  \nNow we come to fitting our model.  \n\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It's very likely that difficulty of a problem is going to influence that it is solved, so we'll control for difficulty. \n```\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\n```\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan). \n```\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\n```\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he's likely to really really struggle with the difficult ones!).  \n\n```{r}\nmmod <- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n```\n\n### test and visualisations of monkey status\n\nTo examine if monkey status has an effect, we can compare with the model without `status`:   \n```{r}\n#| code-fold: true\nmmod0 <- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n```\n\n```{r}\n#| echo: false\nres = anova(mmod0,mmod)\n```\n\n\nAnd we can see that the status of monkeys _is_ associated with differences in the probability of successful problem solving ($\\chi^2(2)$ = `r round(res[2,\"Chisq\"],2)`, p `r format.pval(res[2,\"Pr(>Chisq)\"],eps=.05)`).  \n\n\nAnd if we want to visualise the relevant effect, we can (as we did with `glm()`) plot on the predicted probability scale, which is much easier to interpret:\n\n```{r}\n#| code-fold: true\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |>\n  as.data.frame() |>\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n```\n\n### interpretation\n\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:  \n```{r}\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |>\n  exp()\n```\n\n```{r}\n#| echo: false\nbroom.mixed::tidy(mmod) |>\n  filter(effect==\"fixed\") |>\n  transmute(\n    term,\n    est = round(estimate,2),\n    OR = round(exp(estimate),2),\n    `OR interpretation` = c(\n      \"estimated odds of an adolescent monkey solving an easy problem\",\n      \"odds of successful problem solving are more than halved (0.42 times the odds) when a given monkey moves from an easy to a difficult problem\",\n      \"odds of success would be almost doubled (1.95 times the odds) if a monkey were to change from adolescent to dominant status (NB this is non-significant)\",\n      \"odds of success would quadruple (4.3 times the odds) if a monkey were to change from adolescent to subordinate status\"\n    )\n  ) |> gt::gt()\n```\n\n::: {.column-margin}\n__Side note__  \n\nContrast this with what we would get from a _linear_ multilevel model. If we were instead modelling a \"problem score\" with `lmer()`, rather than \"solved yes/no\" with `glmer()`, our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.  \n\nNote that estimating differences between groups is not quite the same idea as estimating the effect \"if a particular monkey changed from adolescent to subordinate\". In the linear world, these two things are the same, but our odds ratios give us only the latter. \n\n\n:::\n\n\n\n\n\n\n\n\n```{r}\n#| include: false\n# ss = 236154#round(runif(1,1e3,1e6))\n# set.seed(ss)\n# n_groups = 16\n# # npgroup = round(runif(30,2,25))\n# npgroup = round(runif(16,15,45))\n# g = unlist(sapply(1:n_groups, function(x) rep(x,npgroup[x])))\n# N = length(g)\n# x = sample(letters[1:2],N,T,prob=c(.3,.7))\n# b = rnorm(n_groups)\n# b = b[g]\n# res = MASS::mvrnorm(n=n_groups,\n#                     mu=c(0,0),Sigma=matrix(c(2,0,0,.6),nrow=2))\n# re0 = res[,1]\n# re  = re0[g]\n# rex = res[,2]\n# re_x  = rex[g]\n# lp = (-1 + re) + (.9 + re_x)*(x==\"a\") + .4*b\n# y_bin = rbinom(N, size = 1, prob = plogis(lp))\n# y = lp + rnorm(N,0,1.3)\n# df = data.frame(x = x, g=factor(g), b=b,y_bin,y)\n# #set.seed(764)\n# monkeystatus = df |>\n#   transmute(\n#     status = ifelse(x==\"a\",\"sub\",\"alpha\"),\n#     troopsize = round(scale(b)[,1]*13+56),\n#     troop = paste0(\"T\",sample(1e3:1e4,n_groups))[as.numeric(g)],\n#     solved = y_bin,\n#     probscore = round(scale(y)[,1]*10+50,2)\n#   )\n\n# library(lme4)\n# subordinate monkeys (as opposed to alpha monkeys in the same troop) have increased odds of success. \n# adding one more monkey to the troop is associated with increased odds of a monkey succeeding\n# glmer(solved ~ troopsize + status + (1+status|troop), \n#       monkeystatus, family=binomial) |> summary()\n# \n# # subordinate monkeys (as opposed to alpha monkeys) tend to score 6.4 higher on the problem solving scale\n# # every additional 1 in troop size is associated with 0.3 higher scores \n# lmer(probscore ~ troopsize + status + (1+status|troop), \n#       monkeystatus) |> summary()\n\n\n```\n\n\n\n\n# MLMs for Longitudinal Data\n\nMultilevel models are perfectly geared towards dealing with longitudinal data, which can be framed as having essentially the same hierarchical structure as \"children in schools\", or \"observations within participant\".  \n\nThe only difference is that in longitudinal data, the repeated measurements within a participant are ordered according to time. For instance, this could apply to anything from studying people over decades as they age to studying eye movements across the 1 second presentation of a spoken word.  \n\nAs with all applications of multilevel models, we will want our data in long format, so it is usually going to look something like this:  \n\n```{r}\n#| echo: false\ntibble(\n  y = strsplit(\"? ? ... ... ? ? ...\",split=\" \")[[1]],\n  time = strsplit(\"1 2 ... ... 1 2 ...\",split=\" \")[[1]],\n  person = strsplit(\"1 1 ... ... 2 2 ...\",split=\" \")[[1]],\n  `... ` = rep(\"...\",7),\n  `...  ` = rep(\"...\",7),\n) |> as.data.frame() \n```\n\nWe are treating time as continuous here, which has some great benefits - we don't need to have seen everyone at every timepoint, nor do we even need to have everyone with regularly spaced intervals (although knowing the chronology is important for decisions about the shape of trajectory we might want to fit (more on that next week!)).  \n\nGiven the variables above, you might be able to guess the basic form that our models are going to take in order to assess \"change over time\".  \n\nRegressing the outcome variable on to our time variable gives us an estimate of the trajectory - i.e. how much the outcome changes with every 1 additional unit of time. Furthermore, we can fit random slopes of time in order to model how people^[assuming that it is people we are studying!] can vary in the trajectory. \n```{r}\n#| eval: false\nlmer(y ~ 1 + time + (1 + time | person), ...)\n```\n\nAnd we can fit interactions between predictors and time (`y ~ time * x`) to ask questions such as whether different groups have different trajectories of `y`, or whether the association between `x` and `y` changes over time (`x` can also be something that changes over time!). For instance, we can use multilevel models to address question such as does the development of reading ability differ between monolingual and bilingual children? Does the association between physical activity and positive mood change as we age? Do people look at the more unusual of two objects following a speech disfluency?  \n\n\n\n## Example\n\n\n:::frame\n__Data: mindfuldecline.csv__  \n\nA study is interested in examining **whether engaging in mindfulness can prevent cognitive decline in older adults**. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke's Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.   \n\n  \nThe data are available at: [https://uoepsy.github.io/data/msmr_mindfuldecline.csv](https://uoepsy.github.io/data/msmr_mindfuldecline.csv){target=\"_blank\"}.  \n\n```{r}\n#| echo: false\n# read_csv(\"https://uoepsy.github.io/data/dapr3_mindfuldecline.csv\") |>\n#   transmute(ppt,condition,study_visit = visit, age, ACE, imp) |>\n#   write_csv(file=\"../../data/msmr_mindfuldecline.csv\")\nmmd <- read_csv(\"../../data/msmr_mindfuldecline.csv\")\ntibble(\n  variable=names(mmd),\n  description=c(\"Participant Identifier\",\n                \"Whether the participant engages in mindfulness or not (control/mindfulness)\",\n                \"Study Visit Number (1 - 10)\",\n                \"Age (in years) at study visit\",\n                \"Addenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\",\n                \"Clinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\")\n) |> gt::gt()\n\n```\n\n:::\n\n### exploring the data\n\n```{r}\nlibrary(tidyverse)\nlibrary(lme4)\nmmd <- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nhead(mmd)\n```\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n```{r}\nmmd |> \n  group_by(condition) |>\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n```\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. \nFrom the plot below, we can see that on the whole participants' cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:  \n```{r}\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n```\n\n### modelling change over time\n\nInitially, we'll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables `study_visit` and `age` represent exactly the same information (time), so we have a choice of which one to use.  \n\n::: {.callout-note collapse=\"true\"}\n#### Why the age variable (currently) causes problems\n\nAs it is, the `age` variable we have starts at 60 and goes up to 78 or so.  \n\nIf we try and use this in a model, we get an error!  \n```{r}\n#| message: true\nmod1 <- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n```\n<span style=\"color:red\">Model failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)</span>\n\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is \"when all predictors are zero\". So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!  \n\nThis means that trying to fit `(1 + age | ppt)` will try to estimate the variability in people's change in cognition over time, and the variability in cognition at age zero. As we can see in @fig-nonconv, because the intercept is so far away from the data, the angle of each persons' slope has a _huge_ influence over where their intercept is. The more upwards a persons' slope is, the lower down their intercept is.  \n\n```{r} \n#| label: fig-nonconv\n#| fig-cap: \"lines indicate predicted values from the model with random intercepts and random slopes of age, where age. Due to how age is coded, the 'intercept' is estimated back at age 0\"  \n#| echo: false\nbroom.mixed::augment(mod1) |>\n  ggplot(aes(x=age,group=ppt))+\n  geom_point(aes(y=ACE))+\n  stat_smooth(geom=\"line\",method=lm,se=F,fullrange=T,\n              alpha=.4,\n              aes(y=.fitted))+\n  xlim(0,78)\n  \n```\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):  \n\n```{r}\nVarCorr(mod1)\n```\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the `study_visit` variable.  \nEither will do, we just need to remember the units they are measured in!  \n\n\n:::\n\nLet's center age on 60:\n```{r}\nmmd$ageC <- mmd$age-60\n```\n\nAnd fit our model:\n```{r}\nmod1 <- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n```\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, $t$ statistics that are $>|2\\text{-ish}|$ are probably going to be significant when assessed properly).  \n```{r}\n#| eval: false\nsummary(mod1)\n```\n```\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\n```\n\nWe're now ready to add in group differences in their trajectories of cognition:\n```{r}\nmod2 <- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n```\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is `r round(fixef(mod2)[1])` (that's the `(Intercept)`). For these participants, scores are estimated to decrease by `r round(fixef(mod2)[2],2)` points every year (that's the slope of `ageC`). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the `condition [mindfulness]` coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline `r round(fixef(mod2)[4],2)` less than the control group every year.  \n_(note, there are always lots of ways to frame interactions. A \"reduction in decline\" feels most appropriate to me here)_  \n  \n  \nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.  \n```{r}\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n```\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.  \nAnd that's exactly what we see when we visualise those fixed effects:  \n```{r}\n#| code-fold: true\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |>\n  as.data.frame() |>\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n```\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. \nTo do so, we need to combine two data sources - the fixed effects estimation from `effect()`, and the data itself:  \n```{r}\n#| code-fold: true\nploteff <- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |>\n  as.data.frame()\n\nmmd |>\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n```\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person's cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons' trajectory. When we add in the observed trajectories, we see the variability in people's trajectories (one person even goes up over time!).  \n  \nOur model represents this variability in the random effects part. While the estimated average slope is `r round(fixef(mod2)[2],2)` for the control group (and `r round(fixef(mod2)[2],2)`+`r round(fixef(mod2)[4],2)`=`r round(sum(fixef(mod2)[c(2,4)]),2)` for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of `r round(sqrt(VarCorr(mod2)[[1]][2,2]),2)` (remember we can extract this info using `VarCorr()`, or just look in the output of `summary(model)`).  \n```{r}\nVarCorr(mod2)\n```\n\n::: {.column-margin}\n\n```{r}\n#| echo: false\n#| label: fig-slopesmargin\n#| fig-cap: \"Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\"\nggplot(data=data.frame(x=c(-1,1)), aes(x=x))+\n  stat_function(fun=dnorm, \n                args=list(fixef(mod2)[2],sqrt(VarCorr(mod2)[[1]][2,2])),\n                geom=\"line\", col=\"#a64bb0\",lwd=1)+\n  stat_function(fun=dnorm, \n                args=list(sum(fixef(mod2)[c(2,4)]),sqrt(VarCorr(mod2)[[1]][2,2])),\n                geom=\"line\", col=\"#82b69b\",lwd=1) + \n  scale_x_continuous(limits=c(-.7,.3),breaks=seq(-.7,.3,.2)) +\n  scale_y_continuous(NULL, breaks=NULL)+\n  labs(x=\"slopes\")\n```\n\n:::\n\nIf you think about what this means - it means that some participants we _would_ expect to actually increase in their slopes. If we have a normal distribution with a mean of `r round(fixef(mod2)[2],1)` or `r round(sum(fixef(mod2)[c(2,4)]),2)` and a standard distribution of `r round(sqrt(VarCorr(mod2)[[1]][2,2]),2)`, then we would expect some values to to positive (see e.g., @fig-slopesmargin).  \n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"02c_log.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","toc_float":true,"link-citations":true,"theme":["united","assets/style-labs.scss"],"title":"2B: Logistic MLM | Longitudinal MLM","params":{"SHOW_SOLS":false,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}