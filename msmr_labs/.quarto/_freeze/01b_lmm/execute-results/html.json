{
  "hash": "d252d54ed8553438148424beeb9a4f42",
  "result": {
    "markdown": "---\ntitle: \"1B: Linear Mixed Models/Multi-level Models\"\nparams: \n    SHOW_SOLS: FALSE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n:::{.callout-note collapse=\"true\"}\n## A Note on terminology\n\nThe methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![size weighted by hits on google scholar search (sept 2020)](01b_lmm_files/figure-html/fig-wordcloud-1.png){#fig-wordcloud fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n\n\n# LMM\n\nIn the simple linear regression model was written as $\\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{\\ + \\ \\varepsilon}$, the estimated coefficients $\\color{blue}{b_0}$, $\\color{blue}{b_1}$ etc., are estimated as a fixed value.  \n\nIn the example where we model School children's grades as a function of their motivation score, we can fit a simple regression model, and the estimated parameters are two values that define line - the intercept and the slope (as in @fig-schoolplot1).  \n \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line.](01b_lmm_files/figure-html/fig-schoolplot1-1.png){#fig-schoolplot1 fig-align='center' width=80%}\n:::\n:::\n\nThese two values are fixed. It does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of 40.3 (the intercept). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nschoolmot <- read_csv(\"data/schoolmot.csv\")\nsrmod <- lm(grade ~ motiv, data = schoolmot)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n...\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  40.2776     1.8270  22.046  < 2e-16 ***\nmotiv         1.9551     0.3422   5.714  1.5e-08 ***\n---\n```\n:::\n:::\n\n\nTo make it clear why this is, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: \n\n$$\n\\begin{align}\n&\\text{For child }i \\\\\n&\\text{grade}_i = b_0 + b_1 \\cdot \\text{motiv}_i + \\epsilon_i \n\\end{align}\n$$\ni.e. For any child $i$ that we choose, that child's grade ($\\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\\text{motiv}_i$). \nThe issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children we are looking at can be grouped into the different schools we sampled them from, and those school-level differences might actually account for quite a lot of the variation in grades (in [1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable) we actually estimated this to account for c40% of the variation in grades).  \n\nWe saw how we might add in `school` as a predictor to our linear model to estimate all these school-level differences. This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing we want to account for. However, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), and at the school-level (funding). For this, we really need a multilevel model.  \n\n\n::: {.callout-note collapse=\"true\"}\n#### clusters as fixed effects\n\nWe have already seen that we can include fixed effects for cluster differences (we referred to this as \"no pooling\").  \n\ne.g. to fit school-level differences in grades, we could use:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(grade ~ motiv + school, data = schoolmot)\n```\n:::\n\n\nThe model equation for this would look something like:\n$$\n\\begin{align}\n\\text{For child }i& \\\\\n\\text{grade}_i =\\, &b_0 + b_1 \\cdot \\text{motiv}_i + b_2 \\cdot \\text{isSchool2}_i + b_3 \\cdot \\text{isSchool3}_i\\,\\, + \\,\\, ... \\,\\, + \\\\\n& ... + \\,\\, ... \\,\\, + \\,\\, ... \\,\\, + \\\\\n& b_p \\cdot \\text{isSchoolP}_i\\,\\, + \\epsilon_i \n\\end{align}\n$$\n\nThe school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. \n\n\n:::\n\n\n\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_1 \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n$$\n\n$$\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n\\end{align}\n$$\n\n\nthe linear mixed model (LMM) \n\nfits a distribution of intercepts.\na center ( a single value and a spread)\n\nformula here\n\n\n\nso for a given school, the intercept is b0 + z0i  \na fixed number plus some random deviation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b_lmm_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggforce)\nlibrary(ggfx)\n\n\nspecg = plotlines |> filter(g==10) |>\n  mutate(f = fixef(rimod)[1])\n\n\nbasep + \n  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + \n  geom_line(data = specg,lwd=1,\n            aes(x=x,y=.fitted,group=g),alpha=1,col=\"orange\") +\n  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col=\"#a41ae4\") +\n  \n  geom_curve(\n    data=specg[1,],\n    aes(x=0,xend=0,y=.fitted,yend=f,col=\"orange\"),\n    curvature=.2,lwd=1\n  ) +\n  annotate(\"text\",x=-.1,y=fixef(rimod)[1],\n           label=expression(gamma*\"00\"),size=5,\n           hjust=1,vjust=1.3,col=\"#a41ae4\",parse=TRUE) +\n  annotate(\"text\",x=-.1,y=mean(unlist(specg[1,5:6])),\n           label=expression(zeta*\"0i\"),size=5,\n           hjust=1.2,col=\"orange\")+\n  guides(col=\"none\")\n```\n\n::: {.cell-output-display}\n![](01b_lmm_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n\n```{.r .cell-code}\n  # stat_eye(side=\"left\",\n  #          data=tibble(motiv=-1,grade=50),\n  #          aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), \n  #          alpha=.3, fill=\"#a41ae4\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b_lmm_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nintercepts vary\nslopes vary\n\nno pooling vs partial pooling: \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b_lmm_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nhow is it different to fixed eff?\n\"partial pooling\" (link back to above)\nshrinkage\n- socialist vs liberal analogy?  \n\nhow/why does it do this?\nby modelling a distribution of lines  \n\n\n$$\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n$$\n\n# fitting LMM in R\n\nlme4\nlmer\n\n# model parameters\n\n_what_ are the model parameters? \ni.e. variance components. \n\neq with model params coloured\n$$\n\n$$\n\n\nwe _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. \n\n# terminology: fixed effects, random effects, variance components\n\nwe often use \"random effects\" to just mean the distribution of random deviations. i.e. \n\nsometimes you might hear \n\"random effect of group\"\n\"random effect for group\"\n\"random effect [of x] by group\"\n\ngenerally, people are referring to the `(1 + ... | cluster)` bit. \n\ngraphic on how to read it.\n\nintercept >> 1\nslope of x >> x\n| >> varies by\nthese groups >> cluster\n\na common stumbling block. \n\"effect of x varies by cluster\" is not the same as \"x varies by cluster\".  \n\n\n\n\n\n# estimation\n\n## ML and REML\n\nMLE explainer\n\n- problem for lmm\nest fix > est varcorr > est fix > est varcorr\nest of varcorr assumes fixed effects are known. \nthis biases var ests to be slightly smaller  \na bit like n-1 in formula for sd\n\nREML\n- OLS to partial out fixef > \n  est varcorr > est varcorr > est varcorr > \n  use GLS to est fixef\n- in the estimation of varcorr, the fixed effects are 0 _by definition_\n  \n\n\n\n\n\n\n\n\n\n## fitting issues\n\nconvergence warnings, singular fits \n\n\n\n\n\n\n",
    "supporting": [
      "01b_lmm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.2.6/panelset.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/panelset-0.2.6/panelset.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}