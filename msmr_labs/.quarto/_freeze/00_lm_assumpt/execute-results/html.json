{
  "hash": "1004da7501618995456afef49bb01d10",
  "result": {
    "markdown": "---\ntitle: \"LM Troubleshooting\"\nparams: \n    SHOW_SOLS: FALSE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nIn the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below.  \n\n::: {.callout-note collapse=\"true\"}\n# LM assumptions in brief\n\nWhen we fit linear regression models, we are fitting a line (or a regression surface, when we add in more predictors), to a cloud of datapoints. The discrepancy between the fitted model and the observed data is taken up by the residuals. \n\n$$\n\\begin{align}\n\\color{red}{y} &= \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\ \n\\color{red}{\\text{observed }y} &= \\color{blue}{\\text{fitted }\\hat y} \\,\\, \\color{black}{+ \\text{ residual }\\hat \\varepsilon}\\\\ \n\\end{align}\n$$\n\nWe are theorising that our model contains all the systematic relationships with our outcome variable, we assume that the residuals - the leftovers - are essentially random noise. This is the $\\epsilon \\sim N(0, \\sigma)$ bit, which is a way of specifying our assumption that the errors are normally distributed with a mean of zero (see @fig-slr2).   \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Simple linear regression model, with the systematic part of the model in blue, and residuals in red. The distributional assumption placed on the residuals is visualised by the orange normal curves - the residuals are normally distributed with a mean of zero, and this does not change across the fitted model.](00_lm_assumpt_files/figure-html/fig-slr2-1.png){#fig-slr2 fig-align='center' width=80%}\n:::\n:::\n\n\nWe typically want to check our model residuals (by plotting or performing statistical tests) to determine if we have reason to believe our assumptions are violated. The easiest way to do this in R is with `plot(model)`, which provides us with a series of visuals to examine for unusual patterns and conspicuous observations.  \n\nWhen model assumptions appear problematic, then our inferential tools go out the window. While our specific point estimates for our regression coefficients are our best linear estimates for the sample that we have, our standard errors rely on the distributional assumptions of the residuals^[Why is this? It's because the formula to calculate the standard error involves $\\sigma^2$ - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient]. It is our standard errors that allow us to construct test statistics and compute p-values (@fig-inf1) and construct confidence intervals. Our assumptions underpin our ability to generalise from our specific sample to make statements about the broader population.  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![inference for regression coefficients](images/sum4.png){#fig-inf1 fig-align='center' width=80%}\n:::\n:::\n\n\n\n::: {.callout-tip collapse=\"true\"}\n## Refresher: Standard Error   \n\nTaking **samples** from a **population** involves an element of _randomness_. The mean height of 10 randomly chosen Scottish people will not be exactly equal to the mean height of the entire Scottish population. Take another sample of 10, and we get _another_ mean height (@fig-se).  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Estimates from random samples vary randomly around the population parameter](00_lm_assumpt_files/figure-html/fig-se-1.png){#fig-se fig-align='center' width=80%}\n:::\n:::\n\n\nThe standard error of a statistic is the standard deviation of all the statistics we _might have_ computed from samples of that size (@fig-se2). We can calculate a standard error using formulae (e.g. for a mean, the standard error is $\\frac{\\sigma}{\\sqrt{n}}$) but we can also use more computationally intensive approaches such as \"bootstrapping\" to actually generate an empirical sampling distribution of statistics which we can then summarise.  \n\nWe use the standard error to quantify the uncertainty around our sample statistic as an estimate of the population parameter, or to construct standardised test statistics in order to perform tests against some null hypothesis.  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The standard error is the standard deviation of the 'sampling distribution' - the distribution of sample statistics that we _could_ see.](00_lm_assumpt_files/figure-html/fig-se2-1.png){#fig-se2 fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n\n:::\n\n\n::: {.callout-important collapse=\"false\"}\n# Measurement!\n\nDetailed below are various methods that allow us to be flexible with regard to the distributional assumptions we have to hold when using linear regression. \n\nThe elephant in the room is that _whatever_ we do, we should be cognizant of the validity of our measurements. If we're not measuring what we think we're measuring, then we're in trouble. \n\n:::\n\n# Transformation of the outcome variable.  \n\nA somewhat outdated approach, transforming our outcome variable prior to fitting the model, using something such as `log(y)` or `sqrt(y)`, will sometimes allow us to estimate a model for which our assumptions are satisfied.  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A model of a transformed outcome variable can sometimes avoid violations of assumptions that arise when modeling the outcome variable directly. Data from https://uoepsy.github.io/data/trouble1.csv](00_lm_assumpt_files/figure-html/fig-trouble1-1.png){#fig-trouble1 fig-align='center' width=80%}\n:::\n:::\n\n\nThe major downside of this is that we are no longer modelling $y$, but some transformation $f(y)$ ($y$ with some function $f$ applied to it). Interpretation of the coefficients changes accordingly, such that we are no longer talking in terms of changes in y, but changes in $f(y)$. When the transformation function used is non-linear (see the Right-Hand of @fig-logtr) a change in $f(y)$ is **not the same** for every $y$. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The log transformation is non-linear](00_lm_assumpt_files/figure-html/fig-logtr-1.png){#fig-logtr fig-align='center' width=80%}\n:::\n:::\n\n\nFinding the optimal transformation to use can be difficult, but there are methods out there to help you. One such method is the BoxCox transformation, which can be conducted using `BoxCox(variable, lambda=\"auto\")`, from the __forecast__ package.^[This method finds an appropriate value for $\\lambda$ such that the transformation $(sign(x) |x|^{\\lambda}-1)/\\lambda$ results in a close to normal distribution.] \n\nFor certain transformations, we _can_ re-express coefficients to be interpretable with respect to $y$ itself. For instance, the model using a log transform $ln(y) = b_0 + b_1(x)$ gives us a coefficient that represents statement __A__ below. We can re-express this by taking the opposite function to logarithm, the exponent, `exp()`. Similar to how this works in logistic regression, the exponentiated coefficients obtained from `exp(coef(model))` are _multiplicative_, meaning we can say something such as statement __B__\n\n:::int\n\n- __A:__ \"a 1 unit change in $x$ is associated with a $b$ unit change in $ln(y)$\".  \n- __B:__ \"a 1 unit change in $x$ is associated with $e^b$ __percent__ change in $y$.\"\n\n:::\n\n\n\n# Heteroscedastic robust standard errors (Huber White)\n\nOften, when faced with residuals that appear to violate our assumption of constant variance (also called \"homoscedasticity\" or \"equal variance\"), a suitable option is simply to apply a correction to ensure that we make the correct inferences. \n\nThere are various alternative calculations of standard errors that are robust to non-constance variance (or \"heteroscedasticity\"). The most commonly used are the \"Huber-White\" standard errors^[This is a special formulation of something called a 'Sandwich' estimator!], which are robust to heteroscedasticity and/or non-normality. Fortunately, we don't have to do any complicated calculations ourselves, as R will do all of the hard work for us. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Residual plots showing heteroscedasticity. The residuals vs fitted plot shows fanning out of residuals, and this is reflected in the scale-location plot showing the increasing variance. Data from https://uoepsy.github.io/data/trouble2.csv](00_lm_assumpt_files/figure-html/fig-trouble2-1.png){#fig-trouble2 fig-align='center' width=80%}\n:::\n:::\n\n\nOur original __uncorrected__ standard errors:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntroubledf2 <- read_csv(\"https://uoepsy.github.io/data/trouble2.csv\")\nmod <- lm(y ~ 1 + x + x2, data = troubledf2)\nsummary(mod)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Estimate Std. Error      t value   Pr(>|t|)\n(Intercept) -0.038356059  0.9837690 -0.038988886 0.96898022\nx            0.492474335  0.2445796  2.013554062 0.04685567\nx2b          1.230574302  0.8311491  1.480569879 0.14199579\nx2c         -0.001012855  0.8451027 -0.001198499 0.99904622\n```\n:::\n:::\n\n\nUsing the __lmtest__ and __sandwich__ packages, we can use the Huber-White estimation to do both coefficient tests and model comparisons. \n\n:::panelset\n:::panel\n## Tests of the coefficients\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n:::panel\n## Model comparisons\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_res <- lm(y ~ 1 + x, data = troubledf2)\nmod_unres <- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(>F)\n1     98                 \n2     96  2 1.8704 0.1596\n```\n:::\n:::\n\n:::\n:::\n\n# Weighted Least Squares (WLS)\n\nIf we have some specific belief that our non-constant variance is due to differences in the variances of the outcome between various groups, then we might be better suited to use __Weighted Least Squares__.   \n\nAs an example, imagine we are looking at weight of different dog breeds (@fig-dogweight). The weights of chihuahuas are all quite close together (between 2 to 5kg), but the weight of, for example, spaniels is anywhere from 8 to 25kg - a much bigger variance. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The weights of 49 dogs, of 7 breeds](00_lm_assumpt_files/figure-html/fig-dogweight-1.png){#fig-dogweight fig-align='center' width=80%}\n:::\n:::\n\n\nRecall that the default way that `lm()` deals with categorical predictors such as `dog breed`, is to compare each one to a reference level. In this case, that reference level is \"beagle\" (first in the alphabet). Looking at @fig-dogweight above, which comparison do you feel more confident in? \n\n- **A:** Beagles (14kg) vs Pugs (9.1kg). A difference of 4.9kg.  \n- **B:** Beagles (14kg) vs Spaniels (19kg). A difference of 5kg.  \n\nHopefully, your intuition is that **A** looks like a clearer difference than **B** because there's less overlap between Beagles and Pugs than between Beagles and Spaniels. Our standard linear model, however, assumes the standard errors are identical for each comparison:  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ breed, data = dogdf)\n...\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             13.996      1.649   8.489 1.17e-10 ***\nbreedpug                -4.858      2.332  -2.084   0.0433 *  \nbreedspaniel             5.052      2.332   2.167   0.0360 *  \nbreedchihuahua         -10.078      2.332  -4.322 9.28e-05 ***\nbreedboxer              20.625      2.332   8.846 3.82e-11 ***\nbreedgolden retriever   17.923      2.332   7.687 1.54e-09 ***\nbreedlurcher             5.905      2.332   2.533   0.0151 *  \n---\n```\n:::\n:::\n\n\nFurthermore, we can see that we have heteroscedasticity in our residuals - the variance is not constant across the model:  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00_lm_assumpt_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nWeighted least squares is a method that allows us to apply weights to each observation, where the size of the weight indicates the precision of the information contained in that observation.  \nWe can, in our dog-breeds example, allocate different weights to each breed. Accordingly, the Chihuahuas are given higher weights (and so Chihuahua comparisons result in a smaller SE), and Spaniels and Retrievers are given lower weights. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(nlme)\nload(url(\"https://uoepsy.github.io/data/dogweight.RData\"))\ndogmod_wls = gls(weight ~ breed, data = dogdf, \n                 weights = varIdent(form = ~ 1 | breed))\nsummary(dogmod_wls)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients:\n                           Value Std.Error   t-value p-value\n(Intercept)            13.995640  1.044722 13.396516  0.0000\nbreedpug               -4.858097  1.271562 -3.820576  0.0004\nbreedspaniel            5.051696  2.763611  1.827933  0.0747\nbreedchihuahua        -10.077615  1.095964 -9.195207  0.0000\nbreedboxer             20.625429  1.820370 11.330351  0.0000\nbreedgolden retriever  17.922779  2.976253  6.021927  0.0000\nbreedlurcher            5.905261  1.362367  4.334559  0.0001\n```\n:::\n:::\n\n\nWe _can_ also apply weights that change according to continuous predictors (e.g. observations with a smaller value of $x$ are given more weight than observations with larger values). \n\n# Bootstrap\n\nThe bootstrap method is an alternative non-parametric method of constructing a standard error. <!-- It is asymptotically equivalent to the Huber-White corrected standard error discussed above.--> Instead of having to rely on calculating the standard error with a formula and potentially applying fancy mathematical corrections, bootstrapping involves mimicking the idea of \"repeatedly sampling from the population\". It does so by repeatedly **re**sampling **with replacement** from our _original sample_.  \nWhat this means is that we don't have to rely on any assumptions about our model residuals, because we actually generate an actual distribution that we can take as an approximation of our sampling distribution, meaning that we can actually _look_ at where 95% of the distribution falls, without having to rely on any summing of squared deviations.  \n\n\n- Step 1: Resample with replacement from the sample.  \n- Step 2: Fit the model to the resample from Step 1, and obtain a coefficient estimate.  \n- Step 3: Repeat Steps 1 and 2 thousands of times, to get thousands of estimates.  \n- Step 4: The distribution of all our bootstrap estimates will approximate the sampling distribution of the coefficient. It gives us an idea of \"what we would get if we collected another sample of the same size\". We can use the standard deviation of these estimates as our standard error.  \n\nWe can do this really easily in R, as there are various packages/functions that do it all for us. \nFor instance, we might have the following model:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntroubledf3 <- read_csv(\"https://uoepsy.github.io/data/trouble3.csv\")\nmod <- lm(y ~ x + x2, data = troubledf3)\nsummary(mod)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\n...\nlm(formula = y ~ x + x2, data = troubledf3)\n...\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   1.5156     0.6385   2.374   0.0196 *\nx             0.3770     0.1699   2.219   0.0289 *\nx2b           0.2497     0.5368   0.465   0.6429  \nx2c          -0.1306     0.5833  -0.224   0.8233  \nx2d           1.1534     0.5323   2.167   0.0327 *\n---\n```\n:::\n:::\n\n\nHowever, a quick look at our residual plots (@fig-trouble3) give us reason to hesitate. We can see clear deviations from normality in the QQ-plot. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Residual plots showing non-normality in the residuals. The points in the QQplot _should_ follow the diagonal line, but they don't. This is also shown in plotting a histogram of the residuals (bottom right). Data from https://uoepsy.github.io/data/trouble3.csv](00_lm_assumpt_files/figure-html/fig-trouble3-1.png){#fig-trouble3 fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::panelset\n:::panel\n## Boostrapped Coefficients\n\nWe can get out some bootstrapped confidence intervals for our coefficients using the __car__ package:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod <- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965\n```\n:::\n:::\n\n\n:::\n:::panel\n## Bootstrapped ANOVA\n\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the _incremental_ addition of each predictor, we can get bootstrapped p-values from the `ANOVA.boot` function in the __lmboot__ package. \n\nOur original ANOVA:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova( lm(y~x+x2, data = df) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nAnd our bootstrapped p-values:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lmboot)\nmy_anova <- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.023 0.100\n```\n:::\n\n```{.r .cell-code}\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Df    Sum Sq   Mean Sq  F value     Pr(>F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA\n```\n:::\n:::\n\n\n:::\n:::panel\n## Other things  \n\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.  \n\nFor example, to bootstrap the $R^2$ for the model `lm(y~x+x2)`, we would create a little function called `rsq`:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsq <- function(data, indices){\n  # this is the bootstrap resample\n  bdata <- data[indices,]\n  # this is the model, fitted to the resample\n  fit <- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n```\n:::\n\n\nWe then use the __boot__ package, giving 1) our original data and 2) our custom function to the `boot()` function, and compute some confidence intervals:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(boot)\nbootrsq_results <- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable\n```\n:::\n:::\n\n\n:::\n\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n## Cautions\n\n- The bootstrap may give no clue there is bias, when the cause is lack of model fit.\n- The bootstrap does not \"help with small samples\". Bootstrap distributions tend to be slightly too narrow (by a factor of $\\sqrt{(n-1)/n}$ for a single mean), and can actually perform _worse_ than standard methods for small samples.  \n- If your sample is not representative of the population, bootstrapping will not help at all.  \n- There are many different types of bootstrap (e.g. we can resample the observations, or we can resample the residuals), and also different ways we can compute confidence intervals (e.g. take percentiles of the bootstrap distribution, or take the standard deviation of the bootstrap distribution, or others).  \n\n\n:::\n\n\n\n\n# Non-Parametric Tests\n\nMany of the standard hypothesis tests that we have seen ($t$-tests, correlations etc) have got equivalent tests that, instead of examining change in $y$, examine change in something like $rank(y)$^[or $sign( rank(|y|) )$]. By analyising ranked data, we don't have to rely on the same distributional assumptions (ranked data always follows a uniform distribution), and any outliers will have exert less influence on our results.  \nHowever, these tests tend to be less powerful (if there _is_ a true effect in the population, these tests have a lower chance of detecting it), and you lose a lot of interpretability.  \n\nWilcoxon tests are a non-parametric equivalent to the various $t$-tests, and the Kruskal-Wallis test is the non-parametric version of the one-way ANOVA.  \n\n\n| parametric     | non-parametric       |\n| --------------------------------- | ------------------------------- |\n| one sample t-test<br>`t.test(y)`                                    | wilcoxon signed-rank test<br>`wilcox.test(y)`   |\n| paired sample t-test<br>`t.test(y1,y2,paired=TRUE)` or<br>`t.test(y2-y1)` | wilcoxon matched pairs<br>`wilcox.test(y1,y2,paired=TRUE)` or<br>`wilcox.test(y2-y1)` |\n| independent samples t-test<br>`t.test(y1, y2)` or<br>`t.test(y ~ group)`  | mann-whitney U <br>`wilcox.test(y1, y2)` or<br>`wilcox.test(y ~ group)`| \n| one-way ANOVA<br>`anova(lm(y ~ g))`<br>post-hoc tests:<br>`TukeyHSD(aov(y ~ g))` | kruskal-wallis<br>`kruskal.test(y ~ g)`<br>post-hoc tests:<br>`library(FSA)`<br>`dunnTest(y ~ g)` | \n\n\n\n",
    "supporting": [
      "00_lm_assumpt_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.3.0/panelset.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/panelset-0.3.0/panelset.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}