---
title: "LM Troubleshooting"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
library(ggdist)
xaringanExtra::use_panelset()
```

In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below.  

::: {.callout-note collapse="true"}
# LM assumptions in brief

When we fit linear regression models, we are fitting a line (or a regression surface, when we add in more predictors), to a cloud of datapoints. The discrepancy between the fitted model and the observed data is taken up by the residuals. 

$$
\begin{align}
\color{red}{y} &= \color{blue}{b_0 + b_1x_1 \ + \ ... \ + \ b_px_p} \color{black}{+ \varepsilon}\\ 
\color{red}{\text{observed }y} &= \color{blue}{\text{fitted }\hat y} \,\, \color{black}{+ \text{ residual }\hat \varepsilon}\\ 
\end{align}
$$

We are theorising that our model contains all the systematic relationships with our outcome variable, we assume that the residuals - the leftovers - are essentially random noise. This is the $\epsilon \sim N(0, \sigma)$ bit, which is a way of specifying our assumption that the errors are normally distributed with a mean of zero (see @fig-slr2).   

```{r}
#| label: fig-slr2
#| echo: false
#| fig-cap: "Simple linear regression model, with the systematic part of the model in blue, and residuals in red. The distributional assumption placed on the residuals is visualised by the orange normal curves - the residuals are normally distributed with a mean of zero, and this does not change across the fitted model."
#| fig-height: 3.5
set.seed(235)
df <- tibble(
  x = rnorm(100,3,1),
  y = 0.5+.8*x + rnorm(100,0,1)
)
df$y=df$y+1
model1 <- lm(y ~ x, data = df)
betas <- coef(model1)
intercept <- betas[1]
slope <- betas[2]

dfd<-tibble(
  x = c(0:4),
  m = predict(model1,newdata=tibble(x=c(0:4))),
  s = sigma(model1)
)

broom::augment(model1) %>%
ggplot(., aes(x = x, y = y)) +
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=x,dist="norm",arg1=m,arg2=s),alpha=.15, fill="orange") + 
  geom_point(size=3,alpha=.5)+
  geom_abline(intercept = intercept, slope = slope, 
              color = 'blue', size = 1) + 
  #xlim(0,6)+ylim(0,7)+
  geom_vline(xintercept=0,lty="dashed")+
  scale_x_continuous(breaks=0:6)+
  labs(x = "X (predictor)", 
       y = "Y (outcome)")+
  geom_segment(aes(x=x, xend=x, y=y, yend=.fitted), col="red",lty="dotted", linewidth=0.7)
```

We typically want to check our model residuals (by plotting or performing statistical tests) to determine if we have reason to believe our assumptions are violated. The easiest way to do this in R is with `plot(model)`, which provides us with a series of visuals to examine for unusual patterns and conspicuous observations.  

When model assumptions appear problematic, then our inferential tools go out the window. While our specific point estimates for our regression coefficients are our best linear estimates for the sample that we have, our standard errors rely on the distributional assumptions of the residuals^[Why is this? It's because the formula to calculate the standard error involves $\sigma^2$ - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient]. It is our standard errors that allow us to construct test statistics and compute p-values (@fig-inf1) and construct confidence intervals. Our assumptions underpin our ability to generalise from our specific sample to make statements about the broader population.  


```{r}
#| label: fig-inf1
#| echo: false
#| fig-cap: "inference for regression coefficients" 
knitr::include_graphics("images/sum4.png")
```


::: {.callout-tip collapse="true"}
## Refresher: Standard Error   

Taking **samples** from a **population** involves an element of _randomness_. The mean height of 10 randomly chosen Scottish people will not be exactly equal to the mean height of the entire Scottish population. Take another sample of 10, and we get _another_ mean height (@fig-se).  

```{r}
#| echo: false
#| label: fig-se
#| fig-cap: "Estimates from random samples vary randomly around the population parameter"
set.seed(nchar("i'm going dotty"))
tibble(
  m = replicate(750, mean(rnorm(10,165,11)))
) |>
  ggplot(aes(x=m))+
  geom_vline(xintercept = 165, col="red",lty="dashed", lwd=1) +
  geom_dotplot(dotsize=.5,binwidth=.5) +
  scale_y_continuous(NULL, breaks=NULL)+
  scale_x_continuous("mean height (cm)",breaks=seq(-10,10,5)+165) +
  theme_minimal()+
  annotate("text",
           x=170, y=.88, 
           label="Mean height of\nentire Scottish population", col="red",
           hjust=0)+
  geom_curve(aes(x=170, xend=165, y=.88, yend=.88), col="red", size=0.5, 
             curvature = 0, arrow = arrow(length = unit(0.03, "npc"))) +
  
  annotate("text",x=156, y=.57, label="the mean height of\n10 randomly selected\nScottish people", col="grey30") +
  geom_curve(aes(x=156, xend=160.5, y=.5, yend=.40), col="grey30", size=0.5, curvature = 0.2, arrow = arrow(length = unit(0.03, "npc"))) +
  
  annotate("text",x=173, y=.62, label="the mean height of\nanother random sample of 10", col="grey30") +
  annotate("text",x=174, y=.45, label="and another", col="grey30") +
  annotate("text",x=175, y=.25, label="and another", col="grey30") +
  geom_curve(aes(x=173, xend=167.5, y=.57, yend=.45), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=174, xend=169, y=.43, yend=.35), col="grey30", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=175, xend=173.9, y=.23, yend=.015), col="grey30", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))
```

The standard error of a statistic is the standard deviation of all the statistics we _might have_ computed from samples of that size (@fig-se2). We can calculate a standard error using formulae (e.g. for a mean, the standard error is $\frac{\sigma}{\sqrt{n}}$) but we can also use more computationally intensive approaches such as "bootstrapping" to actually generate an empirical sampling distribution of statistics which we can then summarise.  

We use the standard error to quantify the uncertainty around our sample statistic as an estimate of the population parameter, or to construct standardised test statistics in order to perform tests against some null hypothesis.  

```{r}
#| echo: false
#| label: fig-se2
#| fig-cap: "The standard error is the standard deviation of the 'sampling distribution' - the distribution of sample statistics that we _could_ see."
set.seed(2394)
samplemeans <- replicate(2000, mean(rnorm(10,0,11)))
g <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+
  #geom_histogram(alpha=.3)+
  stat_function(geom="line",fun=~dnorm(.x, mean=0,sd=sd(samplemeans)),lwd=1)

ld <- layer_data(g) %>% filter(x <= (11/sqrt(10)) & x >= (-11/sqrt(10)))
ld2 <- layer_data(g) %>% filter(x <= 2*(11/sqrt(10)) & x >= 2*(-11/sqrt(10)))
g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_vline(xintercept = 0, col="red",lty="dashed", lwd=1) +
  annotate("text",
           x=5, y=.12, 
           label="Mean height of\nentire Scottish population", col="red",
           hjust=0)+
  geom_curve(aes(x=5, xend=0, y=.12, yend=.12), col="red", size=0.5, 
             curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_segment(x=0,xend=(-10.5/sqrt(10)),y=.06,yend=.06) +
  annotate("text",x=-7, y=.07, label="standard error\n(standard deviation of\nsampling distribution)", col="grey30")+
  geom_curve(aes(x=-7, xend=-1.5, y=.06, yend=.06), col="grey30", size=0.5, curvature = 0.8, arrow = arrow(length = unit(0.03, "npc")))+

  scale_y_continuous(NULL, limits=c(0,.135),breaks=NULL)+
  labs(x="mean height (cm)") +
  theme_minimal()+
  scale_x_continuous("mean height (cm)",breaks=seq(-10,10,5),labels=seq(-10,10,5)+165)

```


:::

:::


::: {.callout-important collapse="false"}
# Measurement!

Detailed below are various methods that allow us to be flexible with regard to the distributional assumptions we have to hold when using linear regression. 

The elephant in the room is that _whatever_ we do, we should be cognizant of the validity of our measurements. If we're not measuring what we think we're measuring, then we're in trouble. 

:::

# Transformation of the outcome variable.  

A somewhat outdated approach, transforming our outcome variable prior to fitting the model, using something such as `log(y)` or `sqrt(y)`, will sometimes allow us to estimate a model for which our assumptions are satisfied.  

```{r}
#| echo: false
#| label: fig-trouble1
#| fig-cap: "A model of a transformed outcome variable can sometimes avoid violations of assumptions that arise when modeling the outcome variable directly. Data from https://uoepsy.github.io/data/trouble1.csv"

# set.seed(4)
# df <- tibble(
#   x = runif(100,1,100),
#   ly = x*.01 + rnorm(100),
#   y = exp(ly)
# )
# write_csv(df %>% select(x,y), file="../../uoepsy/data/trouble1.csv")
df <- read_csv("https://uoepsy.github.io/data/trouble1.csv") |> mutate(ly=log(y))
par(mfrow=c(2,2))
hist(df$y, breaks=30, main="marginal distribution of y",xlab="y")
with(df,plot(y~x, main="lm(y ~ x)\nassumptions violated",xlab="x",ylab="y"))
abline(lm(y~x,df),col="red")
hist(df$ly,  breaks=30, main="marginal distribution of log(y)",xlab="log(y)")
with(df,plot(ly~x, main="lm(log(y) ~ x)\nassumptions satisfied",xlab="x",ylab="log(y)"))
abline(lm(ly~x,df),col="red")
```

The major downside of this is that we are no longer modelling $y$, but some transformation $f(y)$ ($y$ with some function $f$ applied to it). Interpretation of the coefficients changes accordingly, such that we are no longer talking in terms of changes in y, but changes in $f(y)$. When the transformation function used is non-linear (see the Right-Hand of @fig-logtr) a change in $f(y)$ is **not the same** for every $y$. 

```{r}
#| label: fig-logtr
#| fig-cap: "The log transformation is non-linear"
#| fig-height: 3.5
#| echo: false
par(mfrow=c(1,2))
with(df,plot(y,2*y,main="y and 2*(y)",xlab="y",ylab="2(y)"))
with(df,plot(y,ly,main="y and log(y)",xlab="y",ylab="log(y)"))
```

Finding the optimal transformation to use can be difficult, but there are methods out there to help you. One such method is the BoxCox transformation, which can be conducted using `BoxCox(variable, lambda="auto")`, from the __forecast__ package.^[This method finds an appropriate value for $\lambda$ such that the transformation $(sign(x) |x|^{\lambda}-1)/\lambda$ results in a close to normal distribution.] 

For certain transformations, we _can_ re-express coefficients to be interpretable with respect to $y$ itself. For instance, the model using a log transform $ln(y) = b_0 + b_1(x)$ gives us a coefficient that represents statement __A__ below. We can re-express this by taking the opposite function to logarithm, the exponent, `exp()`. Similar to how this works in logistic regression, the exponentiated coefficients obtained from `exp(coef(model))` are _multiplicative_, meaning we can say something such as statement __B__

:::int

- __A:__ "a 1 unit change in $x$ is associated with a $b$ unit change in $ln(y)$".  
- __B:__ "a 1 unit change in $x$ is associated with $e^b$ __percent__ change in $y$."

:::



# Heteroscedastic robust standard errors (Huber White)

Often, when faced with residuals that appear to violate our assumption of constant variance (also called "homoscedasticity" or "equal variance"), a suitable option is simply to apply a correction to ensure that we make the correct inferences. 

There are various alternative calculations of standard errors that are robust to non-constance variance (or "heteroscedasticity"). The most commonly used are the "Huber-White" standard errors^[This is a special formulation of something called a 'Sandwich' estimator!], which are robust to heteroscedasticity and/or non-normality. Fortunately, we don't have to do any complicated calculations ourselves, as R will do all of the hard work for us. 

```{r}
#| echo: false
#| label: fig-trouble2
#| fig-height: 3
#| fig-cap: "Residual plots showing heteroscedasticity. The residuals vs fitted plot shows fanning out of residuals, and this is reflected in the scale-location plot showing the increasing variance. Data from https://uoepsy.github.io/data/trouble2.csv"

# set.seed(4)
# df <- tibble(
#   x = rep(1:5,e=20),
#   x2 = sample(letters[1:3],100,T),
#   y = unlist(map(1:5, ~rnorm(20, .56*., 1*.))) + (x2=="b")
# )
# write_csv(df,file="../../uoepsy/data/trouble2.csv")
df <- read_csv("https://uoepsy.github.io/data/trouble2.csv")
mod <- lm(y ~ 1 + x + x2, df)
par(mfrow=c(1,3));plot(mod,which=1:3);par(mfrow=c(1,1))
```

Our original __uncorrected__ standard errors:  
```{r}
troubledf2 <- read_csv("https://uoepsy.github.io/data/trouble2.csv")
mod <- lm(y ~ 1 + x + x2, data = troubledf2)
summary(mod)$coefficients
```

Using the __lmtest__ and __sandwich__ packages, we can use the Huber-White estimation to do both coefficient tests and model comparisons. 

:::panelset
:::panel
## Tests of the coefficients

```{r}
library(lmtest)
library(sandwich)
coeftest(mod, vcov = vcovHC(mod, type = "HC0"))
```
:::
:::panel
## Model comparisons

```{r}
mod_res <- lm(y ~ 1 + x, data = troubledf2)
mod_unres <- lm(y ~ 1 + x + x2, data = troubledf2)
waldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = "HC0"))
```
:::
:::

# Weighted Least Squares (WLS)

If we have some specific belief that our non-constant variance is due to differences in the variances of the outcome between various groups, then we might be better suited to use __Weighted Least Squares__.   

As an example, imagine we are looking at weight of different dog breeds (@fig-dogweight). The weights of chihuahuas are all quite close together (between 2 to 5kg), but the weight of, for example, spaniels is anywhere from 8 to 25kg - a much bigger variance. 

```{r}
#| echo: false
#| label: fig-dogweight
#| fig-cap: "The weights of 49 dogs, of 7 breeds"
set.seed(4)
df <- tibble(
  breed = c("chihuahua","boxer","pug","spaniel","golden retriever","beagle","lurcher"),
  m = c(8,70,18,24,70,30,45),
  sd = c(2,10,6,24,15,8,5)
  ) |>
  mutate(
    weight = map2(m,sd, ~rnorm(7,.x,.y)*0.454)
  ) |> select(breed,weight) |> unnest()
df$breed = factor(df$breed, levels=c("beagle","pug","spaniel","chihuahua","boxer","golden retriever","lurcher"))
dogdf<-df
#save(dogdf, file="../../uoepsy/data/dogweight.RData")


df_ms = df |> group_by(breed) |> summarise(m = round(mean(weight),1))
ggplot(df, aes(x=breed,y=weight))+
  geom_boxplot(fatten = NULL) + 
  stat_summary(geom="segment", 
               mapping=aes(xend=..x..-.375,yend=..y..,col=breed)) +
  stat_summary(geom="segment", 
               mapping=aes(xend=..x..+.375,yend=..y..,col=breed)) + 
  geom_label(data=df_ms,aes(label=m,y=m,col=breed))+
  guides(col="none") +
  labs(x="Dog Breed",y="Weight (kg)")

```

Recall that the default way that `lm()` deals with categorical predictors such as `dog breed`, is to compare each one to a reference level. In this case, that reference level is "beagle" (first in the alphabet). Looking at @fig-dogweight above, which comparison do you feel more confident in? 

- **A:** Beagles (14kg) vs Pugs (9.1kg). A difference of 4.9kg.  
- **B:** Beagles (14kg) vs Spaniels (19kg). A difference of 5kg.  

Hopefully, your intuition is that **A** looks like a clearer difference than **B** because there's less overlap between Beagles and Pugs than between Beagles and Spaniels. Our standard linear model, however, assumes the standard errors are identical for each comparison:  
```{r}
#| echo: false
dogmodel <- lm(weight~breed, dogdf)
.pp(summary(dogmodel),l = list(3,9:18))
```

Furthermore, we can see that we have heteroscedasticity in our residuals - the variance is not constant across the model:  
```{r}
#| echo: false
#| fig-height: 3
par(mfrow=c(1,2));plot(dogmodel,which=c(1,3));par(mfrow=c(1,1))
```

Weighted least squares is a method that allows us to apply weights to each observation, where the size of the weight indicates the precision of the information contained in that observation.  
We can, in our dog-breeds example, allocate different weights to each breed. Accordingly, the Chihuahuas are given higher weights (and so Chihuahua comparisons result in a smaller SE), and Spaniels and Retrievers are given lower weights. 

```{r}
#| eval: false
library(nlme)
load(url("https://uoepsy.github.io/data/dogweight.RData"))
dogmod_wls = gls(weight ~ breed, data = dogdf, 
                 weights = varIdent(form = ~ 1 | breed))
summary(dogmod_wls)
```
```{r}
#| echo: false
library(nlme)
dogmod_wls = gls(weight ~ breed, data = dogdf, weights = varIdent(form = ~ 1 | breed))
.pp(summary(dogmod_wls),l = list(16:25))
```

We _can_ also apply weights that change according to continuous predictors (e.g. observations with a smaller value of $x$ are given more weight than observations with larger values). 

# Bootstrap

The bootstrap method is an alternative non-parametric method of constructing a standard error. <!-- It is asymptotically equivalent to the Huber-White corrected standard error discussed above.--> Instead of having to rely on calculating the standard error with a formula and potentially applying fancy mathematical corrections, bootstrapping involves mimicking the idea of "repeatedly sampling from the population". It does so by repeatedly **re**sampling **with replacement** from our _original sample_.  
What this means is that we don't have to rely on any assumptions about our model residuals, because we actually generate an actual distribution that we can take as an approximation of our sampling distribution, meaning that we can actually _look_ at where 95% of the distribution falls, without having to rely on any summing of squared deviations.  


- Step 1: Resample with replacement from the sample.  
- Step 2: Fit the model to the resample from Step 1, and obtain a coefficient estimate.  
- Step 3: Repeat Steps 1 and 2 thousands of times, to get thousands of estimates.  
- Step 4: The distribution of all our bootstrap estimates will approximate the sampling distribution of the coefficient. It gives us an idea of "what we would get if we collected another sample of the same size". We can use the standard deviation of these estimates as our standard error.  

We can do this really easily in R, as there are various packages/functions that do it all for us. 
For instance, we might have the following model:  

```{r}
#| eval: false
troubledf3 <- read_csv("https://uoepsy.github.io/data/trouble3.csv")
mod <- lm(y ~ x + x2, data = troubledf3)
summary(mod)
```

```{r}
#| echo: false
set.seed(nchar("strap on your boots")*0275)
df <- tibble(
  x = runif(100,1,5),
  x2 = sample(letters[1:4],100,TRUE),
  y = 0.33*x+rchisq(100,2)
)
# write_csv(df, "../../uoepsy/data/trouble3.csv")
# troubledf3 <- read_csv("https://uoepsy.github.io/data/trouble3.csv")
troubledf3<-df
mod <- lm(y ~ x + x2, data = troubledf3)
.pp(summary(mod), l=list(1,3,9:16))
```

However, a quick look at our residual plots (@fig-trouble3) give us reason to hesitate. We can see clear deviations from normality in the QQ-plot. 

```{r}
#| echo: false
#| label: fig-trouble3
#| fig-cap: "Residual plots showing non-normality in the residuals. The points in the QQplot _should_ follow the diagonal line, but they don't. This is also shown in plotting a histogram of the residuals (bottom right). Data from https://uoepsy.github.io/data/trouble3.csv"
par(mfrow=c(2,2));plot(mod,which=1:3);hist(resid(mod));par(mfrow=c(1,1))
```


:::panelset
:::panel
## Boostrapped Coefficients

We can get out some bootstrapped confidence intervals for our coefficients using the __car__ package:  

```{r}
library(car)
# bootstrap our model coefficients
boot_mod <- Boot(mod)
# compute confidence intervals
Confint(boot_mod)
```

:::
:::panel
## Bootstrapped ANOVA

If we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the _incremental_ addition of each predictor, we can get bootstrapped p-values from the `ANOVA.boot` function in the __lmboot__ package. 

Our original ANOVA:
```{r}
anova( lm(y~x+x2, data = df) )
```

And our bootstrapped p-values:  
```{r}
library(lmboot)
my_anova <- ANOVA.boot(y~x+x2, data = df, 
                       B = 1000)
# these are our bootstrapped p-values:
my_anova$`p-values`

#let's put them alongside our original ANOVA table:
cbind(
  anova( lm(y~x+x2, data = df) ),
  p_bootstrap = c(my_anova$`p-values`,NA)
)
```

:::
:::panel
## Other things  

We can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.  

For example, to bootstrap the $R^2$ for the model `lm(y~x+x2)`, we would create a little function called `rsq`:  
```{r}
rsq <- function(data, indices){
  # this is the bootstrap resample
  bdata <- data[indices,]
  # this is the model, fitted to the resample
  fit <- lm(y ~ x + x2, data = bdata)
  # this returns the R squared
  return(summary(fit)$r.square)
}
```

We then use the __boot__ package, giving 1) our original data and 2) our custom function to the `boot()` function, and compute some confidence intervals:  
```{r}
library(boot)
bootrsq_results <- boot(data = df, statistic = rsq, R = 1000)
boot.ci(bootrsq_results, type = "bca")
```

:::

:::


::: {.callout-caution collapse="true"}
## Cautions

- The bootstrap may give no clue there is bias, when the cause is lack of model fit.
- The bootstrap does not "help with small samples". Bootstrap distributions tend to be slightly too narrow (by a factor of $\sqrt{(n-1)/n}$ for a single mean), and can actually perform _worse_ than standard methods for small samples.  
- If your sample is not representative of the population, bootstrapping will not help at all.  
- There are many different types of bootstrap (e.g. we can resample the observations, or we can resample the residuals), and also different ways we can compute confidence intervals (e.g. take percentiles of the bootstrap distribution, or take the standard deviation of the bootstrap distribution, or others).  


:::




# Non-Parametric Tests

Many of the standard hypothesis tests that we have seen ($t$-tests, correlations etc) have got equivalent tests that, instead of examining change in $y$, examine change in something like $rank(y)$^[or $sign( rank(|y|) )$]. By analyising ranked data, we don't have to rely on the same distributional assumptions (ranked data always follows a uniform distribution), and any outliers will have exert less influence on our results.  
However, these tests tend to be less powerful (if there _is_ a true effect in the population, these tests have a lower chance of detecting it), and you lose a lot of interpretability.  

Wilcoxon tests are a non-parametric equivalent to the various $t$-tests, and the Kruskal-Wallis test is the non-parametric version of the one-way ANOVA.  


| parametric     | non-parametric       |
| --------------------------------- | ------------------------------- |
| one sample t-test<br>`t.test(y)`                                    | wilcoxon signed-rank test<br>`wilcox.test(y)`   |
| paired sample t-test<br>`t.test(y1,y2,paired=TRUE)` or<br>`t.test(y2-y1)` | wilcoxon matched pairs<br>`wilcox.test(y1,y2,paired=TRUE)` or<br>`wilcox.test(y2-y1)` |
| independent samples t-test<br>`t.test(y1, y2)` or<br>`t.test(y ~ group)`  | mann-whitney U <br>`wilcox.test(y1, y2)` or<br>`wilcox.test(y ~ group)`| 
| one-way ANOVA<br>`anova(lm(y ~ g))`<br>post-hoc tests:<br>`TukeyHSD(aov(y ~ g))` | kruskal-wallis<br>`kruskal.test(y ~ g)`<br>post-hoc tests:<br>`library(FSA)`<br>`dunnTest(y ~ g)` | 



