---
title: "2A: Inference for MLM"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
```

:::lo
This reading:  

Conducting inference (i.e. getting confidence intervals or p-values, model comparisons) for MLMs can be tricky partly because there are a variety of different methods that have been developed.  
This reading briefly explains why getting p-values from `lmer()` is not as easy as it was for `lm()`, before giving an outline of some of the main approaches people tend to take. Don't feel like you have to remember all of these, just be aware that they exist, and refer back to this page whenever you need to.  


:::

# Inference

The term "inference" is used to refer to the process of moving from beyond a description of our specific sample to being able to make statements about the broader population from which we have sampled. 

In the framework for statistics that we have been learning, this centers on the idea of _sample statistics that we might see in the long run_ (i.e. if we did the experiment again and again and again with a new sample each time, see @fig-se3). In USMR, we saw various ways in which this logic was applied, combining the observed sample statistic with the standard error to create a _test statistic_ ($z$, $t$, $\chi^2$, $F$, then compared to the appropriate standard distribution). 

```{r}
#| echo: false
#| label: fig-se3
#| fig-cap: "The standard error is the standard deviation of the 'sampling distribution' - the distribution of sample statistics that we _could_ see. We use this to ask how likely we are to see our observed sample in a universe where the null hypothesis is true. This probability gives us reason to reject (or not) said null hypothesis."
#| fig-height: 3.5
set.seed(2394)
samplemeans <- seq(-5,5,.1)
g <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+
  #geom_histogram(alpha=.3)+
  stat_function(geom="line",fun=~dnorm(.x, mean=0,sd=1),lwd=1)


ld <- layer_data(g) |> filter(x <= 1 & x >= -1)
ld2 <- layer_data(g) |> filter(x <= 2 & x >= -2)
ld3 <- layer_data(g) |> filter(x >= 2.5)

g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_area(data=ld3,aes(x=x,y=y),fill="red",alpha=.1) +
  geom_vline(xintercept=2.5,col="red")+
  annotate("text",x=3,y=.3,label="observed\nsample statistic",col="red",hjust=0,vjust=0)+
  geom_curve(aes(x=3, xend=2.5, y=.3, yend=.2), col="red", size=0.5, 
             curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_vline(xintercept = 0, col="black",lty="dashed", lwd=.5) +
  annotate("text",
           x=-2, y=.3, 
           label="Null Hypothesis", col="black",
           hjust=1)+
  geom_curve(aes(x=-2, xend=0, y=.3, yend=.3), col="black", size=0.5, 
             curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  
  geom_segment(x=0,xend=-1,y=.15,yend=.15) +
  annotate("text",x=-3.5, y=.16, label="Standard Error (SE)\n(standard deviation of\nsampling distribution)", col="grey30")+
  geom_curve(aes(x=-2.3, xend=-.5, y=.2, yend=.15), col="grey30", size=0.5, curvature = -0.3, arrow = arrow(length = unit(0.03, "npc")))+

  scale_y_continuous(NULL,breaks=NULL)+
  theme_minimal()+
  scale_x_continuous("sample means under the null hypothesis",breaks=NULL) 
  
  

```

In the linear models we were fitting with `lm()`, these were $t$ (for the coefficient estimate) and $F$ (the reduction in residual sums of squares) tests, and accordingly they had an associated degrees of freedom. If we fit a linear model `lm(y~x)` to 10 datapoints, then our tests would have $10-2=8$^[$n$ observations minus $k$ parameters (slope of `x`) minus 1 intercept] degrees of freedom, and test statistics would be compared against, e.g. a $t$ distribution with 8 degrees of freedom. Alternatively, if we were to fit that model to 100 datapoints, we would be working with 98 degrees of freedom. The degrees of freedom reflects the fact that there is more variability in statistics from smaller samples. Another way of thinking of degrees of freedom is that they are the number of independent datapoints that are left "free to vary" around our model parameters. 

But we are now working with multilevel data, and in the scenario where we have, e.g. $n_p$ pupils clustered into $n_s$ schools, how many independent bits of information do we have to begin with? Is it $n_p$? Or $n_s$? Or somewhere in between? Our random effects are not "free to vary" in the sense that they are estimated under certain constraints (such as following a normal distribution).  

In very specific situations that correspond to classical experimental designs (in which, e.g., we have perfectly balanced numbers across experimental factors and equal sizes within groups) it is possible to conduct similar $F$ tests (and hence $t$-tests too) with a known degrees of freedom. Unfortunately, transferring this to more general scenarios is problematic (e.g., any missing data, unbalanced designs, more complex random effect structures). Partly because defining the necessarily follow an $F$ distribution with _any_ degrees of freedom. It is for these reasons that the author of the __lme4__ removed p-values from the output.     

However, there are various strategies that we can use to conduct inferences that either attempt to approximate the degrees of freedom, or use an alternative method based on, e.g., likelihoods or bootstrapping.  

Below, we'll go through each method in R, applying it to the following model (recall this is the model we ended with in reading [1B](01b_lmm.html#a-more-complex-model){target="_blank"}).  
```{r}
library(tidyverse)
library(lme4)

schoolmot <- read_csv("https://uoepsy.github.io/data/schoolmot.csv")

smod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), 
              data = schoolmot)
```

::: {.callout-note collapse="true"}
#### df approximations (Satterthwaite)

Two methods have been suggested as approximations for the denominator degrees of freedom for multilevel models. The maths behind these are pretty intense, so we're going to focus on how to implement them in R, and emphasise some of their respective benefits/drawbacks.  

The Satterthwaite approximation is easily implemented by packages such as __lmerTest__. This package simply overwrites the `lmer()` function to use a version that has the degrees of freedom and associated p-values displayed in the summary.  

It can be used for models fitted with either ML or REML, and generally scales well, so if you are fitting models to big datasets, it won't take too much extra time.  

::::panelset
:::panel
#### tests of single parameters

```{r}
smod3sat <- lmerTest::lmer(grade ~ motiv * funding + 
                (1 + motiv | schoolid), 
              data = schoolmot)
summary(smod3sat)
```

:::int
__Reporting__  

... degrees of freedom in the coefficients tests have been corrected via Satterthwaite's method.   
...   
...   
The association between childrens' motivation level and their school grades was moderated by the type of school attended (state/private), with a 1 unit increase in motivation associated with an additional 2.85 point increase for children in state school in comparison to those attending private schools ($b = 2.85,\ SE = 1.06,\ t(26.54^*) = 2.69,\ p = .012$).

:::



:::
:::panel
#### model comparisons

You can conduct model comparisons with an $F$ test and the Satterthwaite df approximation using the function `SATmodcomp()` from the __pbkrtest__ package:  

```{r}
smod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), 
              data = schoolmot)
smod3_res <- lmer(grade ~ motiv + funding + (1 + motiv | schoolid), 
              data = schoolmot)
library(pbkrtest)
SATmodcomp(largeModel = smod3, smallModel = smod3_res)
```

:::int
__Reporting__  

... the interaction between motivation level and school funding resulted in a significant improvement in model fit ($F(1,26.54^*)=7.23, p=.012$, with degrees of freedom approximated using the Satterthwaite method).  

:::


:::
::::

:::


::: {.callout-note collapse="true"}
#### df approximations (Kenward Rogers)  

The Kenward Rogers (KR) approximation involves a correcting the standard errors for small samples, and then approximating the degrees of freedom similarly to Satterthwaite. Because the standard errors are adjusted in KR, $t$-statistics will be slightly different too. 

The KR approach is generally a good option for smaller sample sizes. The adjustment for smaller samples in KR relies on estimates obtained via REML, which means that to use this method we must fit models with `REML=TRUE`. 
One thing to note is that the calculation can be computationally demanding, and so as n increases, it will take more and more time to implement.  

::::panelset
:::panel
#### tests of single parameters

We can use the __parameters__ package to get out tests of coefficients using the KR method. It displays both confidence intervals and p-values:  
```{r}
library(parameters)
model_parameters(smod3, ci_method="kr")
```

:::int
__Reporting__  

... standard errors and degrees of freedom for the coefficients tests have been corrected via the Kenward Rogers method.   
...   
...   
The association between childrens' motivation level and their school grades was moderated by the type of school attended (state/private), with a 1 unit increase in motivation associated with an additional 2.85 point increase for children in state school in comparison to those attending private schools ($b = 2.85,\ SE = 1.07,\ t(28.79^*) = 2.67,\ p = .012$).

:::


:::
:::panel
#### model comparisons

And the __pbkrtest__ package allows for the model comparison:  
```{r}
smod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), 
              data = schoolmot, REML=TRUE)
smod3_res <- lmer(grade ~ motiv + funding + (1 + motiv | schoolid), 
              data = schoolmot, REML=TRUE)
library(pbkrtest)
KRmodcomp(largeModel = smod3, smallModel = smod3_res)
```


:::int
__Reporting__  

... the interaction between motivation level and school funding resulted in a significant improvement in model fit ($F(1,28.79^*)=7.12, p=.012$, with degrees of freedom approximated using the Kenward Rogers method).  

:::


:::
::::

:::


::: {.callout-note collapse="true"}
#### likelihood based methods 

Remember that multilevel models are typically fitted using maximum likelihood estimation - i.e. a process that iteratively tries to find the set of estimates that result in the greatest probability of observing the data that we have observed (@fig-mlee).  

```{r}
#| label: fig-mlee
#| fig-cap: "likelihood is the probability of observing the data, given some model"
#| out-height: "200px"
#| out-wight: "200px"
#| echo: false
knitr::include_graphics("images/mle_single.png")
```

There are two main things to be aware of with likelihood based methods.    

- Because these methods rely on the likelihood, then in in order to assess significance of fixed effects, models must be fitted with `REML=FALSE` (functions like `anova()` and `confint()` shown below will re-fit models for you!). This is because when using REML, the likelihood is indexing the fit of the random effects only. 

- Comparisons of two likelihoods (i.e. likelihood ratio tests) are only __asymptotically__ $\chi^2$ distributed (i.e. as $n \rightarrow \infty$), meaning that that this may not be appropriate for smaller sample sizes.  


::::panelset
:::panel
#### likelihood ratio tests (LRT)

If we consider two competing models, e.g., one with an interaction in it and one without, then we can examine how the inclusion of the model changes the likelihood of seeing our data.  

```{r}
smod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), 
              data = schoolmot, REML = FALSE)
smod3_res <- lmer(grade ~ motiv + funding + (1 + motiv | schoolid), 
              data = schoolmot, REML = FALSE)
anova(smod3_res, smod3) # a likelihood ratio test!  
```

We can see the (log)likelihood of the two models, which have been multiplied by -2 to get "deviance", and the _difference_ in the deviance is under the 'Chisq' column of the output, with the associated degrees of freedom (how many parameters we've added) under the 'Df' column. Differences in two deviances are asymptotically $\chi^2$ distributed, and under this assumption we can compare the change in deviance between our two models to the appropriate $\chi^2$ distribution in order to obtain a p-value.  

:::int
__Reporting__  

... the interaction between motivation level and school funding resulted in a significant improvement in model fit, as indicated by a likelihood ratio test ($\chi^2(1)=6.84,p=.009$). 
:::


:::
:::panel
#### profile likelihood confidence intervals

Another way in which we can use likelihoods is to construct confidence intervals around each parameter. Rather than simply comparing two distinct likelihoods (i.e. two models), we can create a profile of the curvature of the likelihood surface around an estimate when holding other parameters constant. If the curvature is sharp, we have more certainty in the estimate, whereas if it is gradual, we have less certainty. We can use this to create confidence intervals. 

```{r}
confint(smod3, method = "profile")
```

:::int
__Reporting__  

... the association between childrens' motivation level and their school grades was moderated by the type of school attended (state/private), with a 1 unit increase in motivation associated with an additional 2.8 point increase for children in state school in comparison to those attending private schools ($b = 2.8$, 95% profile likelihood CI $[0.75, 4.90]$).

:::

:::
::::


:::

::: {.callout-note collapse="true"}
#### parametric bootstrap

There are also various "bootstrapping" methods which it is worth looking into. Think back to USMR when we first learned about hypothesis testing. Remember that we did some simulating of data, so that we could compare what we actually observe with what we would expect if the null hypothesis were true? By doing this, we were essentially _creating_ a null distribution, so that calculating a p-value can become an issue of summarising data (e.g. calculate the proportion of our simulated null distribution that is more extreme than our observed statistic).  

We can use this same logic to perform tests or construct confidence intervals for multilevel models. However, this particular flavour of _parametric_ bootstrapping does not involve resampling with replacement from our data. Instead, it involves 1) simulating data from our model parameters, then 2) fitting model(s) to that simulated data to get an estimate, then 3) using the distribution of estimates obtained from doing steps 1 and 2 a thousand times.  

Some key things to note:

- This can be time consuming! and might not work well depending on how stable your random effect variances are (i.e. if some variance estimates are close to 0, some of the bootstrap iterations may fail).  
- Parametric bootstrapping has all the normal assumptions of the multilevel model (which we'll learn about next week) - by simulating from the model, we're assuming the model distributions ($\zeta_{0i} \sim N(0,\sigma_0)$, $\varepsilon \sim N(0,\sigma_e)$ etc.) are correct. 

::::panelset
:::panel
#### parametric bootstrapped likelihood ratio test

Instead of assuming that the likelihood ratio test statistics are $\chi^2$-distributed, we can bootstrap this test instead. This approach simulates data from the simpler model, fits both the simple model and the complex model and evaluates the change in log-likelihood. By doing this over and over again, we build a distribution of what changes in log-likelihood we would be likely to see if the more complex model is not any better. In this way it actually constructs a distribution reflecting our null hypothesis, against which we can then compare our actual observed effect:  

The __pbkrtest__ package does this for us:  
```{r}
#| eval: false
smod3 <- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), 
              data = schoolmot)
smod3_res <- lmer(grade ~ motiv + funding + (1 + motiv | schoolid), 
              data = schoolmot)
library(pbkrtest)
PBmodcomp(largeModel = smod3, smallModel = smod3_res)
```
```{r}
#| echo: false
library(pbkrtest)
load("data/pbres.rdata")
pbres
```

:::int
__Reporting__  

... the interaction between motivation level and school funding resulted in a significant improvement in model fit, as indicated by a parametric bootstrapped likelihood ratio test ($\Delta2LL=6.84,p=.013$). 
:::



:::
:::panel
#### parametric bootstrapped CIs

We can easily get parametric bootstrapped confidence intervals from `confint()`:  

```{r}
#| eval: false
confint(smod3, method="boot")
```
```{r}
#| echo: false
load("data/pbci.rdata")
pbci
```

:::int
__Reporting__  

... the association between childrens' motivation level and their school grades was moderated by the type of school attended (state/private), with a 1 unit increase in motivation associated with an additional 2.8 point increase for children in state school in comparison to those attending private schools ($b = 2.8$, 95% parametric bootstrapped CI $[0.69, 5.06]$).

:::

:::

::::

:::



## Summary


|  | df approximations | likelihood based | parametric bootstrap |
| ---- | ---- | ---- | ---- |
| tests/CIs of individual parameters | Tests of individual parameters can be done by refitting with `lmerTest::lmer(...)` for the Satterthwaite (S) method, or using `parameters::model_parameters(model, ci_method="kr")` for Kenward Rogers (KR). | Profile likelihood CIs for individual parameters can be obtained via `confint(m, method="profile")`, but this can be computationally demanding. | Parametric Bootstrapped CIs for individual parameters can be obtained via `confint(m, method="boot")` |
| model comparisons<br><small>(different fixed effects, same random effects)</small> | Comparisons of models that differ _only_ in their fixed effects can be done via $F$ tests in the __pbkrtest__ package:<br>`SATmodcomp(m2, m1)` for S and `KRmodcomp(m2, m1)` for KR. | Comparisons of models that differ _only_ in their fixed effects can be done via LRT using `anova(m1, m2)` | Comparisons of models that differ _only_ in their fixed effects can be done via a bootstrapped LRT using `PBmodcomp(m2, m1)` from the __pbkrtest__ package. |
|  | For KR, models must be fitted with `REML=TRUE` (a good option for small samples). For S, models can be fitted with either. | For likelihood based methods for fixed effects, models must be fitted with `REML=FALSE`.<br>Likelihood based methods are asymptotic (i.e. hold when $n \rightarrow \infty$). Best avoided with smaller sample sizes (i.e. a small number of clusters)  | Time consuming, but considered best available method (can be problematic with unstable models) |



::: {.callout-caution collapse="true"}
#### optional: testing random effects?

Tests of random effects are difficult because the null hypothesis (the random effect variance is zero) lies on a boundary (you can't have a negative variance). Comparisons of models that differ _only_ in their random effects can be done by comparing ratio of likelihoods when fitted with `REML=TRUE` (this has to be done manually), but these tests should be treated with caution.   

We _can_ obtain confidence intervals for our random effect variances using both the profile likelihood and the parametric boostrap methods discussed above.  

As random effects are typically part of the experimental design, there is often little need to test their significance. In most cases, the maximal random effect structure can be conceptualised without reference to the data or any tests, and the inclusion/exclusion of specific random effects is more a matter of what simplifications are required for the model to converge. Inclusion/exclusion of parameters based on significance testing is rarely, if ever a sensible approach.  

:::












