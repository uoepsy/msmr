---
title: "More SEM"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
library(tidyverse)
library(lavaan)
source('assets/setup.R')
options(digits=3)
library(pander)
panderOptions('round', 3)
panderOptions('keep.trailing.zeros', TRUE)
```

In this final week we are going to address some practical issues that commonly arise in Structural Equation Modeling: Non-normality, ordinal-categorical endogenous variables, and missingness. After that, we're going to look briefly into how to simulate data based on a structural equation model.

# Practical issues in SEM

One of the key assumptions of the models we have been fitting so far is that our variables are all normally distributed, and are continuous. Why?  

Recall how we first introduced the idea of estimating a SEM: comparing the **observed covariance matrix** with our **model implied covariance matrix**. This heavily relies on the assumption that our observed covariance matrix provides a good representation of our data - i.e., that $var(x_i)$ and $cov(x_i,x_j)$ are adequate representations of the relations between variables.  
Unfortunately, this is true if we have a "multivariate normal distribution", but becomes more difficult when our variables are either not continuous or not normally distributed. 

:::frame
**Multivariate Normal Distribution (MVN)**

The multivariate normal distribution is fundamentally just the extension of our univariate normal (the bell-shaped curve we are used to seeing) to more dimensions. The condition which needs to be met is that every linear combination of the $k$ variables has a univariate normal distribution. It is denoted by $N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ (note that the bold font indicates that $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ are, respectively, a vector of means and a matrix of covariances).  
The idea of the multivariate normal can be a bit difficult to get to grips with in part because there's not an intuitive way to visualise it once we move beyond 2 or [3 variables](https://demonstrations.wolfram.com/JointDensityOfTrivariateGaussianRandomVariables/).  

```{r echo=FALSE, fig.cap="bivariate normal"}
x <- mvtnorm::rmvnorm(n = 1e3, mean = c(0,0), sigma = matrix(c(4,2,2,3), ncol = 2))
d <- as.data.frame(x)
d$density <- mvtnorm::dmvnorm(x = d)
library(plotly)
plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) %>% 
  add_markers()
```
You may also see this represented by a contour plot, which should look similar to the above 3D plot viewed from above. 
```{r echo=FALSE}
ggplot(d,aes(x=V1,y=V2))+
  geom_point(alpha=.2)+
  geom_density_2d()
```


`r optbegin("Optional: covariance matrix fails to capture non-normality",olabel=FALSE,toggle=params$TOGGLE)`

Suppose we had the following variables: 
```{r echo=FALSE}
op <- list(xi=c(0,1), Psi=matrix(c(2,2,2,3), 2, 2), lambda=c(4, -2))
rnd <- sn::rmsn(1e3, dp=sn::op2dp(op,"SN"))
d<-as.data.frame(rnd)
d$density <- mvtnorm::dmvnorm(x = d)
psych::multi.hist(d[,1:2])
```
The joint probability distribution of these two variables can be visualised in Figure \@ref(fig:mvnn). 
```{r mvnn, echo=FALSE, fig.cap="Joint distribution of some skewed variables"}
plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) %>% 
  add_markers()

```

If we compute our covariance matrix from these variables, we get:
```{r echo=FALSE}
cov(d[,1:2])
```
But this fails to capture information about important properties of the data relating to features such as skew (lop-sided-ness) and kurtosis (pointiness). If we simulate data based on this covariance matrix, we get something like the below, which is clearly limited in its reflection of our actual data.  
```{r echo=FALSE, fig.cap="Probability distribution of simulations based on a covariance matrix (of some skewed variables)"}
d <- as.data.frame(mvtnorm::rmvnorm(n = 1e3, mean = c(0,0), sigma = cov(d[,1:2])))
d$density <- mvtnorm::dmvnorm(x = d)
plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) %>% 
  add_markers()
```
`r optend()`

:::

:::frame
__Prenatal Stress & IQ data__  

A researcher is interested in the effects of prenatal stress on child cognitive outcomes.  
She has a 5-item measure of prenatal stress and a 5 subtest measure of child cognitive ability, collected for 500 mother-infant dyads. 

+ The data is available as a .csv file here: [https://uoepsy.github.io/data/stressIQ.csv](https://uoepsy.github.io/data/stressIQ.csv)


:::

`r qbegin("A1")`
Before we do anything with the data, grab some paper and sketch out the full model that you plan to fit to address the researcher's question.   

Tip: everything you need is in the description of the data. Start by drawing the specific path(s) of interest. Are these between latent variables? If so, add in the paths to the observed variables for each latent variable.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The main parameter of interest here is "the effects of prenatal stress on child cognitive outcomes". So we have an arrow going from Stress to IQ.  
Each of these are latent variables, for which we have observed 5 indicator variables, so we have an arrow going from "IQ" to each of the 5 IQ items, and from "Stress" to the 5 stress items. 
```{r echo=FALSE}
diagmod <- '
#IQ measurement model
IQ=~IQ1+IQ2+IQ3+IQ4+IQ5 
#stress measurement model 
Stress=~stress1+stress2+stress3+stress4+stress5 
#structural part of model
IQ~Stress'
semPlot::semPaths(lavaanify(diagmod),rotation=2)
```


`r solend()`

`r qbegin("A2")`

Okay, let's get into the data then.  
Read in the data and explore it.  

You may remember the `pairs.panels()` function from the **psych** package. Another useful one from the same package is `multi.hist()`.  

Pay attention to the scales on which the items are measured.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
Notice that the prenatal stress questionnaire items are measured on a 3-point scale.  
```{r message=FALSE}
library(tidyverse)
library(psych)

stress_IQ_data <- read_csv("https://uoepsy.github.io/data/stressIQ.csv")

describe(stress_IQ_data) # from psych package
multi.hist(stress_IQ_data, global = FALSE) # from psych package
```

The argument global = TRUE would use the same x-axis for all histograms, and FALSE otherwise.
`r solend()`

:::frame
__Ordered-Categorical Endogenous Variables__  

Sometimes we can treat ordinal data as if it is continuous. When exactly this is appropriate is a contentious issue - some statisticians might maintain that ordinal data is simply __not__ continuous, so we should never treat it as such. In psychology, much research using SEM centers around questionnaire data, which lends itself to *likert* data (for instance, "strongly agree","agree","neither agree nor disagree","disagree","strongly disagree"). An often used rule of thumb, is that likert data with $\geq 5$ levels can be treated as if they are continuous without unduly influencing results (see [Johnson, D.R., & Creech, J.C. (1983). Ordinal measures in multiple indicator models: A simulation study of categorization error](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_2307_2095231)).  

:::blue
**What we can do**  

In R, **lavaan** will automatically switch to a categorical estimator if we tell it that we have some ordered-categorical variables. We can use the `ordered = c("item1name","item2name","item3name", ...)` argument.  
This is true for both the `cfa()` and `sem()` functions.  

:::

:::

`r qbegin("A3")`
The prenatal stress questionnaire items are measured on a 3-point scale.   

Fit a one-factor confirmatory factor analysis for the latent factor of Stress, specifying any ordered-categorical variables in order to use an appropriate estimation method.  
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r categorical estimation}
library(lavaan)
library(semPlot)
# specify the model
model_stress <- 'Stress =~ stress1 + stress2 + stress3 + stress4 + stress5'

# estimate the model - cfa will automatically switch to a categorical estimator if we mention that our five variables are ordered-categorical, using the 'ordered' function
model_stress.est <- 
  cfa(model_stress, data=stress_IQ_data,
      ordered=c('stress1','stress2','stress3','stress4','stress5'))

```

`r solend()`

`r qbegin("A4")`
Inspect your model output. Notice that you have an extra column - we now have 'robust' values for the fit statistics (those that appear in the right-hand column under 'robust') to check that our model fits well.  

Another new thing we have is the presence of 'thresholds' in our output. These are two thresholds per item in this example because we have a three-point response scale. Estimation of ordered-categorical variables involves constructing a hypothetical continuum underlying our categories, with "thresholds" being the points on this continuum where an observation moves from being one category to the next. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
# inspect the output
summary(model_stress.est, fit.measures=T, standardized=T)
```
`r solend()`

:::frame
__Non-normality__  

****

For determining what consititutes deviations from normality, there are various measures of both skewness and kurtosis (see [Joanes, D.N. and Gill, C.A (1998). Comparing measures of sample skewness and kurtosis](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_1111_1467_9884_00122) if you're interested). In addition, there are various suggested rules of thumb we can follow. Below are the most common:  


Skewness rules of thumb:  

- $|skew| < 0.5$: fairly symmetrical
- $0.5 < |skew| < 1$: moderately skewed
- $1 < |skew|$: highly skewed

Kurtosis rule of thumb:

- $Kurtosis > 3$: Heavier tails than the normal distribution. Possibly problematic


:::blue
**What we can do**

When faced with variables which appear to deviate from normality, we should ideally use a robust estimator that corrects for any bias in the standard errors induced by non-normality, while also providing a corrected $\chi^2$ statistic to more accurately capture the misfit of our model. 

There are a few robust estimators in **lavaan**, but one of the more frequently used ones is "MLR" (maximum likelihood with robust SEs). You can find all the other options at [https://lavaan.ugent.be/tutorial/est.html](https://lavaan.ugent.be/tutorial/est.html).  
We can make use of these with: `sem(model, estimator="MLR")` or `cfa(model, estimator="MLR")`.  

:::

:::

`r qbegin("A5")`
We're now going to conduct a CFA for the IQ items.  
  
Check their distributions (both numerically and visually) and fit a one-factor CFA using an appropriate estimation method.   

__Tip:__ the `describe()` function from the __psych__ package will give you measures of skew and kurtosis.  
`r qend()` 

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We should check the item distributions for evidence of non-normality (skewness and kurtosis). We can use the `describe()` function from psych and plot the data using histograms or density curves.

```{r CFA for IQ items}
library(kableExtra)

# this is just describe() but dressed up to make a nice table output (the one we see below)
stress_IQ_data %>% 
    select(contains("IQ")) %>% 
    describe %>% 
    as.data.frame() %>%
    rownames_to_column(., var = "variable") %>% 
    select(variable,mean,sd,skew,kurtosis) %>%
    kable(digits = 2) %>%
    kable_styling(full_width = FALSE)
```

It looks like some of our IQ items are pretty skewed. Let's plot them. 

```{r}
stress_IQ_data %>% 
  select(contains("IQ")) %>% 
  multi.hist
```

If you want a ggplot way:
```{r}
## GGPLOT
# temporarily reshape the data to long format to make it quicker to plot
stress_IQ_data %>% 
  pivot_longer(IQ1:IQ5, names_to="variable",values_to="score") %>%
  ggplot(aes(x=score))+
  geom_density()+
  facet_wrap(~variable)+
  theme_light()
```

Because our variables seem to be non-normal, therefore, we should use a robust estimator such as MLR for our CFA

```{r robust estimator}
model_IQ <- 'IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5'

model_IQ.est <- cfa(model_IQ, data=stress_IQ_data, estimator='MLR')
```

`r solend()`

`r qbegin("A6")`
Examine the fit of the CFA model.  
If it doesn't fit very well, consider checking for areas of local misfit (i.e., check your `modindices()`), and adjust your model accordingly.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
summary(model_IQ.est, fit.measures=T, standardized=T)
```

The model doesn't fit very well so we could check the modification indices for local mis-specifications

```{r check mods}
modindices(model_IQ.est, sort=T)
```

It looks like we might need to include residual covariances between subtests 1 and 2 and between subtests 4 and 5, though if this were a real analysis we would want to double check this makes substantive sense (for example, do subtests 1 and 2 both measure memory while subtests 4 and 5 both test spatial ability?)

```{r make modifications}
model2_IQ <- '
    IQ=~IQ1+IQ2+IQ3+IQ4+IQ5
    IQ1~~IQ2
    IQ4~~IQ5
'
model2_IQ.est <- cfa(model2_IQ, data=stress_IQ_data, estimator='MLR')
```

The fit of the model is now much improved!
```{r}
summary(model2_IQ.est, fit.measures=T, standardized=T)
```

`r solend()`


`r qbegin("A7")`
Now its time to build a full SEM. 
Estimate the effect of prenatal stress on IQ.  

**Remember:** We know that IQ is non-normal, so we would like to use a robust estimator (e.g. MLR). However, as lavaan will tell you if you try using `estimator="MLR"`, this is not supported for ordered data (i.e., the Stress items). It suggests instead using the WLSMV (weighted least square mean and variance adjusted) estimator. The WLSMV estimator **is the default estimator** in lavaan when ordered categorical variables are present. The WLSMV estimator is DWLS with a correction to return robust standard errors. In fact, if you notice in the output, you have a "robust" column too. Hence, you're all good for non-normality as you have robust standard errors!

Specify and estimate your SEM model. 

`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r full SEM}
SEM_model <- '
    #IQ measurement model
    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 
    IQ1 ~~ IQ2
    IQ4 ~~ IQ5

    #stress measurement model 
    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 

    #structural part of model
    IQ ~ Stress
'
```


```{r}
SEM_model.est <- sem(SEM_model, data=stress_IQ_data,
                     ordered=c('stress1','stress2','stress3','stress4','stress5'),
                     estimator="WLSMV")

summary(SEM_model.est, fit.measures=T, standardized=T)
```
```{r echo=FALSE}
d <- partable(SEM_model.est)
```



Don't be misled by the fact that the summary here says that the estimator is DWLS. When we have *any* ordered-categorical endogenous variables in the model lavaan uses DWLS (diagonally weighted least squares) estimation for the model parameters. However, because we specified it in fitting our model, WLSMV is being used to correct the standard errors.  

We can see that the effect of prenatal stress on offspring IQ is $\beta$ = `r round(d %>% filter(lhs=="IQ",rhs=="Stress") %>% pull(est), 3)` and is statistically significant at $p<.05$.

`r solend()`

# Missing Data

:::yellow
**Missingness**

It is very common to have missing data. Participants may stop halfway through the study, may decline to be followed up (if it is longitudinal) or may simply decline to answer certain sections. In addition, missing data can occur for all sorts of technical reasons (e.g, website crash and auto-submit a questionnaire, etc.). 

It is important to understand the possible reasons for missing data in order to appropriately consider what data you *do* have. If missing data are missing completely random, then the data you do have should still be representative of the population. But suppose you are studying cognitive decline in an aging population, and people who are suffering from cognitive impairment are less likely to attend the follow-up assessments. Is this missingness random? No. Does it affect how you should consider your available data to represent the population of interest? Yes. 

There are three main explanations for missing data:

- **MCAR: Missing Completely At Random.** Data which are MCAR are missing data for which the propensity for missingness is completely independent of any observed or unobserved variables. It is truly random if the data are missing or not.  

- **MAR: Missing At Random.** Data which are MAR are missing data for which the propensity for missingness is not random, but it can be fully explained by some variables for which there is complete data. In other words, there is a systematic relationship between missing values and observed values. For example, people who are unemployed at time of assessment will likely not respond to questions on job satisfaction. Missing values on job satisfaction is unrelated to the levels of job satisfaction, but related to their employment status. 

- **MNAR: Missing Not At Random.** Data which are MNAR are missing data for which the propensity for missingness is related to the value which is missing. For example, suppose an employer tells employees that there are a limited number of bonuses to hand out, and then employees are asked to fill out a questionnaire. Thos who are less satisfied with their job may not respond to questions on job satisfaction (if they believe it will negatively impact their chances of receiving a bonus). 

<small>  
*To me, these are some of the most confusing terms in statistics, because "at random" is used to mean "not completely random"!?? It might be easier to think of "missing at random" as "missing conditionally at random", but then it gives the same acronym as "completely at random"!*
</small>

:::

:::blue
**FIML (Full Information Maximum Likelihood)**

<div style="display:inline-block; width: 60%; vertical-align:top;">

One approach to dealing with missing data (not discussed in depth here) is *imputation*. This involves substituting missing values with values predicted by some model of the process which reflects the data generating process for that variable (sometimes including some stochasticity in these imputed values, sometimes not).^[You will often see things like *mean imputation* and *median imputation*, but think carefully about why this might not be a great approach. It assumes that a single value is exactly the value which we would have observed if it were not missing. The lack of variability around this estimate of missing values will shrink the standard errors because it is assumed that no deviations exist among the substituted values.]

In SEM, there is an extremely useful method known as **full information maximum likelihood** (FIML) which, similar way to imputation, uses observed data to supplement missingness. Intuitively, this is a bit like using all rest of the picture to fill in the missing bits (e.g. Figure \@ref(fig:dougalmiss)). 
</div>
<div style="display:inline-block; width: 30%;vertical-align:middle;">
```{r dougalmiss, echo=FALSE, out.width="300px", fig.cap="Missingness"}
knitr::include_graphics("images/missing.png")
```
</div>

FIML utilises *all* observed variables (hence "full information") to estimate missing values by maximising the likelihood of the sample as a function of the joint probability distribution of all variables. What is really neat about this is that it allows us to make full use of all our data, as it estimates missing values on all variables in our system, regardless of whether they are exogenous/endogenous variables. Compare this to functions such as `lm()` and `lmer()`, in which missing values on our explanatory variables result in simple list-wise deletion from the model. A downside, one could argue, is that you may have specific theoretical considerations about which observed variables should weigh in on estimating missingness in variable $x$ (rather than *all* variables), in which case imputation techniques may be preferable.
  
In lavaan, we can make use of full information maximum likelihood by using `missing = "FIML"` in the functions `cfa()` and `sem()`. 
:::

`r qbegin("A8")`
In order to try and replicate the IQ CFA, our researcher collects a new sample of size $n=500$. However, she has some missing data (specifically, those who scored poorly on earlier tests tended to feel discouraged and chose not to complete further tests).  
  
Read in the new dataset, plot and numerically summarise the univariate distributions of the measured variables, and then conduct a CFA using the new data, taking account of the missingness (don't forget to also use an appropriate estimator to account for any non-normality). Does the model fit well?    
  
+ The data can be found at [https://uoepsy.github.io/data/IQdatam.csv](https://uoepsy.github.io/data/IQdatam.csv), and is in .csv format. 
 
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can fit the model setting `missing='FIML'`. If data are missing at random (MAR) - i.e., missingness is related to the measured variables but not the unobserved missing values - then this gives us unbiased parameter estimates. Unfortunately we can never know whether data are MAR for sure as this would require knowledge of the missing values. 

```{r message=FALSE}
IQ_data_new <- read_csv("https://uoepsy.github.io/data/IQdatam.csv")
multi.hist(IQ_data_new, global = FALSE)

IQ_data_new %>% select(contains("IQ")) %>% 
    describe %>% 
    as.data.frame() %>%
    rownames_to_column(., var = "variable") %>% 
    select(variable,mean,sd,skew,kurtosis) %>%
    kable(digits = 2) %>%
    kable_styling(full_width = FALSE)
```

```{r missingness}
IQ_model_missing <- '
  IQ=~IQ1+IQ2+IQ3+IQ4+IQ5
  IQ1~~IQ2
  IQ4~~IQ5
'

IQ_model_missing.est <- cfa(IQ_model_missing, 
                            data=IQ_data_new, 
                            missing='FIML', estimator="MLR")

summary(IQ_model_missing.est, fit.measures=T, standardized=T)
```

Our fit indices all look very good!  
`r solend()`


# Simulating Data  

:::yellow
**simulating data as an aid to learning**  

An *extremely* useful approach to learning both R and statistics is to create yourself some fake data on which you can try things out. Because you create the data, you can control any relationships, group differences etc. In doing so, you can make yourself a target to aim for.  

Many of you will currently be in the process of collecting/acquiring data for your thesis. If you are yet to obtain your data, we **strongly** recommend that you start to simulate some data with the expected distributions in order to play around and test how your analyses works, and how to interpret the results.  

For myself, I estimate that I generate fake data on average several times a week just to help me work out things I don't understand. It also enables for easy sharing of reproducible chunks of code which can perfectly replicate issues and help discussions. 

:::

We're going to walk through the process of fitting a structural equation model by first simulating some data.  

The data simulated was generated with the following parameters (expressed in lavaan syntax). You might describe this as the "data generating process". The goal of statistics is to shed light on the data generating process when we don't know what it is (when we have real data).  

We're going to simulate some data for which the follow model structure holds well:  

```
likes_tea =~ rate_breakfast + rate_darjeeling + rate_mint + rate_chamomile
likes_biscuits =~ rate_oreo + rate_digestive + rate_custardcream + rate_bourbon + rate_crackers
rate_bourbon ~~ rate_oreo

likes_biscuits ~ likes_tea
```

As you can see, in the data generating process, there are 2 factors each measured by 4 or 5 items. Factor "likes_tea" reflects people's enjoyment of tea, which is measured by various ratings of specific teas (these things are the things we would directly measure). The second factor is the same idea, but for biscuits.  
We've then regressed liking biscuits onto liking tea (e.g. do people who like tea more, also like biscuits more?). Additionally, we've added in a covariance between ratings of oreos with ratings of bourbons, suggesting that these are related in some way _beyond_ their representation of "liking biscuits" (perhaps this covariance is the preference for specifically chocolate flavours?).  

You can find the data at [https://uoepsy.github.io/data/teabiscuit.csv](https://uoepsy.github.io/data/teabiscuit.csv), or run the code below to simulate it yourself.  

The __lavaan__ package makes it really easy to simulate data, we just need to specify the magnitude of the paths, and give it to the `simulateData()` function.  

Before simulating random data, however, it is good practice to set the random seed to ensure you will be able to reproduce your results. If you place the line `set.seed(<any whole number>)` at the start of your R code, every time you will run your file line by line and in order, you will get the same results.

```{r echo=TRUE}
set.seed(987)
mdl <- "
likes_tea =~ 0.9*rate_breakfast + 0.7*rate_darjeeling + 0.6*rate_mint + 0.6*rate_chamomile
likes_biscuits =~ 0.8*rate_oreo + 0.8*rate_digestive + 0.7*rate_custardcream + 0.9*rate_bourbon + 0.3*rate_crackers
rate_bourbon ~~ 0.4*rate_oreo

likes_biscuits ~ 0.3*likes_tea
"
semPaths(lavaanify(mdl),rotation=2)

df <- simulateData(mdl, sample.nobs = 300)
```


`r qbegin("B1: Open-ended")`
Now that we have our data, we can try fitting models to it. Doing research with real data is like coming in at this point, and _not knowing_ the model that was used to generate the data.   

Try playing around and fitting different models to the data above, and evaluating how well they fit. What happens if you misspecify the measurement model? What happens if you leave out estimating the covariance between bourbons and oreos? Check your modification indices.  
`r qend()`

# Optional: Extensions of SEM

We have really only scraped the surface of the different things we can do with SEM. If you are interested in taking you learning further, then some of the next things to start looking into:   

  - Multigroup analysis (testing the model across two or more populations)  
    - Jöreskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36(4), 409-426.  
    - Sorbom, D. (1974). A general method for studying differences in factor means and factor structures between groups. British Journal of Mathematical and Statistical Psychology, 27, 229-239.  

  - Latent Growth Curves (actually just the same as a multilevel model!! &#129327; )  
    - [Michael Clark has a great lot of resources on this](https://m-clark.github.io/mixed-models-with-R/supplemental.html), and it makes the link between random effects and latent variables super clear.  

<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

