---
title: "Week 11 Exercises: More SEM!"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
qcounter <- function(){
  if(!exists("qcounter_i")){
    qcounter_i <<- 1
  }else{
    qcounter_i <<- qcounter_i + 1
  }
  qcounter_i
}
library(psych)
library(semPlot)
library(lavaan)
```




:::frame
__Prenatal Stress & IQ data__  

A researcher is interested in the effects of prenatal stress on child cognitive outcomes.  
She has a 5-item measure of prenatal stress and a 5 subtest measure of child cognitive ability, collected for 500 mother-infant dyads. 

+ The data is available as a .csv file here: [https://uoepsy.github.io/data/stressIQ.csv](https://uoepsy.github.io/data/stressIQ.csv)

```{r}
#| echo: false
tibble(
  variable = names(read_csv("https://uoepsy.github.io/data/stressIQ.csv")),
  description = c("Participant ID",
                  "acute stress",
                  "chronic stress",
                  "environmental stress",
                  "psychological stress",
                  "physiological stress",
                  "verbal ability",
                  "verbal memory",
                  "inductive reasoning",
                  "spatial orientiation",
                  "perceptual speed")
) |> gt::gt()
```


:::

`r qbegin(qcounter())`
Before we do anything with the data, grab some paper and sketch out the full model that you plan to fit to address the researcher's question.   

Tip: everything you need is in the description of the data. Start by drawing the specific path(s) of interest. Are these between latent variables? If so, add in the paths to the indicators for each latent variable.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The main parameter of interest here is "the effects of prenatal stress on child cognitive outcomes". So we have an arrow going from Stress to IQ.  
Each of these are latent variables, for which we have observed 5 indicator variables, so we have an arrow going from "IQ" to each of the 5 IQ items, and from "Stress" to the 5 stress items. 
```{r echo=FALSE}
diagmod <- '
#IQ measurement model
IQ=~IQ1+IQ2+IQ3+IQ4+IQ5 
#stress measurement model 
Stress=~stress1+stress2+stress3+stress4+stress5 
#structural part of model
IQ~Stress'
semPlot::semPaths(lavaanify(diagmod),rotation=1)
```


`r solend()`

`r qbegin(qcounter())`
Read in the data and explore it. Look at the individual distributions of each variable to get a sense of univariate normality, as well as the number of response options each item has.  


::: {.callout-tip collapse="true"}
#### Hints

The `multi.hist()` function from the __psych__ package is pretty useful for getting quick histograms of multiple variables. If necessary, you can set `multi.hist(data, global = FALSE)` to let each histogram's x-axis be on a different scale.

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


```{r message=FALSE}
library(tidyverse)
library(psych)

stress_IQ_data <- read_csv("https://uoepsy.github.io/data/stressIQ.csv")
```

This gives us a whole load of descriptives, means, standard deviations, and other things like skew and kurtosis.  
```{r}
describe(stress_IQ_data) # from psych package
```

Here we can see the distribution of each variable. Some immediate things of note - the stress items are only measured on 3 response options, and the IQ items are quite positively skewed (we can see this matches with the skew metrics above).  
```{r}
multi.hist(stress_IQ_data, global = FALSE) # from psych package
```
`r solend()`

## Non-normality


`r qbegin(qcounter())`
Fit a one factor CFA for the IQ items using an appropriate estimation method.   
  
Be sure to first check their distributions (both numerically and visually). The `describe()` function from the __psych__ package will give you measures of skew and kurtosis.  



::: {.callout-note collapse="true"}
#### Multivariate Normality (MVN)


One of the key assumptions of the models we have been fitting so far is that our variables are all normally distributed, and are continuous. Why?  

Recall how we first introduced the idea of estimating a SEM: comparing the **observed covariance matrix** with our **model implied covariance matrix**. This heavily relies on the assumption that our observed covariance matrix provides a good representation of our data - i.e., that $var(x_i)$ and $cov(x_i,x_j)$ are adequate representations of the relations between variables. While this is true if we have a "multivariate normal distribution", things become more difficult when our variables are either not continuous or not normally distributed. 

The multivariate normal distribution is fundamentally just the extension of our univariate normal (the bell-shaped curve we are used to seeing) to more dimensions. The condition which needs to be met is that every linear combination of the $k$ variables has a univariate normal distribution. It is denoted by $N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ (note that the bold font indicates that $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ are, respectively, a vector of means and a matrix of covariances).^[We've actually seen this previously in multilevel modelling, when we had random intercepts, random slopes, and the covariance between the two!]  
The idea of the multivariate normal can be a bit difficult to get to grips with in part because there's not an intuitive way to visualise it once we move beyond 2 or [3 variables](https://demonstrations.wolfram.com/JointDensityOfTrivariateGaussianRandomVariables/).  

With 2 variables, you can think of it just like a density plot but in 3 dimensions, as in @fig-bivnorm. You can also represent this as a "contour" plot, which is like looking down on @fig-bivnorm from above (just like a map!)  

```{r}
#| echo: false
#| label: fig-bivnorm
#| fig-cap: "Bivariate Normal"
#| out-width: "100%"
#| out-height: "300px"
x <- mvtnorm::rmvnorm(n = 1e3, mean = c(0,0), sigma = matrix(c(4,2,2,3), ncol = 2))
d <- as.data.frame(x)
d$density <- mvtnorm::dmvnorm(x = d)
library(plotly)
plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) |> 
  add_markers()
```


Now let's look at a situation where we have some skewed variables. @fig-mvnn shows such a distribution, and @fig-bivnon shows the contour plot (i.e. it is like @fig-mvnn viewed from above). 

::::panelset
:::panel
#### 3D

```{r}
#| echo: false
#| label: fig-mvnn
#| fig-cap: "Joint distribution of some skewed variables"
#| out-width: "100%"
#| out-height: "300px"
op <- list(xi=c(0,1), Psi=matrix(c(2,2,2,3), 2, 2), lambda=c(6, -1.5))
rnd <- sn::rmsn(1e3, dp=sn::op2dp(op,"SN"))
d<-as.data.frame(rnd)
d$density <- mvtnorm::dmvnorm(x = d)

plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) |> 
  add_markers()
```

:::
:::panel
#### Contour

```{r}
#| label: fig-bivnon
#| fig-cap: "Contour plot for a non-normal bivariate distribution. Marginal distributions of each variable are presented along each axis"
#| echo: false
p1 <- ggplot(d,aes(x=V1,y=V2))+
  geom_point(alpha=.2)+
  geom_density_2d()
ggExtra::ggMarginal(p1, type="histogram")
```

:::
::::


::: {.callout-caution collapse="true"}
#### simulations from a covariance matrix

If we compute our covariance matrix from the 2 skewed variables, we get:
```{r}
#| echo: false
cov(d[,1:2])
```
But this fails to capture information about important properties of the data relating to features such as skew (lop-sided-ness) and kurtosis (pointiness). As an example, if we were to simulate data based on this covariance matrix, we get something like @fig-bivsim, which is clearly limited in its reflection of our actual data.  

::::panelset
:::panel
#### 3D
```{r }
#| echo: false
#| label: fig-bivsim
#| fig-cap: "Probability distribution of simulations based on a covariance matrix (of some skewed variables). The covariance matrix contains only 2 'moments' of the variables: their scale (spread) and location (center)"
#| out-width: "100%"
#| out-height: "300px"
d <- as.data.frame(mvtnorm::rmvnorm(n = 1e3, mean = apply(d,2,mean)[1:2], sigma = cov(d[,1:2])))
d$density <- mvtnorm::dmvnorm(x = d)
plot_ly(d, x = ~ V1, y = ~ V2, z = ~ density,
              marker = list(color = ~ density,
                            showscale = TRUE)) |> 
  add_markers()
```

:::
:::panel
#### Contour
```{r}
#| label: fig-bivnon2
#| fig-cap: "Contour plot for simulated data from a covariance matrix"
#| echo: false
p1 <- ggplot(d,aes(x=V1,y=V2))+
  geom_point(alpha=.2)+
  geom_density_2d()
ggExtra::ggMarginal(p1, type="histogram")
```
:::
::::

:::



:::


::: {.callout-note collapse="true"}
#### Non-normality & Robust Estimation

For determining what constitutes deviations from normality, there are various ways to calculate the skewness and kurtosis of univariate distributions (see [Joanes, D.N. and Gill, C.A (1998). Comparing measures of sample skewness and kurtosis](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_1111_1467_9884_00122) if you're interested). In addition, there are various suggested rules of thumb we can follow. Below are the most common:  

Skewness rules of thumb:  

- $|skew| < 0.5$: fairly symmetrical
- $0.5 < |skew| < 1$: moderately skewed
- $1 < |skew|$: highly skewed

Kurtosis rule of thumb:

- $Kurtosis > 3$: Heavier tails than the normal distribution. Possibly problematic


:::blue
**What we can do**

In the event of non-normality, we can still use maximum likelihood to find our estimated factor loadings, coefficients and covariances. However, our standard errors (and our $\chi^2$ model fit) will be biased. We can use a robust maximum likelihood estimator that essentially applies corrections to the standard errors^[These are similar corrections to ones that we apply in a regression world, see [LM Troubleshooting](00_lm_assumpt.html#heteroscedastic-robust-standard-errors-huber-white){target="_blank"}] and $\chi^2$ statistic.  

There are a few robust estimators in **lavaan**, but one of the more frequently used ones is "MLR" (maximum likelihood with robust SEs). You can find all the other options at [https://lavaan.ugent.be/tutorial/est.html](https://lavaan.ugent.be/tutorial/est.html).  
We can make use of these with: `sem(model, estimator="MLR")` or `cfa(model, estimator="MLR")`.  

:::

:::

`r qend()` 

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We should check the item distributions for evidence of non-normality (skewness and kurtosis). We can use the `describe()` function from psych and plot the data using histograms or density curves.

```{r}
#| code-fold: true
library(kableExtra)

# this is just describe() but dressed up to make a nice table output (the one we see below)
stress_IQ_data |> 
    select(contains("IQ")) |> 
    describe() |> 
    as.data.frame() |>
    rownames_to_column(var = "variable") |> 
    select(variable,mean,sd,skew,kurtosis) |>
    kable(digits = 2) |>
    kable_styling(full_width = FALSE)
```

It looks like some of our IQ items are pretty skewed. Let's plot them. 

::::panelset
:::panel
#### multi.hist

```{r}
stress_IQ_data |> 
  select(contains("IQ")) |> 
  multi.hist()
```

:::
:::panel
#### ggplot

```{r}
## GGPLOT
# temporarily reshape the data to long format to make it quicker to plot
stress_IQ_data |> 
  pivot_longer(IQ1:IQ5, names_to="variable",values_to="score") |>
  ggplot(aes(x=score))+
  geom_density()+
  facet_wrap(~variable)+
  theme_light()
```

:::
::::

Because our variables seem to be non-normal, therefore, we should use a robust estimator such as MLR for our CFA

```{r robust estimator}
model_IQ <- 'IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5'

model_IQ.est <- cfa(model_IQ, data=stress_IQ_data, estimator='MLR')
```

`r solend()`

`r qbegin(qcounter())`
Examine the fit of the CFA model of IQ.  
If it doesn't fit very well, consider checking for areas of local misfit (i.e., check your `modindices()`), and adjust your model accordingly.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

We also get out robust fit measures, so we should ask for them:
```{r}
fitmeasures(model_IQ.est)[c("rmsea.robust","srmr","cfi.robust","tli.robust")]
```

The model doesn't fit very well so we could check the modification indices for local mis-specifications

```{r check mods}
modindices(model_IQ.est, sort=T) |> head()
```

It looks like we might need to include residual covariances between the items IQ1 and IQ2 and maybe also between items IQ4 and IQ5. 
As always, we need to double check this makes substantive sense. Items IQ1 and IQ2 measure verbal comprehension and verbal memory - people might be likely to score low/high on both of these due to their verbal ability. IQ4 and IQ5 might be both related due to both being tests requiring visual perception, but it's less obvious. We'd probably want to know more about the specific tests undertaken. 

```{r make modifications}
model2_IQ <- '
    IQ=~IQ1+IQ2+IQ3+IQ4+IQ5
    IQ1~~IQ2
'
model2_IQ.est <- cfa(model2_IQ, data=stress_IQ_data, estimator='MLR')
```

The fit of the model is now much improved! and our loadings are all significant and $>|0.3|$.   
The RMSEA is still in that grey area between 0.05 and 0.08, so we would probably want to flag this when writing up. We could keep trying to add stuff in order to get it below 0.05, but that means a high risk of overfitting.  
```{r}
fitmeasures(model2_IQ.est)[c("rmsea.robust","srmr","cfi.robust","tli.robust")]

summary(model2_IQ.est, standardized=T)
```

`r solend()`

## Ordered Categoricals


`r qbegin(qcounter())`
Fit a one-factor confirmatory factor analysis for the latent factor of Stress. Note that the items are measured on a 3-point scale!  

::: {.callout-note collapse="true"}
#### Ordered-Categorical Endogenous Variables

Sometimes we can treat ordinal data as if it is continuous. When exactly this is appropriate is a contentious issue - some statisticians might maintain that it is *never* appropriate to treat ordinal data as continuous. In psychology, much research using SEM centers around questionnaire data, which lends itself to *likert* data (for instance, "strongly agree","agree","neither agree nor disagree","disagree","strongly disagree"). An often used rule of thumb is that likert data with $\geq 5$ levels can be treated as if they are continuous without unduly influencing results (see [Johnson, D.R., & Creech, J.C. (1983). Ordinal measures in multiple indicator models: A simulation study of categorization error](https://discovered.ed.ac.uk/permalink/f/1s15qcp/TN_cdi_crossref_primary_10_2307_2095231)).  

But what if we don't have 5+ levels? An important note is that even if your questionnaire included 5 response options for people to choose from, if participants only _used_ 3 of them, then you actually have a variable with just 3 levels. 

Estimation techniques exist that allow us to model such variables by considering the set of ordered responses to correspond to portions of an underlying continuum. This involves estimating the 'thresholds' of that underlying distribution at which people switch from giving one response to the next. 

::::{.columns}
:::{.column width="40%"}
Rather than working with correlations, this method involves using "polychoric correlations", which are estimates of the correlation between two theorized normally distributed continuous variables, based on their observed ordinal manifestations. 

:::
:::{.column width="60%"}
```{r}
#| echo: false
#| label: fig-polychor
#| fig-cap: "the idea of polychoric correlation is to estimate the correlation between two unobserved continuous variables by examining the distributions across the set of ordered categories that we *do* observe"
set.seed(444)
df <- tibble(
  item1 = rnorm(1e3),
  item2 = .6*item1 + rnorm(1e3)
)
df <- as.data.frame(apply(df,2,scale))
df2 <- apply(df,2,function(x) cut(x,3,labels=c("disagree","neither","agree")))
names(df) <- paste0("l",names(df))
df <- cbind(df,df2)


thresh1 <- df |> group_by(item1) |>
  summarise(
    min = min(litem1),
    max = max(litem1),
    lab = max-((max-min)/2)
  )

thresh2 <- df |> group_by(item2) |>
  summarise(
    min = min(litem2),
    max = max(litem2),
    lab = max-((max-min)/2)
  )

ggplot(df,aes(x=litem1,y=litem2,col=interaction(item1,item2)))+
  geom_point(size=3,alpha=.5)+
  guides(col="none") +
  geom_vline(data=thresh1[-2,], aes(xintercept=min))+
  geom_hline(data=thresh2[-2,], aes(yintercept=min))+
  geom_text(data=thresh1,inherit.aes=F,aes(x=lab,y=-3,label=item1))+
  geom_text(data=thresh2,inherit.aes=F,aes(y=lab,x=-3.3,label=item2),angle=90)+
  labs(x="underlying agreement with item 1",
       y="underlying agreement with item 2")
```
:::

::::


:::blue
**What we can do**  

In R, **lavaan** will automatically switch to a categorical estimator (called "DWLS"^[Diagonally Weighted Least Squares]) if we tell it that we have some ordered-categorical variables. We can use the `ordered = c("item1name","item2name","item3name", ...)` argument.  
This is true for both the `cfa()` and `sem()` functions.  




:::

:::




`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


```{r categorical estimation}
library(lavaan)
library(semPlot)
# specify the model
model_stress <- 'Stress =~ stress1 + stress2 + stress3 + stress4 + stress5'

# estimate the model - cfa will automatically switch to a categorical estimator if we mention that our five variables are ordered-categorical, using the 'ordered' function
model_stress.est <- 
  cfa(model_stress, data=stress_IQ_data,
      ordered=c('stress1','stress2','stress3','stress4','stress5'))

```

`r solend()`


`r qbegin(qcounter())`
Inspect your summary model output. Notice that we have a couple of additional things - we have 'scaled' and 'robust' values for the fit statistics (we have a second column for all the fit indices but using a scaled version of the $\chi^2$ statistic, and then we also have some extra rows of 'robust' measures), and we have the estimated 'thresholds' in our output (there are two thresholds per item in this example because we have a three-point response scale). The estimates themselves are not of great interest to us.  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
# inspect the output
fitmeasures(model_stress.est)[c("rmsea.robust","srmr","cfi.robust","tli.robust")]
summary(model_stress.est, standardized=TRUE)
```
`r solend()`


`r qbegin(qcounter())`
Now its time to build the full SEM.   
Estimate the effect of prenatal stress on IQ.  



::: {.callout-tip collapse="true"}
#### Hints

**Remember:** We know that IQ is non-normal, so we would like to use a robust estimator (e.g. MLR). However, as lavaan will tell you if you try using `estimator="MLR"`, this is not supported for ordered data (i.e., the Stress items). It suggests instead using the WLSMV (weighted least square mean and variance adjusted) estimator.  

As it happens, the WLSMV estimator is just DWLS with a correction to return robust standard errors. If you specify `estimator="WLSMV"` then your standard errors *will* be corrected, but don't be misled by the fact that the summary here will still say that the estimator is DWLS.   

:::

`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r full SEM}
SEM_model <- '
    #IQ measurement model
    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 
    IQ1 ~~ IQ2
    IQ4 ~~ IQ5

    #stress measurement model 
    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 

    #structural part of model
    IQ ~ Stress
'
```


```{r}
SEM_model.est <- sem(SEM_model, data=stress_IQ_data,
                     ordered=c('stress1','stress2','stress3','stress4','stress5'),
                     estimator="WLSMV")
```

Let's print out the full summary.  
Note that when we have a mix of ordered categoricals _and_ continuous variables, then we can't get the robust estimates of the fit indices. These are now NA. We do still get the scaled versions though:  
```{r}
summary(SEM_model.est, fit.measures=T, standardized=T)
```
```{r}
#| echo: false
d <- partable(SEM_model.est)
```


We can see that the effect of prenatal stress on offspring IQ is $\beta$ = `r round(d |> filter(lhs=="IQ",rhs=="Stress") |> pull(est), 3)` and is statistically significant at $p<.05$.

`r solend()`

## Missingness


`r qbegin(qcounter())`
In order to try and replicate the IQ CFA, our researcher collects a **new** sample of size $n=500$. However, she has some missing data (specifically, those who scored poorly on earlier tests tended to feel discouraged and chose not to complete further tests).  
  
Read in the new dataset, plot and numerically summarise the univariate distributions of the measured variables, and then conduct a CFA using the new data, taking account of the missingness (don't forget to also use an appropriate estimator to account for any non-normality). Does the model fit well?    
  
+ The data can be found at [https://uoepsy.github.io/data/IQdatam.csv](https://uoepsy.github.io/data/IQdatam.csv), and is in .csv format.  
 
 
::: {.callout-note collapse="true"}
#### Missingness

It is very common to have missing data. Participants may stop halfway through the study, may decline to be followed up (if it is longitudinal) or may simply decline to answer certain sections. In addition, missing data can occur for all sorts of technical reasons (e.g, website crash and auto-submit a questionnaire, etc.). 

It is important to understand the possible reasons for missing data in order to appropriately consider what data you *do* have. If missing data are missing completely random, then the data you do have should still be representative of the population. But suppose you are studying cognitive decline in an aging population, and people who are suffering from cognitive impairment are less likely to attend the follow-up assessments. Is this missingness random? No. Does it affect how you should consider your available data to represent the population of interest? Yes. 

There are three main reasons that we certain data may be missing, and the acronyms are a nightmare:^[To me, these are some of the most confusing terms in statistics, because "at random" is used to mean "not completely random"!?? It might be easier to think of "missing at random" as "missing conditionally at random", but then it gives the same acronym as "completely at random"!]  

- **MCAR: Missing Completely At Random.** Data which are MCAR are missing data for which the propensity for missingness is completely independent of any observed or unobserved variables. It is truly random if the data are missing or not.  

- **MAR: Missing At Random.** Data which are MAR are missing data for which the propensity for missingness is not random, but it can be fully explained by some variables for which there is complete data. In other words, there is a systematic relationship between missing values and observed values. For example, people who are unemployed at time of assessment will likely not respond to questions on work-life satisfaction. Missing values on work-life satisfaction is unrelated to the levels of work-life satisfaction, but related to their employment status. 

- **MNAR: Missing Not At Random.** Data which are MNAR are missing data for which the propensity for missingness is related to the value which is missing. For example, suppose an employer tells employees that there are a limited number of bonuses to hand out, and then employees are asked to fill out a questionnaire. Thos who are less satisfied with their job may not respond to questions on job satisfaction (if they believe it will negatively impact their chances of receiving a bonus). 

:::blue
**What we can do**

In SEM, there is an extremely useful method known as **full information maximum likelihood (FIML)** which, allows us to use all our available data (*including* those that have only _some_ values present).  

FIML separates data into the different patterns of missingness (i.e. those that are missing on `IQ1`, those missing on `IQ1` and `IQ2`, those missing on just `IQ2`, and so on). The likelihood of each group can be estimated from the parameters that are available for that pattern. The estimation process then finds the set of parameters that maximise the likelihood of the full dataset (we can sum up the loglikelihoods of the datsets with each missingness pattern). This way it utilises *all* observed variables (hence "full information") to estimate the model parameters.   

```{r}
#| include: false
#| echo: false
set.seed(345)
d <- data.frame(x=rnorm(1e5),y=rnorm(1e5)) |>
  filter(abs(x)<3,abs(y)<3)
kd <- with(d, MASS::kde2d(x,y,n=100))
kd$y[sample(1:length(kd$y),5)] <- NA
kd$x[sample(1:length(kd$x),5)] <- NA

library(plotly)
plot_ly(x=kd$x, y = kd$y, z = kd$z) |> 
  add_surface()
```

A downside, one could argue, is that you may have specific theoretical considerations about which observed variables *should* weigh in on estimating missingness in variable $x$ (rather than *all* variables), in which case imputation techniques may be preferable. FIML only provides unbiased estimates if the data are MAR (i.e. if the missingness depends on other variables present in the data).  
  
In lavaan, we can make use of full information maximum likelihood by using either `missing = "FIML"` or `missing = "ML"` (they're the same) in the functions `cfa()` and `sem()`.  

::: 


::: {.callout-caution collapse="true"}
#### optional: other approaches to missingness

Thus far in our analyses, we have been simply excluding any row for which there is missingness. This is typically known as "listwise deletion", and is the default for many modelling approaches. Any person who is missing _some_ data on a variable in our model will simply not be included. This method _assumes_ that the data are "Missing Completely At Random", because we make the assumption that studying only the people who have complete data does not bias our results in any way.  

Other traditional approaches are to use basic imputation methods such as "mean/median imputation" (replace missing values with the average of that variable). These can be okay, but they assume that the imputed values are **exactly** what we _would_ have observed had it not been missing, rather than estimates. The lack of variability around this imputed value will shrink the standard errors because it is assumed that no deviations exist among the substituted values. 
Another option is "regression imputation", where we replace missing values with the predicted values from a regression model of the variable with missingness onto some relevant predictors. This is better because our estimates of the values of missing observations could be much better, but we still make an assumption that these estimates are perfect, when we know they are just estimates.  

A more sophisticated approach to this is "Multiple Imputation" (MI). Multiple imputation is similar to the regression imputation approach described above in that it predicts the value of missing observations based on the covariates, but instead of predicting just one value, it predicts many. Rather than saying "the predicted value for this missing observation is 5", we say "the predicted value for this missing observation is from the distribution $\sim N(5,2)$". So we simulate $m$ draws from that distribution, and we end up with $m$ predicted values. We make $m$ copies of our data and replace the missing observations with each of our $m$ predicted values. We then do our analysis on each $m$ imputed dataset, and pool the results!  

:::

:::
 
 
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can fit the model setting `missing='FIML'`. If data are missing at random (MAR) - i.e., missingness is related to the measured variables but not the unobserved missing values - then this gives us unbiased parameter estimates. Unfortunately we can never know whether data are MAR for sure as this would require knowledge of the missing values. 

```{r message=FALSE}
IQ_data_new <- read_csv("https://uoepsy.github.io/data/IQdatam.csv")
multi.hist(IQ_data_new, global = FALSE)

IQ_data_new |> select(contains("IQ")) |> 
    describe() |> 
    as.data.frame() |>
    rownames_to_column(var = "variable") |> 
    select(variable,mean,sd,skew,kurtosis) |>
    kable(digits = 2) |>
    kable_styling(full_width = FALSE)
```

```{r missingness}
IQ_model_missing <- '
  IQ=~IQ1+IQ2+IQ3+IQ4+IQ5
  IQ1~~IQ2
  IQ4~~IQ5
'

IQ_model_missing.est <- cfa(IQ_model_missing, 
                            data=IQ_data_new, 
                            missing='FIML', estimator="MLR")

summary(IQ_model_missing.est, fit.measures=T, standardized=T)
```

Our fit indices all look very good!  
`r solend()`


`r qbegin(qcounter())`
Note that the summary of the model output when we used FIML also told us that there are 3 patterns of missingness.  

Can you find out a) what the patterns are, and b) how many people are in each pattern?  



::: {.callout-tip collapse="true"}
#### Hints

`is.na()` will help a lot here, as will `distinct()` and `count()`.  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


This will turn all the NAs into TRUE, and everything else into FALSE:  
```{r}
#| eval: false
is.na(IQ_data_new)
```

If we turn this back to a dataframe, and then pass it to `distinct()`, we get out the different patterns!  
```{r}
is.na(IQ_data_new) |>
  as.data.frame() |>
  distinct()
```

Getting the counts is more difficult, but one way would be to do it with `count()`, but we have to give it all of the variables, to get all the different combinations:  
```{r}
is.na(IQ_data_new) |>
  as.data.frame() |>
  count(ID1,IQ1,IQ2,IQ3,IQ4,IQ5)
```


`r solend()`


# Extras: Equivalent models

`r qbegin(paste0("Extra ", qcounter()))`
Returning to our full SEM, adjust your model so that instead of `IQ ~ Stress` we are fitting `Stress ~ IQ` (i.e. child cognitive ability $\rightarrow$ prenatal stress).  
Does this model fit better or worse than our first one?  

`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`

Here's our first model:  
```{r}
fitmeasures(SEM_model.est)[c("rmsea.scaled","srmr","tli.scaled","cfi.scaled")]
```

And here's the model with the direction reversed:  
```{r}
SEM_model2 <- '
    #IQ measurement model
    IQ =~ IQ1 + IQ2 + IQ3 + IQ4 + IQ5 
    IQ1 ~~ IQ2
    IQ4 ~~ IQ5
    
    #stress measurement model 
    Stress =~ stress1 + stress2 + stress3 + stress4 + stress5 
    
    #structural part of model
    Stress ~ IQ
'

SEM_model.est2 <- sem(SEM_model2, data=stress_IQ_data,
                    ordered = c('stress1', 'stress2', 'stress3', 'stress4', 'stress5'),
                    estimator="WLSMV")

fitmeasures(SEM_model.est2)[c("rmsea.scaled","srmr","tli.scaled","cfi.scaled")]
```


The fit is exactly the same!!  

One of the strengths of SEM is that we are fitting quantitative models that very closely correspond to theoretical models. In fact, researchers may often start with two slightly different theoretical models, and SEM will allow them to make a formal comparison of the two, because they will lead to differences in model fit. 

However, it is important to be aware that for every structural equation model, there are always equivalent representations of the model structure that leads to _precisely_ the same model fit. These are essentially re-parameterisations of our model - they contain the same information, and lead to the exact same model-implied covariance matrix, but they involve different theoretical assumptions. Many of these we can rule out on conceptual grounds (i.e. a child's cognitive ability cannot influence their mothers' pre-natal stress levels), but others we cannot.  

`r solend()`


# Extras: Simulating SEM data  


`r qbegin(paste0("Extra", qcounter()))`

::: {.callout-note collapse="true"}
#### Simulating data as an tool for learning

An *extremely* useful approach to learning both R and statistics is to create yourself some fake data on which you can try things out. Because you create the data, you can control any relationships, group differences etc. In doing so, you essentially make yourself a target to aim for.  

Many of you will currently be in the process of collecting/acquiring data for your thesis. If you are yet to obtain your data, we **strongly** recommend that you start to simulate some data with the expected distributions in order to play around and test how your analyses works, and how to interpret the results.  

I estimate that I generate fake data at least a couple of times every week just to help me work out things I don't understand. It also enables for easy sharing of reproducible chunks of code which can perfectly replicate issues and help discussions.  

While structural equation models are a fairly complex method, simulating data from a pre-defined SEM is actually pretty straightforward. We just specify a population model (i.e. giving it some values of each parameter) and then sample from it.  
For example:  

```{r}
popmod <- "
biscuits =~ .7*oreo + .6*digestive + .7*custardcream + 
            .6*jammydodger + .8*bourbon + .3*jaffacake
"

newdata <- simulateData(popmod, sample.nobs = 300)

head(newdata)
```

:::

1. Do a little bit of research and find a paper that fits a structural equation model. Simulate some data from it.  
2. Save the dataset (`write_csv()` will help), and share it with someone else, along with a description of the theoretical model. Can they estimate a model that fits well? 

`r qend()`



# Extras: Extensions of SEM

We have really only scraped the surface of the different things we can do with SEM. If you are interested in taking you learning further, then some of the next things to start looking into:   

  - Multigroup analysis (testing the model across two or more populations)  
    - Jöreskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36(4), 409-426.  
    - Sorbom, D. (1974). A general method for studying differences in factor means and factor structures between groups. British Journal of Mathematical and Statistical Psychology, 27, 229-239.  

  - Latent Growth Curves (actually just the same as a multilevel model!! &#129327; )  
    - [Michael Clark has a great lot of resources on this](https://m-clark.github.io/mixed-models-with-R/supplemental.html), and it makes the link between random effects and latent variables super clear.  



