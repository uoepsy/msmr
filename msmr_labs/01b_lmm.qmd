---
title: "1B: Linear Mixed Models/Multi-level Models"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()

schoolmot <- read_csv("data/schoolmot.csv") |>
  mutate(schoolid=factor(schoolid))
srmod <- lm(grade ~ motiv, data = schoolmot)
```

:::{.callout-caution collapse="true"}
## A Note on terminology

The methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. 

```{r}
#| echo: false
#| label: fig-wordcloud
#| fig-cap: "size weighted by hits on google scholar search (sept 2020)"  
tribble(
  ~word, ~freq,
  "multi-level model", 154000 + 31300,
  "hierarchical linear model", 24000,
  "mixed-effects model", 56500 + 191000,
  "mixed model", 1500000,
  "random coefficient model", 11200+6920,
  "random-effects model", 101000 + 501000,
  "random parameter model", 2140 + 1460,
  "random-intercept model", 17100 + 2930, 
  "variance components model", 6210 + 5560,
  "partial pooling", 5120,
  "mixed error-component model", 62,
  "random slope model", 4010 + 1620,
  "panel data model", 55400,
  "latent curve model", 1520,
  "growth curve model", 18400
) -> mlmname


mlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5

#wordcloud2(mlmname, shape="diamond", size=.4)
library(wordcloud)
wordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,
          min.freq=1,
          scale=c(4,.5),
          rot.per=0,
          fixed.asp=T,
          #ordered.colors=T,
          colors="#a41ae4")
```

:::


# LMM

In the simple linear regression model was written as $\color{red}{y} = \color{blue}{b_0 + b_1x_1 \ + \ ... \ + \ b_px_p} \color{black}{\ + \ \varepsilon}$, the estimated coefficients $\color{blue}{b_0}$, $\color{blue}{b_1}$ etc., are estimated as a fixed value.  

In the example where we model School children's grades as a function of their motivation score, we can fit a simple regression model, and the estimated parameters are two values that define the intercept and the slope of the line in @fig-schoolplot1.  
 
These two values are fixed. It does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). 

```{r}
schoolmot <- read_csv("data/schoolmot.csv")
srmod <- lm(grade ~ motiv, data = schoolmot)
```
```{r}
#| echo: false
.pp(summary(srmod),l=list(0,9:13))
```




the linear mixed model (LMM) 

fits a distribution of intercepts.
a center ( a single value and a spread)

so for a given school, the intercept is b0 + z0i  
a fixed number plus some random deviation

```{r}
#| echo: false
#| label: fig-schoolplot1
#| fig-cap: "Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line."
library(lme4)
library(ggdist)
library(distributional)
fmod = lm(grade~motiv,schoolmot)
rimod = lmer(grade~motiv+(1|schoolid),schoolmot)
rsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)
basep = ggplot(schoolmot, aes(x=motiv,y=grade))+
  geom_point(alpha=.1) + 
  geom_vline(xintercept=0,lty="dashed")+
  scale_x_continuous(limits=c(0,10),breaks=0:10)

basep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +
  geom_point(x=0,y=coef(fmod)[1],size=3)
  
```

```{r}
#| eval: false
#| echo: false
plotlines = 
  as.data.frame(coef(rimod)$schoolid) |> 
  mutate(
    g = 1:n(),
    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))
  ) |> unnest(data)

basep + 
  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + 
  stat_eye(side="left", aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), 
           alpha=.3, fill="#a41ae4") + 
  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1) 
```


```{r}
#| echo: false
#| eval: false
plotlines = 
  as.data.frame(coef(rsmod)$schoolid) |> 
  mutate(
    g = 1:n(),
    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))
  ) |> unnest(data)

basep + 
  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + 
  stat_eye(side="left", aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), 
           alpha=.3, fill="#a41ae4") + 
  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1) 

```


intercepts vary
slopes vary

no pooling vs partial pooling: 

```{r}
#| echo: false
#| eval: false
set.seed(123)
# sort(unique(schoolmot$schoolid))[c(1:4)]
bind_rows(
  schoolmot,
  tibble(
    schoolid = "Hypothetical School X",
    motiv = c(-1,0.1,1.4)+5,
    grade = 10*motiv + rnorm(3,0,10)
  )
) |> bind_rows(x=_, 
               tibble(schoolid="Hypothetical School Y",motiv = -2+5, grade = 65)
               ) -> tdf 

rsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)
femod2 = lm(grade~motiv*schoolid,tdf)

feplot = expand_grid(
  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),
               "Hypothetical School X"),
  motiv = seq(0,10,.1)
) %>% mutate(.fitted = predict(femod2, newdata = .)) 


rsplot = 
  as.data.frame(coef(rsmod2)$schoolid) |> 
  rownames_to_column(var="schoolid") |>
  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | 
           grepl("Hypothetical", schoolid)) |>
  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> 
  unnest(data)

tdf |> 
  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | 
           grepl("Hypothetical", schoolid)) |>
  ggplot(aes(x=motiv,y=grade))+
  geom_point()+
  facet_wrap(~schoolid) + 
  geom_line(data=feplot,
            aes(y=.fitted),col="blue",lwd=1)+
  geom_line(data=rsplot,
            aes(x=x,y=.fitted),col="orange",lwd=1) +
  ylim(0,100)

```


how is it different to fixed eff?
"partial pooling" (link back to above)
shrinkage
- socialist vs liberal analogy?  

how/why does it do this?
by modelling a distribution of lines  


$$
\begin{align}\\
& \color{red}{y} = \color{blue}{b_0 + b_1x_1 \ + \ ... \ + \ b_px_p} \color{black}{+ \varepsilon}\\ 
& \text{Where:} \\
& \epsilon \sim N(0, \sigma) \text{ independently}
\end{align}
$$

# fitting LMM in R

lme4
lmer

# model parameters

_what_ are the model parameters? 
i.e. variance components. 

eq with model params coloured
$$

$$


we _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. 

# terminology: fixed effects, random effects, variance components

we often use "random effects" to just mean the distribution of random deviations. i.e. 

sometimes you might hear 
"random effect of group"
"random effect for group"
"random effect [of x] by group"

generally, people are referring to the `(1 + ... | cluster)` bit. 

graphic on how to read it.

intercept >> 1
slope of x >> x
| >> varies by
these groups >> cluster

a common stumbling block. 
"effect of x varies by cluster" is not the same as "x varies by cluster".  





# estimation

## ML and REML

MLE explainer

- problem for lmm
est fix > est varcorr > est fix > est varcorr
est of varcorr assumes fixed effects are known. 
this biases var ests to be slightly smaller  
a bit like n-1 in formula for sd

REML
- OLS to partial out fixef > 
  est varcorr > est varcorr > est varcorr > 
  use GLS to est fixef
- in the estimation of varcorr, the fixed effects are 0 _by definition_
  









## fitting issues

convergence warnings, singular fits 






