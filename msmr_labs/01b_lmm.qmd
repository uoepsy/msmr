---
title: "1B: Linear Mixed Models/Multi-level Models"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()

schoolmot <- read_csv("data/schoolmot.csv") |>
  mutate(schoolid=factor(schoolid))
srmod <- lm(grade ~ motiv, data = schoolmot)
```

The methods we're going to start to look at are known by lots of different names (see @fig-wordcloud). The core idea is that **model parameters vary at more than one level.**. 

```{r}
#| echo: false
#| label: fig-wordcloud
#| fig-cap: "size weighted by hits on google scholar search (sept 2020)"  
tribble(
  ~word, ~freq,
  "multi-level model", 154000 + 31300,
  "hierarchical linear model", 24000,
  "mixed-effects model", 56500 + 191000,
  "mixed model", 1500000,
  "random coefficient model", 11200+6920,
  "random-effects model", 101000 + 501000,
  "random parameter model", 2140 + 1460,
  "random-intercept model", 17100 + 2930, 
  "variance components model", 6210 + 5560,
  "partial pooling", 5120,
  "mixed error-component model", 62,
  "random slope model", 4010 + 1620,
  "panel data model", 55400,
  "latent curve model", 1520,
  "growth curve model", 18400
) -> mlmname


mlmname$freq[mlmname$freq > 100000] <- c(85000,85000, 110000,70000,95000)*1.5

#wordcloud2(mlmname, shape="diamond", size=.4)
library(wordcloud)
wordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,
          min.freq=1,
          scale=c(4,.5),
          rot.per=0,
          fixed.asp=T,
          #ordered.colors=T,
          colors="#a41ae4")
```


<div class="divider div-transparent div-dot"></div>

# Fixed effects

In the simple linear regression model was written as $\color{red}{y} = \color{blue}{b_0 + b_1x_1 \ + \ ... \ + \ b_px_p} \color{black}{\ + \ \varepsilon}$, the estimated coefficients $\color{blue}{b_0}$, $\color{blue}{b_1}$ etc., are estimated as **fixed** values - i.e. we estimate just one number for $b_0$, and one number for $b_1$, for $b_2$ and so on, and that's it.  

In the example where we model School children's grades as a function of their motivation score, when we fit a simple regression model of `lm(grade ~ motiv)`, the estimated parameters are two values that define a line - an intercept and a slope (as in @fig-schoolplot1).  
 
```{r}
#| echo: false
#| label: fig-schoolplot1
#| fig-cap: "Grade predicted by motivation. The simple linear regression model defines the height and slope of the black line."
library(lme4)
library(ggdist)
library(distributional)
fmod = lm(grade~motiv,schoolmot)
rimod = lmer(grade~motiv+(1|schoolid),schoolmot)
rsmod = lmer(grade~motiv+(1+motiv|schoolid),schoolmot)
basep = ggplot(schoolmot, aes(x=motiv,y=grade))+
  geom_point(alpha=.1) + 
  geom_vline(xintercept=0,lty="dashed")+
  scale_x_continuous(limits=c(-1,12),breaks=0:10)

basep + geom_abline(intercept=coef(fmod)[1],slope=coef(fmod)[2], lwd=1) +
  geom_point(x=0,y=coef(fmod)[1],size=3,col="blue") +
  geom_segment(x=0,xend=1,
               y=(coef(fmod) %*% c(1,0))[1],
               yend=(coef(fmod) %*% c(1,0))[1]) +
  geom_segment(x=1,xend=2,
               y=(coef(fmod) %*% c(1,1))[1],
               yend=(coef(fmod) %*% c(1,1))[1]) +
  geom_segment(x=2,xend=3,
               y=(coef(fmod) %*% c(1,2))[1],
               yend=(coef(fmod) %*% c(1,2))[1]) + 
  geom_segment(x=3,xend=4,
               y=(coef(fmod) %*% c(1,3))[1],
               yend=(coef(fmod) %*% c(1,3))[1]) + 
  geom_segment(x=1,xend=1,
               y=(coef(fmod) %*% c(1,0))[1],
               yend=(coef(fmod) %*% c(1,1))[1],col="blue",lwd=2) +
  geom_segment(x=2,xend=2,
               y=(coef(fmod) %*% c(1,1))[1],
               yend=(coef(fmod) %*% c(1,2))[1],col="blue",lwd=2) +
  geom_segment(x=3,xend=3,
               y=(coef(fmod) %*% c(1,2))[1],
               yend=(coef(fmod) %*% c(1,3))[1],col="blue",lwd=2) +
  geom_segment(x=4,xend=4,
               y=(coef(fmod) %*% c(1,3))[1],
               yend=(coef(fmod) %*% c(1,4))[1],col="blue",lwd=2) 
  
  
```

The intercept and slope here are 'fixed' in the sense that it does not matter what school a child is from, if they score 0 on the motivation scale, then our model predicts that they will get a grade of `r round(coef(srmod)[1],1)` (the intercept). 

```{r}
schoolmot <- read_csv("data/schoolmot.csv")
srmod <- lm(grade ~ motiv, data = schoolmot)
```
```{r}
#| echo: false
.pp(summary(srmod),l=list(0,9:13))
```

To make this point really clear, we can write our model equation with the addition of a suffix $i$ to indicate that the equation for the $i^{th}$ child is: 

$$
\begin{align}
&\text{For child }i \\
&\text{grade}_i = b_0 + b_1 \cdot \text{motiv}_i + \epsilon_i 
\end{align}
$$
i.e. For any child $i$ that we choose, that child's grade ($\text{grade}_i$) is some fixed number ($b_0$) plus some fixed amount ($b_1$) times that child's motivation ($\text{motiv}_i$).  
  
The issue, as we saw in [1A #clustered-data](01a_clustered.html#clustered-data), is that the children in our study are actually related to one another in that they can be grouped into the schools that we sampled them from. It's entirely possible (and likely) that there are school-level differences might actually account for quite a lot of the variation in grades (in [1A #ICC](01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable) we actually estimated this to account for approx 40% of the variation in grades).  

TODO CHECK below

We saw how we might add in `school` as a predictor to our linear model to estimate all these school-level differences (`lm(grade ~ schoolid + motiv)`). This is a good start, and may oftentimes be perfectly acceptable if our clustering is simply a nuisance thing that we want to account for - adding in the clustering as another predictor will completely account for *all* cluster-level variability in our outcome variable.  

However, more frequently these clusters are themselves additional units of observation that have features of interest to us. For instance, we may be interested in how the funding that a school receives moderates the association between childrens motivation and grades - i.e. we're interested in things that happen both at the child-level (motivation, grades), and at the school-level (funding). For these scenarios, we really need a multilevel model.  


::: {.callout-note collapse="true"}
#### clusters as fixed effects

We have already seen that we can include fixed effects for cluster differences (we referred to this as "no pooling").  

e.g. to fit school-level differences in grades, we could use:
```{r}
#| eval: false
femod <- lm(grade ~ motiv + schoolid, data = schoolmot)
```

The model equation for this would look something like:
$$
\begin{align}
\text{For child }i& \\
\text{grade}_i =\, &b_0 + b_1 \cdot \text{motiv}_i + b_2 \cdot \text{isSchool2}_i + b_3 \cdot \text{isSchool3}_i\,\, + \,\, ... \,\, + \\
& ... + \,\, ... \,\, + \,\, ... \,\, + \\
& b_p \cdot \text{isSchoolP}_i\,\, + \epsilon_i 
\end{align}
$$

The school coefficients are a series of dummy variables that essentially toggle on or off depending on which school child $i$ is from. 

Because these set of coefficients accoutn for **all** of the school-level differences in grades, it means we are then unable to consider other school-level variables like `funding` (how much govt funding the school receives). If we try, we can see that a coefficient for `funding` is not able to be estimated because `schoolid` is explaining everything school-related:    

```{r}
#| eval: false
femod2 <- lm(grade ~ motiv + schoolid + funding, data = schoolmot)
summary(femod2)
```
```
Coefficients: (1 not defined because of singularities)
                                   Estimate  Std. Error  t value  Pr(>|t|)    
(Intercept)                        36.2655      2.5667    14.129   < 2e-16 ***
motiv                              1.5723       0.3673    4.281   2.07e-05 ***
schoolidBalfron High School        1.0683       2.8923    0.369   0.711946 
schoolidBanff Academy             -3.3253       2.9142   -1.141   0.254163 
...                                ...          ...       ...     ... 
...                                ...          ...       ...     ... 
funding                            NA           NA        NA      NA  
```

:::


<div class="divider div-transparent div-dot"></div>

# The multi-level model

The multi-level model is an alternative model structure that accounts for cluster-level differences in a more flexible and parsimonious way. It achieves this by taking some of the estimated coefficients $b_?$ in our linear regression model and modelling these as randomly varying by clusters (i.e. each cluster gets its own value of $b_?$).  


Let's see how this works by starting with the intercept, $b_0$.  

## random intercept 

To extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.^[Some books use "cluster $j$ >> observation $i$", others use "cluster $i$ >> observation $j$". We use the latter here] Then, we can take our coefficients $b_?$ and allow them to be different for each cluster $i$ by adding the suffix $b_{?i}$. Below, we have done this for our intercept $b_0$.  
  
However, we also need to _define_ these differences, and the multilevel model does this by expressing each cluster's intercept as a deviation ($\zeta_{0i}$ for cluster $i$, below) from a fixed number ($\gamma_{00}$, below). Because these differences are to do with the _clusters_ (and not the individual observations within them), we often write these as a "level 2 equation":    

$$
\begin{align}
\text{For observation }j&\text{ in cluster }i \\
\text{Level 1:}& \\
y_{ij} &= b_{0i} + b_1 \cdot x_{ij} + \epsilon_{ij} \\
\text{Level 2:}& \\
b_{0i} &= \gamma_{00} + \zeta_{0i} \\
\end{align}
$$

::: {.callout-tip collapse="true"}
#### mixed-effects notation  

Instead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:  

$$
\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{b_{0i}}} \cdot 1 + \color{blue}{b_{1}} \cdot x_{ij}  +  \varepsilon_{ij}
$$  

This notation typically corresponds with the "mixed effects" terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:  

$$
\color{red}{y_{ij}} = \underbrace{(\underbrace{\gamma_{00}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{0i}}_{\textrm{random}}})}_{\text{intercept, }b_{0i}} \cdot 1 + \underbrace{b_2}_{\textrm{fixed}} \cdot x_{ij} +  \varepsilon_{ij}
$$

:::

Returning to our school children's grade example, we can fit a model with "random intercepts for schools", which would account for the possibility that some schools have higher grades, some have lower grades, etc.  

$$
\begin{align}
\text{For Child }j\text{ in School }i& \\
\text{Level 1 (child):}& \\
\text{grade}_{ij} &= b_{0i} + b_1 \cdot \text{motiv}_{ij} + \epsilon_{ij} \\
\text{Level 2 (school):}& \\
b_{0i} &= \gamma_{00} + \zeta_{0i} \\
\end{align}
$$
If we consider one of our schools (e.g. "Dyce Academy") we can see that our model predicts that this school has higher grades than most other schools (@fig-ri_1school). We can see how this is modelled as a deviation $\zeta_{0\text{DA}}$ (DA for Dyce Academy) from some fixed value $\gamma_{00}$.  

```{r}
#| label: fig-ri_1school
#| echo: false
#| fig-cap: "Fitted values from a multilevel model with random intercepts for schools"
library(ggforce)
library(ggfx)
plotlabs = tibble(schoolid=unique(schoolmot$schoolid),motiv=10,x=10)
plotlabs$y = predict(rimod, newdata=plotlabs)

plotlines = 
  as.data.frame(coef(rimod)$schoolid) |> 
#  rownames_to_column() |>
  mutate(
    g = 1:n(),
    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))
  ) |> unnest(data)

specg = plotlines |> filter(g==10) |>
  mutate(f = fixef(rimod)[1])

basep + 
  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.3),sigma=2) + 
  geom_line(data = specg,lwd=1,
            aes(x=x,y=.fitted,group=g),alpha=1,col="orange") +
  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col="#a41ae4") +
  geom_curve(
    data=specg[1,],
    aes(x=0,xend=0,y=.fitted,yend=f,col="orange"),
    curvature=.2,lwd=1
  ) +
  annotate("text",x=-.1,y=fixef(rimod)[1],
           label=expression(gamma*"00"),size=5,
           hjust=1,vjust=1.3,col="#a41ae4",parse=TRUE) +
  annotate("text",x=-.1,y=mean(unlist(specg[1,5:6])),
           label=expression(zeta*"0DA"),size=5,
           hjust=1.2,col="orange")+
  geom_text(data=plotlabs,
             aes(x=x,y=y,label=schoolid),
             hjust=0,alpha=.3)+
  geom_label(data=plotlabs[grepl("Dyce",plotlabs$schoolid),],
             aes(x=x,y=y,label=schoolid),
             hjust=0,col="orange")+
  guides(col="none")
```

TODO PANEL PLOT

At this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of `lm(grade ~ motiv + schoolid)`). This would also estimate an difference for each cluster?  

:::sticky
The key to the multilevel model is that we are not actually estimating the cluster-specific differences themselves (although we can_ get these out). We are estimating a **distribution** of differences. 
:::

Specifically, the parameters of the multilevel model that are being estimated are the mean and _variance_ of a _normal_ distribution of clusters.  

So the parameters that are estimated from our model with a random intercept by-schools, are:

$$
\begin{align}
\text{For Child }j\text{ in School }i& \\
\text{Level 1 (child):}& \\
\text{grade}_{ij} &= b_{0i} + b_1 \cdot \text{motiv}_{ij} + \epsilon_{ij} \\
\text{Level 2 (school):}& \\
b_{0i} &= \gamma_{00} + \zeta_{0i} \\
\text{where: }& \\
&\varepsilon_{ij} \sim N(0,\sigma_\varepsilon) \\
&\zeta_{0i} \sim N(0,\sigma_0) \\
\end{align}
$$


- a fixed intercept $\gamma_{00}$  
- the variance with which schools deviate from the fixed intercept $\sigma_0$  
- a fixed slope for `motiv` $b_1$  
- (and we also get the residual variance too, in $\sigma_\varepsilon$)  

```{r}
#| echo: false
basep + 
  with_blur(geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.2), sigma=2) + 
  stat_eye(side="left",
           data=tibble(motiv=-1,grade=50),
           aes(x=0,ydist=dist_normal(fixef(rimod)[1],sqrt(VarCorr(rimod)[[1]][1]))), 
           alpha=.3, fill="#a41ae4")  +

  geom_abline(intercept=fixef(rimod)[1],slope=fixef(rimod)[2], lwd=1,col="#a41ae4")+
    geom_text(data=plotlabs,
             aes(x=x,y=y,label=schoolid),
             hjust=0,alpha=.1) +
  annotate("text",x=-.1,y=fixef(rimod)[1],
           label=expression(gamma*"00"),size=5,
           hjust=1,vjust=1.3,col="#a41ae4",parse=TRUE) + 
  geom_segment(x=-.1,xend=-.1,y=fixef(rimod)[1],
               yend=fixef(rimod)[1]+sqrt(VarCorr(rimod)[[1]][1])-.5,
               col="darkorange",lwd=1) + 
  annotate("text",x=-.1,y=47,
           label=expression(sigma*"0"),size=5,
           hjust=1.2,col="darkorange")
```

## random slopes

$$
\begin{align}
\text{For observation }j&\text{ in cluster }i \\
\text{Level 1:}& \\
y_{ij} &= b_{0i} + b_{1i} \cdot x_{ij} + \epsilon_{ij} \\
\text{Level 2:}& \\
b_{0i} &= \gamma_{00} + \zeta_{0i} \\
b_{1i} &= \gamma_{10} + \zeta_{1i} \\
\end{align}
$$

```{r}
#| echo: false
plotlines = 
  as.data.frame(coef(rsmod)$schoolid) |> 
  mutate(
    g = 1:n(),
    data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))
  ) |> unnest(data)

basep + 
  geom_line(data = plotlines, aes(x=x,y=.fitted,group=g),alpha=.5) + 
  stat_eye(side="left", 
           data=tibble(motiv=-1,grade=50),
           aes(x=0,ydist=dist_normal(fixef(rsmod)[1],sqrt(VarCorr(rsmod)[[1]][1]))), 
           alpha=.3, fill="#a41ae4") + 
  geom_abline(intercept=fixef(rsmod)[1],slope=fixef(rsmod)[2], lwd=1, col="#a41ae4") 

# TODO add separate curve for slope dist, with rug (coloured)

```


intercepts vary
slopes vary

TODO PANEL PLOT


## partial pooling

It's tempting to think that we could get the fixed intercept $\gamma_{00}$ by calculating a simple linear model for each school and taking the average intercept. However, the multilevel model is more clever than that. The amount by which Each cluster in a multilevel model contributes to the estimate of the fixed intercept by an amount that depends on:   

a) how much between-cluster variation there is relative to within-cluster variation (TODO if clusters are very distinct )
b) the number of observations in each cluster

This is a really useful feature, because it means that we 

less data
and, when clusters are quite similar/less distinct, then they contribute similar amounts


- socialist vs liberal analogy?  

how/why does it do this?
by modelling a distribution of lines  


```{r}
#| echo: false
set.seed(123)
# sort(unique(schoolmot$schoolid))[c(1:4)]
bind_rows(
  schoolmot,
  tibble(
    schoolid = "Hypothetical School X",
    motiv = c(-1,0.1,1.4)+5,
    grade = 10*motiv + rnorm(3,0,10)
  )
) |> bind_rows(x=_, 
               tibble(schoolid="Hypothetical School Y",motiv = -2+5, grade = 65)
               ) -> tdf 

rsmod2 = lmer(grade~motiv+(1+motiv|schoolid),tdf)
femod2 = lm(grade~motiv*schoolid,tdf)

feplot = expand_grid(
  schoolid = c(as.character(sort(unique(schoolmot$schoolid))[c(1:4)]),
               "Hypothetical School X"),
  motiv = seq(0,10,.1)
) %>% mutate(.fitted = predict(femod2, newdata = .)) 


rsplot = 
  as.data.frame(coef(rsmod2)$schoolid) |> 
  rownames_to_column(var="schoolid") |>
  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | 
           grepl("Hypothetical", schoolid)) |>
  mutate(data = map2(`(Intercept)`,motiv, ~tibble(x = 0:10, .fitted = ..1 + ..2*(0:10)))) |> 
  unnest(data)

tdf |> 
  filter(schoolid %in% sort(unique(schoolmot$schoolid))[c(1:4)] | 
           grepl("Hypothetical", schoolid)) |>
  ggplot(aes(x=motiv,y=grade))+
  geom_point()+
  facet_wrap(~schoolid) + 
  geom_line(data=feplot,
            aes(y=.fitted),col="blue",lwd=1)+
  geom_line(data=rsplot,
            aes(x=x,y=.fitted),col="orange",lwd=1) +
  ylim(0,100)

```


::: {.callout-caution collapse="true"}
#### optional: how does it work?  

$$
\frac{\sigma^2_{b} }{\sigma^2_b + \frac{\sigma^2_e }{n_i}}
$$

This means that the fixed center of this distribution (the $\gamma_00$) However, 

TODO - what defines the amount to which a cluster contributes to the fixed estimate?   
how do far away/few n clusters influence the estimate?   

in the multilevel modelling approach, each school gets its own intercept and slope, but these 'borrow strength' from the others.   

The borrowing of strength is more apparent for the (what would be) more extreme clusters, as well as those that have fewer datapoints. What happens to these cluster estimates is that they are shrunk towards the population average. 

$\frac{\sigma^2_{b} }{\sigma^2_b + \frac{\sigma^2_e }{n_i}}$

more shrinkage when:
  - smaller n_j
  - when within var is large relative to between var
  - both of these are basically 'when we have less information about that group'  


https://jeanettemumford.org/MixedModelSeries/v4-introduction-to-regularization-in-mixed-models.html#introduction-2

https://jeanettemumford.org/MixedModelSeries/v5-conditional-modes-vs-means.html#the-equation-for-shrinkage

:::




# fitting multilevel models in R


fixed estimates = average cluster  

lme4 lmer

_what_ are the model parameters? 
i.e. variance components. 

eq with model params coloured

we _can_ get out estimates of the specific group-lines if we want, but really the model is estimating the variances. 

::: {.callout-note collapse="true"}
#### terminology: fixed effects, random effects, variance components

we often use "random effects" to just mean the distribution of random deviations. i.e. 

sometimes you might hear 
"random effect of group"
"random effect for group"
"random effect [of x] by group"

generally, people are referring to the `(1 + ... | cluster)` bit. 

graphic on how to read it.

intercept >> 1
slope of x >> x
| >> varies by
these groups >> cluster

a common stumbling block. 
"effect of x varies by cluster" is not the same as "x varies by cluster".  
:::


## model parameters

_what_ are the model parameters? 
i.e. variance components. 

eq with model params coloured
$$

$$


## cluster predictions  

ranef

fixef + ranef = coef

dotplot.ranef.mer

```{r}
#| echo: false
#| eval: false

expand_grid(
  schoolid=unique(schoolmot$schoolid),
  motiv=0:10
) |> broom::augment(
  lm(grade~motiv+schoolid,schoolmot),
  newdata=_, interval="confidence"
) -> plotlinesfix

basep + 
  with_blur(geom_line(data = plotlines,
                      aes(x=x,y=.fitted,group=g),
                      alpha=.4,col="orange"),sigma=2) +
  with_blur(geom_line(data = plotlinesfix,
                      aes(x=motiv,y=.fitted,group=schoolid),
                      alpha=.4,col="blue"),sigma=2) +
  geom_line(data = plotlinesfix[grepl("Calderglen",plotlinesfix$schoolid), ], 
            aes(x=motiv,y=.fitted,group=schoolid),col="blue")+
  geom_line(data = plotlines[plotlines$g==6,], 
            aes(x=x,y=.fitted,group=g),col="orange")


```





# model estimation

## ML and REML

MLE explainer

- problem for lmm
est fix > est varcorr > est fix > est varcorr
est of varcorr assumes fixed effects are known. 
this biases var ests to be slightly smaller  
a bit like n-1 in formula for sd

REML
- OLS to partial out fixef > 
  est varcorr > est varcorr > est varcorr > 
  use GLS to est fixef
- in the estimation of varcorr, the fixed effects are 0 _by definition_
  


## convergence issues

convergence warnings, singular fits 





