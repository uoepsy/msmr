---
title: "Path Analysis"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(lavaan)
library(semPlot)
# knitr::opts_chunk$set(cache = TRUE)

theme_set(theme_light(base_size = 12) +
            theme(
              panel.grid = element_blank()
            ))
```
:::red

**Packages for today**

+ lavaan  
+ semPlot or tidySEM

::: 

# Introducing Path Analysis

over last couple of weeks we have applied exploratory and then confirmatory factor analysis to develop and then test factor analysis models of 'conduct problems'. Factor models posit the existence of some underlying latent variable which is thought of as resulting in the scores on our measured items. Especially in Week 8 we began to depict the variables and parameters involved in these models visually, in what get called 'path' or 'SEM' diagrams. Specifically, by using rectangles (observed variables), ovals (latent variables), single headed arrows (regression paths) and double headed arrows (covariances), we could draw various model structures.  

This week, we are temporarily putting aside the latent variables (no ovals in the drawings today!) and focusing on some of the fundamentals that motivate this modeling approach. 

:::blue

**Mountains cannot be surmounted except by winding paths**

:::

Over the course of USMR and the first block of this course, you have hopefully become pretty comfortable with the regression world, and can see how it is extended to lots of different types of outcome and data stuctures. 

If we are for the time being ignoring the latent variables, then what exactly do we gain by this approach of drawing out our variables and drawing various lines between them? Surely our regression toolkit can do all the things we need?  

Let's imagine we are interested in peoples' intention to get vaccinated, and we observe the following variables: 

- Intention to vaccinate (scored on a range of 0-100)  
- Health Locus of Control (HLC) score (average score on a set of items relating to perceived control over ones own health)  
- Religiosity (average score on a set of items relating to an individual's religiosity). 

We are assuming here that we do not have the individual items, but only the scale scores (if we had the individual items we might be inclined to model religiosity and HLC as latent variables!).   
If we draw out our variables, and think about this in the form of a standard regression model with "Intention to vaccinate" as our outcome variable, then all the lines are filled in for us (see Figure \@ref(fig:mregpath))

```{r mregpath, echo=FALSE, fig.cap="Multiple regression as a path model"}
knitr::include_graphics("images/path/path1.png")
```

But what if our theory suggests that some other model might be of more relevance? For instance, what if we believe that participants' religiosity has an effect on their Health Locus of Control score, which in turn affects the intention to vaccinate (see Figure \@ref(fig:medpath1))?   
In this case, the HLC variable is thought of as a **mediator**, because is mediates the effect of religiosity on intention to vaccinate. We are specifying presence of a distinct type of effect: **direct** and **indirect**. 

:::red
**Direct vs Indirect**  

In path diagrams:

- Direct effect = one single-headed arrow between the two variables concerned  
- Indirect effect = An effect transmitted via some other variables   

:::

```{r medpath1, echo=FALSE, fig.cap="Mediation as a path model"}
knitr::include_graphics("images/path/path2.png")
```

The only option here is to conduct several regression models, because we have multiple **endogenous** variables. Fortunately, path analysis allows us to do just that - fit simultaneous regression equations! 

:::yellow
**Terminology refresher**  

- **Exogenous variables** are a bit like what we have been describing with words like "independent variable" or "predictor". In a path diagram, they have no paths coming from other variables in the system, but have paths *going to* other variables.  
- **Endogenous variables** are more like the "outcome"/"dependent"/"response" variables we are used to. They have some path coming from another variable in the system (and may also - but not necessarily - have paths going out from them).  

:::

Recall our way of drawing path diagrams *(excluding any mention of latent variables and factors for now)*:

- **Observed variables** are represented by squares or rectangles. These are the named variables of interest which exist in our dataset - i.e. the ones which we have measured directly. 
- **Covariances** are represented by double-headed arrows. In many diagrams these are curved. 
- **Regressions** are shown by single headed arrows (e.g., an arrow from $x$ to $y$ for the path $y~x$).  

## Some key assumptions  

There are a few assumptions of a complete path diagram:

- all our exogenous variables are correlated (unless we specifically assume that their correlation is zero)
- All models are 'recursive' (no two-way causal relations, no feedback loops)
- Residuals are uncorrelated with exogenous variables
- Endogenous variables are not connected by correlations (we would use correlations between residuals here, because the residuals are not endogenous)
- All 'causal' relations are linear and additive
- 'causes' are unitary (if A -> B and A -> C, then it is presumed that this is the same aspect of A resulting in a change in both B and C, and not two distinct aspects of A, which would be better represented by two correlated variables A1 and A2). 

:::red
**Causal??**

It is a slippery slope to start using the word 'cause', and personally I am not that comfortable using it here. However, you will likely hear it a lot in resources about path analysis and SEM, so it is best to be warned.  

Please keep in mind that we are using a very broad definition of 'causal', simply to reflect the one way nature of the relationship we are modeling. In Figure \@ref(fig:causes), a change in the variable **X1** is associated with a change in **Y**, but not vice versa. 

```{r causes, echo=FALSE, fig.cap="Paths are still just regressions."}
knitr::include_graphics("images/path/causes.png")
```
:::

## Tracing rules

Thanks to [Sewal Wright](https://www.jstor.org/stable/pdf/2527551.pdf?casa_token=3QF0ad2ZoBcAAAAA:MbEkDNNdoLZr1SXE4LrnK--qrhhsTXLgsRtcWre1UvWxiQiGNUl5vWytGp34XIxhAYMZJe-MbIcBnEwXSfX6MAONevz04-sMXpEDI3IaYKk6mMX46QvX), we can express the correlation between any two variables in the system as the sum of all *compound paths* between the two variables. 

*compound paths* are any paths you can trace between A and B for which there are: 

- no loops
- no going forward then backward
- maximum of one curved arrow per path

Let's consider the example below, for which the paths are all labelled with lower case letters $a, b, c, \text{and } d$. 

```{r patheq1, echo=FALSE, fig.cap="A multiple regression model as a path diagram"}
knitr::include_graphics("images/path/patheq1.png")
```

According to Wright's tracing rules above, write out the equations corresponding to the 3 correlations between our observed variables (remember that $r_{a,b} = r_{b,a}$, so it doesn't matter at which variable we start the paths). 

- $r_{x1,x2} =  c$  
- $r_{x1,y} = a + bc$  
- $r_{x2,y} =  b + ac$  

Now let's suppose we observed the following correlation matrix:
```{r}
egdat <- read_csv("https://uoepsy.github.io/data/patheg.csv")
round(cor(egdat),2)
```
We can plug these into our system of equations:

- $r_{x1,x2} = c = 0.36$  
- $r_{x1,y} = a + bc = 0.75$   
- $r_{x2,y} = b + ac = 0.60$

And with some substituting and rearranging, we can work out the values of $a$, $b$ and $c$. 

`r optbegin("We've hidden the answers so you can test yourself. Grab a piece of paper and solve some equations!", olabel=FALSE, toggle=params$TOGGLE)`
This is what I get:

a = 0.61  
b = 0.38  
c = 0.36  

`r optend()`

We can even work out what the path labeled $d$ (the residual variance) is. 
First we sum up all the equations for the paths from Y to Y. These are:

- $a^2$ (from Y to X1 and back)  
- $b^2$ (from Y to X2 and back)  
- $acb$ (from Y to X1 to X2 to Y)
- $bca$ (from Y to X2 to X1 to Y)
  
Summing them all up and solving gives us:  
$$
\begin{align}
 r_{y \cdot x1, x2} & = a^2 + b^2 + acb + bca\\
 & = 0.61^2 + 0.38^2 + 2 \times(0.61 \times 0.38 \times 0.36)\\
 & = 0.68 \\
\end{align}
$$
We can think of this as the portion of the correlation of Y with itself that occurs *via the predictors*. Put another way, this is the amount of variance in Y explained jointly by X1 and X2, which sounds an awful lot like an $R^2$!  
This means that the path $d = \sqrt{1-R^2}$.  

Hooray! We've just worked out regression coefficients when all we had was the correlation matrix of the variables! It's important to note that we have been using the correlation matrix, so, somewhat unsurprisingly, our estimates are *standardised* coefficients. 

Because we have the data itself, let's quickly find them with `lm()`
```{r}
# quickly scale all the columns in the data
egdat <- egdat %>% mutate_all(~scale(.)[,1])
# extract the coefs
coef(lm(y~x1+x2, egdat))
# extract the r^2
summary(lm(y~x1+x2, egdat))$r.squared
```
# Path Mediation 

Now that we've seen how path analysis works, we can use that same logic to investigate models which have quite different structures, such as those including mediating variables. So if we can't fit our theoretical model into a regression framework, let's just fit it into a framework which is lots of regressions smushed together!  
Luckily, we can just get the **lavaan** package to do all of this for us. So let's look at fitting the model below. 
```{r medpath2, echo=FALSE, fig.cap="If you're interested, you can find the inspiration for this data from the paper [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7596314/). I haven't properly read it though!"}
knitr::include_graphics("images/path/path2.png")
```

First we read in our data: 
```{r}
vax <- read_csv("https://uoepsy.github.io/data/vaxdat.csv")
summary(vax)
```
Then we specify the relevant paths:
```{r}
med_model <- " 
    intention ~ religiosity
    intention ~ hlc
    hlc ~ religiosity
"
```

If we fit this model as it is, we won't actually be testing the indirect effect, we will simply be fitting a couple of regressions.  

To do that, we need to explicitly define the indirect effect in the model, by first creating a label for each of its sub-component paths, and then defining the indirect effect itself as the product (why the product? [Click here for a lovely pdf explainer from Aja](https://uoepsy.github.io/msmr/indirect_effects.pdf)).  
To do this, we use a new operator, `:=`. 

```{r}
med_model <- " 
    intention ~ religiosity
    intention ~ a*hlc
    hlc ~ b*religiosity
    
    indirect:=a*b
"
```


:::yellow
**:=** 

_This operator 'defines' new parameters which take on values that are an arbitrary function of the original model parameters. The function, however, must be specified in terms of the parameter labels that are explicitly mentioned in the model syntax._
  
([the lavaan project](https://lavaan.ugent.be/))

Note. The labels we use are completely up to us. This would be equivalent:
```{r eval=FALSE}
med_model <- " 
    intention ~ religiosity
    intention ~ peppapig * hlc
    hlc ~ kermit * religiosity
    
    indirect:= kermit * peppapig
"
```

:::

## Estimating the model 

It is common to estimate the indirect effect using bootstrapping (a method of resampling the data with replacement, thousands of times, in order to empirically generate a sampling distribution). We can do this easily in lavaan:

```{r}
mm1.est <- sem(med_model, data=vax, se = "bootstrap") 
summary(mm1.est, ci = TRUE)
```

# Exercises 

This week's lab focuses on the technique of path analysis using the same context as previous weeks: conduct problems in adolescence. In this week's example, a researcher has collected data on n=557 adolescents and would like to know whether there are associations between conduct problems (both aggressive and non-aggressive) and academic performance and whether the relations are mediated by the quality of relationships with teachers. 

`r qbegin("A1")`
First, read in the dataset from https://uoepsy.github.io/data/cp_teachacad.csv     
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r message=FALSE}
cp_teach<-read_csv("https://uoepsy.github.io/data/cp_teachacad.csv")
summary(cp_teach)
```
`r solend()`

`r qbegin("A2")`
Use the `sem()` function in lavaan to specify and estimate a straightforward linear regression model to test whether aggressive and non-aggressive conduct problems significantly predict academic performance.  

How do your results compare to those you obtain using the `lm()` function?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
``` {r simple regression in lavaan}
# we can fit the model in lavaan as follows:
# first we specify the model using lavaan syntax
sr_lavaan<-'Acad~Non_agg+Agg'
# next we can estimate the model using the sem() function
sr_lavaan.est<-sem(sr_lavaan, data=cp_teach)
# we can inspect the results using the summary() function
summary(sr_lavaan.est)

# the same model can be fit using lm():

sr_lm<-lm(Acad~Non_agg+Agg, data=cp_teach)
summary(sr_lm)
```

We can see that both non-aggressive and aggressive conduct problems significantly predict academic perfofmance.We can also see that we get the same results when we use the `sem()` function as we do when we use the `lm()` function. Lavaan will give essentially the same results as `lm()` for simple  and multiple regression problems. However, if we have multiple outcome variables in our model it is advantageous to do this using path mediation model with lavaan. This allows us to include all the regressions in a single model.
`r solend()`

`r qbegin("A3")`
Now specify a model in which non-aggressive conduct problems have both a direct and indirect effect (via teacher relationships) on academic performance
`r qend()` 

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r path analysis for non-aggressive conduct problems}
model1<-'
    #we regress academic performance on non-aggressive conduct problems (the direct effect)
    Acad~Non_agg
    
    #we regress academic peformance on teacher relationship quality
    Acad~Teach_r
    
    #we regress teacher relationship quality on non-aggressive conduct problems
    Teach_r~Non_agg 
'
```
`r solend()`

`r qbegin("A4")`
Now define the indirect effect in order to test the hypothesis that non-aggressive conduct problems have both a direct and an indirect effect (via teacher relationships) on academic performance. 

Fit the model and examine the 95% CI.
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r path analysis for non-aggressive conduct problems with constraints}

#model specification
model1<-'
    Acad~Non_agg
    #we label the two parameters that comprise the indirect effect b and c
    Acad~b*Teach_r    
    Teach_r~c*Non_agg  
    
    # the indirect effect is the product of b and c. We create a new parameter (ind) to estimate the indirect effect
    ind:=b*c   
'

#model estimation
model1.est<-sem(model1, data=cp_teach, se='bootstrap') 

# we request bootstrapped standard errors to assess the signifance of the indirect effect
summary(model1.est, ci=T)
```

We can see that the 95% bootstrapped confidence interval for the indirect effect of non-aggressive conduct problems on academic performance ('ind') does not include zero. We can conclude that the indirect effect is significant at $p <.05$. The direct effect is also statistically significant at $p < .05$.
`r solend()`

`r qbegin("A5")`
Specify a new parameter which is the total (direct+indirect) effect of non-aggressive conduct problems on academic performance
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can create a new parameter that is the sum of the direct and indirect effect to evaluate the total effect of non-aggressive conduct problems on academic performance.

```{r path analyis total}

#model specification

model1<-'
    # we now also label the indirect effect of non-aggressive conduct problems on academic performance
    Acad~a*Non_agg    
    Acad~b*Teach_r    
    Teach_r~c*Non_agg  
    
    ind:=b*c   
    #the total effect is the indirect effect plus the direct effect
    total:=b*c+a
'

#model estimation
model1.est<-sem(model1, data=cp_teach,se='bootstrap') 

# we request bootstrapped standard errors to assess the signifance of the indirect effect
summary(model1.est, ci=T)
```
`r solend()`


`r qbegin("A6")`
Now visualise the estimated model and its parameters using the `semPaths()` function from the `semPlot` package.  
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r visualise the model}
#to include the parameter estimates we set what='est'
semPaths(model1.est, what='est') 
```

`r solend()`

## A more complex model

`r qbegin("B1")`
Now specify a model in which both aggressive and non-aggressive conduct problems have both direct and indirect effects (via teacher relationships) on academic performance. Include the parameters for the indirect effects.
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r two predictors}
model2<-
   'Acad~Agg+Non_agg+b*Teach_r
    Teach_r~c1*Agg+c2*Non_agg
   
    ind1:=b*c1 #indirect effect for aggressive conduct problems
    ind2:=b*c2 #indirect effect for non-aggressive conduct problems
'
```

We now have two predictors, one mediator and one outcome (and two indirect effects, one for each predictor). We can represent this in two lines: one where we specify academic performance as the outcome variable and one where we specify teacher relationships (the mediator) as the outcome variable. `r solend()`

`r qbegin("B2")`
Now estimate the model and test the significance of the indirect effects
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r estimate two predictor model}
model2.est<-sem(model2,  data=cp_teach,se='bootstrap') 
summary(model2.est, ci=T)
```

We can see that the 95% confidence intervals for both indirect effects do not include zero, therefore, we can conclude that they are significant at $p < .05$. 
`r solend()`

`r qbegin("B3")`
Write a brief paragraph reporting on the results of the model estimates in Question B2. Include a Figure or Table to display the parameter estimates.
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

:::int 

A path mediation model was used to test the direct and indirect effects (via teacher relationship quality) of aggressive and non-aggressive conduct problems on academic performance. In the model, academic performance was regressed on teacher relationship quality, non-aggressive conduct problems and aggressive conduct problems while teacher relationship quality (the mediator) was regressed on aggressive and non-aggressive conduct problems. The indirect effects were tested using the product of the coefficient for the regression of outcome on mediator and the coefficient for the regression of mediator on predictor. The statistical significance of the indirect effects were evaluated using boostrapped 95% confidence intervals with 1000 draws.

Unstandardised parameter estimates are provided in Figure \@ref(fig:pathcp). Solid lines indicate that a parameter is significant at $p <. 05$, while dashed lines represent non-significant paths.The indirect effects of both non-aggressive  ($b = 0.09$, 95% CI=0.05-0.14) and aggressive ($b = 0.15$, 95% CI=0.08-0.22) conduct problems on academic performance were statistcally significant. 

```{r pathcp, echo=FALSE, out.width="1200px", fig.cap="Effect of conduct problems on academic performance mediated by quality of teacher relationship."}
knitr::include_graphics("images/path/pathanaly.png")
```
:::

`r solend()`

`r optbegin("Optional: Mediation the more manual way: back to lm()", olabel=FALSE, toggle=params$TOGGLE)`  

Following [Baron & Kenny 1986](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.917.2326&rep=rep1&type=pdf), we can conduct mediation analysis by using three separate regression models.  

1. $y \sim x$
2. $x \sim m$
3. $y \sim x + m$

Step 1. Determine the presence of `y ~ x`:  
*if x predicts y, then there is possibility to detect mediation*  
```{r}
vax <- read_csv("https://uoepsy.github.io/data/vaxdat.csv")

mod1 <- lm(intention ~ religiosity, data = vax)
summary(mod1)$coefficients
```

Step 2. Determine the presence of `m ~ x`:
*if x predicts m, then there is possibility to detect mediation*
```{r}
mod2 <- lm(hlc ~ religiosity, data = vax)
summary(mod2)$coefficients
```
Step 3. Examine the effect of `y ~ x + m`:  
*If the x no longer predicts y after partialling out effects due to m, then there is __full__ mediation. If the effect of x on y is smaller, then there is __partial__ mediation.* 
```{r}
mod3 <- lm(intention ~ religiosity + hlc, data = vax)
summary(mod3)$coefficients
```
Step 4. Test for the mediation.  
There are various ways to do this, but the simplest is probably:
```{r}
library(mediation)
summary(mediate(mod2, mod3, treat='religiosity', mediator='hlc', boot=TRUE, sims=500))
```
ACME: Average Causal Mediation Effects
ADE: Average Direct Effects
Total Effect: sum of the mediation (indirect) effect and the direct effect. 

`r optend()`


<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

