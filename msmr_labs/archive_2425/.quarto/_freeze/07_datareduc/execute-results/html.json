{
  "hash": "2217fc59b0e6bd88af6728f8305a12fa",
  "result": {
    "markdown": "---\ntitle: \"7A: Wide Data, PCA and EFA\"\nparams: \n    SHOW_SOLS: FALSE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n:::lo\nThis reading:  \n\n- working with wide data\n- data reduction techniques: PCA and EFA\n\n:::\n\n\n# Working with wide data\n\ncopy in stuff from standalone pages\n\n\n# PCA\n\n- the goal of PCA\n  - pca cov vs cor\n- the math (either rewrite or remove)\n- pca in R\n- doing PCA\n- the output  \n- methods for evaluating number of components\n- extracting and using component scores\n\n# EFA\n\n- the goal of EFA\n  - often multiple times and comparing solutions\n  - rotation vs the orthogonality of PCA\n  - this brings indeterminacy.\n  - factor scores must be estimated, rather than extracted\n- methods of evaluating number of components\n- doing EFA\n  - the output\n- estimating factor scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### The goal of PCA  \n\nThe goal of principal component analysis (PCA) is to find a _smaller_ number of uncorrelated variables which are linear combinations of the original ( _many_ ) variables and explain most of the variation in the data.\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. \nHow we measure such constructs is a very important consideration for research. The things we're interested in are very rarely the things we are *directly* measuring. \n\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety? ^[Even if we cut open someone's brain, it's unclear what we would be looking for in order to 'measure' it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings]. More often than not, we measure these things using questionnaire based methods, to capture the multiple dimensions of the thing we are trying to assess. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of \"anxiety\"). But they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess \"effects on anxiety\", rather than \"effects on anxiety q1 + effects on anxiety q2 + ...\", etc.  \n\nThis leads us to the idea of *reducing the dimensionality of our data*. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? \n\n:::\n\n\n\n::: {.callout-caution collapse=\"true\"}\n#### Optional: (some of) the math behind it  \n\nDoing data reduction can feel a bit like magic, and in part that's just because it's quite complicated. \n\n**The intuition**  \n\nOne way we might construct a square matrix that is symmetric along the diagonal is to compute the product of a vector $\\mathbf{f}$ with its transpose $\\mathbf{f'}$:  \n$$\n\\begin{equation*}\n\\mathbf{f} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\qquad \n\\mathbf{f} \\mathbf{f'} = \n\\begin{bmatrix}\n0.60 \\\\ \n0.77 \\\\\n0.69 \\\\\n0.83 \\\\\n0.60 \\\\\n0.88 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n0.60, 0.77, 0.69, 0.83, 0.60, 0.88 \\\\\n\\end{bmatrix} \n\\qquad = \\qquad\n\\begin{bmatrix}\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 0.54, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 0.41, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 0.66, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 0.79 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nThe difference between this an a correlation matrix is that in the correlation matrix the diagonal has values of 1 (the correlation of a variable with itself is 1).  and lets call it **R**.\n$$\n\\begin{equation*}\n\\mathbf{R} = \n\\begin{bmatrix}\n1.00, 0.42, 0.36, 0.48, 0.30, 0.54 \\\\\n0.38, 1.00, 0.46, 0.62, 0.38, 0.69 \\\\\n0.34, 0.48, 1.00, 0.55, 0.34, 0.62 \\\\\n0.42, 0.58, 0.50, 1.00, 0.42, 0.75 \\\\\n0.30, 0.42, 0.36, 0.48, 1.00, 0.54 \\\\\n0.44, 0.62, 0.53, 0.70, 0.44, 1.00 \\\\\n\\end{bmatrix} \n\\end{equation*} \n$$\n\nPCA is about trying to determine the vector **f** which gets close to generating the correlation matrix **R**. It's a bit like unscrambling eggs!  \n\nWe start by expressing the correlation matrix $R$ as the product of a matrix $C$ and it's inverse $C'$, where $\\mathbf{C}$ are our \"principal components\" - a set of orthogonal vectors that together can reproduce the correlation matrix.  \n$\\mathbf{R = CC'}$.  \n\n<!-- If $n$ is number of variables in $R$, then $i^{th}$ component $C_i$ is the linear sum of each variable multiplied by some weighting:   -->\n<!-- $$ -->\n<!-- C_i = \\sum_{j=1}^{n}w_{ij}x_{j} -->\n<!-- $$ -->\n\n**How do we find $C$?**\n\nThis is where \"eigen decomposition\" comes in.  \nFor the $n \\times n$ correlation matrix $\\mathbf{R}$, there is an **eigenvector** $x_i$ that solves the equation \n$$\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n$$\nWhere the vector multiplied by the correlation matrix is equal to some **eigenvalue** $\\lambda_i$ multiplied by that vector.  \nWe can write this without subscript $i$ as: \n$$\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n$$\nthe vectors which make up $\\mathbf{X}$ must be orthogonal [($\\mathbf{XX' = I}$)](https://miro.medium.com/max/700/1*kyg5XbrY1AOB946IE5nWWg.png), which means that $\\mathbf{R = X \\lambda X'}$\n \nWe can actually do this in R manually. \nCreating a correlation matrix:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# lets create a correlation matrix, as the product of ff'\nf <- c(.5,.7,.6,.8,.5,.9)\nR <- f %*% t(f)\n#give rownames and colnames\nrownames(R)<-colnames(R)<-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)<-1\nR\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     V1   V2   V3   V4   V5   V6\nV1 1.00 0.35 0.30 0.40 0.25 0.45\nV2 0.35 1.00 0.42 0.56 0.35 0.63\nV3 0.30 0.42 1.00 0.48 0.30 0.54\nV4 0.40 0.56 0.48 1.00 0.40 0.72\nV5 0.25 0.35 0.30 0.40 1.00 0.45\nV6 0.45 0.63 0.54 0.72 0.45 1.00\n```\n:::\n:::\n\n\nEigen Decomposition\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# do eigen decomposition\ne <- eigen(R)\nprint(e, digits=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3.26 0.75 0.70 0.59 0.44 0.26\n\n$vectors\n      [,1]     [,2]   [,3]  [,4]  [,5]   [,6]\n[1,] -0.33  7.1e-01  0.579  0.20  0.11 -0.066\n[2,] -0.43  1.6e-15 -0.197 -0.56  0.66 -0.182\n[3,] -0.38  7.4e-15 -0.515  0.73  0.20 -0.103\n[4,] -0.46  5.4e-16 -0.126 -0.22 -0.67 -0.523\n[5,] -0.33 -7.1e-01  0.579  0.20  0.11 -0.066\n[6,] -0.49  8.7e-16 -0.095 -0.14 -0.24  0.821\n```\n:::\n:::\n\n\nThe eigenvectors are orthogonal ($\\mathbf{XX' = I}$):\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(e$vectors %*% t(e$vectors),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n```\n:::\n:::\n\n\nThe Principal Components $\\mathbf{C}$ are the eigenvectors scaled by the square root of the eigenvalues:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#eigenvectors\ne$vectors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]          [,2]        [,3]       [,4]       [,5]        [,6]\n[1,] -0.3326559  7.071068e-01  0.57920379  0.1951175  0.1068041 -0.06621997\n[2,] -0.4250458  1.582068e-15 -0.19655129 -0.5624420  0.6566682 -0.18207040\n[3,] -0.3824152  7.424616e-15 -0.51514632  0.7338344  0.1983883 -0.10252830\n[4,] -0.4606244  5.412337e-16 -0.12644568 -0.2207487 -0.6701921 -0.52340157\n[5,] -0.3326559 -7.071068e-01  0.57920379  0.1951175  0.1068041 -0.06621997\n[6,] -0.4894896  8.743006e-16 -0.09512771 -0.1423882 -0.2397026  0.82074293\n```\n:::\n\n```{.r .cell-code}\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 1.805024 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[2,] 0.000000 0.8660254 0.0000000 0.0000000 0.0000000 0.0000000\n[3,] 0.000000 0.0000000 0.8385558 0.0000000 0.0000000 0.0000000\n[4,] 0.000000 0.0000000 0.0000000 0.7671097 0.0000000 0.0000000\n[5,] 0.000000 0.0000000 0.0000000 0.0000000 0.6627507 0.0000000\n[6,] 0.000000 0.0000000 0.0000000 0.0000000 0.0000000 0.5108962\n```\n:::\n\n```{.r .cell-code}\nC <- e$vectors %*% diag(sqrt(e$values))\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]          [,2]       [,3]       [,4]       [,5]        [,6]\n[1,] -0.6004520  6.123724e-01  0.4856947  0.1496765  0.0707845 -0.03383153\n[2,] -0.7672181  1.370111e-15 -0.1648192 -0.4314547  0.4352073 -0.09301908\n[3,] -0.6902688  6.429906e-15 -0.4319790  0.5629315  0.1314820 -0.05238132\n[4,] -0.8314383  4.687222e-16 -0.1060318 -0.1693385 -0.4441703 -0.26740389\n[5,] -0.6004520 -6.123724e-01  0.4856947  0.1496765  0.0707845 -0.03383153\n[6,] -0.8835406  7.571666e-16 -0.0797699 -0.1092274 -0.1588631  0.41931448\n```\n:::\n:::\n\n\nAnd we can reproduce our correlation matrix, because $\\mathbf{R = CC'}$. \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nC %*% t(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.35 0.30 0.40 0.25 0.45\n[2,] 0.35 1.00 0.42 0.56 0.35 0.63\n[3,] 0.30 0.42 1.00 0.48 0.30 0.54\n[4,] 0.40 0.56 0.48 1.00 0.40 0.72\n[5,] 0.25 0.35 0.30 0.40 1.00 0.45\n[6,] 0.45 0.63 0.54 0.72 0.45 1.00\n```\n:::\n:::\n\n\nNow lets imagine we only consider 1 principal component.  \nWe can do this with the `principal()` function: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(psych)\npc1<-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 1, rotate = \"none\", covar = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n    PC1   h2   u2 com\nV1 0.60 0.36 0.64   1\nV2 0.77 0.59 0.41   1\nV3 0.69 0.48 0.52   1\nV4 0.83 0.69 0.31   1\nV5 0.60 0.36 0.64   1\nV6 0.88 0.78 0.22   1\n\n                PC1\nSS loadings    3.26\nProportion Var 0.54\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.96\n```\n:::\n:::\n\n\nLook familiar? It looks like the first component we computed manually. The first column of $\\mathbf{C}$:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncbind(pc1$loadings, C=C[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         PC1          C\nV1 0.6004520 -0.6004520\nV2 0.7672181 -0.7672181\nV3 0.6902688 -0.6902688\nV4 0.8314383 -0.8314383\nV5 0.6004520 -0.6004520\nV6 0.8835406 -0.8835406\n```\n:::\n:::\n\nWe can now ask \"how well does this component (on its own) recreate our correlation matrix?\" \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nC[,1] %*% t(C[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.3605425 0.4606776 0.4144732 0.4992388 0.3605425 0.5305237\n[2,] 0.4606776 0.5886235 0.5295867 0.6378945 0.4606776 0.6778683\n[3,] 0.4144732 0.5295867 0.4764710 0.5739159 0.4144732 0.6098805\n[4,] 0.4992388 0.6378945 0.5739159 0.6912897 0.4992388 0.7346095\n[5,] 0.3605425 0.4606776 0.4144732 0.4992388 0.3605425 0.5305237\n[6,] 0.5305237 0.6778683 0.6098805 0.7346095 0.5305237 0.7806440\n```\n:::\n:::\n\nIt looks close, but not quite. How much not quite? Measurably so!\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR - (C[,1] %*% t(C[,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            V1          V2         V3          V4          V5          V6\nV1  0.63945745 -0.11067758 -0.1144732 -0.09923876 -0.11054255 -0.08052369\nV2 -0.11067758  0.41137646 -0.1095867 -0.07789449 -0.11067758 -0.04786832\nV3 -0.11447323 -0.10958666  0.5235290 -0.09391590 -0.11447323 -0.06988050\nV4 -0.09923876 -0.07789449 -0.0939159  0.30871033 -0.09923876 -0.01460953\nV5 -0.11054255 -0.11067758 -0.1144732 -0.09923876  0.63945745 -0.08052369\nV6 -0.08052369 -0.04786832 -0.0698805 -0.01460953 -0.08052369  0.21935596\n```\n:::\n:::\n\n\nNotice the values on the diagonals of $\\mathbf{c_1}\\mathbf{c_1}'$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiag(C[,1] %*% t(C[,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3605425 0.5886235 0.4764710 0.6912897 0.3605425 0.7806440\n```\n:::\n:::\n\nThese aren't 1, like they are in $R$. But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar? \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npc1$communality\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       V1        V2        V3        V4        V5        V6 \n0.3605425 0.5886235 0.4764710 0.6912897 0.3605425 0.7806440 \n```\n:::\n:::\n\nAnd likewise the 1 minus these is the unexplained variance:  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - diag(C[,1] %*% t(C[,1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6394575 0.4113765 0.5235290 0.3087103 0.6394575 0.2193560\n```\n:::\n\n```{.r .cell-code}\npc1$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       V1        V2        V3        V4        V5        V6 \n0.6394575 0.4113765 0.5235290 0.3087103 0.6394575 0.2193560 \n```\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/panelset-0.2.6/panelset.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/panelset-0.2.6/panelset.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}