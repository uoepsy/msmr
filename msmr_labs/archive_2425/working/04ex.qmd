---
title: "Week 4 Exercises: Nested and Crossed"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
library(ggdist)
xaringanExtra::use_panelset()
qcounter <- function(){
  if(!exists("qcounter_i")){
    qcounter_i <<- 1
  }else{
    qcounter_i <<- qcounter_i + 1
  }
  qcounter_i
}
library(lme4)
```


# Psychoeducation Treatment Effects

```{r}
#| eval: false
simm2<-function(seed=NULL,b0=0,b1=1,b2=1,z0=1,z1=1,e=1){
  if(!is.null(seed)){
    set.seed(seed)
  }
  n_groups = round(runif(1,1,15))*2
  npg = 5
  g = rep(1:n_groups, e = 5)      # the group identifier
  x = rep(0:4,n_groups)
  b = rep(0:1,e=n_groups/2)
  b = b[g]
  re0 = rnorm(n_groups, sd = z0)  # random intercepts
  re  = re0[g]
  rex = rnorm(n_groups, sd = z1)  # random effects
  re_x  = rex[g]
  lp = (b0 + re) + (b1 + re_x)*x + b2*x*b 
  y = rnorm(length(g), mean = lp, sd = e) # create a continuous target variable
  # y_bin = rbinom(N, size = 1, prob = plogis(lp)) # create a binary target variable
  data.frame(x, b=factor(b), g=factor(g), y)
}
eseed = round(runif(1,1e3,1e6))
set.seed(645533)
big = tibble(
    school = 1:30,
    int = rnorm(30,20,1),
    sl = rnorm(30,-.3,.5),
    intr = rnorm(30,-1,.5),
    z0 = runif(30,.5,1),
    z1 = runif(30,.5,1),
    e = runif(30,.5,1)
  )
  big = big |> mutate(
    data = pmap(list(int,sl,intr,z0,z1,e), ~simm2(b0=..1,b1=..2,b2=..3,z0=..4,z1=..5,e=..6))
  ) |> unnest(data)

  # m = lmer(round(y)~x*b+(1+x*b|school)+(1+x|school:g),big)
  # broom.mixed::augment(m) |>
  #   ggplot(aes(x=x,y=.fitted,col=factor(b)))+
  #   geom_point(aes(y=`round(y)`))+
  #   geom_line(aes(group=interaction(school,g)))

tnames = unique(replicate(100,paste0(sample(LETTERS,2),collapse="")))
  
big |> transmute(
    therapist = tnames[school],
    group = ifelse(b==0,"Control","Treatment"),
    patient = pmap_chr(list(therapist,group,g),~paste(..1,..2,..3,sep="_")),
    visit = x,
    GAD = pmin(35,pmax(7,round(y)+5))
  ) |> select(patient,visit,GAD) |>
  pivot_wider(names_from=visit,values_from=GAD, names_prefix="visit_") |>
  write_csv(file="../../data/msmr_gadeduc.csv")

```



:::frame
__Data: gadeduc.csv__

```{r}
#| include: false
geduc = read_csv("../../data/msmr_gadeduc.csv")
geduc1 = geduc |> 
  pivot_longer(2:last_col(), names_to="visit",values_to="GAD") |>
  mutate(
    visit = as.numeric(gsub("visit_","",visit))
  ) |>
  separate(patient, into=c("therapist","group","patient"), sep="_")
# m = lmer(GAD~visit*group+(1+visit*group|therapist)+(1+visit|therapist:patient),geduc1)
# summary(m)
tn = geduc1 |> group_by(therapist) |> summarise(np = n_distinct(patient))
```

This is synthetic data from a randomised controlled trial, in which `r nrow(tn)` therapists randomly assigned participants (each therapist saw between `r min(tn[,'np'])` and `r max(tn[,'np'])` patients) to a control or treatment group, and monitored the participants' scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).  
The control group of participants received standard sessions offered by the therapists. 
For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and participants were given relevant tasks to complete between each session. All participants had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).  

The data are available at [https://uoepsy.github.io/data/msmr_gadeduc.csv](https://uoepsy.github.io/data/msmr_gadeduc.csv){target="_blank"}

You can find a data dictionary below:
```{r}
#| echo: false
tibble(
    variable = names(geduc),
    description = c("A patient code in which the labels take the form <Therapist initials>_<group>_<patient number>.","Score on the GAD7 at baseline", 
                    "GAD7 at 1 month assessment",
                    "GAD7 at 2 month assessment",
                    "GAD7 at 3 month assessment",
                    "GAD7 at 4 month assessment"
                    )
) |> gt::gt()
```

:::


`r qbegin(qcounter())`

- read in the data. uh-oh... this isn't data in the same shape as we've been giving you thus far.  
- can you get it into a suitable format for modelling?  


::: {.callout-tip collapse="true"}
#### Hints

- it's wide, and we want it long
- once it's long. "visit_0", "visit_1",.. needs to become the numbers 0, 1, ...
- one variable (`patient`) contains lots of information that we want to separate out.     
`data |> separate(variable, into=c("thing1","thing2","thing3",...), sep = "separator")`  


:::


`r qend()`
`r solbegin(label="1 - reshaping", slabel=F,show=T, toggle=params$TOGGLE)`
```{r}
geduc = read_csv("../../data/msmr_gadeduc.csv")

geduc |> 
  pivot_longer(2:last_col(), names_to="visit",values_to="GAD")
```


`r solend()`
`r solbegin(label="2 - time is numeric", slabel=F,show=T, toggle=params$TOGGLE)`
```{r}
geduc |> 
  pivot_longer(2:last_col(), names_to="visit",values_to="GAD") |>
  mutate(
    visit = as.numeric(gsub("visit_","",visit))
  ) 
```


`r solend()`
`r solbegin(label="3 - splitting up the patient variable", slabel=F,show=T, toggle=params$TOGGLE)`
```{r}
geduc_long <- geduc |> 
  pivot_longer(2:last_col(), names_to="visit",values_to="GAD") |>
  mutate(
    visit = as.numeric(gsub("visit_","",visit))
  ) |>
  separate(patient, into=c("therapist","group","patient"), sep="_")

geduc_long
```

`r solend()`


`r qbegin(qcounter())`
Visualise the data. Does it look like the treatment had an effect?  
Does it look like it had an effect for every therapist?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +
  stat_summary(geom="pointrange")
```


```{r}
ggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +
  stat_summary(geom="pointrange") +
  facet_wrap(~therapist)
```

`r solend()`


`r qbegin(qcounter())`
fit a model to test if the psychoeducational treatment is associated with more improvement in anxiety over time.  
`r qend()`
`r solbegin(label="1 - fixed effects", slabel=F,show=T, toggle=params$TOGGLE)`

```{r}
#| eval: false
lmer(GAD ~ visit * group + ...
       ...
     data = geduc_long)
```

`r solend()`
`r solbegin(label="2 - grouping structure", slabel=F,show=T, toggle=params$TOGGLE)`

```{r}
#| eval: false
lmer(GAD ~ visit * group + ...
       ( ... | therapist) + 
       ( ... | therapist:patient),
     data = geduc_long)
```


`r solend()`
`r solbegin(label="3 - random effects", slabel=F,show=T, toggle=params$TOGGLE)`

```{r}
mod1 <- lmer(GAD ~ visit*group + 
               (1+visit*group|therapist)+
               (1+visit|therapist:patient),
             geduc_long)
```

`r solend()`

`r qbegin(qcounter())`
for each of the models below, what is wrong or suboptimal about the random effect structure?  

```{r}
#| eval: false
modelA <- lmer(GAD ~ visit*group + 
               (1+visit*group|therapist)+
               (1+visit|patient),
             geduc_long)

modelB <- lmer(GAD ~ visit*group + 
               (1+visit*group|therapist/patient),
             geduc_long)
```


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

- patient doesn't capture the different patients _within_ therapists. so it actually fits crossed random effects and treats all data where `patient==1` as from the same group (even if this includes several different patients' worth of data from different therapists!)

- using the `/` means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can't vary by patient, so this doesn't work. hence why we need to split them up into `(...|therapist)+(...|therapist:patient)`.  


`r solend()`


`r qbegin(qcounter())`
let's suppose I don't want the psychoeducation treatment, I just want the standard therapy sessions that the 'Control' group received. Which therapist should I go to?  

::: {.callout-tip collapse="true"}
#### Hints

dotplot.ranef.mer might help here

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
WG, EQ, or EI.. 

why? they all have the most negative slope of visit  

```{r}
dotplot.ranef.mer(ranef(mod1))$therapist
```

`r solend()`


`r qbegin(qcounter())`
recreate this plot.  

The faint lines represent the model estimated lines for each patient.  The points and ranges represent our fixed effect estimates and their uncertainty.  

```{r} 
#| echo: false
effplot <- effects::effect("visit*group",mod1) |>
  as.data.frame()

broom.mixed::augment(mod1) |> 
  mutate(
    upatient = paste0(therapist,patient)
  ) |>
  ggplot(aes(x=visit,y=.fitted,col=group))+
  stat_summary(geom="line", aes(group=upatient,col=group), alpha=.1)+
  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+
  labs(x="- Month -",y="GAD7")

```


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
effplot <- effects::effect("visit*group",mod1) |>
  as.data.frame()

broom.mixed::augment(mod1) |> 
  mutate(
    upatient = paste0(therapist,patient)
  ) |>
  ggplot(aes(x=visit,y=.fitted,col=group))+
  stat_summary(geom="line", aes(group=upatient,col=group), alpha=.1)+
  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+
  labs(x="- Month -",y="GAD7")
```


`r solend()`











# Test Enhanced Learning

TODO switch to logistic

:::frame
__Data: Test-enhanced learning__  

An experiment was run to conceptually replicate "test-enhanced learning" (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (`StudyStudy`), the other group studied the material once then did a test (`StudyTest`). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (`Test_word`). 
One of the researchers' questions concerned how test-enhanced learning influences time-to-recall. 

The critical (replication) prediction is that the `StudyStudy` group should perform somewhat better on the immediate recall test, but the `StudyTest` group will retain the material better and thus perform better on the 1-week follow-up test.

The following code loads the data into your R environment by creating a variable called `tel`:

```{r eval=F}
load(url("https://uoepsy.github.io/data/testenhancedlearning.RData"))
```

```{r echo=FALSE} 
load(url("https://uoepsy.github.io/data/testenhancedlearning.RData"))
tibble(
  variable=names(tel),
  description=c("Unique Participant Identifier", "Group denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)","Time of recall test ('min' = Immediate, 'week' = One week later)","Word being recalled (175 different test words)","Whether or not the word was correctly recalled","Time to recall word (milliseconds)")
) %>% 
    kableExtra::kbl() %>%
    kableExtra::kable_styling(full_width = FALSE)
```

:::



`r qbegin(qcounter())`
Load and plot the data. Does it look like the effect was replicated?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
load(url("https://uoepsy.github.io/data/testenhancedlearning.RData"))

ggplot(tel, aes(Delay, Correct, col=Group)) + 
  stat_summary(fun.data=mean_se, geom="pointrange")+
  theme_light()
```

That looks like test-enhanced learning to me!  


`r solend()`

`r qbegin(qcounter())`

Test the critical hypothesis using a mixed-effects model.  

Fit the maximal random effect structure supported by the experimental design. Simplify the random effect structure until you reach a model that converges.  

Some of the models you attempt here might take time to fit. This is normal, and you can cancel the estimation at any time by pressing the escape key.  
I suggest you write your initial model, set it running, and then look at the first solution to see if it converged for me. 
Assume that if it didn't work for me, it also won't work for you. In which case cancel it and write your next model (then look at the next solution as that model is fitting, and so on.. )  

::: {.callout-tip collapse="true"}
#### Hints

What we're aiming to do here is to follow [Barr et al.'s](https://doi.org/10.1016/j.jml.2012.11.001) advice of defining our maximal model and then removing only the terms to allow a non-singular fit.  

+ What kind of model will you use? What is our outcome? is it binary, or continuous? 
+ We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects?

:::

`r qend()` 
`r solbegin(label="1", slabel=F,show=T, toggle=params$TOGGLE)`

```{r}
#| echo: false
# save(mod1,mod2,mod3,mod4,file="data/telmodels.Rdata")
load("data/telmodels.Rdata")
```


This one took my computer about 6 minutes.  
```{r}
#| eval: false
mod1 <- glmer(Correct ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay * Group | Test_word),
             family=binomial, data=tel)
```
<p style="color:red;font-size:.8em">
Warning message:<br>
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :<br>
  Model failed to converge with max|grad| = 0.0184773 (tol = 0.002, component 1)
</p>

```{r}
VarCorr(mod1)
```

`r solend()`
`r solbegin(label="2", slabel=F,show=T, toggle=params$TOGGLE)`

lets remove the interaction in the by-word random effects.  
This one took about 5 minutes...
```{r}
#| eval: false
mod2 <- glmer(Correct ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay + Group | Test_word),
             family=binomial, data=tel)
```
<p style="color:red;font-size:.8em">
Warning message:<br>
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :<br>
  Model failed to converge with max|grad| = 0.00887744 (tol = 0.002, component 1)
</p>

```{r}
VarCorr(mod2)
```

`r solend()`
`r solbegin(label="3", slabel=F,show=T, toggle=params$TOGGLE)`

We still have a singular fit here. Thinking about the study, if we are going to remove __one__ of the by-testword random effects (`Delay` or `Group`), which one do we consider to be more theoretically justified? Is the effect of Delay likely to vary by test-words? More so than the effect of group is likely to vary by test-words? Quite possibly - there's no obvious reason for _certain_ words to be more memorable for people in one group vs another. But there is reason for words to vary in the effect that delay of one week has - how familiar a word is will likely influence the amount to which a week's delay has on recall.   

Let's remove the by-testword random effect of group. 
```{r}
#| eval: false
mod3 <- glmer(Correct ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 + Delay | Test_word),
             family=binomial, data=tel)
```

This one converges! But we still have a correlation of -1. Why did we not get a warning message?  
```{r}
VarCorr(mod3)
```

```{r}
isSingular(mod3)
```

```{r}
rr = allFits(mod3)
```




`r solend()`
`r solbegin(label="4", slabel=F,show=T, toggle=params$TOGGLE)`

```{r}
#| eval: false
mod4 <- glmer(Correct ~ Delay*Group +
             (1 + Delay | Subject_ID) +
             (1 | Test_word),
             family=binomial, data=tel)
```
```{r}
isSingular(mod4)
```





`r solend()`
`r solbegin(label="4", slabel=F,show=T, toggle=params$TOGGLE)`

fixed effects from each model: 
```{r}
#| echo: false
bind_rows(
 broom.mixed::tidy(mod1) |> filter(effect=="fixed") |> mutate(mod="mod1"), 
 broom.mixed::tidy(mod2) |> filter(effect=="fixed") |> mutate(mod="mod2"), 
 broom.mixed::tidy(mod3) |> filter(effect=="fixed") |> mutate(mod="mod3"), 
 broom.mixed::tidy(mod4) |> filter(effect=="fixed") |> mutate(mod="mod4")
) |> select(mod,term,estimate) |> pivot_wider(values_from=estimate,names_from=mod)
```

standard errors of fixed effects for each model:  
```{r}
#| echo: false
bind_rows(
 broom.mixed::tidy(mod1) |> filter(effect=="fixed") |> mutate(mod="mod1"), 
 broom.mixed::tidy(mod2) |> filter(effect=="fixed") |> mutate(mod="mod2"), 
 broom.mixed::tidy(mod3) |> filter(effect=="fixed") |> mutate(mod="mod3"), 
 broom.mixed::tidy(mod4) |> filter(effect=="fixed") |> mutate(mod="mod4")
) |> select(mod,term,std.error) |> pivot_wider(values_from=std.error,names_from=mod)
```


`r solend()` 



`r qbegin(qcounter())`
plot!  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(effects)
effplot <- effect("Delay:Group", mod4) |>
  as.data.frame()

ggplot(effplot, aes(Delay, fit, color=Group)) + 
  geom_pointrange(aes(ymax=upper, ymin=lower), 
                  position=position_dodge(width = 0.2))+
  theme_classic() # just for a change :)
```

`r solend()`


`r qbegin(qcounter())`
What should we do with this information? How can we apply test-enhanced learning to learning R and statistics?
`r qend()` 
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
You'll get the benefits of test-enhanced learning if you try yourself before looking at the solutions! If you don't test yourself, you're more likely to forget it in the long run. 
`r solend()` 


# Vocab Development




:::frame
__Data: pvt_bilingual.csv__ 

```{r}
#| echo: false
pvt <- read_csv("../../data/pvt_bilingual.csv")
pvtsc = pvt |> count(school,child,isBilingual)
```

`r row(pvtsc)` children from `r length(unique(pvt$school))` schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). `r sum(pvtsc$isBilingual==1)` were monolingual English speakers, and `r sum(pvtsc$isBilingual==1)` were bilingual (english + another language). 

Previous research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. 

The data are available at [https://uoepsy.github.io/data/pvt_bilingual.csv](https://uoepsy.github.io/data/pvt_bilingual.csv).  

```{r echo=FALSE}
tibble(variable = names(pvt),
       description = c(
         "Child's name",
         "School Identifier",
         "Binary variable indicating whether the child is monolingual (0) or bilingual (1)",
         "Age (years)",
         "Score on the Picture Vocab Test (PVT). Scores range 0 to 60")
) %>% gt::gt()
```

:::

`r qbegin(qcounter())`
Let's start by thinking about our clustering - we'd like to know how much of the variance in PVT scores is due to the clustering of data within children, who are themselves within schools. One easy way of assessing this is to fit an _intercept only_ model, which has the appropriate random effect structure.  

Using the model below, calculate the proportion of variance attributable to the clustering of data within children within schools.  

```{r}
pvt_null <- lmer(PVT ~ 1 +  (1 | school/child), data = pvt)
```

::: {.callout-tip collapse="true"}
#### Hints
the random intercept variances are the building blocks here. There are no predictors in this model, so all the variance in the outcome gets attributed to either school-level nesting, child-level nesting, or else is lumped into the residual.   
:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
# pvt <- read_csv("../../data/bntmono.csv")
pvt_null <- lmer(PVT ~ 1 +  (1 | school/child), data = pvt)
summary(pvt_null)
```

```{r}
#| echo: false
vcres = VarCorr(pvt_null) |> as.data.frame()
vcres = round(vcres$vcov,2)
```


As we can see from `summary(bnt_null)`, the random intercept variances are `r vcres[1]` for child-level, `r vcres[2]` for school-level, and the residual variance is `r vcres[3]`.  

So child level differences account for $\frac{`r vcres[1]`}{`r paste0(vcres,collapse=" + ")`} = `r round(vcres[1]/sum(vcres),2)`$ of the variance in PVT scores, and child & school differences together account for $\frac{`r paste0(vcres[1:2],collapse=" + ")`}{`r paste0(vcres,collapse=" + ")`} = `r round(sum(vcres[1:2])/sum(vcres),2)`$ of the variance.  


`r solend()`

`r qbegin(paste0(qcounter(), " - Less Guided"))`
Conduct an analysis to estimate the differences in trajectories of vocabulary development between children attending bilingual schools vs those attending monolingual schools.  

Write up your results.  


::: {.callout-tip collapse="true"}
#### Hints

- make things factors
- always plot your data!
- 

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
ggplot(pvt, aes(x=age,y=PVT,col=factor(isBilingual)))+
  stat_summary(geom="pointrange")+
  stat_summary(geom="line")
```





```{r}
pvt <- pvt |> mutate(
  poly1 = poly(age, 3)[,1],
  poly2 = poly(age, 3)[,2],
  poly3 = poly(age, 3)[,3],
)
```

```{r}
#| eval: false
mod1 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + (poly1 + poly2 + poly3)*isBilingual | school) + 
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)

mod2 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + isBilingual * (poly1 + poly2) + poly3 | school) +
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)

mod3 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + isBilingual * (poly1 + poly2) | school) +
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)

mod4 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + isBilingual * (poly1) + poly2 | school) +
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)

mod5 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + isBilingual * (poly1) | school) +
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)

mod6 <- lmer(PVT ~ 1 + (poly1 + poly2 + poly3) * isBilingual + 
       (1 + isBilingual + poly1 | school) +
       (1 + (poly1 + poly2 + poly3) | school:child),
     data = pvt)
```








`r solend()`






