[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariate Statistics and Methodology in R",
    "section": "",
    "text": "Multivariate Statistics and Methodology in R (MSMR) is an advanced semester-long course designed for Masters students in psychology seeking a deeper understanding of statistical techniques to analyze complex data sets with multiple sources of variation. Building on the foundation laid by the Univariate Statistics and Methodology in R (USMR) course, MSMR extends students’ analytical repertoire to encompass multilevel models, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM).\nThe initial half of the course introduces students to the intricacies of multilevel models, providing a solid theoretical framework for understanding hierarchical data structures. Students will gain practical insights into applying these models to address research questions involving nested data and varying sources of variation.\nThe second half of the course delves into methods such as PCA and EFA for reducing dimensionality of data, before moving to Confirmatory Factor models and subsequently Structural Equation Models as a means of modeling and testing our theories about psychological constructs."
  },
  {
    "objectID": "07ex.html",
    "href": "07ex.html",
    "title": "Week 7 Exercises: PCA & EFA",
    "section": "",
    "text": "New packages\nWe’re going to be needing some different packages this week (no more lme4!).\nMake sure you have these packages installed:\n\npsych\n\nGPArotation\n\ncar",
    "crumbs": [
      "Week 7 Exercises: PCA & EFA"
    ]
  },
  {
    "objectID": "07ex.html#footnotes",
    "href": "07ex.html#footnotes",
    "title": "Week 7 Exercises: PCA & EFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎",
    "crumbs": [
      "Week 7 Exercises: PCA & EFA"
    ]
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: lmm_gadeduc.csv\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nSolution 1. Here’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient      visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 VC_Control_1      24      24      26      29      28\n2 VC_Control_2      24      26      28      29      30\n3 VC_Control_3      25      29      27      29      30\n4 VC_Control_4      24      25      25      26      26\n5 VC_Control_5      28      28      27      29      28\n6 VC_Control_6      26      28      25      27      28\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 and shoving their values into one variable, and keeping the name of the column they come from as another variable:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,410 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 VC_Control_1 visit_0    24\n 2 VC_Control_1 visit_1    24\n 3 VC_Control_1 visit_2    26\n 4 VC_Control_1 visit_3    29\n 5 VC_Control_1 visit_4    28\n 6 VC_Control_2 visit_0    24\n 7 VC_Control_2 visit_1    26\n 8 VC_Control_2 visit_2    28\n 9 VC_Control_2 visit_3    29\n10 VC_Control_2 visit_4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n2 - time is numeric\n\n\n\nSolution 2. Now we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" with nothingness \"\", and then convert them to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,410 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 VC_Control_1     0    24\n 2 VC_Control_1     1    24\n 3 VC_Control_1     2    26\n 4 VC_Control_1     3    29\n 5 VC_Control_1     4    28\n 6 VC_Control_2     0    24\n 7 VC_Control_2     1    26\n 8 VC_Control_2     2    28\n 9 VC_Control_2     3    29\n10 VC_Control_2     4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nSolution 3. Finally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,410 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 VC        Control 1           0    24\n 2 VC        Control 1           1    24\n 3 VC        Control 1           2    26\n 4 VC        Control 1           3    29\n 5 VC        Control 1           4    28\n 6 VC        Control 2           0    24\n 7 VC        Control 2           1    26\n 8 VC        Control 2           2    28\n 9 VC        Control 2           3    29\n10 VC        Control 2           4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect?\nDoes it look like it worked for every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Here’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists EU and OD don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\n\n\n\n\n\n1 - fixed effects\n\n\n\nSolution 5. We want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nSolution 6. We have multiple observations for each of the 482 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nSolution 7. Note that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can’t vary by patient, so this doesn’t work. hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndotplot.ranef.mer() might help here!\nYou can read about ranef in Chapter 2 #making-model-predictions.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. It would be best to go to one of the therapists SZ, YS, or IT…\nWhy? These therapists all have the most negative slope of visit:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using the effects package.\nremember that the “patient” column doesn’t group observations into unique patients.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\nsee more in Chapter 2 #visualising-models\n\n\n\n\n\n\n\n\n\n1 - the relevant parts\n\n\n\nSolution 10. The effects package will give us the fixed effect estimates:\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\nWe want to get the fitted values for each patient. We can get fitted values using augment(). But the patient variable doesn’t capture the unique patients, it just captures their numbers (which aren’t unique to each therapist).\nSo we can create a new column called upatient which pastes together the therapists initials and the patient numbers\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n\n# A tibble: 2,410 × 17\n     GAD visit group   therapist patient upatient .fitted .resid  .hat .cooksd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    24     0 Control VC        1       VC1         24.2 -0.198 0.454 0.0210 \n 2    24     1 Control VC        1       VC1         25.3 -1.28  0.239 0.239  \n 3    26     2 Control VC        1       VC1         26.4 -0.360 0.186 0.0128 \n 4    29     3 Control VC        1       VC1         27.4  1.56  0.294 0.508  \n 5    28     4 Control VC        1       VC1         28.5 -0.522 0.563 0.284  \n 6    24     0 Control VC        2       VC2         24.8 -0.843 0.454 0.383  \n 7    26     1 Control VC        2       VC2         26.2 -0.171 0.239 0.00426\n 8    28     2 Control VC        2       VC2         27.5  0.502 0.186 0.0250 \n 9    29     3 Control VC        2       VC2         28.8  0.174 0.294 0.00633\n10    30     4 Control VC        2       VC2         30.2 -0.153 0.563 0.0246 \n# ℹ 2,400 more rows\n# ℹ 7 more variables: .fixed &lt;dbl&gt;, .mu &lt;dbl&gt;, .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;,\n#   .sqrtrwt &lt;dbl&gt;, .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\n\n\n\n\n\n2 - constructing the plot\n\n\n\nSolution 11. \n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")",
    "crumbs": [
      "Week 4 Exercises: Nested and Crossed"
    ]
  },
  {
    "objectID": "04ex.html#footnotes",
    "href": "04ex.html#footnotes",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎",
    "crumbs": [
      "Week 4 Exercises: Nested and Crossed"
    ]
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "Week 2 Exercises: Logistic and Longitudinal",
    "section": "",
    "text": "Data: msmr_apespecies.csv & msmr_apeage.csv\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/msmr_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/msmr_apespecies.csv.\n\n\n\n\n\n\nTable 1: Data Dictionary: msmr_apespecies.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nspecies\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Data Dictionary: msmr_apeage.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nape\nApe Name\n\n\nage\nAge at assessment (years)\n\n\ndominance\nDominance (Z-scored)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and check over it. Do any relevant cleaning/wrangling that might be necessary.\n\n\n\n\n\n1 - reading and joining\n\n\n\nSolution 1. We’ll read in both datasets, and then join them together.\n\nlibrary(tidyverse)\nlibrary(lme4)\nape_species &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apespecies.csv\")\nape_age &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apeage.csv\")\n\nSometimes is handy to check that all our participants are in both datasets:\n\n# are all the apes in ape_age also in ape_species?\nall(ape_age$ape %in% ape_species$ape)\n\n[1] TRUE\n\n# and vice versa?\nall(ape_species$ape %in% ape_age$ape)\n\n[1] TRUE\n\n\n\n\n\n\n\n\noptional - working with sets!\n\n\n\n\n\nI often default to using %in% and asking several questions to carefully make sure I know what’s going on - i.e. “is all of A %in% B? and is all of B %in% A, … etc.”\nThere is actually a neat way to ask these questions both at once using some handy functions (that I often forget about) to perform operations on “sets” (i.e. collections of things). These include:\n\nunion(x,y) - return everything that is in set x or in set y (or both)\nintersect(x,y) - return everything that is in both x and y\nsetdiff(x,y) - return everything in x that is not in y (not there’s asymmetry here!)\nsetequal(x,y) - are sets x and y equal?\n\nSo we can use these to ask if, e.g., the two sets of apes in each dataset are equal:\n\nsetequal(ape_species$ape,ape_age$ape)\n\n[1] TRUE\n\n\nIf you want a fun1 challenge, there are lots of other (less concise ways) that we can ask the same thing. Try and come up with a few different ways.\nE.g.: “is there anything in the union of the two sets that is not in the intersection of the two sets?”\n\nsetdiff(union(ape_species$ape,ape_age$ape), \n        intersect(ape_species$ape,ape_age$ape))\n\ncharacter(0)\n\n\n\n\n\nOkay, both datasets contain data for the same set of apes.\nLet’s join them:\n\napedat &lt;- full_join(ape_age, ape_species)\nhead(apedat)\n\n# A tibble: 6 × 4\n  ape     age dominance species   \n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 Joel      7       0.6 chimpanzee\n2 Joel      5       1.2 chimpanzee\n3 Joel      8       1.1 chimpanzee\n4 Joel      1       0.2 chimpanzee\n5 Joel      2       0.5 chimpanzee\n6 Joel      6       1   chimpanzee\n\n\n\n\n\n\n\n2 - identifying issues\n\n\n\nSolution 2. First off, we can see that we’ve got some weird typos. Some apes have been identified as “gorrila” but it is actually spelled “gorilla”.\nAlso, we’ve got people using two alternatives for the chimps: “chimp” and “chimpanzee”. We’ll need to combine those.\n\ntable(apedat$species)\n\n\n    bonobo      chimp chimpanzee    gorilla    gorrila  orangutan \n       187        146        127        211          2        157 \n\n\nAge looks like it has some weird values (possibly “-99”?), and there are possibly a few outliers in the dominance variable. Given that dominance is standardised, it is extremely unlikely that we would see values around 20.. They’re not “impossible”, but they’re so incredibly unlikely that I’d be more comfortable assuming they are typos:\n\nhist(apedat$age, breaks=20)\nhist(apedat$dominance, breaks=20)\n\n\n\n\n\n\n\n\nJust to see what the most extreme values of dominance are:\n\n# show the biggest 5 absolute values in dominance variable\nsort(abs(apedat$dominance), decreasing = TRUE)[1:5]\n\n[1] 21.2 19.4  3.9  2.9  2.9\n\n\n\n\n\n\n\n3 - cleaning up\n\n\n\nSolution 3. \n\napedat &lt;- apedat |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\n\n\n\n\nQuestion 2\n\n\nHow is this data structure “hierarchical” (or “clustered”)? How many levels do we have, and what are the observational units at each level?\n\n\n\n\n\nSolution\n\n\n\nSolution 4. We have a random sample of \\(\\underbrace{\\text{timepoints}}_{\\text{level 1}}\\) from a random sample of \\(\\underbrace{\\text{apes}}_{\\text{level 2}}\\).\n\n\n\n\nQuestion 3\n\n\nFor how many apes do we have data? How many of each species?\nHow many datapoints does each ape have?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’ve seen this last week too - counting the different levels in our data. See Chapter 4: logisticMLM - #getting-to-know-my-monkeys for an example (also about monkeys!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. We have 168 apes in our dataset:\n\nlength(unique(apedat$ape))\n\n[1] 168\n\n\nHere’s how many of each species:\n\napedat |&gt; \n  group_by(species) |&gt;\n  summarise(\n   n_apes = n_distinct(ape) \n  )\n\n# A tibble: 4 × 2\n  species   n_apes\n  &lt;chr&gt;      &lt;int&gt;\n1 bonobo        36\n2 chimp         56\n3 gorilla       46\n4 orangutan     30\n\n\nLet’s create a table of how many observations for each ape, and then we can create a table from that table, to show how many apes have 2 datapoints, how many have 3, 4, and so on:\n\ntable(apedat$ape) |&gt;\n  table() |&gt;\n  barplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nMake a plot to show how dominance changes for each ape as they get older.\n\n\n\n\n\n\nHints\n\n\n\n\n\nIn Chapter 5: Longitudinal MLM - #exploring-the-data we made a facet for each cluster (each participant). That was fine because we had only 20 people. In this dataset we have 168! That’s too many to facet. We could try using group aesthetic instead, to plot multiple lines on the same plot, or we could just plot a sample of our apes. This is all just an initial look at the data, after all.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Here’s a line for each ape, and a facet for each species:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_line(aes(group = ape)) + \n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nIt’s kind of hard to see the trend for each ape, so let’s also make a separate little linear model for each ape:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_smooth(aes(group = ape), method=lm, se=FALSE) + \n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nAlternatively, let’s take a sample of apes, and plot the same stuff but facetted:\n\napedat |&gt;\n  # choose the rows where ape ID is one of a random sample of 16 ape IDs\n  filter(ape %in% sample(unique(apedat$ape), 16) ) |&gt;\n  ggplot(aes(x = age, y = dominance, col = species))+\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE)+\n  facet_wrap(~ape)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nRecenter the age variable so that “0” becomes a 1-year old, which is the youngest age that we’ve got data on for any of our species.\nThen fit a model that estimates the differences between primate species in how dominance changes over time.\n\n\n\n\n\n\nHints\n\n\n\n\n\nthink slowly about “differences between primate species in how dominance changes over time”.\n\n“how dominance changes over time” – sounds like dominance ~ time\nso differences between primate species in this would require the interaction dominance ~ time * species\nwe have a random sample of apes, and for each of these we have multiple observations. We can model these as by-ape random effects - + (...... | ape)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\napedat$age = apedat$age-1 \n\nm.full &lt;- lmer(dominance ~ 1 + age * species + (1 + age | ape), data = apedat)\n\n\n\n\n\nQuestion 6\n\n\nDo primate species differ in the growth of dominance?\nPerform an appropriate test/comparison.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is asking about the age*species interaction, which in our model is represented by 3 parameters. To assess the overall question, it might make more sense to do a model comparison.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nm.int &lt;- lmer(dominance ~ 1 + age + species + (1 + age | ape), data = apedat)\n\nanova(m.int, m.full)\n\nData: apedat\nModels:\nm.int: dominance ~ 1 + age + species + (1 + age | ape)\nm.full: dominance ~ 1 + age * species + (1 + age | ape)\n       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \nm.int     9 806.67 849.11 -394.34    788.67                        \nm.full   12 801.16 857.74 -388.58    777.16 11.517  3   0.009237 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies differ in how dominance changes over adolescence (\\(\\chi^2(3) = 11.52, p = 0.009\\)).\n\n\n\n\n\nQuestion 7\n\n\nPlot the average model predicted values for each age.\nBefore you plot.. do you expect to see straight lines? (remember, not every ape is measured at age 2, or age 3, etc).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot (see the lecture).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. Averaging fitted values would give us straight lines if every ape had data at all ages, but in our study we have some apes with only 2 data points, and each ape has different set of ages (e.g., one ape might be measured at age 3, 6, and 10, another ape might be at ages 2 and 16).\n\nlibrary(broom.mixed)\n\naugment(m.full) |&gt;\nggplot(aes(age,dominance, color=species)) +\n  # the point ranges are our observations\n  stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n  # the lines are our average predictions  \n  stat_summary(aes(y=.fitted, linetype=species), fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the model based fixed effects:\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\neffects::effect(\"age*species\", m.full, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=age+1,y=fit,col=species))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=species),col=NA,alpha=.3) +  \n  scale_color_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  scale_fill_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  facet_wrap(~species) + \n  guides(col=\"none\",fill=\"none\") +\n  labs(x=\"Age (years)\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nIdentify what each of the fixed effects from the model represents (you might also want to get some p-values or confidence intervals).\n\n\n\n\n\n\nHints\n\n\n\n\n\nEach of the estimates should correspond to part of our plot from the previous question.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. Let’s get some confidence intervals:\n\nconfint(m.full, method=\"profile\",\n        parm = \"beta_\")\n\n                           2.5 %      97.5 %\n(Intercept)          -0.67066925 -0.17299177\nage                   0.02361398  0.08142209\nspecieschimp          0.13383485  0.77009884\nspeciesgorilla        0.28124162  0.94933844\nspeciesorangutan     -0.38909919  0.34257548\nage:specieschimp     -0.03973125  0.03392308\nage:speciesgorilla   -0.05012759  0.02799393\nage:speciesorangutan -0.10625760 -0.02167806\n\n\n\n\n\n\n\n\n\n\nterm\nest\nCI\ninterpretation\n\n\n\n\n(Intercept)\n-0.42\n[-0.67, -0.17]*\nestimated dominance of 1 year old bonobos (at left hand side of plot, bonobo line is lower than 0)\n\n\nage\n0.05\n[0.02, 0.08]*\nestimated change in dominance score for every year older a bonobo gets (slope of bonobo line)\n\n\nspecieschimp\n0.45\n[0.13, 0.77]*\nestimated difference in dominance scores at age 1 between bonobos and chimps (at left hand side of plot, chimp line is higher than bonobo line)\n\n\nspeciesgorilla\n0.62\n[0.28, 0.95]*\nestimated difference in dominance scores at age 1 between bonobos and gorillas (at left hand side of plot, gorilla line is higher than bonobo line)\n\n\nspeciesorangutan\n-0.02\n[-0.39, 0.34]\nno significant difference in dominance scores at age 1 between bonobos and orangutans (at the left hand side of our plot, orangutan line is similar height to bonobo line)\n\n\nage:specieschimp\n0.00\n[-0.04, 0.03]\nno significant difference between chimps and bonobos in the change in dominance for every year older (slope of chimp line is similar to slope of bonobo line)\n\n\nage:speciesgorilla\n-0.01\n[-0.05, 0.03]\nno significant difference between gorillas and bonobos in the change in dominance for every year older (slope of gorilla line is similar to slope of bonobo line)\n\n\nage:speciesorangutan\n-0.06\n[-0.11, -0.02]*\nestimated difference between orangutans and bonobos in the change in dominance for every year older (slope of orangutan line is less steep than slope of bonobo line)",
    "crumbs": [
      "Week 2 Exercises: Logistic and Longitudinal"
    ]
  },
  {
    "objectID": "02ex.html#footnotes",
    "href": "02ex.html#footnotes",
    "title": "Week 2 Exercises: Logistic and Longitudinal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nreally!?!?↩︎",
    "crumbs": [
      "Week 2 Exercises: Logistic and Longitudinal"
    ]
  },
  {
    "objectID": "00ex.html",
    "href": "00ex.html",
    "title": "Extra Exercises: Regression Refresher",
    "section": "",
    "text": "Workplace Pride\n\nData: lmm_jsup.csv\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nDataset: https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some descriptive statistics.\n\n\n\n\n\n\nHints\n\n\n\n\n\nDon’t remember how to do descriptives? Think back to previous courses - it’s time for some means, standard deviations, mins and maxes. For categorical variables we can do counts or proportions.\nWe’ve seen various functions such as summary(), and also describe() from the psych package.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the dataset:\n\nlibrary(tidyverse) # for data wrangling\nlibrary(psych) \n\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nLet’s take just the numeric variables and get some descriptives:\n\njsup |&gt; \n  select(employment_length, wp) |&gt; \n  describe()\n\n                  vars   n  mean   sd median trimmed  mad  min   max range\nemployment_length    1 295 12.62 4.28  13.00   12.60 4.45 0.00 30.00 30.00\nwp                   2 295 25.49 5.27  25.44   25.46 5.93 6.34 38.46 32.12\n                   skew kurtosis   se\nemployment_length  0.08     0.38 0.25\nwp                -0.05    -0.14 0.31\n\n\nAnd make frequency tables for the categorical ones:\n\ntable(jsup$role)\n\n\n  A   B   C \n109  95  91 \n\n\nI’m going to use dept rather than department_name as the output will be easier to see:\n\ntable(jsup$dept)\n\n\n   ACE    CMA    CPS    FSA    GLD   HMRC    NCA   NS&I  OFGEM OFQUAL OFSTED \n    17     21     13     25     17     16     20     20     15      5     17 \n OFWAT    ORR    SFO   UKSA   UKSC \n    16     17     18     45     13 \n\ntable(jsup$virtual)\n\n\n  0   1 \n175 120 \n\n\n\n\n\n\nQuestion 2\n\n\nAre there differences in ‘workplace-pride’ between people in different roles?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndoes y [continuous variable] differ by x [three groups]? lm(y ~ x)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nmod1 &lt;- lm(wp ~ role, data = jsup)\n\nRather than doing summary(model) - I’m just going to use the broom package to pull out some of the stats in nice tidy dataframes.\nThe glance() function will give us things like the \\(R^2\\) values and \\(F\\)-statistic (basically all the stuff that is at the bottom of the summary()):\n\nlibrary(broom)\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.216         0.211  4.68      40.3 3.44e-16     2  -872. 1753. 1768.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe tidy() function will give us the coefficients, standard errors, t-statistics and p-values. It’s the same information, just neater!\n\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    28.1      0.448     62.6  3.66e-171\n2 roleB          -2.24     0.657     -3.41 7.33e-  4\n3 roleC          -5.95     0.665     -8.95 4.38e- 17\n\n\nAlternatively, we can get some quick confidence intervals for our coefficients:\n\nconfint(mod1)\n\n                2.5 %     97.5 %\n(Intercept) 27.167804 28.9329302\nroleB       -3.536088 -0.9494885\nroleC       -7.255030 -4.6382319\n\n\nIt looks like roles do differ in their workplace pride. Specifically, compared to people in role A, people who are in roles B and C on average report less pride in the workplace.\n\n\n\n\n\nQuestion 3\n\n\nIs it something about the roles that make people report differences in workplace-pride, or is it possibly just that people who are newer to the company tend to feel more pride than those who have been there for a while (they’re all jaded), and the people in role A tend to be much newer to the company (making it look like the role A results in taking more pride). In other words, if we were to compare people in role A vs role B vs role C but hold constant their employment_length, we might see something different?\nFit another model to find out.\nTo help with interpreting the model, make a plot that shows all of the relevant variables that are in the model in one way or another.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSo we want to adjust for how long people have been part of the company..\nRemember - if we want to estimate the effect of x on y while adjusting for z, we can do lm(y ~ z + x).\nFor the plot - put something on the x, something on the y, and colour it by the other variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         36.1      0.709     50.9   6.90e-147\n2 employment_length   -0.834    0.0637   -13.1   4.32e- 31\n3 roleB                0.510    0.563      0.906 3.65e-  1\n4 roleC               -0.704    0.663     -1.06  2.89e-  1\n\n\nNote that, after adjusting for employment length, there are no significant differences in wp between roles B or C compared to A.\nIf we plot the data to show all these variables together, we can kind of see why! Given the pattern of wp against employment_length, the wp for different roles are pretty much where we would expect them to be if role doesn’t make any difference (i.e., if role doesn’t shift your wp up or down).\n\nggplot(jsup, aes(x=employment_length,y=wp,col=role))+\n  geom_point(size=3,alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nDo roles differ in their workplace-pride, when adjusting for time in the company?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis may feel like a repeat of the previous question, but note that this is not a question about specific group differences. It is about whether, overall, the role groups differ. So it’s wanting to test the joint effect of the two additional parameters we’ve just added to our model. (hint hint model comparison!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nmod2a &lt;- lm(wp ~ employment_length, data = jsup)\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\nanova(mod2a, mod2)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length\nModel 2: wp ~ employment_length + role\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    293 4090.8                           \n2    291 4028.9  2    61.864 2.2342 0.1089\n\n\nThis is no surprise given the previous question, we just now have a single test to report if we wanted to - after accounting for employment length, role does not explain a significant amount of variance in workplace pride.\n\n\n\n\nQuestion 5\n\n\nLet’s take a step back and remember what data we actually have. We’ve got 295 people in our dataset, from 16 departments.\nDepartments may well differ in the general amount of workplace-pride people report. People love to say that they work in the “National Crime Agency”, but other departments might not elicit such pride (*cough* HM Revenue & Customs *cough*). We need to be careful not to mistake department differences as something else (like differences due to the job role).\nMake a couple of plots to look at:\n\nhow many of each role we have from each department\nhow departments differ in their employees’ pride in their workplace\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nggplot(jsup, aes(x = role)) + \n  geom_bar()+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nIn this case, it looks like most of the departments have similar numbers of each role, apart from the UKSA (“UK Statistics Authority”), where we’ve got loads more of role A, and very few role C..\nNote also that in the plot below, the UKSA is, on average, full of employees who take a lot of pride in their work. Is this due to the high proportion of people in role A? or is the effect of role we’re seeing more due to differences in departments?\n\nggplot(jsup, aes(x = dept, y = wp)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = label_wrap_gen(35)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nEven if we had perfectly equal numbers of roles in each department, we’re also adjusting for other things such as employment_length, and the extent to which this differs by department can have trickle-on effects on our coefficient of interest (the role coefficients).\n\n\n\n\nQuestion 6\n\n\nAdjusting for both length of employment and department, are there differences in ‘workplace-pride’ between the different roles?\nCan you make a plot of all four of the variables involved in our model?\n\n\n\n\n\n\nHints\n\n\n\n\n\nMaking the plot might take some thinking. We’ve now added dept into the mix, so a nice way might be to use facet_wrap() to make the same plot as the one we did previously, but for each department.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nmod3 &lt;- lm(wp ~ employment_length + dept + role, data = jsup)\ntidy(mod3)\n\n# A tibble: 19 × 5\n   term              estimate std.error statistic   p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)        36.4       0.631    57.6    7.17e-156\n 2 employment_length  -0.882     0.0344  -25.7    4.71e- 75\n 3 deptCMA            -3.80      0.649    -5.85   1.39e-  8\n 4 deptCPS            -0.217     0.730    -0.298  7.66e-  1\n 5 deptFSA             4.74      0.625     7.60   4.71e- 13\n 6 deptGLD             0.0582    0.682     0.0853 9.32e-  1\n 7 deptHMRC           -3.79      0.692    -5.47   1.02e-  7\n 8 deptNCA            -3.85      0.655    -5.88   1.18e-  8\n 9 deptNS&I           -0.574     0.654    -0.878  3.81e-  1\n10 deptOFGEM          -0.648     0.705    -0.919  3.59e-  1\n11 deptOFQUAL         -4.94      1.01     -4.89   1.71e-  6\n12 deptOFSTED         -5.88      0.683    -8.61   5.52e- 16\n13 deptOFWAT          -1.21      0.692    -1.75   8.17e-  2\n14 deptORR            -2.85      0.681    -4.18   3.98e-  5\n15 deptSFO            -1.36      0.672    -2.02   4.47e-  2\n16 deptUKSA            4.28      0.576     7.43   1.32e- 12\n17 deptUKSC           -2.31      0.732    -3.16   1.77e-  3\n18 roleB               1.42      0.303     4.68   4.47e-  6\n19 roleC               1.31      0.366     3.59   3.92e-  4\n\n\nIn a way, adding predictors to our model is kind of like splitting up our plots by that predictor to see the patterns. This becomes more and more difficult (/impossible) as we get more variables, but right now we can split the data into all the constituent parts.\n\nggplot(jsup, aes(x = employment_length, y = wp, col = role)) +\n  geom_point(size=3,alpha=.4)+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nThe association between wp and employment_length is clear in all these little sub-plots - there’s a downward trend. The department differences can be seen too: UKSA is generally a bit higher, HMRC and UKSC a bit lower, and so on. By default, the model captures these coefficients as ‘differences from the reference group’, so all these coefficients are in relation to the “ACE” department.\nSeeing the role differences is a bit harder in this plot, but think about what you would expect to see if there were no differences in roles (i.e. imagine if they were all in role A). Take for instance the FSA department, where this is easiest to see - for the people who are in role C, for people of their employment length we would expect their wp to be lower if they were in role A. Likewise for those in role B. Across all these departments, the people in role B and C (green and blue dots respectively) are a bit higher than we would expect. This is what the model coefficients tell us!\n\n\n\n\nQuestion 7\n\n\nNow we’re starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on..\nLet’s try to describe our sample in a bit more detail.\n\nhow many participants do we have, and from how many departments?\nhow many participants are there, on average, from each department? what is the minimum and maximum?\nwhat is the average employment length for our participants?\nhow many departments are ‘virtual departments’ vs office-based?\n\nwhat is the overall average reported workplace-pride?\nhow much variation in workplace-pride is due to differences between departments?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe first lot of these questions can be answered using things like count(), summary(), table(), mean(), min() etc. See 1: Clustered Data #determining-sample-sizes\nFor the last one, we can use the ICC! See 1: Clustered Data #icc\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. How many respondents do we have, and from how many departments?\n\nnrow(jsup)\n\n[1] 295\n\nlength(table(jsup$dept))\n\n[1] 16\n\n\nHow many respondents are there, on average, from each dept? What is the minimum and maximum number of people in any one department?\n\njsup |&gt;\n  count(dept) |&gt; \n  summarise(min=min(n),\n            max=max(n),\n            median=median(n)\n  )\n\n# A tibble: 1 × 3\n    min   max median\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     5    45     17\n\n\nWhat is the average employment length of respondents?\n\nmean(jsup$employment_length)\n\n[1] 12.61695\n\n\nHow many departments are virtual vs office based? This requires a bit more than just table(jsup$virtual), because we are describing a variable at the department level.\n\njsup |&gt; \n  group_by(virtual) |&gt;\n  summarise(\n    ndept = n_distinct(dept)\n  )\n\n# A tibble: 2 × 2\n  virtual ndept\n    &lt;dbl&gt; &lt;int&gt;\n1       0    11\n2       1     5\n\n\nWhat is the overall average ‘workplace-pride’? What is the standard deviation?\n\nmean(jsup$wp)\n\n[1] 25.49373\n\nsd(jsup$wp)\n\n[1] 5.270847\n\n\nFinally, how much variation in workplace-pride is attributable to department-level differences?\n\nICC::ICCbare(x = dept, y = wp, data = jsup)\n\n[1] 0.4394114\n\n\n\n\n\n\nQuestion 8\n\n\nWhat if we would like to know whether, when adjusting for differences due to employment length and roles, workplace-pride differs between people working in virtual-departments compared to office-based ones?\nCan you add this to the model? What happens?\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Let’s add the virtual predictor to our model. Note that we don’t actually get a coefficient here - it is giving us an NA!\n\nmod4 &lt;- lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n\nsummary(mod4)\n\n\nCall:\nlm(formula = wp ~ employment_length + dept + role + virtual, \n    data = jsup)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6901 -1.4040 -0.0269  1.1782  5.0542 \n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       36.35219    0.63098  57.612  &lt; 2e-16 ***\nemployment_length -0.88169    0.03436 -25.658  &lt; 2e-16 ***\ndeptCMA           -3.79695    0.64901  -5.850 1.39e-08 ***\ndeptCPS           -0.21732    0.73037  -0.298 0.766271    \ndeptFSA            4.74482    0.62453   7.597 4.71e-13 ***\ndeptGLD            0.05816    0.68221   0.085 0.932117    \ndeptHMRC          -3.78587    0.69236  -5.468 1.02e-07 ***\ndeptNCA           -3.85033    0.65486  -5.880 1.18e-08 ***\ndeptNS&I          -0.57367    0.65372  -0.878 0.380951    \ndeptOFGEM         -0.64794    0.70497  -0.919 0.358850    \ndeptOFQUAL        -4.94134    1.01041  -4.890 1.71e-06 ***\ndeptOFSTED        -5.88455    0.68314  -8.614 5.52e-16 ***\ndeptOFWAT         -1.20866    0.69172  -1.747 0.081692 .  \ndeptORR           -2.84522    0.68127  -4.176 3.98e-05 ***\ndeptSFO           -1.35503    0.67189  -2.017 0.044689 *  \ndeptUKSA           4.28197    0.57593   7.435 1.32e-12 ***\ndeptUKSC          -2.31312    0.73248  -3.158 0.001765 ** \nroleB              1.41790    0.30286   4.682 4.47e-06 ***\nroleC              1.31478    0.36632   3.589 0.000392 ***\nvirtual                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.982 on 276 degrees of freedom\nMultiple R-squared:  0.8673,    Adjusted R-squared:  0.8587 \nF-statistic: 100.2 on 18 and 276 DF,  p-value: &lt; 2.2e-16\n\n\nSo what is happening? If we think about it, if we separate out “differences due to departments” then there is nothing left to compare between departments that are virtual vs office based. Adding the between-department predictor of virtual doesn’t explain anything more - the residual sums of squares doesn’t decrease at all:\n\nanova(\n  lm(wp ~ employment_length + dept + role, data = jsup),\n  lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length + dept + role\nModel 2: wp ~ employment_length + dept + role + virtual\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1    276 1083.8                      \n2    276 1083.8  0         0         \n\n\nAnother way of thinking about this: knowing the average workplace-pride for the department that someone is in tells me what to expect about that person’s workplace pride. But once I know their department’s average workplace-pride, knowing whether it is ‘virtual’ or ‘office-based’ doesn’t tell me anything new, for the very fact that the virtual/office-based distinction comes from comparing different departments.\nBut we’re not really interested in these departments specifically! What would be nice would be if we can look at the relevant effects of interest (things like role and virtual), but then just think of the department differences as just some sort of random variation. So we want to think of departments in a similar way to how we think of our individual employees - they vary randomly around what we expect - only they’re at a different level of observation. Such an approach is what we will learn about this semester - “multilevel models”!",
    "crumbs": [
      "Extra Exercises: Regression Refresher"
    ]
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models",
    "crumbs": [
      "Week 1 Exercises: Intro to MLM"
    ]
  },
  {
    "objectID": "01ex.html#footnotes",
    "href": "01ex.html#footnotes",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎",
    "crumbs": [
      "Week 1 Exercises: Intro to MLM"
    ]
  },
  {
    "objectID": "03ex.html",
    "href": "03ex.html",
    "title": "Week 3 Exercises: Non-Linear Change",
    "section": "",
    "text": "Cognitive Task Performance\n\nDataset: Az.rda\nThese data are available at https://uoepsy.github.io/data/Az.rda. You can load the dataset using:\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n\nand you will find the Az object in your environment.\nThe Az object contains information on 30 Participants with probable Alzheimer’s Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100.\nWe’re interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nSubject\nUnique Subject Identifier\n\n\nTime\nTime point of the study (1 to 10)\n\n\nTask\nTask type (Memory, cADL, sADL)\n\n\nPerformance\nScore on test (range 0 to 100)\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nLoad in the data and examine it.\nHow many participants, how many observations per participant, per task?\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n\n    Subject         Time          Task      Performance   \n 1      : 30   Min.   : 1.0   cADL  :300   Min.   : 2.00  \n 2      : 30   1st Qu.: 3.0   sADL  :300   1st Qu.:40.00  \n 3      : 30   Median : 5.5   Memory:300   Median :52.00  \n 4      : 30   Mean   : 5.5                Mean   :49.27  \n 5      : 30   3rd Qu.: 8.0                3rd Qu.:61.00  \n 6      : 30   Max.   :10.0                Max.   :85.00  \n (Other):720                                              \n\n\n30 participants:\n\nlength(unique(Az$Subject))\n\n[1] 30\n\n\nDoes every participant have 10 datapoints for each Task type? Yes!\n\nany( table(Az$Subject, Az$Task) != 10 )\n\n[1] FALSE\n\n\n\n\n\n\nQuestion 2\n\n\nNo modelling just yet.\nPlot the performance over time for each type of task.\nTry using stat_summary so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor an example plot, see Chapter 6: polynomial growth #example-in-mlm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. You can use “pointranges”, or “line” and “ribbon”.\nstat_summary will take the data and for each value of x calculate some function (in this case the mean, or the mean and SE):\n\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhy do you think raw/natural polynomials might be more useful than orthogonal polynomials for these data?\n\n\n\n\n\n\nHints\n\n\n\n\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rate of change) at a specific point in time?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. Because we’re interested in whether there are task differences at the starting point, raw polynomials are probably what we want here.\n\n\n\n\nQuestion 4\n\n\nRe-center the Time variable so that the intercept is the first timepoint.\nThen choose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.\n\n\n\n\n\n\nHints\n\n\n\n\n\nNote there is no part of the research question that specifically asks about how “gradual” or “quick” the change is (which would suggest we are interested in the quadratic term).\nHowever, plots (summarised at the task level, and of individual participant’s data separately) can help to give us a sense of what degree of polynomial terms might be suitable to succinctly describe the trends.\nIn many cases, fitting higher and higher order polynomials will likely result in a ‘better fit’ to our sample data, but these will be worse and worse at generalising to new data - i.e. we run the risk of overfitting.\n\n\n\n\n\n\n\n\n1 - how many polynomials?\n\n\n\nSolution 4. In our plot, there were 2 straight line and one slightly curvy one. It wasn’t S-shaped or ‘wiggly’ or anything, there was just a bit of a bend in it, which suggests that the quadratic term could be a good approximation here.\nIt’s worth also looking at the individual participant trajectories. In these we can also see the curvi-ness of the blue line - it’s more pronounced for some people than others, but it does seem like individual’s trajectories on the Memory task may be curvilinear.\n\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")+\n  facet_wrap(~Subject)\n\n\n\n\n\n\n\n\n\nlibrary(lme4)\nAz &lt;- Az |&gt; mutate(\n  Time1 = Time-1,\n  poly1 = poly(Time1,2,raw=T)[,1],\n  poly2 = poly(Time1,2,raw=T)[,2]\n)\n\n\n\n\n\n\n\nOr using Dan’s code:\n\n\n\n\n\n\n# import Dan's code:\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\n# this also produces a nice little plot to show the polynomials\nAz$Time1 &lt;- Az$Time-1\nAz &lt;- code_poly(df = Az, predictor = 'Time1',\n                poly.order = 2, orthogonal = FALSE)\n\n\n\n\n\n\n\n\n\n2 - fixed effects\n\n\n\nSolution 5. We’re interested in how performance changes over time, but we have poly1 and poly2 for time, so we’re at:\nlmer(Performance ~ poly1 + poly2 ... \nOur research aims are to investigate differences between task performance (both at baseline and change over time). So we want to interact time with task:\nlmer(Performance ~ (poly1 + poly2)*Task ... \n\n\n\n\n\n3 - grouping structure\n\n\n\nSolution 6. \n\nhead(Az)\n\n  Subject Time Task Performance Time1 poly1 poly2\n1       1    1 cADL          65     0     0     0\n2       1    2 cADL          61     1     1     1\n3       1    3 cADL          53     2     2     4\n4       1    4 cADL          46     3     3     9\n5       1    5 cADL          42     4     4    16\n6       1    6 cADL          35     5     5    25\n\n\nWe have 900 observations, and 30 for each participant.\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + .... | Subject)\n\n\n\n\n\n\nFixed vs Random\n\n\n\n\n\nWe can account for group differences in models either by estimating group differences, or by estimating variance between groups:\n\ngroup as a fixed effect (y ~ 1 + group) = groups differ in \\(y\\) by \\(b_1, b_2, ..., b_k\\)\n\ngroup as a random effect (y ~ 1 + (1|group)) = groups vary in \\(y\\) with a standard deviation of \\(\\sigma_0\\)\n\nOne way to think about whether a group is best in the random effects part or in the fixed part of our model is to think about “what would happen if I repeated the experiment?”\nShould variable g be fixed or random?\n\n\n\n\n\n\n\n\n\nRepetition:  If the experiment were repeated:\nDesired inference:  The conclusions refer to:\n\n\n\n\nFixed\\(y\\,\\sim\\,~\\,...\\, +\\, g\\)\nSame groups would be used\nThe groups used\n\n\nRandom\\(y\\,\\sim\\,...\\,+\\,(\\,... |\\,g)\\)\nDifferent groups would be used\nA population from which the groups used are just a (random) sample\n\n\n\nPractical points:\n- Sometimes there isn’t enough variability between groups to model as random effects (i.e. the variance gets estimated as too close to zero). - Sometimes you might not have sufficient number of groups to model as random effects (e.g. for groups of fewer than c8 things, estimates of the variance are often not a reliable reflection of the population).\n\n\n\n\n\n\n\n\n4 - random effects\n\n\n\nSolution 7. What slopes could vary by participant?\nQ: Could participants vary in their performance over time?\nA: Yes, (poly1 + poly2 | Subject)\nQ: Could participants vary in how performance differs between Tasks?\nA: Yes, (poly1 + poly2 + Task | Subject). E.g., Some participants might be much better at the memory task than other tasks, some might be better at the other tasks.\nQ: Could participants vary in how tasks differ in their performance over time?\nA: Yes, ((poly1 + poly2)*Task | Subject). E.g., For some participants, memory could decline more than cADL, for other participants it could decline less.\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + (poly1 + poly2)*Task | Subject)\n\n\n\n\n\n5 - the model\n\n\n\nSolution 8. small note: I’m using the bobyqa optimizer here because it tends to be slightly quicker than the default.\n\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\nboundary (singular) fit: see help(‘isSingular’) Warning messages: 1: In commonArgs(par, fn, control, environment()) : maxfun &lt; 10 * length(par)^2 is not recommended. 2: In optwrap(optimizer, devfun, getStart(start, rho$pp), lower = rho$lower, : convergence code 1 from bobyqa: bobyqa – maximum number of function evaluations exceeded\n\n\n\n\n\nQuestion 5\n\n\nOkay, so the model didn’t converge. It’s trying to estimate a lot of things in the random effects (even though it didn’t converge, try looking at VarCorr(model) to see all the covariances it is trying to estimate).\n\n\n\n\n\n\nCategorical random effects on the RHS\n\n\n\n\n\nWhen we have a categorical random effect (i.e. where the x in (1 + x | g) is a categorical variable), then model estimation can often get tricky, because “the effect of x” for a categorical variable with \\(k\\) levels is identified via \\(k-1\\) parameters, meaning we have a lot of variances and covariances to estimate when we include x|g.\n\n\nWhen x is numeric:\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n\n\n\nWhen x is categorical with \\(k\\) levels:\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n\n\nHowever, we can use an alternative formation of the random effects by putting a categorical x into the right-hand side:\nInstead of (1 + x | g) we can fit (1 | g) + (1 | g:x).\nThe symbol : in g:x is used to refer to the combination of g and x.\n\n\n      g        x     g:x\n1    p1        a    p1.a\n2    p1        a    p1.a\n3    p1        b    p1.b\n4   ...      ...     ...\n5    p2        a    p2.a\n6    p2        b    p2.b\n7   ...      ...     ...\n\n\nIt’s a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:\n\n(1 + x | g): each group of g can have a different intercept and a different effect of x\n\n(1 | g) + (1 | g:x): each group of g can have a different intercept, and each level of x within each g can have a different intercept.\n\nBoth of these allow the outcome y to change across x differently for each group in g (i.e. both of them result in y being different for each level of x in each group g).\nThe first does so explicitly by estimating the group level variance of the y~x effect.\nThe second one estimates the variance of \\(y\\) between groups, and also the variance of \\(y\\) between ‘levels of x within groups’. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of x, we don’t have to worry about lots of covariances:\n\n\n(1 + x | g)\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n\n\n\n(1 | g) + (1 | g:x)\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n\n\n\n\n\nTry adjusting your model by first moving Task to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one)\nThis is our first experience of our random effect structures becoming more complex than simply (.... | group). This is going to feel confusing, but don’t worry, we’ll see more structures like this next week.\n\n\n\n\n\n\nHints\n\n\n\n\n\n... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nTo then start simplifying (if this model doesn’t converge), it can be helpful to look at the VarCorr() of the non-converging model to see if anything looks awry. Look for small variances, perfect (or near perfect) correlations. These might be sensible things to remove.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. Here’s our model with subject-task effects on the right hand side.\nAgain we have problems, as we have a singular fit:\n\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\nboundary (singular) fit: see help(‘isSingular’)\n\nLooking at the random effects of our model, note that the poly2|Subject random effect has very little variance (and high correlations).\nNote that it makes sense that by including the random effects for Subject:Task, there might not be much above that leftover in Subject random effects.\n\nVarCorr(m2)\n\n Groups       Name        Std.Dev. Corr         \n Subject:Task (Intercept) 3.100837              \n              poly1       0.923485  0.275       \n              poly2       0.052644 -0.329  0.021\n Subject      (Intercept) 3.461349              \n              poly1       1.565908 -0.177       \n              poly2       0.007638  0.075 -0.995\n Residual                 1.019574              \n\n\nWhen we remove it, our model converges!!\n\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\nQuestion 6\n\n\nConduct a series of model comparisons investigating whether\n\nTasks differ only in their linear change\n\nTasks differ in their quadratic change\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we’re interested in the how the average participant performs over the study). So our models want to have the same random effect structure, but different fixed effects.\nSee the end of Chapter 6: polynomial growth #example-in-mlm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. As I’m comparing these with a likelihood ratio test, I’ll fit them with REML=FALSE\n\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3full = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3full)\n\nData: Az\nModels:\nm3int: Performance ~ poly1 + poly2 + Task + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nm3lin: Performance ~ poly1 * Task + poly2 + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nm3full: Performance ~ (poly1 + poly2) * Task + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\n       npar    AIC    BIC  logLik -2*log(L)   Chisq Df Pr(&gt;Chisq)    \nm3int    15 3801.8 3873.8 -1885.9    3771.8                          \nm3lin    17 3775.4 3857.0 -1870.7    3741.4  30.413  2  2.488e-07 ***\nm3full   19 3607.8 3699.1 -1784.9    3569.8 171.532  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe linear change over time differs between Tasks (\\(\\chi^2(2) = 30.41, p &lt; 0.001\\)).\nThe quadratic change over time differs between Tasks (\\(\\chi^2(2) = 171.53, p &lt; 0.001\\)).\n\n\n\n\n\n\n\ncompletely optional - comparisons and raw/orthogonal\n\n\n\n\n\nThere’s an argument to be made that to ask “do tasks differ in Linear change?” we could test either (as above):\nA: Performance ~ poly1 + poly2 + Task\nvs\nB: Performance ~ poly1*Task + poly2\nOr\nC: Performance ~ poly1 + poly2*Task\nvs\nD: Performance ~ (poly1 + poly2)*Task\nThe comparison between models A and B assumes that all the Tasks have the same curvature. Visually, this is the comparison of which of the two patterns in plots A and B in Figure 1 best fits the data. In A, the lines have the same linear slope and the same curvature. In B, the lines have the same curvature but different linear slopes.\nThe comparison between models C and D is similar, but not quite the same. C has lines that have different curvatures but the same linear slope, and plot D has lines that have different curvatures and different linear slopes.\n\n\n\n\n\n\n\n\nFigure 1: Model comparisons with raw polynomials\n\n\n\n\n\nBUT… in all these descriptions, I have used the term “linear slope” a little ambiguously. Because we are using raw polynomials, this is actually meaning specifically the tangent at the start (i.e., at the left hand side of the plot).\nNote that plots for A and B both have the same amount of curvature - i.e., the lines in B are equally curvy as those in A. But this isn’t true for C and D. This is because in model C, by not having the poly1:Task interaction in there, all we are saying is that the tangent at 0 is the same for all tasks. The quadratic term interaction poly2:Task then tries to “make up for” this by making the green line curvy (when actually it’s just a shallower straight line).\nSo the comparison between C and D (when using raw polynomials) doesn’t ask “do tasks differ in their overall linear trend?”. Instead it asks “do tasks differ in linear trend at the outset? With orthogonal polynomials, we would have something more like Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Model comparisons with orthogonal polynomials\n\n\n\n\n\nThis whole thing is because essentially in raw polynomials, poly1 and poly2 are correlated. So excluding one of the terms (or an interaction with one of the terms) will change the estimates for the others. In orthogonal polynomials, we don’t have that issue.\n\n\n\n\n\n\n\nQuestion 7\n\n\nGet some confidence intervals and provide an interpretation of each coefficient from the full model.\n\n\n\n\n\nSolution\n\n\n\nSolution 11. As we’ve used likelihood ratio tests above, we’ll get some profile likelihood confidence intervals for our parameters.\nnote this took me about 4 mins to run\nIf you want to get some quicker (slightly less robust) confidence intervals, then switch \"profile\" to \"Wald\".\n\nconfint(m3full, method=\"profile\", parm=\"beta_\")\n\n\n\n\n\n\n\n\n\nterm\nest\nCI\ninterpretation\n\n\n\n\n(Intercept)\n63.88\n[62.19, 65.57]\nestimated score on the cADL task at baseline\n\n\npoly1\n-3.27\n[-3.93, -2.61]\nestimated linear change in cADL scores from baseline\n\n\npoly2\n0.01\n[-0.02, 0.03]\nno significant curvature to the cADL trajectory\n\n\nTasksADL\n1.44\n[-0.18, 3.06]\nno significant difference between sADL and cADL tasks at baseline\n\n\nTaskMemory\n-2.40\n[-4.02, -0.78]\nat baseline, scores on memory task are 2.4 lower than cADL\n\n\npoly1:TasksADL\n1.34\n[0.82, 1.85]\nperformance on sADL task is not decreasing from baseline as much as performance on cADL\n\n\npoly1:TaskMemory\n-3.30\n[-3.81, -2.78]\nperformance on Memory task is decreasing (linearly) more than performance on cADL\n\n\npoly2:TasksADL\n-0.01\n[-0.05, 0.02]\nno significant difference between quadratic change of sADL from that of cADL\n\n\npoly2:TaskMemory\n0.34\n[0.3, 0.37]\nsignificant difference in quadratic change between performance on Memory vs performance on cADL\n\n\n\n\n\n\n\n\n\n\n\n\n\nsome tables of predictions (in case they help)\n\n\n\n\n\nTo get a sense of the quadratic term ‘in action’, think about the predictions across time for each task:\nFor cADL, this is just the linear change. Every timepoint, performance decreases by 3.27.\n\n\n\n\n\n\n\ntimepoint\ncADL\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) = 63.88\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) = 60.61\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2) = 57.34\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3) = 54.07\\)\n\n\n\nFor sADL, the additional change is +1.34, so at every timepoint performance decreases by -1.93 (this is -3.27+1.34)\n\n\n\n\n\n\n\ntimepoint\nsADL\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +\\) \\((1.44) + (1.34 \\times time) + (-0.01 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time) + (1.34 \\times time)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) + (1.34 \\times 0) = 63.88\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) + (1.34 \\times 1) = 61.95\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2)+ (1.34 \\times 2)  = 60.02\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3)+ (1.34 \\times 3) = 58.09\\)\n\n\n\nFor the Memory task, the quadratic term is in play. at every timepoint performance decreases by \\(-3.27-3.30 +(0.34 \\times time^2)\\). So for low timepoints, the quadratic term doesn’t make much of a difference as it does for bigger time points.\n\n\n\n\n\n\n\ntimepoint\nMemory\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +\\) \\((-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time) +\\) \\((-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) +\\) \\((-2.40) + (-3.30 \\times 0) + (0.34 \\times 0^2) = 61.48\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) +\\) \\((-2.40) + (-3.30 \\times 1) + (0.34 \\times 1^2) = 55.25\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2) +\\) \\((-2.40) + (-3.30 \\times 2) + (0.34 \\times 2^2) = 49.70\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3) +\\) \\((-2.40) + (-3.30 \\times 3) + (0.34 \\times 3^2) = 44.83\\)\n\n\n…\n…\n\n\n9\n\\(63.88 + (-3.27 \\times 9) +\\) \\((-2.40) + (-3.30 \\times 9) + (0.34 \\times 9^2) = 29.89\\)\n\n\n10\n\\(63.88 + (-3.27 \\times 10) +\\) \\((-2.40) + (-3.30 \\times 10) + (0.34 \\times 10^2) = 29.78\\)\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectories for each task.\n\n\n\n\n\nSolution\n\n\n\nSolution 12. \n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nMake a plot showing both the average performance and the average model predicted performance across time.\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\nlibrary(broom.mixed)\naugment(m3) |&gt;\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")",
    "crumbs": [
      "Week 3 Exercises: Non-Linear Change"
    ]
  },
  {
    "objectID": "05ex.html",
    "href": "05ex.html",
    "title": "Week 5 Exercises: Assumptions, Diagnostics, Writing up",
    "section": "",
    "text": "Video game aggression and the dark triad\n\nDataset: NGV.csv\nThese data are from an experiment designed to investigate how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.\nThe experiment involved playing 10 levels of a game in which the objective was to escape a maze. Various obstacles and other characters were present throughout the maze, and players could interact with these by side-stepping or jumping over them, or by pushing or shooting at them. All of these actions took the same amount of effort to complete (pressing a button), and each one achieved the same end (moving beyond the obstacle and being able to continue through the maze).\nEach participant completed all 10 levels twice, once in which all characters were presented as cartoons, and once in which all characters were presented as realistic humans and animals. The layout of the level was identical in both, the only difference being the depiction of objects and characters. For each participant, these 20 levels (\\(2 \\times 10\\) mazes) were presented in a random order. Half of the participants played via a screen, and the other half played via a VR headset. For each level played, we have a record of “needless game violence” (NGV) which was calculated via the number of aggressive (pushing/shooting) actions taken (+0.5 for every action that missed an object, +1 for every action aimed at an inanimate object, and +2 for every action aimed at an animate character).\nPrior to the experiment, each participant completed the Short Dark Triad 3 (SD-3), which measures the three traits of machiavellianism, narcissism, and psychopathy.\nDataset: https://uoepsy.github.io/data/NGV.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nPID\nParticipant number\n\n\nage\nParticipant age (years)\n\n\nlevel\nMaze level (1 to 10)\n\n\ncharacter\nWhether the objects and characters in the level were presented as 'cartoon' or as 'realistic'\n\n\nmode\nWhether the participant played via a screen or with a VR headset\n\n\nP\nPsycopathy Trait from SD-3 (score 1-5)\n\n\nN\nNarcissism Trait from SD-3 (score 1-5)\n\n\nM\nMachiavellianism Trait from SD-3 (score 1-5)\n\n\nNGV\nNeedless Game Violence metric\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nConduct an analysis to address the research aims!\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThere’s a lot to unpack in the research aim: “how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.”\n\n\n\n\n\n\n\n\n\n1 - some quick checks\n\n\n\nSolution 1. \n\nngv &lt;- read_csv(\"https://uoepsy.github.io/data/NGV.csv\")\n\nLet’s make things factors and see where we stand:\n\nngv &lt;- ngv |&gt; \n  mutate(\n    PID = factor(PID),\n    level = factor(level),\n    character = factor(character),\n    mode = factor(mode)\n  )\n\nsummary(ngv)\n\n      PID            age            level         character       mode    \n ppt_1  :  20   Min.   :18.00   level1 :152   cartoon  :760   Screen:740  \n ppt_10 :  20   1st Qu.:28.00   level10:152   realistic:760   VR    :780  \n ppt_11 :  20   Median :36.50   level2 :152                               \n ppt_12 :  20   Mean   :34.99   level3 :152                               \n ppt_13 :  20   3rd Qu.:42.25   level4 :152                               \n ppt_14 :  20   Max.   :48.00   level5 :152                               \n (Other):1400                   (Other):608                               \n       P               N               M              NGV        \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 0.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 8.875  \n Median :2.000   Median :3.000   Median :3.000   Median :11.000  \n Mean   :2.171   Mean   :2.763   Mean   :2.842   Mean   :10.788  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:13.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :29.000  \n                                                                 \n\n\neverything looks good to me! All ranges looks fine.\nHow many people do we have?\n\ndim(table(ngv$PID, ngv$character))\n\n[1] 76  2\n\n\n76!\nDoes everyone have 10 datapoints for cartoon and 10 for realistic?\n\nany(table(ngv$PID, ngv$character)!=10)\n\n[1] FALSE\n\n\nyes, they do!\n\n\n\n\n\n2 - some exploratory plots\n\n\n\nSolution 2. Okay. Let’s plot.. The question asks about “how the realism of video games is associated with more/less unnecessarily aggressive gameplay”.\nSo we’ll put the character on the x-axis and our outcome NGV on the y:\nI like jitters, but you could put boxplots or violin plots too!\n\nggplot(ngv, aes(x = character, y = NGV)) +\n  geom_jitter(height=0, width=.2, alpha=.2)\n\n\n\n\n\n\n\n\nWe’re also interested in whether this differs depending on the mode of gameplay (screen vs VR headset). So we could facet_wrap perhaps? or colour?\nLet’s also plot the means - i’ll put those to the side of all the jittered points with a “nudge”..\n\nggplot(ngv, aes(x = character, y = NGV, col = mode)) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\n\nAs well as looking at whether the NGV~character relationship differs between modes, we’re also interested in the differences in this relationship due to individual differences in the dark triad personality traits. We have these measured for each person, so we can just use a similar idea:\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(P))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(M))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(N))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\n\nWhat do we get from all these plots? Well, it looks like mode might be changing the relationship between character and violence. It also looks like there’s a considerable effect of the dark triad on the amount of violence people use! Of course, in these individual plots, it’s hard to ascertain the extent to which plots showing differences between levels of Narcissism are due to Narcissism or due to differences in Psychopathy (all the dark triad traits are fairly correlated)\n\nngv |&gt; select(P,N,M) |&gt;\n  cor() |&gt; round(2)\n\n     P    N    M\nP 1.00 0.43 0.51\nN 0.43 1.00 0.58\nM 0.51 0.58 1.00\n\n\nSo we need to do some modelling!\n\n\n\n\n\n3 - fitting a model\n\n\n\nSolution 3. NOTE: This is how I might approach this question. There are lots of other things that we could quite sensibly do!\n\nWe know that we’re interested in NGV ~ character.\n\nWe also have the additional question of whether this is different between modes - NGV ~ character * mode.\n\nAnd whether the NGV ~ character association is modulated by Psychopathy NGV ~ character * P, and by Narcissism NGV ~ character * N, and by Machiavellianism NGV ~ character * M.\n\nWe could fit these all in one:\nNGV ~ character * (mode + P + M + N)\nWe have multiple observations per participant PID, and we also have multiple observations for each level level. All participants see every level, and every level is seen by all participants. It’s not the case that a level is unique to a single participant, so these are crossed.\n( ?? | PID) + ( ?? | level)\nParticipants plays both the cartoon and the realistic versions, so we could have variation between participants in how realism affects violence - (1 + character | PID). Beyond this, all variables are measured at the participant level, so we can’t have anything else.\nFor the levels, each level is played by some participants in VR headsets and some participants on a screen, so we could have some levels for which VR feels very different (and makes you play more violently?) - (mode | level). We could also have some levels for which the realism has a bigger effect - (character | level), and also have some levels for which people high on the dark triad play differently - i.e. the dark triad could result in lots of violence in level 2, but not in level 3 - (P + M + N | level).\n(you could also try to fit the interactions in the random effects here but i’m not going to even try!)\n\nm0 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + character + mode + P + M + N | level), data = ngv)\n\nafter some simplification, I end up at the model below. You might end up at a slightly different random effect structure, and that is completely okay! The important thing is to be transparent in your decisions.\n\nm1 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), data = ngv)\n\n\n\n\n\nQuestion 2\n\n\nCheck the assumptions of your model\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe have a multilevel model, so we have assumptions at multiple levels! See Chapter 9 #mlm-assumptions-diagnostics.\nBe careful - QQplots with few datapoints can make things look weirder than they are - try a histogram too\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Here are some assumption plots:\n\nplot(m1)\n\n\n\n\n\n\n\nplot(m1,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nThere’s something weird going on in the left hand side with a bunch of points looking funny! My guess is that this may well come out when examining influence.\nIn the QQplots of the random effects below we can see a couple of participants are a bit off - this may well be what we are seeing above.\n\nqqnorm(ranef(m1)$PID[,1],main=\"1|PID\");qqline(ranef(m1)$PID[,1])\nqqnorm(ranef(m1)$PID[,2],main=\"character|PID\");qqline(ranef(m1)$PID[,2])\nqqnorm(ranef(m1)$level[,1],main=\"1|level\");qqline(ranef(m1)$level[,1])\nqqnorm(ranef(m1)$level[,2],main=\"mode|level\");qqline(ranef(m1)$level[,2])\n\n\n\n\n\n\n\n\nThe QQplots for the level random effects are hard to evaluate in part because there aren’t many levels (only 10). Let’s do some histograms too. They don’t look terrible..\n\nhist(ranef(m1)$PID[,1],main=\"1|PID\")\nhist(ranef(m1)$PID[,2],main=\"character|PID\")\nhist(ranef(m1)$level[,1],main=\"1|level\")\nhist(ranef(m1)$level[,2],main=\"mode|level\")\n\n\n\n\n\n\n\n\nOur model predictions don’t look great either. Something is happening at values of 0?\n\nlibrary(performance)\ncheck_predictions(m1)\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nCheck the extent to which your results may be sensitive to certain influential observations, or participants, or levels!\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Chapter 9 #influence for two packages that can assess influence.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. let’s check for influential participants first:\n\nlibrary(HLMdiag)\ninf2 &lt;- hlm_influence(m1,level=\"PID\")\ndotplot_diag(inf2$cooksd, index=inf2$PID, cutoff=\"internal\")\n\n\n\n\n\n\n\n\nand let’s re-fit without those two weird people..\n\nm1a &lt;- lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), \n            data = ngv |&gt; filter(!(PID %in% c(\"ppt_59\",\"ppt_53\"))))\n\nOur conclusions change!\nThe significance of P (which is the association between psychopathy and needless violence in the cartoon condition) depends upon exclusion of these two participants. I’m showing the table with Satterthwaite p-values as it’s a bit quicker, but given that we have only 10 groups for the level random effect, it might be worth switching to KR\n\nlibrary(sjPlot)\ntab_model(m1,m1a, df.method=\"satterthwaite\")\n\n\n\n \nNGV\nNGV\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n6.25\n3.14 – 9.35\n&lt;0.001\n5.52\n3.10 – 7.93\n&lt;0.001\n\n\ncharacter [realistic]\n-2.98\n-4.48 – -1.49\n&lt;0.001\n-2.98\n-4.50 – -1.45\n&lt;0.001\n\n\nmode [VR]\n-1.00\n-2.34 – 0.34\n0.140\n-0.36\n-1.41 – 0.70\n0.500\n\n\nP\n0.90\n-0.29 – 2.08\n0.135\n1.19\n0.25 – 2.13\n0.014\n\n\nM\n0.89\n-0.20 – 1.97\n0.107\n0.81\n-0.03 – 1.65\n0.060\n\n\nN\n0.22\n-0.90 – 1.34\n0.693\n0.33\n-0.58 – 1.24\n0.470\n\n\ncharacter [realistic] ×mode [VR]\n-0.41\n-1.05 – 0.23\n0.206\n-0.40\n-1.06 – 0.26\n0.228\n\n\ncharacter [realistic] × P\n0.76\n0.19 – 1.33\n0.010\n0.79\n0.19 – 1.38\n0.010\n\n\ncharacter [realistic] × M\n0.52\n-0.00 – 1.04\n0.051\n0.53\n-0.01 – 1.06\n0.053\n\n\ncharacter [realistic] × N\n-0.00\n-0.54 – 0.54\n0.988\n-0.03\n-0.61 – 0.55\n0.920\n\n\nRandom Effects\n\n\n\nσ2\n3.33\n3.41\n\n\n\nτ00\n7.91 PID\n4.58 PID\n\n\n\n0.06 level\n0.06 level\n\n\nτ11\n1.26 PID.characterrealistic\n1.29 PID.characterrealistic\n\n\n\n0.05 level.modeVR\n0.05 level.modeVR\n\n\nρ01\n-0.11 PID\n-0.18 PID\n\n\n\n-0.51 level\n-0.45 level\n\n\nICC\n0.71\n0.59\n\n\nN\n76 PID\n74 PID\n\n\n\n10 level\n10 level\n\nObservations\n1520\n1480\n\n\nMarginal R2 / Conditional R2\n0.226 / 0.777\n0.306 / 0.714\n\n\n\n\n\n\nNote that our model predictions look much better\n\ncheck_predictions(m1a)\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\nIf we look at those two people - they just didn’t do much (or any) “needless game violence”.\n\nngv |&gt; filter(PID %in% c(\"ppt_59\",\"ppt_53\")) |&gt;\n  ggplot(aes(x=character, y=NGV)) +\n  geom_jitter(height=0, width=.2) + \n  facet_wrap(~PID)\n\n\n\n\n\n\n\n\nLet’s check for influential levels now:\n\ninf3 &lt;- hlm_influence(m1a,level=\"level\")\ndotplot_diag(inf3$cooksd)\n\n\n\n\n\n\n\n\nand for influential observations\n\ninf1 &lt;- hlm_influence(m1a,level=1)\ndotplot_diag(inf1$cooksd)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll the datasets!\nThe link below will take you to a page with all the datasets that we have seen across the readings and exercises, as well as a few extra ones that should be new! For each one, there is a quick explanation of the study design which also details the research aims of the project.\n\nDATASETS PAGE\n\nPick one of the datasets and:\n\nexplore the data, and do any required cleaning (most of them are clean already)\nconduct an analysis to address the research aims\nwrite a short description of the sample data (see Chapter 11 #the-sample-data)\nwrite a short explanation of your methods (see Chapter 11 #the-methods)\nwrite a short summary of your results, along with suitable visualisations and tables (see Chapter 11 #the-results)\nPost some of your writing on Piazza and we can collectively discuss it!\n\nIf you like, work on this as a group - set up a google doc to collaboratively write together (it’s much more fun that way!)",
    "crumbs": [
      "Week 5 Exercises: Assumptions, Diagnostics, Writing up"
    ]
  },
  {
    "objectID": "08ex.html",
    "href": "08ex.html",
    "title": "Week 8 Exercises: CFA",
    "section": "",
    "text": "New packages\nMake sure you have these packages installed:\n\nlavaan\nsemPlot\n\n\n\nExercises for the Enthusiastic\n\nDataset: radakovic_das.csv\nApathy is lack of motivation towards goal-directed behaviours. It is pervasive in a majority of psychiatric and neurological diseases, and impacts everyday life. Traditionally, apathy has been measured as a one-dimensional construct but is in fact composed of different types of demotivation.\nThe Dimensional Apathy Scale (DAS) is a multidimensional assessment for demotivation, in which 3 subtypes of apathy are assessed:\n\nExecutive: lack of motivation for planning, attention or organisation\nEmotional: lack of emotional motivation (indifference, affective or emotional neutrality, flatness or blunting)\nInitiation: lack of motivation for self-generation of thoughts and/or actions\n\nThe DAS measures these subtypes of apathy and allows for quick and easy assessment, through self-assessment, observations by informants/carers or administration by researchers or healthcare professionals.\nYou can find data for the DAS when administered to 250 healthy adults at https://uoepsy.github.io/data/radakovic_das.csv, and information on the items is below.\n\n\n\n\n\n\nDAS Dictionary\n\n\n\n\n\nAll items are measured on a 6-point Likert scale of Always (0), Almost Always (1), Often (2), Occasionally (3), Hardly Ever (4), and Never (5). Certain items (indicated in the table below with a - direction) are reverse scored to ensure that higher scores indicate greater levels of apathy.\n\n\n\n\n\n\n\n\nitem\ndirection\ndimension\nquestion\n\n\n\n\n1\n+\nExecutive\nI need a bit of encouragement to get things started\n\n\n2\n-\nInitiation\nI contact my friends\n\n\n3\n-\nEmotional\nI express my emotions\n\n\n4\n-\nInitiation\nI think of new things to do during the day\n\n\n5\n-\nEmotional\nI am concerned about how my family feel\n\n\n6\n+\nExecutive\nI find myself staring in to space\n\n\n7\n-\nEmotional\nBefore I do something I think about how others would feel about it\n\n\n8\n-\nInitiation\nI plan my days activities in advance\n\n\n9\n-\nEmotional\nWhen I receive bad news I feel bad about it\n\n\n10\n-\nExecutive\nI am unable to focus on a task until it is finished\n\n\n11\n+\nExecutive\nI lack motivation\n\n\n12\n+\nEmotional\nI struggle to empathise with other people\n\n\n13\n-\nInitiation\nI set goals for myself\n\n\n14\n-\nInitiation\nI try new things\n\n\n15\n+\nEmotional\nI am unconcerned about how others feel about my behaviour\n\n\n16\n-\nInitiation\nI act on things I have thought about during the day\n\n\n17\n+\nExecutive\nWhen doing a demanding task, I have difficulty working out what I have to do\n\n\n18\n-\nInitiation\nI keep myself busy\n\n\n19\n+\nExecutive\nI get easily confused when doing several things at once\n\n\n20\n-\nEmotional\nI become emotional easily when watching something happy or sad on TV\n\n\n21\n+\nExecutive\nI find it difficult to keep my mind on things\n\n\n22\n-\nInitiation\nI am spontaneous\n\n\n23\n+\nExecutive\nI am easily distracted\n\n\n24\n+\nEmotional\nI feel indifferent to what is going on around me\n\n\n\n\n\n\n\nHere are the item numbers that correspond to each dimension.\n\nExecutive: 1, 6, 10, 11, 17, 19, 21, 23\nEmotional: 3, 5, 7, 9, 12, 15, 20, 24\nInitiation: 2, 4, 8, 13, 14, 16, 18, 22\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data. It will need a little bit of tidying before we can get to fitting a CFA.\nRemember that most of the actions needed for working with those sort of data are described in the Chapter on Data Wrangling for Questionnaires.\n\n\n\n\n\n\nHints\n\n\n\n\n\nBy the looks of things, this is what I would consider doing:\n\nRename the variables to easy-to-read strings like q1, q2, q3, etc.\nSet up a data dictionary that records the text of the item q1 corresponds to, the text that q2 corresponds to, etc.\nRecode the Likert scale labels to numbers.\nReverse-code the questions with a negative direction. Note, you don’t need to this, as they’ll just end up with loadings in the opposite direction, but I would strongly recommend it for interpretation purposes.\n\nCheck if there is missing data and if there is, removing those observations.\n\n\n\n\n\n\n\n\n\n1 - Read and check\n\n\n\nSolution 1. First let’s just read in the dataset:\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)\n\nrdas &lt;- read_csv(\"https://uoepsy.github.io/data/radakovic_das.csv\")\nhead(rdas)\n\n# A tibble: 6 × 24\n  I need a bit of encouragement …¹ `I contact my friends` I express my emotion…²\n  &lt;chr&gt;                            &lt;chr&gt;                  &lt;chr&gt;                 \n1 Often                            Almost Always          Almost Always         \n2 Almost Always                    Hardly Ever            Occasionally          \n3 Often                            Occasionally           Occasionally          \n4 Hardly Ever                      Occasionally           Almost Always         \n5 Occasionally                     Hardly Ever            Occasionally          \n6 Occasionally                     Occasionally           Almost Always         \n# ℹ abbreviated names: ¹​`I need a bit of encouragement to get things started`,\n#   ²​`I express my emotions`\n# ℹ 21 more variables: `I think of new things to do during the day` &lt;chr&gt;,\n#   `I am concerned about how my family feel` &lt;chr&gt;,\n#   `I find myself staring in to space` &lt;chr&gt;,\n#   `Before I do something I think about how others would feel about it` &lt;chr&gt;,\n#   `I plan my days activities in advance` &lt;chr&gt;, …\n\n\nThe names we’re getting are useful in that they show the items, but they’re horrible to have to use in R, so we will ideally replace them with easy to use names. Note also that the data is being read in as the actual response option - e.g., “Almost Always” - and we want to treat these as a numeric scale. So those will have to change too.\n\n\n\n\n\n2 - Renaming variables\n\n\n\nSolution 2. I like to make a “data dictionary” whenever I get data like this. While I want to rename the variables to make it easier for me to use, I also want to keep track of what the questions were.\nHere I make a “tibble” (the function data.frame() would work too, tibble is just tidyverse version). I indicate what I am going to rename things as (“q1”,“q2”, …, “q24”), and then I have the current names of the variables\n\nrdas_dict &lt;- tibble(\n  variable = paste0(\"q\",1:24),\n  item = names(rdas)\n)\n\nDoing this is really useful because I can’t keep track in my head of what “q5” was.\nIf I want to know, then I can just do:\n\nrdas_dict[5,]\n\n# A tibble: 1 × 2\n  variable item                                   \n  &lt;chr&gt;    &lt;chr&gt;                                  \n1 q5       I am concerned about how my family feel\n\n\nNow let’s actually change the names in our data to what we said we would:\n\nnames(rdas) &lt;- paste0(\"q\", 1:24)\n\n\n\n\n\n\n3 - Recoding responses\n\n\n\nSolution 3. Okay, so we have all our data in words, not numbers. Views on how to treat Likert data are mixed, but it’s very common to treat it as continuous in Psychology.\nLet’s check the response values we have. Just in question 1 for now:\n\nunique(rdas$q1)\n\n[1] \"Often\"         \"Almost Always\" \"Hardly Ever\"   \"Occasionally\" \n[5] \"Always\"        NA              \"Never\"        \n\n\nA little trick that we can use to find the unique values in an entire dataset is to quickly convert the dataframe into one big long vector. Technically, a dataframe is a “list of vectors”, and the function unlist() will remove this structure.\nSo we can find all the unique values in all the questions with:\n\nunique(unlist(rdas))\n\n[1] \"Often\"         \"Almost Always\" \"Hardly Ever\"   \"Occasionally\" \n[5] \"Always\"        NA              \"Never\"         \"[NO ENTRY]\"   \n\n\nPerfect. So we know we have uniformity of spelling. It happens less often these days as questionnaire software is improving, but you might occasionally encounter typos in some of the questions, or things with and without capital letters (R is a bit thick, and doesn’t recognise that “Often” and “often” are the same thing).\nNote that we have the 6 responses that we would expect given the description of the scale, but we also have some NA values, and some [NO ENTRY] values. Not sure how those got there.\nWe want to turn each “Always” in to 0, each “Almost Always” in to 1, “Often” in to 2, and so on. If we simply leave out the “[NO ENTRY]”, then this will be turned into a missing value NA, which is handy.\n\nrdas &lt;- rdas |&gt; \n  mutate(across(q1:q24, ~case_match(.,\n    \"Always\" ~ 0,\n    \"Almost Always\" ~ 1,\n    \"Often\" ~ 2,\n    \"Occasionally\" ~ 3,\n    \"Hardly Ever\" ~ 4,\n    \"Never\" ~ 5\n  )))\nhead(rdas)\n\n# A tibble: 6 × 24\n     q1    q2    q3    q4    q5    q6    q7    q8    q9   q10   q11   q12   q13\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2     1     1     4     1     3     1     4     2     2     3     2     3\n2     1     4     3     4    NA     0    NA     2    NA     2     1     2     3\n3     2     3     3     4     3     3     2     2     4     1     3     2     3\n4     4     3     1     3     1    NA     2     2    NA     2     3     4     2\n5     3     4     3     5     2     2     2     3     3     2     2     2     1\n6     3     3     1     0     1     2     1     1     0     2     1     4     3\n# ℹ 11 more variables: q14 &lt;dbl&gt;, q15 &lt;dbl&gt;, q16 &lt;dbl&gt;, q17 &lt;dbl&gt;, q18 &lt;dbl&gt;,\n#   q19 &lt;dbl&gt;, q20 &lt;dbl&gt;, q21 &lt;dbl&gt;, q22 &lt;dbl&gt;, q23 &lt;dbl&gt;, q24 &lt;dbl&gt;\n\n\n\n\n\n\n\n4 - Reverse Coding\n\n\n\nSolution 4. According to the table of items, the ones which need to be reverse scored are:\n\nreversed &lt;- c(2,3,4,5,7,8,9,10,13,14,16,18,20,22)\n\nFor these items, we want 5s to become 0s, 4s become 1s, and so on.\nThe tidyverse solution shown in the readings and solutions to previous labs will work just fine, but if you’re curious, here’s a different way to accomplish the same thing using functions from base R:\n\nrdas[, reversed] &lt;- apply(\n  rdas[, reversed], MARGIN = 2, function(x) 5-x)\n\nNote: The above code works nicely because our dataset is currently ordered such that the first column is item 1, 2nd column is item 2, and so on. This means we can use numbers to index the appropriate variables, rather than names. It would need adjusting if, for instance, our first column contained “participant ID”, and our items only began later.\n\n\n\n\n\n5 - Removing missingness\n\n\n\nSolution 5. We haven’t learned about more sophisticated methods of handling missing data, so for now we will just remove any rows in which there is missingness - i.e., we’ll do “listwise deletion”:\n\ncompl_rdas &lt;- na.omit(rdas)\n\n\n\n\n\nQuestion 2\n\n\nSpecify the theoretical model proposed by Radakovic et al. \nFor reference, check out the example in the readings.\n\ndasmod &lt;- \"\n\n\n\n\n\n\"\n\nChallenge: Before you estimate the model, how many degrees of freedom do you think the model will have? (The readings will help here!)\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll have to use the data dictionary to see which items are associated with which dimensions.\n\n\n\n\n\n\n\nHere is the model structure, according to the list of items in the dictionary.\nI’m calling my factors “Em”, “Ex”, and “BCI” for “emotional”, “executive” and “behavioural/cognitive initiation” respectively. The factor correlations will be estimated by default, but I like to write things explicitly.\n\ndasmod = \"\nEx =~ q1 + q6 + q10 + q11 + q17 + q19 + q21 + q23\nEm =~ q3 + q5 + q7 + q9 + q12 + q15 + q20 + q24\nBCI =~ q2 + q4 + q8 + q13 + q14 + q16 + q18 + q22\nEm ~~ Ex\nEm ~~ BCI\nEx ~~ BCI\n\"\n\n\n\n\n\nDegrees of freedom is computed as the number of “knowns” minus the number of “unknowns”.\nLet’s start with figuring out the number of “knowns”: the number of values in the dataset. This number comes from the observed covariance matrix. Let’s imagine a smaller dataset with only five items. It’ll create a covariance matrix like this:\nvar\ncovar   var\ncovar   covar   var\ncovar   covar   covar   var\ncovar   covar   covar   covar   var\nHow many values are in this matrix? In the first row, there’s 1, plus the second row with 2, plus the third row with 3, plus the fourth row with 4, plus the fifth row with 5. In other words, there are\n\nsum(1:5)\n\n[1] 15\n\n\nvalues in this covariance matrix.\nFor the present scenario with 24 items, we will have\n\nsum(1:24)\n\n[1] 300\n\n\nvalues in the covariance matrix. (Twenty-four of these will be each item’s own variance, and the other 276 will be covariances between items.)\nThe alternative formula to calculate this is \\(\\frac{k \\cdot (k+1)}{2}\\), and we get the same number by plugging in the number of variables for \\(k\\): \\(\\frac{24 \\cdot (24+1)}{2} = \\frac{600}{2} = 300\\)\nNow let’s look at the number of “unknowns”: the number of parameters the model has to estimate. This number comes from the number of latent variables and how they relate to each item.\n\nEach latent variable has its own variance, and there are three latent variables, so the model will have three latent factor variances.\nEach item will load onto one latent variable, and there are 24 items, so the model will have 24 factor loadings.\nEach item will have residual factor variances, and there are 24 items, so the model will have 24 residual factor variances.\n\nAdding these up, we get\n\n3 + 24 + 24\n\n[1] 51\n\n\nunknown parameters.\nFinally, let’s subtract the knowns from the unknowns to get the degrees of freedom:\n\n300 - 51\n\n[1] 249\n\n\n\n\n\n\nQuestion 3\n\n\nEstimate the model using cfa().\nYou can choose whether you want to standardise the latent factors or fix the first loading of each factor to be 1 (it’s the same model, just scaled differently).\nExamine the model fit - does it fit well?\nWhat modifications do the modification indices suggest? Are the top three suggestions theoretically reasonable, in your opinion?\nRemember, we don’t really want to have to make modifications to our models. If you don’t need to (if the model fits well) then don’t bother! (It’s still worth looking at the modification indices though).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThere’s a whole section on “model fit” in the CFA chapter!\nAnd there’s also a whole section on model modifications.\n\n\n\n\n\n\n\nLet’s fit the model!\n\ndasmod.est = cfa(dasmod, compl_rdas)\n\nThe standard test of model fit is a chi-squared test which compares the observed covariance matrix to the model-implied covariance matrix. Ideally, these two matrices will be fairly similar, so we want a non-significant result.\nWe can get the test statistic and p-value from the model’s chi-squared test as follows:\n\nsummary(dasmod.est)$test$standard\n\n$test\n[1] \"standard\"\n\n$stat\n[1] 274.8334\n\n$stat.group\n[1] 274.8334\n\n$df\n[1] 249\n\n$refdistr\n[1] \"chisq\"\n\n$pvalue\n[1] 0.1251793\n\n\nSo a chi-squared test with 249 degrees of freedom results in a test statistic of 274.8, associated with a p-value of 0.125. The take-away is that our observed covariance matrix is not significantly different from the model-implied covariance matrix—yay!\nNext, let’s check the additional measures of global fit:\n\nfitmeasures(dasmod.est)[c(\"srmr\",\"rmsea\",\"tli\",\"cfi\")]\n\n      srmr      rmsea        tli        cfi \n0.05364507 0.02171599 0.96913532 0.97215469 \n\n\nAll looks pretty good! Cut-offs for SRMR tend to vary, with some using &lt;0.08, or &lt;0.09, and some being stricter with &lt;0.05. Remember, these criteria are somewhat arbitrary.\nModification indices suggest a whole bunch of items that could have some associations beyond that modelled in the factors, but these are all weak correlations at around 0.2.\n\nmodindices(dasmod.est, sort=TRUE) |&gt; head()\n\n    lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n132  q6 ~~  q3 15.896  0.222   0.222    0.284    0.284\n113  q1 ~~  q9 11.929 -0.172  -0.172   -0.267   -0.267\n216 q19 ~~ q20 10.887  0.168   0.168    0.256    0.256\n103  q1 ~~  q6  7.447  0.149   0.149    0.220    0.220\n358  q4 ~~  q8  7.417  0.199   0.199    0.207    0.207\n105  q1 ~~ q11  6.757 -0.149  -0.149   -0.237   -0.237\n\n\nThese are the top 3 being suggested. I can’t see any obvious link between any of these that would make me think they are related beyond their measuring of ‘apathy’.\n\nrdas_dict[c(3,6),]\n\n# A tibble: 2 × 2\n  variable item                             \n  &lt;chr&gt;    &lt;chr&gt;                            \n1 q3       I express my emotions            \n2 q6       I find myself staring in to space\n\nrdas_dict[c(1,9),]\n\n# A tibble: 2 × 2\n  variable item                                               \n  &lt;chr&gt;    &lt;chr&gt;                                              \n1 q1       I need a bit of encouragement to get things started\n2 q9       When I receive bad news I feel bad about it        \n\nrdas_dict[c(19,20),]\n\n# A tibble: 2 × 2\n  variable item                                                                \n  &lt;chr&gt;    &lt;chr&gt;                                                               \n1 q19      I get easily confused when doing several things at once             \n2 q20      I become emotional easily when watching something happy or sad on TV\n\n\n\n\n\n\nQuestion 4\n\n\nAre the (standardised) loadings all “big enough”?\nThere’s no clear threshold that people use here - it depends a lot on the field, and on the wordings of specific items. Ideally, the same value we used in EFA (\\(\\geq|0.3|\\)) would be nice, but not crucial.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo get out standardised loadings, we can do:\n\nmod.est &lt;- cfa(model_syntax, data = ...)\nsummary(mod.est, std=TRUE)\n\nAnd you’ll get out an extra 2 columns in the summary output.\nPay attention to the Estimate, Std.lv and Std.all columns in your output. The way I think of these columns is just to think of how we scale things in regression models:\n\nEstimate column : item ~ Factor\n\nStd.lv column : item ~ scale(Factor)\n\nStd.all column: scale(item) ~ scale(Factor)\n\nSo if, when we fitted the model, we had specified cfa(model, data, std.lv = TRUE), then the factor already has a variance of 1, so Scale(Factor) doesn’t do anything.\nSee Chapter 4#interpretation.\n\n\n\n\n\n\n\nI’m not going to print all of this right now because there’s so much output, but here’s how we would find standardised loadings. We can find them in the Std.all column.\n\nsummary(dasmod.est, std = TRUE)\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Ex =~                                                                 \n    q1                1.000                               0.679    0.694\n    q6                0.679    0.121    5.607    0.000    0.461    0.433\n    ...\n    ...\nThe standardised loadings are all (just) greater than \\(|0.3|\\). Questions 13 and 15 are very close…\n\nrdas_dict[c(13,15),]\n\n# A tibble: 2 × 2\n  variable item                                                     \n  &lt;chr&gt;    &lt;chr&gt;                                                    \n1 q13      I set goals for myself                                   \n2 q15      I am unconcerned about how others feel about my behaviour\n\n\n\n\n\n\nQuestion 5\n\n\nDo the factors correlate in the way you would expect?\nIs more emotional apathy associated with more executive apathy? and with more initiation apathy?\n\n\n\n\n\n\nHints\n\n\n\n\n\nIf you didn’t reverse code the appropriate items, then this might get confusing, because we’d have to look at factor loadings to know in which direction the factor is going (i.e., are higher numbers “more apathy” or “less apathy”?).\nIf you did reverse code the appropriate items, then you’re golden, because you made them all point towards “more” apathy.\n\n\n\n\n\n\n\nHere are the correlations we’re interested in. Note that what we are seeing is that the three factors are all positively correlated, but for Em and Ex this is only weak (and not significant).\nThis isn’t necessary a problem, it just means that these two factors are fairly distinct/orthogonal. We might want to check back in the original paper to see what they proposed!\n\nsummary(dasmod.est, std = TRUE)\n\n...\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Ex ~~                                                                 \n    Em                0.051    0.028    1.807    0.071    0.159    0.159\n  Em ~~                                                                 \n    BCI               0.043    0.018    2.369    0.018    0.263    0.263\n  Ex ~~                                                                 \n    BCI               0.151    0.040    3.746    0.000    0.642    0.642\n\n\n\n\n\n\n\nQuestion 6\n\n\nMake a diagram of the model.\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor a quick look at the structure of the model, try the semPaths() function from the semPlot package Chapter 4 CFA#making diagrams.\nIf you were going to use this sort of diagram in a proper write-up, though, it’d be better to make a nicer graphic manually (e.g., in Powerpoint, your favourite graphics software, or semdiag).\n\n\n\n\n\n\n\nHere’s one example:\n\nlibrary(semPlot)\nsemPaths(dasmod.est, whatLabels = \"std\", rotation=2)\n\n\n\n\n\n\n\n\nThere are lots of options in semPaths(), so if you can make your graphic more elaborate than this one, then be our guest!\n\n\n\n\nOptional Question 7\n\n\n\n\nImagine that you’re a clinician administering the DAS to a patient. In clinical settings, it’s common practice to skip the complex factor analysis we’ve been doing here and just create a sum score or a mean score that describe a patient’s responses. Then clinicians can check whether the score is above some threshold to see whether there’s cause for concern.\nFor each of the dimensions of apathy in the data, calculate sum scores for each of the 250 participants.\n\n\n\n\n\n\nHints\n\n\n\n\n\nGood ol’ rowSums() to the rescue!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Here are the sets of items associated with each dimension:\n\nExitems &lt;- c(1,6,10,11,17,19,21,23)\nEmitems &lt;- c(3,5,7,9,12,15,20,24)\nBCIitems &lt;- c(2,4,8,13,14,16,18,22)\n\nAgain, because the item numbers correspond to the column positions in our data, we can just do rowSums indexing on those column numbers to get our scores:\n\ncompl_rdas$ExSCORE &lt;- rowSums(compl_rdas[,Exitems])\ncompl_rdas$EmSCORE &lt;- rowSums(compl_rdas[,Emitems])\ncompl_rdas$BCIScore &lt;- rowSums(compl_rdas[,BCIitems])\n\n\n\n\n\nOptional Question 8\n\n\nHow might you think about a sum/mean score in terms of a diagram?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWhat does a sum or mean score imply about how each item is weighted compared to the others? How is this different from what a more sophisticated method like EFA or CFA can do?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Computing sum scores can feel like a ‘model free’ calculation, but actually it does pre-suppose a factor structure, and a much more constrained one than those we have been estimating. Specifically, we’re assuming that all items contribute equally to an underlying factor, rather than being weighted differently, and that all items also have the same variance as one another.\n\n\n\n\n\n\n\n\n\nFor a full explanation of this idea, see “Thinking twice about sum scores”, McNeish & Wolf 2020.\n\n\n\n\n\n\n“DOOM” Scrolling\n\nDataset: doom.csv\nThe “Domains of Online Obsession Measure” (DOOM) is a fictitious scale that aims to assess the sub types of addictions to online content. It was developed to measure 2 separate domains of online obsession: items 1 to 9 are representative of the “emotional” relationships people have with their internet usage (i.e. how it makes them feel), and items 10 to 15 reflect “practical” relationship (i.e., how it connects or interferes with their day-to-day life). Each item is measured on a 7-point likert scale from “strongly disagree” to “strongly agree”.\nWe administered this scale to 476 participants in order to assess the validity of the 2 domain structure of the online obsession measure that we obtained during scale development.\nThe data are available at https://uoepsy.github.io/data/doom.csv, and the table below shows the individual item wordings.\n\n\n\n\n\n\n\n\nvariable\nquestion\n\n\n\n\nitem_1\ni just can't stop watching videos of animals\n\n\nitem_2\ni spend hours scrolling through tutorials but never actually attempt any projects.\n\n\nitem_3\ncats are my main source of entertainment.\n\n\nitem_4\nlife without the internet would be boring, empty, and joyless\n\n\nitem_5\ni try to hide how long i’ve been online\n\n\nitem_6\ni avoid thinking about things by scrolling on the internet\n\n\nitem_7\neverything i see online is either sad or terrifying\n\n\nitem_8\nall the negative stuff online makes me feel better about my own life\n\n\nitem_9\ni feel better the more 'likes' i receive\n\n\nitem_10\nmost of my time online is spent communicating with others\n\n\nitem_11\nmy work suffers because of the amount of time i spend online\n\n\nitem_12\ni spend a lot of time online for work\n\n\nitem_13\ni check my emails very regularly\n\n\nitem_14\nothers in my life complain about the amount of time i spend online\n\n\nitem_15\ni neglect household chores to spend more time online\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nAssess whether the 2 domain model of online obsession provides a good fit to the validation sample of 476 participants.\n\n\n\n\nFrom the visual of the correlation matrix, you can see the vague outline of two groups of items correlations. Note there’s a little overlap..\n\ndoom &lt;- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\nheatmap(cor(doom))\n\n\n\n\n\n\n\n\nfirst we write our model:\n\nmoddoom &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n\nThen we fit it to the data:\n\nmoddoom.est &lt;- cfa(moddoom, data = doom)\n\nThen we inspect that fitted model object.\nI’m just going to extract the fit indices here first.\n\nfitmeasures(moddoom.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n     rmsea       srmr        cfi        tli \n0.07331272 0.06227484 0.84791972 0.81790388 \n\n\nUh-oh.. they don’t look great.\n\n\n\n\nQuestion 10\n\n\nAre there any areas of local misfit (certain parameters that are not in the model (and are therefore fixed to zero) but that could improve model fit if they were estimated?).\n\n\n\n\nI’m printing out just the head(), so that I can look at the few parameters with the greatest modification indices.\nThe top three parameters jump out immediately to me.\nitem_1 and item_3 have a suggested correlation of c0.5, as do item_7 and item_8. In addition, it’s suggested that including a loading (estimated to be about 0.6) from item_10 to the emot factor would improve the model fit.\n\nmodindices(moddoom.est, sort=TRUE) |&gt;\n  head()\n\n        lhs op     rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   item_1 ~~  item_3 88.700  0.560   0.560    0.459    0.459\n32     emot =~ item_10 74.374  1.690   0.817    0.640    0.640\n109  item_7 ~~  item_8 61.930  0.370   0.370    0.432    0.432\n34     emot =~ item_13 14.824 -0.655  -0.317   -0.275   -0.275\n122  item_9 ~~ item_10 12.634  0.145   0.145    0.217    0.217\n135 item_13 ~~ item_15  8.956  0.178   0.178    0.171    0.171\n\n\n\n\n\n\nQuestion 11\n\n\nBeware: there’s a slightly blurred line here that we’re about to step over, and move from confirmatory back to ‘exploratory’.\nLook carefully at the item wordings,do any of the suggested modifications make theoretical sense? Add them to the model and re-fit it. Does this new model fit well?\nIn this case, the likely reason for the poor fit of the “DOOM” scale, is that the person who made the items (ahem, me) doesn’t really know anything about the construct they are talking about, and didn’t put much care into constructing the items!\n\n\n\n\nThere are three main proposed adjustments from our initial model:\n\nitem_1 ~~ item_3. These questions are both about animals. It would make sense that these are related over and above the underlying “emotional internet usage” factor.\nitem_7 ~~ item_8. These are both about viewing negative content online, so it makes sense here that they would be related beyond the ‘emotional’ factor.\nemot =~ item_10. This item is about communicating with others. It currently loads highly on the pract factor too. It maybe makes sense here that “communicating with others” will capture both a practical element of internet useage and an emotional one.\n\nPutting them all in at once could be a mistake - if we added in emot =~ item_10, then we change slightly the underlying construct of the emot factor, meaning it might make other suggested modifications (item_7 ~~ item_8) less important. It’s a bit like Whac-A-Mole - you make one modification and then a whole new area of misfits appears!\nLet’s adjust our model:\n\nmoddoom2 &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n\nThen fit it to the data:\n\nmoddoom2.est &lt;- cfa(moddoom2, data = doom)\n\nfitmeasures(moddoom2.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n     rmsea       srmr        cfi        tli \n0.06013775 0.05147585 0.89901514 0.87747171 \n\n\nThe fit is still not great, and the same suggested correlations are present in modification indices:\n\nmodindices(moddoom2.est, sort=TRUE) |&gt;\n  head()\n\n       lhs op     rhs     mi    epc sepc.lv sepc.all sepc.nox\n47  item_1 ~~  item_3 87.679  0.553   0.553    0.455    0.455\n109 item_7 ~~  item_8 61.827  0.365   0.365    0.421    0.421\n43   pract =~  item_7  7.117 -0.300  -0.178   -0.159   -0.159\n48  item_1 ~~  item_4  6.264 -0.139  -0.139   -0.128   -0.128\n117 item_8 ~~ item_10  6.072 -0.110  -0.110   -0.151   -0.151\n44   pract =~  item_8  5.065 -0.244  -0.145   -0.130   -0.130\n\n\nLet’s go ahead and put the covariance between item_1 and item_3 in. I personally went for this first because they seem more similar to me than item_7 and item_8 do.\n\nmoddoom3 &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9 + item_10\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\nitem_1 ~~ item_3\n\"\n\nmoddoom3.est &lt;- cfa(moddoom3, data = doom)\n\nfitmeasures(moddoom3.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n     rmsea       srmr        cfi        tli \n0.03282045 0.03830848 0.97032291 0.96350520 \n\n\nWhoop! It fits well! It may well be that if we inspect modification indices again, we still see that item_7 ~~ item_8 would improve our model fit. The thing to remember however, is that we could simply keep adding parameters until we run out of degrees of freedom, and our model would “fit better”. But such a model would not be useful. It would not generalise well, because it runs the risk of being overfitted to the nuances of this specific sample.\n\n\n\n\nQuestion 12\n\n\nmcq style\nwhich do you think reflects where we’re at now with our analysis\n\nwe have confirmed our theoretical measurement model of DOOM scrolling\nwe have confirmed our theoretical measurement model, but with some caveats\nthe measure of doom scrolling is crap\n\n\n\n\n\n\n\n\n\n\n\n\nMore Conduct Problems\n\nData: conduct_problems_2.csv\nLast week we conducted an exploratory factor analysis of a dataset to try and identify an optimal factor structure for a new measure of conduct (i.e., antisocial behavioural) problems.\nThis week, we’ll conduct some confirmatory factor analyses (CFA) of the same inventory to assess the extent to which this 2-factor structure fits an independent sample. To do this, we have administered our measure to a new sample of n=600 adolescents.\nWe have re-ordered the questionnaire items to be grouped into the two types of behaviours:\n\n\nNon-Aggressive Behaviours\n\n\n\n\n\nitem\nbehaviour\n\n\n\n\nitem 1\nStealing\n\n\nitem 2\nLying\n\n\nitem 3\nSkipping school\n\n\nitem 4\nVandalism\n\n\nitem 5\nBreaking curfew\n\n\n\n\n\n\n\n\n\n\nAggressive Behaviours\n\n\n\n\n\nitem\nbehaviour\n\n\n\n\nitem 6\nThreatening others\n\n\nitem 7\nBullying\n\n\nitem 8\nSpreading malicious rumours\n\n\nitem 9\nUsing a weapon\n\n\nitem 10\nFighting\n\n\n\n\n\n\n\n\n\nThe data are available as a .csv at https://uoepsy.github.io/data/conduct_problems_2.csv\n\n\nQuestion 13\n\n\n\nRead in the data, and take a quick look around (e.g., cor matrix, quick pairs.panels plots etc).\nFit the proposed 2 factor model\nExamine the fit of the 2-factor model of conduct problems to this new sample of 600 adolescents.\n\nEvaluate the fit, and make any model modifications if necessary (and only if you feel that there is substantive support for the modification given the items).\n\nMake a diagram of your model, using the standardised factor loadings as labels.\n\nMake a bullet point list of everything you have done so far, and the resulting conclusions. Then, if you feel like it, turn the bulleted list into written paragraphs, and you’ll have a write-up of your analyses!\n\n\n\n\n\nHere’s the data:\n\nlibrary(tidyverse)\ncp2 &lt;- read_csv(\"https://uoepsy.github.io/data/conduct_problems_2.csv\")\n\ncor(cp2)\n\n           item1     item2      item3     item4     item5      item6     item7\nitem1  1.0000000 0.5254249 0.43498606 0.4831121 0.5644047 0.13450795 0.2806369\nitem2  0.5254249 1.0000000 0.50099054 0.5119842 0.6740107 0.15239358 0.3031997\nitem3  0.4349861 0.5009905 1.00000000 0.4897339 0.5990709 0.09463966 0.2557407\nitem4  0.4831121 0.5119842 0.48973386 1.0000000 0.6199783 0.14847703 0.2492471\nitem5  0.5644047 0.6740107 0.59907089 0.6199783 1.0000000 0.11353641 0.3112458\nitem6  0.1345080 0.1523936 0.09463966 0.1484770 0.1135364 1.00000000 0.5461757\nitem7  0.2806369 0.3031997 0.25574075 0.2492471 0.3112458 0.54617567 1.0000000\nitem8  0.2576481 0.2748175 0.19355398 0.2553712 0.2463290 0.59441335 0.8009591\nitem9  0.2444337 0.2759875 0.20798523 0.2260156 0.2333758 0.36915878 0.5988582\nitem10 0.1783069 0.1618631 0.12924914 0.1433759 0.1394748 0.46316014 0.5316143\n           item8     item9    item10\nitem1  0.2576481 0.2444337 0.1783069\nitem2  0.2748175 0.2759875 0.1618631\nitem3  0.1935540 0.2079852 0.1292491\nitem4  0.2553712 0.2260156 0.1433759\nitem5  0.2463290 0.2333758 0.1394748\nitem6  0.5944133 0.3691588 0.4631601\nitem7  0.8009591 0.5988582 0.5316143\nitem8  1.0000000 0.6169829 0.5557168\nitem9  0.6169829 1.0000000 0.3613253\nitem10 0.5557168 0.3613253 1.0000000\n\n\nJust from the visual, it looks like the same factor structure is present in this sample.\n\nheatmap(cor(cp2), scale = \"none\")\n\n\n\n\n\n\n\n\nThis is our proposed model:\n\nlibrary(lavaan)\ncpmod &lt;- \"\n  # the non-aggressive problems factor\n  nonagg =~ item1 + item2 + item3 + item4 + item5\n\n  # the aggressive problems factor\n  agg =~ item6 + item7 + item8 + item9 + item10\n\n  # covariance between the two factors\n  # (this is included by default in cfa)\n  agg ~~ nonagg\n\"\n\ncpmod.est &lt;- cfa(cpmod, data = cp2)\n\nAnd it appears to fit pretty well!\n\nfitmeasures(cpmod.est)[c(\"srmr\",\"rmsea\",\"cfi\",\"tli\")]\n\n      srmr      rmsea        cfi        tli \n0.03454489 0.03945991 0.98862263 0.98494172 \n\n\nWe can check modification indices anyway, but I don’t plan on making any adjustments given that it already fits well:\n\nmodindices(cpmod.est, sort = TRUE) |&gt; head()\n\n      lhs op    rhs     mi    epc sepc.lv sepc.all sepc.nox\n72  item6 ~~ item10 10.747  0.082   0.082    0.144    0.144\n25 nonagg =~  item7  8.106  0.119   0.080    0.080    0.080\n65  item5 ~~  item7  6.720  0.039   0.039    0.160    0.160\n71  item6 ~~  item9  6.675 -0.065  -0.065   -0.115   -0.115\n24 nonagg =~  item6  6.100 -0.136  -0.092   -0.094   -0.094\n33    agg =~  item5  5.273 -0.122  -0.075   -0.072   -0.072\n\n\nIt maybe makes sense that there is some residual covariance between item6 (“threatening others”) and item10 (“fighting”), but it’s only a weak correlation (0.14). Not worth adding.\nSo let’s get on with making a diagram. We can rotate this however you like. Convention is typically to have it downwards but I like it left to right (not sure why!)\n\nlibrary(semPlot)\nsemPaths(cpmod.est, \n        whatLabels = \"std\", \n        rotation = 2)\n\n\n\n\n\n\n\n\nThe lines from agg =~ item6 and nonagg =~ item1 are dotted to indicate that the model was initially fitted with the loading fixed to 1.\nBecause we’re showing standardised loadings, we could just use the model when fitted with std.lv=TRUE just to stop these dotted lines from appearing:\n\ncpmod.est2 &lt;- cfa(cpmod, data = cp2, std.lv = TRUE)\n\nsemPaths(cpmod.est2, \n        whatLabels = \"std\", \n        rotation = 2)\n\n\n\n\n\n\n\n\nAnd let’s give a brief write-up:\nA two-factor model was tested. Items 1-5 loaded on a ‘non-aggressive conduct problems’ factor and items 6-10 loaded on an ‘aggression’ factor and these factors were allowed to correlate. Scaling and identification were achieved by fixing the loading of item 1 on the non-aggressive conduct problems factor and item 6 on the aggression factor to 1. The model was estimated using maximum likelihood estimation. The model fit well with CFI=.99, TLI=0.99, RMSEA=.04, and SRMR=.04 (Hu & Bentler, 1999). All loadings were statistically significant and &gt;|.3| on the standardised scale. Overall, therefore, a two-factor oblique model was supported for the conduct problems items. The correlation between the factors was \\(r=.38\\,\\, (p&lt;.001)\\).\n\n\n\n\n\nparameter\nest\nstd.est\nse\nz\npvalue\n\n\n\n\nnonagg=~item1\n1.000\n0.664\n0.000\n\n\n\n\nnonagg=~item2\n1.217\n0.765\n0.076\n15.965\n&lt; 0.001\n\n\nnonagg=~item3\n1.012\n0.676\n0.070\n14.424\n&lt; 0.001\n\n\nnonagg=~item4\n1.146\n0.706\n0.077\n14.967\n&lt; 0.001\n\n\nnonagg=~item5\n1.360\n0.872\n0.078\n17.412\n&lt; 0.001\n\n\nagg=~item6\n1.000\n0.636\n0.000\n\n\n\n\nagg=~item7\n1.419\n0.878\n0.082\n17.331\n&lt; 0.001\n\n\nagg=~item8\n1.437\n0.915\n0.081\n17.662\n&lt; 0.001\n\n\nagg=~item9\n1.093\n0.668\n0.077\n14.125\n&lt; 0.001\n\n\nagg=~item10\n0.950\n0.608\n0.073\n13.073\n&lt; 0.001\n\n\nnonagg~~agg\n0.156\n0.375\n0.023\n6.856\n&lt; 0.001\n\n\nitem1~~item1\n0.574\n0.559\n0.037\n15.398\n&lt; 0.001\n\n\nitem2~~item2\n0.477\n0.416\n0.035\n13.778\n&lt; 0.001\n\n\nitem3~~item3\n0.551\n0.543\n0.036\n15.262\n&lt; 0.001\n\n\nitem4~~item4\n0.597\n0.501\n0.040\n14.867\n&lt; 0.001\n\n\nitem5~~item5\n0.264\n0.239\n0.028\n9.537\n&lt; 0.001\n\n\nitem6~~item6\n0.561\n0.595\n0.035\n16.193\n&lt; 0.001\n\n\nitem7~~item7\n0.228\n0.229\n0.021\n10.767\n&lt; 0.001\n\n\nitem8~~item8\n0.153\n0.162\n0.019\n8.122\n&lt; 0.001\n\n\nitem9~~item9\n0.566\n0.554\n0.035\n15.978\n&lt; 0.001\n\n\nitem10~~item10\n0.587\n0.630\n0.036\n16.351\n&lt; 0.001\n\n\nnonagg~~nonagg\n0.453\n1.000\n0.052\n8.701\n&lt; 0.001\n\n\nagg~~agg\n0.381\n1.000\n0.045\n8.396\n&lt; 0.001",
    "crumbs": [
      "Week 8 Exercises: CFA"
    ]
  }
]