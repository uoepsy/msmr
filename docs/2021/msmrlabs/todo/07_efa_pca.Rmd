```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
```

# EFA and PCA  

### Packages {-}  

+ psych  
+ GPArotation  
+ car  
+ GGally (optional)

### Lecture Slides {-}  

+ The lecture slides can be accessed [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/week7_pca.pdf) and [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/week7_efa.pdf).

## Today's Exercises

<div class="noteBox"> 
#### Background {-}  

A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:  

1. Stealing
1. Lying
1. Skipping school
1. Vandalism
1. Breaking curfew
1. Threatening others
1. Bullying
1. Spreading malicious rumours
1. Using a weapon 
1. Fighting

Your task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales 
</div>

`r msmbstyle::question_begin(header="&#x25BA; Question 1")`
Load the `psych` package and read in the dataset from [https://edin.ac/2Vk1BVU]("https://edin.ac/2Vk1BVU").  
The first column is clearly an ID column, and it is easiest just to discard this for when we are doing factor analysis.  
  
Create a correlation matrix for *the items*.  
Inspect the items to check their suitability for exploratory factor analysis.   
  
+ You can use a function such as `corr.test(df)` from the psych package to create the correlation matrix.  
+ You can check the factorability of the correlation matrix using `KMO(df)`.  
+ You can check linearity of relations using `scatterplotMatrix(df)` (from the `car` package). If you add the argument `diagonal=histogram`  
+ You can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). 
+ You can do the same using the `ggpairs` function from the `GGally` package.
  
*NOTE. df=dataframe*
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
library(psych)
df <- read.csv("https://edin.ac/2Vk1BVU")

# discard the first column
df <- df[,-1]

corr.test(df)  

KMO(df)  

car::scatterplotMatrix(df)
```
or alternatively. 
```{r message=FALSE}
library(GGally)
ggpairs(data=df, diag=list(continuous="density"), axisLabels="show")
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 2")`
How many dimensions should be retained?   
  
Use a scree plot, parallel analysis, and MAP test to guide you.   
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can use `fa.parallel(df)` to conduct both parallel analysis and view a scree plot.   
```{r}
fa.parallel(df)
```
In this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors.  
  
We can conduct the MAP test using `vss(df)`.
```{r}
vss(df)
```
The MAP test suggests retaining 2 factors.  
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3")`
Having decided how many dimensions to retain in the previous question, conduct an EFA to extract this many factors, using a suitable rotation and extraction method.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can use the `fa()` function from the `psych` package, for example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method.  
```{r}
conduct_efa <- fa(df, nfactors=2, rotate='oblimin', fm='minres')
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 4")`
Inspect the loadings and give the factors you extracted labels based on the patterns of loadings.  
  
Look back to the description of the items, and suggest a name for you factors  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can inspect the loadings using:
```{r}
conduct_efa$loadings
```
We can see that the first five items have high loadings for one factor and the second five items have high loadings for the other.  
  
The first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 5")`
How correlated are your factors?  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can inspect the factor correlations (if we used an oblique rotation) using:
```{r}
conduct_efa$Phi
```
We can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. 
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>

`r msmbstyle::question_begin(header="&#x25BA; Question 6")`
Drawing on your previous answers and conducting any additional analyses you believe would be necessary to identify an optimal factor structure for the 10 conduct problems, write a brief text that summarises your method and the results from your chosen optimal model.
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin()`
The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).

An example summary would be:

First, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of $.87$ with no variable KMOs $<.50$. 
An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using minres extraction and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings $>|0.3|$ and the one-factor model blurred the distinction between different forms of conduct problems. 
The factor loadings are provided in Table \@ref(tab:loading-table)^[You should provide the table of factor loadings. It is conventional to omit factor loadings $<|0.3|$; however, be sure to ensure that you mention this in a table note.]. Based on the pattern of factor loadings, the two factors were labelled 'aggressive conduct problems' and 'non-aggressive conduct problems'. These factors had a  correlation of $r=.43$. Overall, they accounted for 52% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.

```{r loading-table, echo=FALSE}
loadings = unclass(conduct_efa$loadings)
loadings = round(loadings, 3)
loadings[abs(loadings) < 0.3] = NA
options(knitr.kable.NA = '')
knitr::kable(loadings, digits = 2, caption = "Factor loadings.")
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>

`r msmbstyle::question_begin(header="&#x25BA; Question 7")`
Using the same data, conduct a PCA using the `principal()` function.  
  
What differences do you notice compared to your EFA?  
  
Do you think a PCA or an EFA is more appropriate in this particular case?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can use:
```{r}
principal(df, nfactors=2)
```
We can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low.  
  
In terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data.
`r msmbstyle::solution_end()`

