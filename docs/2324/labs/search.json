[
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below."
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596"
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965"
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA"
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable"
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎"
  },
  {
    "objectID": "01a_clustered.html",
    "href": "01a_clustered.html",
    "title": "1A: Clustered Data | LMM",
    "section": "",
    "text": "TODO this reading"
  },
  {
    "objectID": "01a_clustered.html#clusters-clusters-everywhere",
    "href": "01a_clustered.html#clusters-clusters-everywhere",
    "title": "1A: Clustered Data | LMM",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in patients within medical practices, employees within departments, people within towns etc. These sort of groups exist ‘in the wild’ (i.e. the schools, departments and towns are all just occurrences of groups that we have observed). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters3. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\n\n\nTable 1: Various different study designs will give rise to clustered data.\n\n\n\nCross Sectional\nRepeated Measures\nLongitudinal\n\n\n\n\nLevel n\n...\n...\n...\n\n\n...\n...\n...\n...\n\n\nLevel 3\nSchool\n...\nFamilies\n\n\nLevel 2\nClassroom\nParticipants\nPeople\n\n\nLevel 1 (Observations)\nChildren\nExperimental Stimuli\nTime"
  },
  {
    "objectID": "01a_clustered.html#what-are-clusters",
    "href": "01a_clustered.html#what-are-clusters",
    "title": "1A: Clustered Data | LMM",
    "section": "What are ‘clusters’?",
    "text": "What are ‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we will probably start using the terms “clusters” and “groups” interchangeably, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing can be thought of as a random sample of higher level units.\n\nMore often than not, the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how do drugs placebo/aspirin/beta-blockers influence people’s heart rate?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Calderglen High School vs Gleniffer High School etc. If we were to run our study again, we might not collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the level of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).4\n\n\n\n\n\n\nOptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\nOptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01a_clustered.html#determining-sample-sizes",
    "href": "01a_clustered.html#determining-sample-sizes",
    "title": "1A: Clustered Data | LMM",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n\n\n\n\nUnit\nSample Size\n\n\n\n\nLevel 2\nSchool\n30\n\n\nLevel 1 (Observations)\nChildren\n900\n\n\n\n\n\n\n\nWe can check this in our data:\n\n# TODO\n# schoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Balfron High School and only 10 from Wallace Hall Academy, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot %&gt;%\n  count(schoolid) %&gt;%\n  summary()\n\n                    schoolid        n     \n Alness Academy         : 1   Min.   :30  \n Balfron High School    : 1   1st Qu.:30  \n Banff Academy          : 1   Median :30  \n Boroughmuir High School: 1   Mean   :30  \n Braes High School      : 1   3rd Qu.:30  \n Calderglen High School : 1   Max.   :30  \n (Other)                :24"
  },
  {
    "objectID": "01a_clustered.html#icc",
    "href": "01a_clustered.html#icc",
    "title": "1A: Clustered Data | LMM",
    "section": "ICC",
    "text": "ICC\nThe IntraClass Correlation Coefficient (ICC) is a descriptive measure of how much variation in a variable is explained by the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):6\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can estimate from a fitted model.\nIn the data (visualised above), it’s estimated that 40% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.4026178\n\n\n\n\n\n\n\n\nOptional: Calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot %&gt;% \n  mutate(\n    grand_mean = mean(grade)\n  ) %&gt;%\n  group_by(schoolid) %&gt;%\n  mutate(\n    group_mean = mean(grade)\n  ) %&gt;%\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot %&gt;% \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.4026178\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nSuppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\nOptional: A Little Simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 51.47 52.86\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.3987734\n\n\n\n\n\n\n\n\n\n\n\nOptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n  cluster observation   y\n1 group_1           1   4\n2 group_1           2   2\n3 group_2           1   4\n4 group_2           2   2\n5 group_3           1   7\n6 group_3           2   5\n7     ...         ... ...\n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group."
  },
  {
    "objectID": "01a_clustered.html#visualisations",
    "href": "01a_clustered.html#visualisations",
    "title": "1A: Clustered Data | LMM",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade,col))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01a_clustered.html#footnotes",
    "href": "01a_clustered.html#footnotes",
    "title": "1A: Clustered Data | LMM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nIt’s “turtles all the way down”↩︎\nDepending on the research question and design of the study, we may be only interested in things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎"
  },
  {
    "objectID": "01b_lmm.html",
    "href": "01b_lmm.html",
    "title": "1B: The Multilevel Model",
    "section": "",
    "text": "intercepts vary slopes vary\nhow is it different to fixed eff? “partial pooling” (link back to above) shrinkage - socialist vs liberal analogy?\nhow does it do this? by modelling a distribution of lines\n\\[\n\\begin{align}\\\\\n& \\color{red}{y} = \\color{blue}{b_0 + b_1x_1 \\ + \\ ... \\ + \\ b_px_p} \\color{black}{+ \\varepsilon}\\\\\n& \\text{Where:} \\\\\n& \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\end{align}\n\\]"
  },
  {
    "objectID": "01b_lmm.html#ml-and-reml",
    "href": "01b_lmm.html#ml-and-reml",
    "title": "1B: The Multilevel Model",
    "section": "ML and REML",
    "text": "ML and REML\nmax likelihood, REML"
  },
  {
    "objectID": "01b_lmm.html#fitting-issues",
    "href": "01b_lmm.html#fitting-issues",
    "title": "1B: The Multilevel Model",
    "section": "fitting issues",
    "text": "fitting issues\nconvergence warnings, singular fits"
  },
  {
    "objectID": "02a_inference.html",
    "href": "02a_inference.html",
    "title": "Inference for LMM",
    "section": "",
    "text": "SE, p-vals/confints"
  },
  {
    "objectID": "02a_inference.html#wald-t-balanced-only",
    "href": "02a_inference.html#wald-t-balanced-only",
    "title": "Inference for LMM",
    "section": "wald t (balanced only)",
    "text": "wald t (balanced only)"
  },
  {
    "objectID": "02a_inference.html#df-approximations",
    "href": "02a_inference.html#df-approximations",
    "title": "Inference for LMM",
    "section": "df approximations",
    "text": "df approximations\nsatterthwaite\nkr\n\n# ddf ----\nSATmodcomp(m1,m0) # refits with REML\nKRmodcomp(m1,m0) # refits with REML\n\nparameters::model_parameters(m1, ci_method=\"kr\", ci_random=FALSE)\n# CIs computed via REML, estimates, t, p not unless initial model fitted with REML\nparameters::model_parameters(m1,ci_method=\"sat\", ci_random=FALSE)\n# CIs computed via REML, estimates, t, p not unless initial model fitted with REML"
  },
  {
    "objectID": "02a_inference.html#likelihood-based",
    "href": "02a_inference.html#likelihood-based",
    "title": "Inference for LMM",
    "section": "likelihood based",
    "text": "likelihood based\nprofile likelihood\nlrt"
  },
  {
    "objectID": "02a_inference.html#parametric-bootstrap",
    "href": "02a_inference.html#parametric-bootstrap",
    "title": "Inference for LMM",
    "section": "parametric bootstrap",
    "text": "parametric bootstrap"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n Solution \n\n\nsolution\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  }
]