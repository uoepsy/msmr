[
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below."
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596"
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\nWe can get out some bootstrapped confidence intervals for our coefficients using the car package:\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965"
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA"
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable"
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎"
  },
  {
    "objectID": "00_pca.html",
    "href": "00_pca.html",
    "title": "PCA in 3D",
    "section": "",
    "text": "Imagine we had 3 measured variables: y1, y2, and y3, as visualised in 3-dimensional space in Figure 1\n\n\n\n\n\nFigure 1: 3 measured variables\n\n\n\nThe cloud of datapoints in Figure 1 has a shape - it is longer in one direction (sort of diagonally across y1 and y2), slightly shorter in another (across y3), and then quite narrow in another. You can imagine trying to characterise this shape as the ellipse in Figure 2\n\n\n\n\n\nFigure 2: An ellipsis capturing the cloud of datapoints\n\n\n\nWhen faced with trying to characterise the shape of a 3-dimensional object, we might normally think about its length, width and depth. Imagine being given a ruler and being asked to give two numbers to provide a measurement of your smartphone. What do you pick? Chances are, you will measure its length and then its width. You’re likely to ignore the depth because it is much less than the other two dimensions. This is what PCA is doing. If we take three perpendicular dimensions, we can see that the shape in Figure 2 is longer in one dimension, then slightly shorter in another, and very short in another. These dimensions (seen in Figure 4) are our principal components! Our scree plot (indicating the amount of variance captured by each component) would look like Figure 3 - we can see that each dimension captures less and less of the variance.\n\n\n\n\n\n\nFigure 3: Scree plot for PCA of 3 uncorrelated variables\n\n\n\n\n\n\n\n\nFigure 4: Principal components are the axes\n\n\n\nOur principal components capture sequentially the largest dimensions of the shape, which reflect where the most variance is. If there was no correlation between any of our observed variables (i.e. they’re all unrelated), then we would have a shape that was basically a sphere, and the no single dimension would capture much more variance than any other. This would look something like Figure 6. Our scree plot would look like Figure 5 - we can see that each component captures a similar amount.\n\n\n\n\n\n\nFigure 5: Scree plot for PCA of 3 uncorrelated variables\n\n\n\n\n\n\n\n\nFigure 6: Principal components for 3 uncorrelated variables\n\n\n\nThe “loadings” we get out of a PCA reflect the amount to which each variable changes across the component. Try rotating the plots in Figure 7 and Figure 8, which show the first principal component and second principal component respectively. You will see that the first component (the black line) is much more closely linked to changes in y1 and y2 than it is to changes in y3. The second component is the opposite. This reflected in the relative weight of the loadings below!\n\n\n\nLoadings:\n   PC1    PC2    PC3   \nV1  0.967 -0.130 -0.217\nV2  0.964 -0.155  0.216\nV3  0.288  0.958       \n\n                 PC1   PC2   PC3\nSS loadings    1.948 0.958 0.094\nProportion Var 0.649 0.319 0.031\nCumulative Var 0.649 0.969 1.000\n\n\n\n\n\n\n\nFigure 7: The first principal component\n\n\n\n\n\n\n\n\nFigure 8: The second principal component"
  },
  {
    "objectID": "00_surveywrangle.html",
    "href": "00_surveywrangle.html",
    "title": "Data Wrangling for Surveys & Questionnaires",
    "section": "",
    "text": "Questionnaire data often comes to us in ‘wide’ format, which is often how we want it for many of the analytical methods we use with questionnaire data. However, working with data in the wide format comes with some specific challenges that generally arise because we have lots and lots of variables.\nBelow we will walk through some of the common ways we want to wrangle and clean questionnaire data."
  },
  {
    "objectID": "00_surveywrangle.html#footnotes",
    "href": "00_surveywrangle.html#footnotes",
    "title": "Data Wrangling for Surveys & Questionnaires",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically this is pronounced “LICK-URT” and not “LIE-KURT”. It’s named after Dr Rensis Likert, and that’s how he pronounced his name!↩︎"
  },
  {
    "objectID": "01a_clustered.html",
    "href": "01a_clustered.html",
    "title": "1A: Clustered Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to clustered data\nWorking with clustered data (sample sizes, ICC, visualisations)"
  },
  {
    "objectID": "01a_clustered.html#clusters-clusters-everywhere",
    "href": "01a_clustered.html#clusters-clusters-everywhere",
    "title": "1A: Clustered Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\nTable 1:  Various different study designs will give rise to clustered data. \n  \n    \n       \n      Cross Sectional\n      Repeated Measures\n      Longitudinal\n    \n  \n  \n    Level n\n...\n...\n...\n    ...\n...\n...\n...\n    Level 3\nSchool\n...\nFamilies\n    Level 2\nClassroom\nParticipants\nPeople\n    Level 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n  \n  \n  \n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person)."
  },
  {
    "objectID": "01a_clustered.html#what-are-clusters",
    "href": "01a_clustered.html#what-are-clusters",
    "title": "1A: Clustered Data",
    "section": "What are ‘clusters’?",
    "text": "What are ‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we will probably start using the terms “clusters” and “groups” interchangeably, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing can be thought of as a random sample of higher level units.\n\nMore often than not, the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how do drugs placebo/aspirin/beta-blockers influence people’s heart rate?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01a_clustered.html#determining-sample-sizes",
    "href": "01a_clustered.html#determining-sample-sizes",
    "title": "1A: Clustered Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n  \n    \n       \n      Unit\n      Sample Size\n    \n  \n  \n    Level 2\nSchool\n30\n    Level 1 (Observations)\nChildren\n900\n  \n  \n  \n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30"
  },
  {
    "objectID": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1A: Clustered Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context."
  },
  {
    "objectID": "01a_clustered.html#visualisations",
    "href": "01a_clustered.html#visualisations",
    "title": "1A: Clustered Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01a_clustered.html#footnotes",
    "href": "01a_clustered.html#footnotes",
    "title": "1A: Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nDepending on the research question and design of the study, we may be only interested in things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎"
  },
  {
    "objectID": "01b_lmm.html",
    "href": "01b_lmm.html",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel model (MLM)\nHow the MLM achieves partial pooling\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)"
  },
  {
    "objectID": "01b_lmm.html#random-intercepts",
    "href": "01b_lmm.html#random-intercepts",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)."
  },
  {
    "objectID": "01b_lmm.html#random-slopes",
    "href": "01b_lmm.html#random-slopes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho \\sigma_0 \\sigma_1\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho \\sigma_0 \\sigma_1\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values"
  },
  {
    "objectID": "01b_lmm.html#extracting-model-parameters",
    "href": "01b_lmm.html#extracting-model-parameters",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod2)\n\n(Intercept)       motiv \n  29.233320    4.475717 \n\n\n\n\nrandom effect variances\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\nVarCorr(smod2)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 12.5745        \n          motiv        2.0708  -0.698\n Residual             11.8036"
  },
  {
    "objectID": "01b_lmm.html#making-model-predictions",
    "href": "01b_lmm.html#making-model-predictions",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 7.07 higher than average, and an estimate slope of motivation that is 0.47 lower than average.\n\nranef(smod2)\n\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod2))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slope for each cluster.\nFor example, if we are estimating that “Anderson High School” has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.\nWe can get these out using coef()\n\ncoef(smod2)\n\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n\n\nfixef() + ranef() = coef()"
  },
  {
    "objectID": "01b_lmm.html#a-more-complex-model",
    "href": "01b_lmm.html#a-more-complex-model",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "A more complex model",
    "text": "A more complex model\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child’s education motivation, motiv), and our observations (children) happen to be clustered into groups (schools).\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).\nFor instance, we might have questions that take the form:\n\n“does [Level-2 variable] predict [Level-1 outcome]?”\n\n“does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?\n\n(in our example, Level-1 = children, Level-2 = Schools).\nConsider, for example, if we want to investigate whether the relationship between children’s motivation levels and their grades is different depending upon the source of school funding (private vs state).\nAddressing such questions requires a different fixed effect structure in order to allow us to test the relevant estimate of interest. Specifically here we need the interaction between motiv and funding (private vs state).\nNote that this interaction is ‘cross-level’! It allows us to ask whether something about children (the grade~motiv relationship) depends upon something about the school they’re in (funding type).\n\nsmod3 &lt;- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n\nNote, we cannot include funding in the random effects part of our model, because “the effect of funding on school grades” is something we assess by comparing between schools. We cannot think of that effect varying by-school because every school is either “private” or “state” funded. We never observe “Ardnamurchan High School” as anything other than “state” funded, so “the effect on grades of being state/private funded” does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the funding coefficient and the motiv:funding interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in Figure 7, to having two lines (one for “private” schools and one for “state” schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.\n\n\n\n\n\n\nModel equation\n\n\n\n\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome.\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the lmer() syntax in R, rather than in terms of the mathematical expressions.\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!\nBecause the funding variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n\\]\nIt is sometimes easier to think of this in the “mixed effects notation” we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:\n\\[\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\noptional: an attempted visual explanation\n\n\n\n\n\nFigure 12 shows an attempted visual intuition of how the different parts of the model work:\n\n\n\n\n\nFigure 12: visual explanation of a model with a cross-level interaction\n\n\n\n\n\n\n\n\n\n\n\n\nmodel summary\n\nsummary(smod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: grade ~ motiv * funding + (1 + motiv | schoolid)\n   Data: schoolmot\n\nREML criterion at convergence: 7083.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08250 -0.67269  0.03043  0.63562  3.13012 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 105.124  10.253        \n          motiv         2.595   1.611   -0.48\n Residual             139.030  11.791        \nNumber of obs: 900, groups:  schoolid, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         40.3143     4.6414   8.686\nmotiv                2.6294     0.8652   3.039\nfundingstate       -17.2531     5.7346  -3.009\nmotiv:fundingstate   2.8485     1.0591   2.689\n\nCorrelation of Fixed Effects:\n            (Intr) motiv  fndngs\nmotiv       -0.782              \nfundingstat -0.809  0.633       \nmtv:fndngst  0.639 -0.817 -0.773\n\n\n\n\nplot\nFor plotting the fixed effect estimates (which are often the bit we’re most interested in) from multilevel models, we can’t rely on using predict(), fitted() or augment(), as these return to us the cluster-specific predicted values.\nInstead, we need to use tools like the effects package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:\n\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)"
  },
  {
    "objectID": "01b_lmm.html#convergence-warnings-singular-fits",
    "href": "01b_lmm.html#convergence-warnings-singular-fits",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))"
  },
  {
    "objectID": "01b_lmm.html#footnotes",
    "href": "01b_lmm.html#footnotes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "01ex.html#footnotes",
    "href": "01ex.html#footnotes",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "02a_inference.html",
    "href": "02a_inference.html",
    "title": "2A: Inference for MLM",
    "section": "",
    "text": "This reading:\nConducting inference (i.e. getting confidence intervals or p-values, model comparisons) for MLMs can be tricky partly because there are a variety of different methods that have been developed.\nThis reading briefly explains why getting p-values from lmer() is not as easy as it was for lm(), before giving an outline of some of the main approaches people tend to take. Don’t feel like you have to remember all of these, just be aware that they exist, and refer back to this page whenever you need to."
  },
  {
    "objectID": "02a_inference.html#summary",
    "href": "02a_inference.html#summary",
    "title": "2A: Inference for MLM",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood based\nparametric bootstrap\n\n\n\n\ntests/CIs of individual parameters\nTests of individual parameters can be done by refitting with lmerTest::lmer(...) for the Satterthwaite (S) method, or using parameters::model_parameters(model, ci_method=\"kr\") for Kenward Rogers (KR).\nProfile likelihood CIs for individual parameters can be obtained via confint(m, method=\"profile\"), but this can be computationally demanding.\nParametric Bootstrapped CIs for individual parameters can be obtained via confint(m, method=\"boot\")\n\n\nmodel comparisons(different fixed effects, same random effects)\nComparisons of models that differ only in their fixed effects can be done via \\(F\\) tests in the pbkrtest package:SATmodcomp(m2, m1) for S and KRmodcomp(m2, m1) for KR.\nComparisons of models that differ only in their fixed effects can be done via LRT using anova(m1, m2)\nComparisons of models that differ only in their fixed effects can be done via a bootstrapped LRT using PBmodcomp(m2, m1) from the pbkrtest package.\n\n\n\nFor KR, models must be fitted with REML=TRUE (a good option for small samples). For S, models can be fitted with either.\nFor likelihood based methods for fixed effects, models must be fitted with REML=FALSE.Likelihood based methods are asymptotic (i.e. hold when \\(n \\rightarrow \\infty\\)). Best avoided with smaller sample sizes (i.e. a small number of clusters)\nTime consuming, but considered best available method (can be problematic with unstable models)\n\n\n\n\n\n\n\n\n\noptional: testing random effects?\n\n\n\n\n\nTests of random effects are difficult because the null hypothesis (the random effect variance is zero) lies on a boundary (you can’t have a negative variance). Comparisons of models that differ only in their random effects can be done by comparing ratio of likelihoods when fitted with REML=TRUE (this has to be done manually), but these tests should be treated with caution.\nWe can obtain confidence intervals for our random effect variances using both the profile likelihood and the parametric boostrap methods discussed above.\nAs random effects are typically part of the experimental design, there is often little need to test their significance. In most cases, the maximal random effect structure can be conceptualised without reference to the data or any tests, and the inclusion/exclusion of specific random effects is more a matter of what simplifications are required for the model to converge. Inclusion/exclusion of parameters based on significance testing is rarely, if ever a sensible approach."
  },
  {
    "objectID": "02a_inference.html#footnotes",
    "href": "02a_inference.html#footnotes",
    "title": "2A: Inference for MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(n\\) observations minus \\(k\\) parameters (slope of x) minus 1 intercept↩︎"
  },
  {
    "objectID": "02b_loglong.html",
    "href": "02b_loglong.html",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "",
    "text": "This reading:\nTwo examples!\n\nLogistic multilevel models (lm() is to glm() as lmer() is to glmer())\n\nMuch of the heavy lifting in understanding the transition from linear &gt;&gt; logistic models is just the same as USMR Week 10, so it might be worth looking back over that for a refresher.\n\n“Change over time” - Fitting multilevel models to longitudinal data.\n\nThe application of multilevel models to longitudinal data is very much just that - we are taking the same sort of models we learned last week and simply applying them to a different context in which “time” is a predictor."
  },
  {
    "objectID": "02b_loglong.html#example",
    "href": "02b_loglong.html#example",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: monkeystatus.csv\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nThe data is available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\ngetting to know my monkeys\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in long format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable monkeyID which indicates what monkey each problem has been attempted by. We can see the status of each monkey, and the difficulty of each task, along with whether it was solved:\n\nlibrary(tidyverse)\nlibrary(lme4)\nmstat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n\n# A tibble: 6 × 4\n  status      difficulty monkeyID solved\n  &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;\n1 subordinate easy       Seunghoo      1\n2 subordinate easy       Seunghoo      0\n3 subordinate difficult  Seunghoo      0\n4 subordinate easy       Seunghoo      1\n5 subordinate difficult  Seunghoo      0\n6 subordinate easy       Seunghoo      1\n\n\nWe can do some quick exploring to see how many monkeys we have (50), and how many problems each one attempted (min = 3, max = 11:\n\nmstat |&gt; \n  count(monkeyID) |&gt; # count the monkeys!  \n  summary()\n\n   monkeyID               n        \n Length:50          Min.   : 3.00  \n Class :character   1st Qu.: 6.25  \n Mode  :character   Median : 8.00  \n                    Mean   : 7.94  \n                    3rd Qu.:10.00  \n                    Max.   :11.00  \n\n\nLet’s also see how many monkeys of different statuses we have in our sample:\n\nmstat |&gt; \n  group_by(status) |&gt; # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n\n# A tibble: 3 × 2\n  status      nmonkey\n  &lt;chr&gt;         &lt;int&gt;\n1 adolescent       16\n2 dominant         23\n3 subordinate      11\n\n\nIt’s often worth plotting as much as you can to get to a sense of what we’re working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.\n\n# which monkeys did what type of problems? \nmstat |&gt; count(status, monkeyID, difficulty) |&gt;\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen working with binary outcomes, it’s often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like “subordinate” monkeys solve more problems, and adolescents solve fewer (makes sense - they’re still learning!).\n\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n\n\n\n\n\n\n\n\n\n\nmodels of monkeys\nNow we come to fitting our model.\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It’s very likely that difficulty of a problem is going to influence that it is solved, so we’ll control for difficulty.\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan).\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he’s likely to really really struggle with the difficult ones!).\nFirst, we’ll relevel the difficulty variable so that the reference level is “easy”:\n\nmstat &lt;- mstat |&gt; mutate(\n  difficulty = fct_relevel(factor(difficulty), \"easy\")\n)\n\nand fit our model:\n\nmmod &lt;- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n   Data: mstat\n\n     AIC      BIC   logLik deviance df.resid \n   503.7    531.6   -244.8    489.7      390 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9358 -0.6325 -0.3975  0.6748  2.5161 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n monkeyID (Intercept)         1.552    1.246         \n          difficultydifficult 1.371    1.171    -0.44\nNumber of obs: 397, groups:  monkeyID, 50\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          -0.3945     0.3867  -1.020  0.30770   \ndifficultydifficult  -0.8586     0.3053  -2.812  0.00492 **\nstatusdominant        0.6682     0.4714   1.417  0.15637   \nstatussubordinate     1.4596     0.5692   2.564  0.01034 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) dffclt sttsdm\ndffcltydffc -0.333              \nstatusdmnnt -0.721 -0.031       \nstatssbrdnt -0.594 -0.033  0.497\n\n\n\n\ntest and visualisations of monkey status\nTo examine if monkey status has an effect, we can compare with the model without status:\n\n\nCode\nmmod0 &lt;- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n\n\nData: mstat\nModels:\nmmod0: solved ~ difficulty + (1 + difficulty | monkeyID)\nmmod: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmmod0    5 506.13 526.05 -248.07   496.13                       \nmmod     7 503.70 531.58 -244.85   489.70 6.4367  2    0.04002 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd we can see that the status of monkeys is associated with differences in the probability of successful problem solving (\\(\\chi^2(2)\\) = 6.44, p &lt; 0.05).\nAnd if we want to visualise the relevant effect, we can (as we did with glm()) plot on the predicted probability scale, which is much easier to interpret:\n\n\nCode\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n\n\n\n\n\n\n\n\n\n\n\ninterpretation\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:\n\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |&gt;\n  exp()\n\n                                  2.5 %     97.5 %\n(Intercept)         0.6740333 0.3158677  1.4383264\ndifficultydifficult 0.4237470 0.2329198  0.7709156\nstatusdominant      1.9506801 0.7743073  4.9142675\nstatussubordinate   4.3043097 1.4106363 13.1338476\n\n\n\n\n\n\n\n\n  \n    \n      term\n      est\n      OR\n      OR interpretation\n    \n  \n  \n    (Intercept)\n-0.39\n0.67\nestimated odds of an adolescent monkey solving an easy problem\n    difficultydifficult\n-0.86\n0.42\nodds of successful problem solving are more than halved (0.42 times the odds) when a given monkey moves from an easy to a difficult problem\n    statusdominant\n0.67\n1.95\nodds of success would be almost doubled (1.95 times the odds) if a monkey were to change from adolescent to dominant status (NB this is non-significant)\n    statussubordinate\n1.46\n4.30\nodds of success would quadruple (4.3 times the odds) if a monkey were to change from adolescent to subordinate status\n  \n  \n  \n\n\n\n\n\n\nSide note\nContrast this with what we would get from a linear multilevel model. If we were instead modelling a “problem score” with lmer(), rather than “solved yes/no” with glmer(), our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.\nNote that estimating differences between groups is not quite the same idea as estimating the effect “if a particular monkey changed from adolescent to subordinate”. In the linear world, these two things are the same, but our odds ratios give us only the latter."
  },
  {
    "objectID": "02b_loglong.html#example-1",
    "href": "02b_loglong.html#example-1",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: mindfuldecline.csv\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nThe data are available at: https://uoepsy.github.io/data/msmr_mindfuldecline.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    study_visit\nStudy Visit Number (1 - 10)\n    age\nAge (in years) at study visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\nexploring the data\n\nlibrary(tidyverse)\nlibrary(lme4)\nmmd &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nhead(mmd)\n\n# A tibble: 6 × 6\n  ppt   condition study_visit   age   ACE imp  \n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 PPT_1 control             1    60  84.5 unimp\n2 PPT_1 control             2    62  85.6 imp  \n3 PPT_1 control             3    64  84.5 imp  \n4 PPT_1 control             4    66  83.1 imp  \n5 PPT_1 control             5    68  82.3 imp  \n6 PPT_1 control             6    70  83.3 imp  \n\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n\nmmd |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n\n# A tibble: 2 × 2\n  condition   n_ppt\n  &lt;chr&gt;       &lt;int&gt;\n1 control        10\n2 mindfulness    10\n\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. From the plot below, we can see that on the whole participants’ cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:\n\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\n\n\nmodelling change over time\nInitially, we’ll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables study_visit and age represent exactly the same information (time), so we have a choice of which one to use.\n\n\n\n\n\n\nWhy the age variable (currently) causes problems\n\n\n\n\n\nAs it is, the age variable we have starts at 60 and goes up to 78 or so.\nIf we try and use this in a model, we get an error!\n\nmod1 &lt;- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n\nModel failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is “when all predictors are zero”. So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!\nThis means that trying to fit (1 + age | ppt) will try to estimate the variability in people’s change in cognition over time, and the variability in cognition at age zero. As we can see in Figure 2, because the intercept is so far away from the data, the angle of each persons’ slope has a huge influence over where their intercept is. The more upwards a persons’ slope is, the lower down their intercept is.\n\n\n\n\n\nFigure 2: lines indicate predicted values from the model with random intercepts and random slopes of age, where age. Due to how age is coded, the ‘intercept’ is estimated back at age 0\n\n\n\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):\n\nVarCorr(mod1)\n\n Groups   Name        Std.Dev. Corr  \n ppt      (Intercept) 7.51567        \n          age         0.12696  -0.999\n Residual             0.51536        \n\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the study_visit variable.\nEither will do, we just need to remember the units they are measured in!\n\n\n\nLet’s center age on 60:\n\nmmd$ageC &lt;- mmd$age-60\n\nAnd fit our model:\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, \\(t\\) statistics that are \\(&gt;|2\\text{-ish}|\\) are probably going to be significant when assessed properly).\n\nsummary(mod1)\n\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\nWe’re now ready to add in group differences in their trajectories of cognition:\n\nmod2 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is 85 (that’s the (Intercept)). For these participants, scores are estimated to decrease by -0.27 points every year (that’s the slope of ageC). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the condition [mindfulness] coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline 0.17 less than the control group every year.\n(note, there are always lots of ways to frame interactions. A “reduction in decline” feels most appropriate to me here)\nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.\n\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n\n# Fixed Effects\n\nParameter                      | Coefficient |   SE |         95% CI |      t |    df |      p\n----------------------------------------------------------------------------------------------\n(Intercept)                    |       85.20 | 0.15 | [84.89, 85.52] | 568.00 | 17.75 | &lt; .001\nageC                           |       -0.27 | 0.04 | [-0.36, -0.17] |  -5.93 | 17.95 | &lt; .001\ncondition [mindfulness]        |        0.05 | 0.21 | [-0.39,  0.49] |   0.23 | 17.49 | 0.821 \nageC × condition [mindfulness] |        0.17 | 0.06 | [ 0.04,  0.31] |   2.73 | 17.99 | 0.014 \n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: ppt)       |        0.35\nSD (ageC: ppt)            |        0.14\nCor (Intercept~ageC: ppt) |        0.26\nSD (Residual)             |        0.49\n\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.\nAnd that’s exactly what we see when we visualise those fixed effects:\n\n\nCode\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. To do so, we need to combine two data sources - the fixed effects estimation from effect(), and the data itself:\n\n\nCode\nploteff &lt;- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame()\n\nmmd |&gt;\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person’s cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons’ trajectory. When we add in the observed trajectories, we see the variability in people’s trajectories (one person even goes up over time!).\nOur model represents this variability in the random effects part. While the estimated average slope is -0.27 for the control group (and -0.27+0.17=-0.09 for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of 0.14 (remember we can extract this info using VarCorr(), or just look in the output of summary(model)).\n\nVarCorr(mod2)\n\n Groups   Name        Std.Dev. Corr \n ppt      (Intercept) 0.34615       \n          ageC        0.13866  0.260\n Residual             0.49450       \n\n\n\n\n\n\n\n\n\nFigure 3: Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\n\n\n\n\nIf you think about what this means - it means that some participants we would expect to actually increase in their slopes. If we have a normal distribution with a mean of -0.3 or -0.09 and a standard distribution of 0.14, then we would expect some values to to positive (see e.g., Figure 3)."
  },
  {
    "objectID": "02b_loglong.html#footnotes",
    "href": "02b_loglong.html#footnotes",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that binary outcomes are just a special case of the binomial↩︎\nassuming that it is people we are studying!↩︎"
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "Week 2 Exercises: Logistic and Longitudinal",
    "section": "",
    "text": "Great Apes!\n\nData: msmr_apespecies.csv & msmr_apeage.csv\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/msmr_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/msmr_apespecies.csv.\n\n\n\n\n\n\n\n\nTable 1:  Data Dictionary: msmr_apespecies.csv \n  \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    species\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2:  Data Dictionary: msmr_apeage.csv \n  \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    age\nAge at assessment (years)\n    dominance\nDominance (Z-scored)\n  \n  \n  \n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and check over it. Do any relevant cleaning/wrangling that might be necessary.\n\n\n\n\n\n1 - reading and joining\n\n\n\nWe’ll read in both datasets, and then join them together.\n\nlibrary(tidyverse)\nlibrary(lme4)\nape_species &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apespecies.csv\")\nape_age &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apeage.csv\")\n\nSometimes is handy to check that all our participants are in both datasets:\n\n# are all the apes in ape_age also in ape_species?\nall(ape_age$ape %in% ape_species$ape)\n\n[1] TRUE\n\n# and vice versa?\nall(ape_species$ape %in% ape_age$ape)\n\n[1] TRUE\n\n\nLet’s join them:\n\napedat &lt;- full_join(ape_age, ape_species)\nhead(apedat)\n\n# A tibble: 6 × 4\n  ape     age dominance species   \n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 Joel      7       0.6 chimpanzee\n2 Joel      5       1.2 chimpanzee\n3 Joel      8       1.1 chimpanzee\n4 Joel      1       0.2 chimpanzee\n5 Joel      2       0.5 chimpanzee\n6 Joel      6       1   chimpanzee\n\n\n\n\n\n\n\n2 - identifying issues\n\n\n\nFirst off, we can see that we’ve got some weird typos. Some apes have been identified as “gorrila” but it is actually spelled “gorilla”.\nAlso, we’ve got people using two alternatives for the chimps: “chimp” and “chimpanzee”. We’ll need to combine those.\n\ntable(apedat$species)\n\n\n    bonobo      chimp chimpanzee    gorilla    gorrila  orangutan \n       187        146        127        211          2        157 \n\n\nAge looks like it has some weird values (possibly “-99”?), and there are possibly a few outliers in the dominance variable. Given that dominance is standardised, it is extremely unlikely that we would see values around 20.. They’re not “impossible”, but they’re so incredibly unlikely that I’d be more comfortable assuming they are typos:\n\nhist(apedat$age, breaks=20)\nhist(apedat$dominance, breaks=20)\n\n\n\n\n\n\n\n\nJust to see what the most extreme values of dominance are:\n\n# show the biggest 5 absolute values in dominance variable\nsort(abs(apedat$dominance), decreasing = TRUE)[1:5]\n\n[1] 21.2 19.4  3.9  2.9  2.9\n\n\n\n\n\n\n\n3 - cleaning up\n\n\n\n\napedat &lt;- apedat |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\n\n\n\n\nQuestion 2\n\n\nHow is this data structure “hierarchical” (or “clustered”)? What are our level 1 units, and what are our level 2 units?\n\n\n\n\n\nSolution\n\n\n\nWe have a random sample of \\(\\underbrace{\\text{timepoints}}_{\\text{level 1}}\\) from a random sample of \\(\\underbrace{\\text{apes}}_{\\text{level 2}}\\).\n\n\n\n\nQuestion 3\n\n\nFor how many apes do we have data? How many of each species?\nHow many datapoints does each ape have?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’ve seen this last week too - counting the different levels in our data. See 2B #getting-to-know-my-monkeys for an example (with another monkey example!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have 168 apes in our dataset:\n\nlength(unique(apedat$ape))\n\n[1] 168\n\n\nHere’s how many of each species:\n\napedat |&gt; \n  group_by(species) |&gt;\n  summarise(\n   n_apes = n_distinct(ape) \n  )\n\n# A tibble: 4 × 2\n  species   n_apes\n  &lt;chr&gt;      &lt;int&gt;\n1 bonobo        36\n2 chimp         56\n3 gorilla       46\n4 orangutan     30\n\n\nLet’s create a table of how many observations for each ape, and then we can create a table from that table, to show how many apes have 2 datapoints, how many have 3, 4, and so on:\n\ntable(apedat$ape) |&gt;\n  table() |&gt;\n  barplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nMake a plot to show how dominance changes as apes get older.\n\n\n\n\n\n\nHints\n\n\n\n\n\nIn 2B #exploring-the-data we made a facet for each cluster (each participant). That was fine because we had only 20 people. In this dataset we have 168! That’s too many to facet. The group aesthetic will probably help instead!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s a line for each ape, and a facet for each species:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_line(aes(group = ape)) + \n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nIt’s kind of hard to see the trend for each ape, so let’s also make a separate little linear model for each ape:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_point(alpha=.1) +\n  stat_smooth(aes(group=ape),geom=\"line\",method=lm,se=F,alpha=.5) +\n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nRecenter the age variable on 1, which is the youngest ages that we’ve got data on for any of our species.\nThen fit a model that estimates the differences between primate species in how dominance changes over time.\n\n\n\n\n\nSolution\n\n\n\n\napedat$age = apedat$age-1 \n\nm.full &lt;- lmer(dominance ~ 1 + age * species + (1 + age | ape), data = apedat)\n\n\n\n\n\nQuestion 6\n\n\nDo primate species differ in the growth of dominance?\nPerform an appropriate test/comparison.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is asking about the age*species interaction, which in our model is represented by 3 parameters. To assess the overall question, it might make more sense to do a model comparison.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nm.int &lt;- lmer(dominance ~ 1 + age + species + (1 + age | ape), data = apedat)\n\nanova(m.int, m.full)\n\nData: apedat\nModels:\nm.int: dominance ~ 1 + age + species + (1 + age | ape)\nm.full: dominance ~ 1 + age * species + (1 + age | ape)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nm.int     9 806.67 849.11 -394.34   788.67                        \nm.full   12 801.16 857.74 -388.58   777.16 11.517  3   0.009237 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies differ in how dominance changes over adolescence (\\(\\chi^2(3) = 11.52, p = 0.009\\)).\n\n\n\n\n\nQuestion 7\n\n\nPlot the average model predicted values for each age.\nBefore you plot.. do you expect to see straight lines? (remember, not every ape is measured at age 2, or age 3, etc).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot (see the lecture).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAveraging fitted values would give us straight lines if every ape had data at all ages, but in our study we have some apes with only 2 data points, and each ape has different set of ages (e.g., one ape might be measured at age 3, 6, and 10, another ape might be at ages 2 and 16).\n\nlibrary(broom.mixed)\n\naugment(m.full) |&gt;\nggplot(aes(age,dominance, color=species)) +\n  # the point ranges are our observations\n  stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n  # the lines are our average predictions  \n  stat_summary(aes(y=.fitted, linetype=species), fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the model based fixed effects:\n\n\n\n\n\nSolution\n\n\n\n\neffects::effect(\"age*species\", m.full, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=age+1,y=fit,col=species))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=species),col=NA,alpha=.3) +  \n  scale_color_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  scale_fill_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  facet_wrap(~species) + \n  guides(col=\"none\",fill=\"none\") +\n  labs(x=\"Age (years)\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nInterpret each of the fixed effects from the model (you might also want to get some p-values or confidence intervals).\n\n\n\n\n\n\nHints\n\n\n\n\n\nEach of the estimates should correspond to part of our plot from the previous question.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLet’s get some confidence intervals:\n\nconfint(m.full, method=\"profile\",\n        parm = \"beta_\")\n\n                           2.5 %      97.5 %\n(Intercept)          -0.67066925 -0.17299177\nage                   0.02361398  0.08142209\nspecieschimp          0.13383485  0.77009884\nspeciesgorilla        0.28124162  0.94933844\nspeciesorangutan     -0.38909919  0.34257548\nage:specieschimp     -0.03973125  0.03392308\nage:speciesgorilla   -0.05012759  0.02799394\nage:speciesorangutan -0.10625760 -0.02167806\n\n\n\n\n\n\n\n\n  \n    \n      term\n      est\n      CI\n      interpretation\n    \n  \n  \n    (Intercept)\n-0.42\n[-0.67, -0.17]*\nestimated dominance of 1 year old bonobos (at left hand side of plot, bonobo line is lower than 0)\n    age\n0.05\n[0.02, 0.08]*\nestimated change in dominance score for every year older a bonobo gets (slope of bonobo line)\n    specieschimp\n0.45\n[0.13, 0.77]*\nestimated difference in dominance scores at age 1 between bonobos and chimps (at left hand side of plot, chimp line is higher than bonobo line)\n    speciesgorilla\n0.62\n[0.28, 0.95]*\nestimated difference in dominance scores at age 1 between bonobos and gorillas (at left hand side of plot, gorilla line is higher than bonobo line)\n    speciesorangutan\n-0.02\n[-0.39, 0.34]\nno significant difference in dominance scores at age 1 between bonobos and orangutans (at the left hand side of our plot, orangutan line is similar height to bonobo line)\n    age:specieschimp\n0.00\n[-0.04, 0.03]\nno significant difference between chimps and bonobos in the change in dominance for every year older (slope of chimp line is similar to slope of bonobo line)\n    age:speciesgorilla\n-0.01\n[-0.05, 0.03]\nno significant difference between gorillas and bonobos in the change in dominance for every year older (slope of gorilla line is similar to slope of bonobo line)\n    age:speciesorangutan\n-0.06\n[-0.11, -0.02]*\nestimated difference between orangutans and bonobos in the change in dominance for every year older (slope of orangutan line is less steep than slope of bonobo line)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTrolley problems\n\nData: msmr_trolley.csv\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n  \n    \n      positive frame\n      neutral frame\n      negative frame\n    \n  \n  \n    5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One people will be saved if you do not pull the lever, but 5 people will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n  \n  \n  \n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nThe data are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\nTable 3:  Data Dictionary: trolley.csv \n  \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    frame\nframing of the thought experiment (positive/neutral/negative\n    lives\nlives at stake in the thought experiment (5 or 15)\n    lever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 10\n\n\nRead in the data and check over how many people we have, and whether we have complete data for each participant.\n\n\n\n\n\n\nHints\n\n\n\n\n\nI would maybe try data |&gt; group_by(participant) |&gt; summarise(), and then use the n_distinct() function to count how many “things” each person sees (e.g., 2B #example).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ntrolley &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_trolley.csv\")\nhead(trolley)\n\n# A tibble: 6 × 4\n  PID   frame    lives   lever\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;\n1 PPT_1 positive 5lives      1\n2 PPT_1 positive 15lives     1\n3 PPT_1 positive 5lives      1\n4 PPT_1 positive 15lives     1\n5 PPT_1 neutral  5lives      1\n6 PPT_1 neutral  15lives     0\n\n\nHow many participants?\n\nlength(unique(trolley$PID))\n\n[1] 120\n\n\nHow many trials for each participant in each condition.\nWe can, for each participant, count how many trials they have in total, how many “frames” they see, how many “lives” they see, and how many “frame x lives” combinations they see:\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  )\n\n# A tibble: 120 × 5\n   PID     n_trials n_frame n_lives n_combn\n   &lt;chr&gt;      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 PPT_1         12       3       2       6\n 2 PPT_10        12       3       2       6\n 3 PPT_100       12       3       2       6\n 4 PPT_101       12       3       2       6\n 5 PPT_102       12       3       2       6\n 6 PPT_103       12       3       2       6\n 7 PPT_104       12       3       2       6\n 8 PPT_105       12       3       2       6\n 9 PPT_106       12       3       2       6\n10 PPT_107       12       3       2       6\n# ℹ 110 more rows\n\n\nIf everybody gets the same here (as we can see they do below), then everyone has complete data!\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  ) |&gt;\n  summary()\n\n     PID               n_trials     n_frame     n_lives     n_combn \n Length:120         Min.   :12   Min.   :3   Min.   :2   Min.   :6  \n Class :character   1st Qu.:12   1st Qu.:3   1st Qu.:2   1st Qu.:6  \n Mode  :character   Median :12   Median :3   Median :2   Median :6  \n                    Mean   :12   Mean   :3   Mean   :2   Mean   :6  \n                    3rd Qu.:12   3rd Qu.:3   3rd Qu.:2   3rd Qu.:6  \n                    Max.   :12   Max.   :3   Max.   :2   Max.   :6  \n\n\n\n\n\n\nQuestion 11\n\n\nConstruct an appropriate plot to summarise the data in a suitable way to illustrate the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSomething making use of stat_summary() to give proportions, a bit like the plot in 2B #getting-to-know-my-monkeys?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere is a plot of proportions of trials in which the lever was pulled, split by how the problem was framed, and the number of lives saved:\n\nggplot(trolley, aes(x=frame, y=lever, col=lives)) +\n  stat_summary(geom=\"pointrange\", size=1, \n               position=position_dodge(width=.2)) \n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nFit a model to assess the research aims.\nDon’t worry if it gives you an error, we’ll deal with that in a second.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nRemember, a good way to start is to split this up into 3 parts: 1) the outcome and fixed effects, 2) the grouping structure, and 3) the random slopes.\n\nfitting (or attempting to fit!) glmer models might take time!\n\n\n\n\n\n\n\n\n\n1 - fixed\n\n\n\nThe researchers are “interested in whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved)”.\n“the framing effect on moral judgements” here is operationalised as\n\nlever ~ frame\n\nand the wording “depends upon the stakes” means that we want to know if that effect of frame “is different for” the situations when lives = 5, vs lives = 15 - i.e. we need the interaction!\n\nlever ~ frame * lives\n\nThe outcome here is lever pulled (yes v no), so it’s a binary variable!\n\nglmer(lever ~ frame * lives + ....\n      ....,  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n2 - grouping\n\n\n\nWe know that we have multiple observations for each participant, and those participants are just a random sample (it’s not something we’re interested in testing, we would like to model participant differences as random variation).\n\nglmer(lever ~ frame * lives + ....\n      (1 + .... | PID),  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n3 - random\n\n\n\nFinally, what effects could theoretically vary between our participants?\nEvery participant saw everything (i.e. both frame and lives are “within participant” variables).\nIn theory, all of these are possible given our design:\n\nthe effect of frame on probability of pulling the lever could vary between participants\nthe effect of number of lives on probability of pulling the lever could vary between participants\nthe amount by which number of lives influences the effect of frame on pulling the lever could vary between participants\n\nSo we could theoretically try and fit this model:\n\nmod1 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame * lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning messages: 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, : failure to converge in 10000 evaluations 2: In optwrap(optimizer, devfun, start, rho$lower, control = control, : convergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations 3: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.0795145 (tol = 0.002, component 1)\n\n\n\n\n\nQuestion 13\n\n\nThis is probably the first time we’ve had to deal with a model not converging.\nWhile sometimes changing the optimizer can help, more often than not, the model we are trying to fit is just too complex. Often, the groups in our sample just don’t vary enough for us to estimate a random slope.\nThe aim here is to simplify our random effect structure in order to obtain a converging model, but be careful not to over simplify.\nTry it now. What model do you end up with? (You might not end up with the same model as each other, which is fine. These methods don’t have “cookbook recipes”!)\n\n\n\n\n\n\nHints\n\n\n\n\n\nyou could think of the interaction as the ‘most complex’ part of our random effects, so you might want to remove that first.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThis model still does not converge:\n\nmod2 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame + lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning message: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.00734804 (tol = 0.002, component 1)\n\nWe have a choice here - do we remove frame|PID or lives|PID? One practical point is that each participant has only 4 observations for each frame type, but they have 6 observations for each lives type, which might make it easier to fit.\n\nmod3 &lt;- glmer(lever ~ frame * lives + \n      (1 + lives | PID),  \n      data = trolley, family = binomial)\n\nHooray! it converges!\n\n\n\n\nQuestion 14\n\n\nPlot the predicted probabilities from your model for each combination of frame and lives.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(effects)\neffect(\"frame*lives\", mod3) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x = frame, y = fit, col = lives)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  position=position_dodge(width=.2),\n                  size=1)+\n  labs(y=\"probability of pulling the lever\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional extra: Novel Word Learning\n\nData: nwl.Rdata\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test.  Data were also collect from healthy controls.  Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    group\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n    lesion_location\nLocation of brain lesion: anterior vs posterior\n    block\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n    PropCorrect\nProportion of 30 responses in a given block that the participant got correct\n    NumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n    NumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n    ID\nParticipant Identifier\n    Phase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n  \n  \n  \n\n\n\n\n\n\nQuestion 16\n\n\nLoad the data. Take a look around. Any missing values? Can you think of why?\n\n\n\n\n\nSolution\n\n\n\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\nsummary(nwl)\n\n     group      lesion_location     block    PropCorrect       NumCorrect   \n control:126   anterior : 45    Min.   :1   Min.   :0.2000   Min.   : 6.00  \n patient:117   posterior: 63    1st Qu.:3   1st Qu.:0.5333   1st Qu.:16.00  \n               NA's     :135    Median :5   Median :0.7000   Median :21.00  \n                                Mean   :5   Mean   :0.6822   Mean   :20.47  \n                                3rd Qu.:7   3rd Qu.:0.8333   3rd Qu.:25.00  \n                                Max.   :9   Max.   :1.0000   Max.   :30.00  \n                                                                            \n    NumError              ID         Phase          \n Min.   : 0.000   control1 :  9   Length:243        \n 1st Qu.: 5.000   control10:  9   Class :character  \n Median : 9.000   control11:  9   Mode  :character  \n Mean   : 9.535   control12:  9                     \n 3rd Qu.:14.000   control13:  9                     \n Max.   :24.000   control14:  9                     \n                  (Other)  :189                     \n\n\nThe only missing vales are in the lesion location, and it’s probably because the healthy controls don’t have any lesions. There may also be a few patients for which the lesion_location is missing, but this should be comparatively fewer values compared to controls.\nThe following command creates a two-way frequency table showing the number of controls or patients by lesion location, confirming that controls only have missing values (NAs) and only 9 patients have missing values:\n\ntable(nwl$group, nwl$lesion_location, useNA = \"ifany\")\n\n         \n          anterior posterior &lt;NA&gt;\n  control        0         0  126\n  patient       45        63    9\n\n\n\n\n\n\nQuestion 17\n\n\nOur broader research aim today is to compare the two lesion location groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study.\n\nWhat is the outcome variable?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink carefully: there might be several variables which either fully or partly express the information we are considering the “outcome” here. We saw this back in USMR with the glm()!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe outcome here is (in words) the proportion of correct answers or, equivalently, the probability of answering correctly. A proportion/probability can only vary between 0 and 1 and, as such, we cannot use traditional linear regression or we could end up with predictions outside of the [0, 1] range.\nAs said, the outcome is the proportion of correct answers in each block. This makes it tempting to look at the variable called PropCorrect, but this is encoded as a proportion. We have learned to use logistic models, but these require either:\n\na binary outcome variable, where the values are 0s or 1s\na binomial outcome variable, where the values are aggregated counts of 1s and 0s\n\nBinary data. In the case below you would use the specification correct ~ ...:\n\n\n\n\n\n\n  \n    \n      participant\n      question\n      correct\n    \n  \n  \n    1\n1\n1\n    1\n2\n0\n    1\n3\n1\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nBinomial data. You would use the specification cbind(num_successes, num_failures) which, in the case below, would be:\ncbind(questions_correct, questions_incorrect) ~ ...\n\n\n\n\n\n\n  \n    \n      participant\n      questions_correct\n      questions_incorrect\n    \n  \n  \n    1\n2\n1\n    2\n1\n2\n    3\n3\n0\n    ...\n...\n...\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 18\n\n\n\nResearch Question 1:\nIs the learning rate (training blocks) different between the two lesion location groups?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\nEnsure you are running models on only the data we are actually interested in.\n\nAre the healthy controls included in the research question under investigation?\nAre the testing blocks included in the research question, or only the learning blocks?\n\nWe could use model comparison via likelihood ratio tests (using anova(model1, model2, model3, ...). For this question, we could compare:\n\nA model with just the change over the sequence of blocks\nA model with the change over the sequence of blocks and an overall difference between groups\nA model with groups differing with respect to their change over the sequence of blocks\n\nWhat about the random effects part?\n\nWhat are our observations grouped by?\nWhat variables can vary within these groups?\nWhat do you want your model to allow to vary within these groups?\n\n\n\n\n\n\n\n\n\n\n1 - answers to the hints\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\n\nYes, we don’t a binary variable with correct/incorrect questions but the binomial variables NumCorrect and NumError representing, respectively, the aggregated count (out of 30) of correct and incorrect questions. As such, we will need the following: cbind(NumCorrect, NumError)\n\nEnsure you are running models on only the data we are actually interested in.\n\nThe healthy controls are not included in the research question under investigation, so we will exclude them.\nWe are only interested in the learning blocks, and we will exclude the testing blocks (block &gt; 7)\nYou might want to store this data in a separate object, but in the code for the solution we will just use filter() inside the glmer().\n\nA model with just the change over the sequence of blocks:\n\noutcome ~ block\n\nA model with the change over the sequence of blocks and an overall difference between groups:\n\noutcome ~ block + lesion_location\n\nA model with groups differing with respect to their change *over the sequence of blocks:\n\noutcome ~ block * lesion_location\n\nWhat are our observations grouped by?\n\nrepeated measures by-participant. i.e., the ID variable\n\nWhat variables can vary within these groups?\n\nBlock and Phase. Be careful though - you can create the Phase variable out of the Block variable, so really this is just one piece of information, encoded differently in two variables.\nThe other variables (lesion_location and group) do not vary for each ID. Lesions don’t suddenly change where they are located, nor do participants swap between being a patient vs a control (we don’t need the group variable anyway as we are excluding the controls).\nWhat do you want your model to allow to vary within these groups?\nDo you think the change over the course of the blocks is the same for everybody? Or do you think it varies? Is this variation important to think about in terms of your research question?\n\n\n\n\n\n\n\n2 - modelling\n\n\n\n\nm.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), \n                data = filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\n\nanova(m.base, m.loc0, m.loc1, test=\"Chisq\")\n\nData: filter(nwl, block &lt; 8, !is.na(lesion_location))\nModels:\nm.base: cbind(NumCorrect, NumError) ~ block + (block | ID)\nm.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID)\nm.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nm.base    5 454.12 466.27 -222.06   444.12                     \nm.loc0    6 454.66 469.25 -221.33   442.66 1.4572  1     0.2274\nm.loc1    7 454.47 471.48 -220.23   440.47 2.1974  1     0.1382\n\n\n\nNo significant difference in learning rate between groups (\\(\\chi^2(1)=2.2, p = 0.138\\)).\n\n\n\n\n\nQuestion 19\n\n\n\nResearch Question 2\nIn the testing phase, does performance on the immediate test differ between lesion location groups, and does the retention from immediate to follow-up test differ between the two lesion location groups?\n\nLet’s try a different approach to this. Instead of fitting various models and comparing them via likelihood ratio tests, just fit the one model which could answer both parts of the question above.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThis might required a bit more data-wrangling beforehand. Think about the order of your factor levels (alphabetically speaking, “Follow-up” comes before “Immediate”)!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nnwl_test &lt;- filter(nwl, block &gt; 7, !is.na(lesion_location)) %&gt;%\n    mutate(\n        Phase = factor(Phase), \n        Phase = fct_relevel(Phase, \"Immediate\")\n    )\n\nm.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 | ID), \n                      nwl_test, family=\"binomial\")\n\nsummary(m.recall.loc)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 |      ID)\n   Data: nwl_test\n\n     AIC      BIC   logLik deviance df.resid \n   139.3    145.2    -64.6    129.3       19 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1556 -0.3352  0.0039  0.4963  1.3506 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.3626   0.6021  \nNumber of obs: 24, groups:  ID, 12\n\nFixed effects:\n                                        Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                              -0.1124     0.3167  -0.355   0.7226  \nPhaseFollow-up                           -0.0278     0.2357  -0.118   0.9061  \nlesion_locationposterior                  0.9672     0.4211   2.297   0.0216 *\nPhaseFollow-up:lesion_locationposterior  -0.2035     0.3191  -0.638   0.5236  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PhsFl- lsn_lc\nPhaseFllw-p -0.372              \nlsn_lctnpst -0.752  0.280       \nPhsFllw-p:_  0.275 -0.739 -0.385\n\n\nNote 1:\nIn the above, we have made sure to select the patients by specifying !is.na(lesion_location), meaning that we want those rows where the lesion location is not missing. As a reminder ! is the negation function (not). As we saw in the earlier question, this excludes the 126 healthy controls, as well as the 9 patients for which we have missing values (NAs).\nNote 2:\nWe didn’t specify (Phase | ID) as the random effect because each participant only has 2 data points for Phase, and there is only one line that fits two data points. In other words, there is only one possible way to fit those two data points. As such, as each group of 2 points will have a perfect line fit, and the residuals \\(\\varepsilon_{ij}\\) will all be 0. As a consequence of this, the residuals will have no variability as they are all 0, so \\(\\sigma_{\\epsilon}\\) is 0 which in turn leads to problem with estimating the model coefficients.\n\nsubset(nwl_test, ID == 'patient15')\n\n     group lesion_location block PropCorrect NumCorrect NumError        ID\n1  patient        anterior     8   0.5333333         16       14 patient15\n13 patient        anterior     9   0.5333333         16       14 patient15\n       Phase\n1  Immediate\n13 Follow-up\n\n\nIf you try using (Phase | ID) as random effect, you will see the following message:\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nQuestion 20\n\n\n\nIn family = binomial(link='logit'). What function is used to relate the linear predictors in the model to the expected value of the response variable?\n\nHow do we convert this into something more interpretable?\n\n\n\n\n\n\nSolution\n\n\n\n\nThe link function is the logit, or log-odds (other link functions are available).\nTo convert log-odds to odds, we can use exp(), to get odds and odds ratios.\n\n\n\n\n\nQuestion 21\n\n\nMake sure you pay attention to trying to interpret each fixed effect from your models.\nThese can be difficult, especially when it’s logistic, and especially when there are interactions.\n\nWhat is the increase in the odds of answering correctly in the immediate test if you were to have a posterior legion instead of an anterior legion?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nexp(fixef(m.recall.loc))\n\n                            (Intercept)                          PhaseFollow-up \n                              0.8936639                               0.9725791 \n               lesion_locationposterior PhaseFollow-up:lesion_locationposterior \n                              2.6306343                               0.8158868 \n\n\n\n(Intercept) ==&gt; Anterior lesion group performance in immediate test. This is the odds of them answering correctly in the immediate test.\nPhaseFollow-up ==&gt; Change in performance (for someone with an anterior lesion) from immediate to follow-up test.\nlesion_locationposterior ==&gt; Change in performance in immediate test were a patient to have a posterior lesion instead of an anterior lesion.\nPhaseFollow-up:lesion_locationposterior ==&gt; How change in performance from immediate to follow-up test would differ were a patient to have a posterior lesion instead of an anterior lesion.\n\n\nexp(fixef(m.recall.loc))[3]\n\nlesion_locationposterior \n                2.630634 \n\n\n\nHaving a posterior lesions is associated with 2.63 times the odds of answering correctly in the immediate test compared to having an anterior lesion.\n\n\n\n\n\nQuestion 22\n\n\nRecreate the visualisation in Figure 2.\n\n\n\n\n\nFigure 2: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, \n                                                 color=lesion_location, \n                                                 shape=lesion_location)) +\n    #geom_line(aes(group=ID),alpha=.2) + \n    stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n    stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), \n                 fun=mean, geom=\"line\") + \n    geom_hline(yintercept=0.5, linetype=\"dashed\") + \n    geom_vline(xintercept=c(7.5, 8.5), linetype=\"dashed\") + \n    scale_x_continuous(breaks=1:9, \n                       labels=c(1:7, \"Test\", \"Follow-Up\")) + \n    theme_bw(base_size=10) + \n    labs(x=\"Block\", y=\"Proportion Correct\", \n         shape=\"Lesion\\nLocation\", color=\"Lesion\\nLocation\")"
  },
  {
    "objectID": "03a_poly.html",
    "href": "03a_poly.html",
    "title": "3A: Polynomial Growth",
    "section": "",
    "text": "This reading:\n\nThe basics of modelling non-linear change via polynomial terms.\nAn example with MLM\n\nFor additional reading, Winter & Wieling, 2016 is pretty good (mainly focus on sections 1-3)\nWe have already seen in the last couple of weeks that we can use MLM to study something ‘over the course of X’. This might be “over the course of adolescence” (i.e. y ~ age), or “over the course of an experiment” (y ~ trial_number). The term “longitudinal” is commonly used to refer to any data in which repeated measurements are taken over a continuous domain. This opened up the potential for observations to be unevenly spaced, or missing at certain points.\nIt also, as will be the focus of this week, opens the door to thinking about how many effects of interest may display patterns that are non-linear. There are lots of techniques to try and summarise non-linear trajectories, and here we are going to focus on the method of including higher-order polynomials as predcitors."
  },
  {
    "objectID": "03a_poly.html#raw-polynomials",
    "href": "03a_poly.html#raw-polynomials",
    "title": "3A: Polynomial Growth",
    "section": "Raw Polynomials",
    "text": "Raw Polynomials\nThere are two types of polynomial we can construct. “Raw” (or “Natural”) polynomials are the straightforward ones that you would expect (example in the table below), where the original value of \\(x\\) is squared/cubed.\n\n\n\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n1\n1\n1\n\n\n2\n4\n8\n\n\n3\n9\n27\n\n\n4\n16\n64\n\n\n5\n25\n125\n\n\n…\n…\n…\n\n\n\nWe can quickly get these in R using the poly() function. As we want to create “raw” polynomials, we need to make sure to specify raw = TRUE or we get something else (we’ll talk about what they are in a second!).\n\npoly(1:10, degree = 3, raw=TRUE)\n\n       1   2    3\n [1,]  1   1    1\n [2,]  2   4    8\n [3,]  3   9   27\n [4,]  4  16   64\n [5,]  5  25  125\n [6,]  6  36  216\n [7,]  7  49  343\n [8,]  8  64  512\n [9,]  9  81  729\n[10,] 10 100 1000\nattr(,\"degree\")\n[1] 1 2 3\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nLet’s now use these with our example data we had been plotting above.\nFirst lets add new variables to the dataset, which are the polynomials of our \\(x\\) variable:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      # poly1 is the first column\n      poly1 = poly(age, degree = 3, raw = TRUE)[,1],\n      # poly2 is the second\n      poly2 = poly(age, degree = 3, raw = TRUE)[,2],\n      # poly3 is the third\n      poly3 = poly(age, degree = 3, raw = TRUE)[,3]\n    )\n\nAnd now lets use them in our model as predictors:\n\ncubicmod &lt;- lm(syndens ~ poly1 + poly2 + poly3, data = syndat)\n\n\n\n\n\n\n\nother ways to get polynomials into the model\n\n\n\n\n\nAs we’re working with raw polynomials, we could just do:\n\nsyndat |&gt; \n  mutate(\n    poly1 = age,\n    poly2 = age^2,\n    poly3 = age^3\n  )\n\nOr we could even just specify the calculations for each term inside the call to lm():\n\nlm(syndens ~ age + I(age^2) + I(age^3), data = syndat)\n\nOr even use the poly() function:\n\nlm(syndens ~ poly(age, degree=3, raw=TRUE), data = syndat)\n\n\n\n\n\n\n\n\n\n\nA handy function from Dan\n\n\n\n\n\nDan has a nice function that may be handy. It adds the polynomials to your dataset for you:\n\n# import Dan's code and make it available in our own R session\n# you must do this in every script you want to use this function\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\nsyndat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_synapticdens.csv\")\nsyndat &lt;- code_poly(df = syndat, predictor = 'age', poly.order = 3, \n                    orthogonal = FALSE, draw.poly = FALSE)\nhead(syndat)\n\n# A tibble: 6 × 6\n    age syndens age.Index poly1 poly2 poly3\n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.1   0.615         1   0.1  0.01 0.001\n2   0.2   0.908         2   0.2  0.04 0.008\n3   0.6   0.1           3   0.6  0.36 0.216\n4   0.7   1.81          4   0.7  0.49 0.343\n5   0.9   1.44          5   0.9  0.81 0.729\n6   1     0.615         6   1    1    1    \n\n\n\n\n\nJust to see it in action, let’s take a look at the predicted values from our model.\nTake for instance, the 9th row below. The predicted value of y (shown in the .fitted column) is:\n\\(\\hat y_9 = b_0 + b_1 \\cdot x_9 + b_2 \\cdot x^2_9 + b_3 \\cdot x^3_9\\)\n\\(\\hat y_9 = b_0 + b_1 \\cdot 2 + b_2 \\cdot 4 + b_3 \\cdot 8\\)\n\\(\\hat y_9 = -1.843 + 3.375 \\cdot 2 + -0.332 \\cdot 4 + 0.0097 \\cdot 8\\)\n\\(\\hat y_9 = 3.66\\).\n\nlibrary(broom)\naugment(cubicmod) \n\n# A tibble: 74 × 10\n   syndens poly1 poly2 poly3 .fitted  .resid   .hat .sigma   .cooksd .std.resid\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1   0.615   0.1  0.01 0.001 -1.51    2.12   0.170    1.62 0.104         1.42  \n 2   0.908   0.2  0.04 0.008 -1.18    2.09   0.155    1.63 0.0886        1.39  \n 3   0.1     0.6  0.36 0.216  0.0651  0.0349 0.109    1.65 0.0000155     0.0226\n 4   1.81    0.7  0.49 0.343  0.361   1.45   0.0992   1.64 0.0240        0.933 \n 5   1.44    0.9  0.81 0.729  0.933   0.510  0.0829   1.65 0.00239       0.326 \n 6   0.615   1    1    1      1.21   -0.596  0.0759   1.65 0.00294      -0.379 \n 7   0.615   1.6  2.56 4.10   2.75   -2.13   0.0465   1.63 0.0217       -1.33  \n 8   0.310   1.7  2.89 4.91   2.98   -2.67   0.0433   1.62 0.0316       -1.67  \n 9   1.21    2    4    8      3.66   -2.45   0.0361   1.62 0.0217       -1.52  \n10   2.19    2.1  4.41 9.26   3.87   -1.68   0.0343   1.64 0.00968      -1.04  \n# ℹ 64 more rows\n\n\nIf we plot the predictions with poly1 on the x-axis (poly1 is just the same as our age variable with a different name!), we can see that we are able to model a non-linear relationship between y and x (between synaptic density and age), via a combination of linear parameters!\n\nlibrary(broom)\naugment(cubicmod, interval=\"confidence\") |&gt;\n  ggplot(aes(x=poly1))+\n  geom_point(aes(y=syndens),size=2,alpha=.3) + \n  geom_line(aes(y=.fitted),col=\"darkorange\") +\n  geom_ribbon(aes(ymin=.lower,ymax=.upper),fill=\"darkorange\", alpha=.2)+\n  labs(x=\"age\") # our x-axis, \"poly1\", is just age!  \n\n\n\n\nFigure 6: a cubic model\n\n\n\n\nNow lets look at our coefficients:\n\nsummary(cubicmod)\n\n...\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.842656   0.704193  -2.617   0.0109 *  \npoly1        3.375159   0.345570   9.767 1.06e-14 ***\npoly2       -0.331747   0.044664  -7.428 2.06e-10 ***\npoly3        0.009685   0.001614   6.001 7.79e-08 ***\n---\nWith polynomials the interpretation is a little tricky because we have 3 coefficients that together explain the curvy line we see in Figure 6, and these coefficients are all dependent upon one another.\n\n(Intercept) = When all predictors are zero, i.e. the synaptic density at age 0.\n\npoly1 coefficient = The instantaneous change in \\(y\\) when \\(x=0\\).\npoly2 coefficient = Represents “rate of change of the rate of change” at \\(x=0\\). In other words, the curvature at \\(x=0\\).\n\npoly3 coefficient = Represents how the curvature is changing. It gets more abstract as the order of polynomials increase, so the easiest way to think about it is “the wiggliness”\n\nI’ve tried to represent what each term adds in Figure 7. The intercept is the purple point where age is zero. The poly1 coefficient is represented by the dashed blue line - the tangent of the curve at age zero. The poly2 coef, rperesented by the dashed green line, is how the angle of the blue line is changing at age zero. Finally, the poly3 coefficient tells us how much this curvature is changing (which gets us to our dashed orange line).\nNote that these interpretations are all dependent upon the others - e.g. the interpretation of poly2 refers to how the angle of poly1 is changing.\n\n\n\n\n\nFigure 7: the instantaneous rate of change at x=0 (blue), the rate of change in the rate of change (i.e. curvature, green), and ‘rate of change in rate of change in rate of change’ (i.e. wiggliness, orange)"
  },
  {
    "objectID": "03a_poly.html#orthogonal-polynomials",
    "href": "03a_poly.html#orthogonal-polynomials",
    "title": "3A: Polynomial Growth",
    "section": "Orthogonal Polynomials",
    "text": "Orthogonal Polynomials\nThe poly() function also enables us to compute “orthogonal polynomials”. This is the same information as the raw polynomials, re-expressed into a set of uncorrelated variables.\nRaw polynomials are correlated, which is what results makes their interpretation depend upon one another. For example, if we take the numbers 1,2,3,4,5, then these numbers are by definition correlated with their squares 1,4,9,16,25. As we increase from 1 to 5, we necessarily increase from 1 to 25.\nHowever, if we first center the set of numbers, so that 1,2,3,4,5 becomes -2,1,0,1,2, then their squares are 4,1,0,1,4 - they’re not correlated!\nOrthogonal polynomials essentially do this centering and scaling for \\(k\\) degrees of polynomial terms.\n\n\nSo while raw polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=T), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal polynomials look like this:\n\nmatplot(poly(1:10, 3, raw=F), type=\"l\", lwd=2)\n\n\n\n\n\n\n\n\n\n\nThis orthogonality allows us to essentially capture express the linear trend, curvature, and ‘wiggliness’ of the trajectory independently from one another, rather than relative to one another.\nUltimately, models using raw polynomials and using orthogonal polynomials are identical, but the coefficients we get out represent different things.\nLet’s overwrite our poly variables with orthogonal polynomials, by setting raw = FALSE:\n\nsyndat &lt;- \n  syndat |&gt; \n    mutate(\n      poly1 = poly(age,degree = 3, raw=FALSE)[,1],\n      poly2 = poly(age,degree = 3, raw=FALSE)[,2],\n      poly3 = poly(age,degree = 3, raw=FALSE)[,3],\n    )\n\nAnd fit our model:\n\nOcubicmod &lt;- lm(syndens ~poly1+poly2+poly3,syndat)\nsummary(Ocubicmod)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.5917     0.1902  34.649  &lt; 2e-16 ***\npoly1        12.9161     1.6365   7.892 2.88e-11 ***\npoly2       -14.3156     1.6365  -8.748 7.68e-13 ***\npoly3         9.8212     1.6365   6.001 7.79e-08 ***\n---\nThe interpretation of the estimates themselves are not really very tangible anymore, because the scaling of the orthogonal polynomials has lost a clear link back to “age”.\nAs the polynomial terms are centered on the mean of age, the intercept is the estimated synaptic density at the mean age (the purple dot in Figure 8). The poly1, poly2 and poly3 coefficient represent the independent overall linear trend, centered curvature, and “wiggliness” of the relationship between synaptic density and age (as shown in the blue, green and orange lines in Figure 8 respectively).\n\n\n\n\n\nFigure 8: the independent rate of change (blue), curvature (green) and wiggliness (orange) of the y~x relationship"
  },
  {
    "objectID": "03a_poly.html#raw-vs-orthognal",
    "href": "03a_poly.html#raw-vs-orthognal",
    "title": "3A: Polynomial Growth",
    "section": "Raw vs Orthognal",
    "text": "Raw vs Orthognal\nThe two models we have seen, one with raw polynomials, and one with orthogonal polynomials, are identical.\nFor proof, compare the two:\n\nanova(\n  lm(syndens ~ poly(age, 3, raw = TRUE), data = syndat),\n  lm(syndens ~ poly(age, 3, raw = FALSE), data = syndat)\n)\n\nAnalysis of Variance Table\n\nModel 1: syndens ~ poly(age, 3, raw = TRUE)\nModel 2: syndens ~ poly(age, 3, raw = FALSE)\n  Res.Df    RSS Df   Sum of Sq F Pr(&gt;F)\n1     70 187.47                        \n2     70 187.47  0 -1.1369e-13         \n\n\nSo why would we choose one vs the other?\nThe main reason is if we are interested in evaluating things relative to baseline, in which case raw polynomials allow us to do just that. If we are instead interested in evaluating the trends across the timecourse, then we would want orthogonal polynomials.\nConsider two examples:\n\n\nExample 1\nA student advisor who meets with students as they start university wants to know about how happiness evolves over the course of students’ year at univeristy, and wonders if this is different between introverted and extraverted individuals.\nIn this case, they would want raw polynomials, so that they can assess whether the two personality types differ when they first come to University, and how this is likely to evolve from that point.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\nA company has four stores across the UK, and they want to know if the stores have differed in how variable their earnings have been across the year.\nIn this case, looking at change relative to month 1 isn’t very useful. It would, for instance, tell us that the linear trend for store2’s earnings is upwards, whereas the linear trend for store 1 is flat. This makes store2 look better.\nIf we used orthogonal polynomials instead, we would see that the linear trend for store 2 is actually negative compared to store1.\n\n\n\n\n\n\n\n\n\n\n\n\nRaw? Orthogonal?\nFor non-linear relationships, a good plot is usually the most important thing!"
  },
  {
    "objectID": "03ex.html",
    "href": "03ex.html",
    "title": "Week 3 Exercises: Non-Linear Change",
    "section": "",
    "text": "Cognitive Task Performance\n\nDataset: Az.rda\nThese data are available at https://uoepsy.github.io/data/Az.rda. You can load the dataset using:\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n\nand you will find the Az object in your environment.\nThe Az object contains information on 30 Participants with probable Alzheimer’s Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100.\nWe’re interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    Subject\nUnique Subject Identifier\n    Time\nTime point of the study (1 to 10)\n    Task\nTask type (Memory, cADL, sADL)\n    Performance\nScore on test (range 0 to 100)\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nLoad in the data and examine it.\nHow many participants, how many observations per participant, per task?\n\n\n\n\n\nSolution\n\n\n\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\nsummary(Az)\n\n    Subject         Time          Task      Performance   \n 1      : 30   Min.   : 1.0   cADL  :300   Min.   : 2.00  \n 2      : 30   1st Qu.: 3.0   sADL  :300   1st Qu.:40.00  \n 3      : 30   Median : 5.5   Memory:300   Median :52.00  \n 4      : 30   Mean   : 5.5                Mean   :49.27  \n 5      : 30   3rd Qu.: 8.0                3rd Qu.:61.00  \n 6      : 30   Max.   :10.0                Max.   :85.00  \n (Other):720                                              \n\n\n30 participants:\n\nlength(unique(Az$Subject))\n\n[1] 30\n\n\nDoes every participant have 10 datapoints for each Task type? Yes!\n\nany( table(Az$Subject, Az$Task) != 10 )\n\n[1] FALSE\n\n\n\n\n\n\nQuestion 2\n\n\nNo modelling just yet.\nPlot the performance over time for each type of task.\nTry using stat_summary so that you are plotting the means (and standard errors) of each task, rather than every single data point. Why? Because this way you can get a shape of the average trajectories of performance over time in each task.\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor an example plot, see 3A #example-in-mlm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nYou can use “pointranges”, or “line” and “ribbon”.\nstat_summary will take the data and for each value of x calculate some function (in this case the mean, or the mean and SE):\n\nggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + \n  stat_summary(fun.data=mean_se, geom=\"ribbon\", color=NA, alpha=0.5) +\n  stat_summary(fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhy do you think raw/natural polynomials might be more useful than orthogonal polynomials for these data?\n\n\n\n\n\n\nHints\n\n\n\n\n\nAre we somewhat interested in group differences (i.e. differences in scores, or differences in rate of change) at a specific point in time?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nBecause we’re interested in whether there are task differences at the starting point, raw polynomials are probably what we want here.\n\n\n\n\nQuestion 4\n\n\nRe-center the Time variable so that the intercept is the first timepoint.\nThen choose an appropriate degree of polynomial (if any), and fit a full model that allows us to address the research aims.\n\n\n\n\n\n\nHints\n\n\n\n\n\nNote there is no part of the research question that specifically asks about how “gradual” or “quick” the change is (which would suggest we are interested in the quadratic term).\nHowever, the plot can help to give us a sense of what degree of polynomial terms might be suitable to succinctly describe the trends.\nIn many cases, fitting higher and higher order polynomials will likely result in a ‘better fit’ to our sample data, but these will be worse and worse at generalising to new data - i.e. we run the risk of overfitting.\n\n\n\n\n\n\n\n\n1 - how many polynomials?\n\n\n\nIn our plot, there were 2 straight line and one slightly curvy one. It wasn’t S-shaped or ‘wiggly’ or anything, there was just a bit of a bend in it, which suggests that the quadratic term could be a good approximation here.\n\nlibrary(lme4)\nAz &lt;- Az |&gt; mutate(\n  Time1 = Time-1,\n  poly1 = poly(Time1,2,raw=T)[,1],\n  poly2 = poly(Time1,2,raw=T)[,2]\n)\n\n\n\n\n\n\n\nOr using Dan’s code:\n\n\n\n\n\n\n# import Dan's code:\nsource(\"https://uoepsy.github.io/msmr/functions/code_poly.R\")\n\n# this also produces a nice little plot to show the polynomials\nAz$Time1 &lt;- Az$Time-1\nAz &lt;- code_poly(df = Az, predictor = 'Time1',\n                poly.order = 2, orthogonal = FALSE)\n\n\n\n\n\n\n\n\n\n2 - fixed effects\n\n\n\nWe’re interested in how performance changes over time, but we have poly1 and poly2 for time, so we’re at:\nlmer(Performance ~ poly1 + poly2 ... \nOur research aims are to investigate differences between task performance (both at baseline and change over time). So we want to interact time with task:\nlmer(Performance ~ (poly1 + poly2)*Task ... \n\n\n\n\n\n3 - grouping structure\n\n\n\n\nhead(Az)\n\n  Subject Time Task Performance Time1 poly1 poly2\n1       1    1 cADL          65     0     0     0\n2       1    2 cADL          61     1     1     1\n3       1    3 cADL          53     2     2     4\n4       1    4 cADL          46     3     3     9\n5       1    5 cADL          42     4     4    16\n6       1    6 cADL          35     5     5    25\n\n\nWe have 900 observations, and 30 for each participant.\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + .... | Subject)\n\n\n\n\n\n\nFixed vs Random\n\n\n\n\n\nWe can account for group differences in models either by estimating group differences, or by estimating variance between groups:\n\ngroup as a fixed effect (y ~ 1 + group) = groups differ in \\(y\\) by \\(b_1, b_2, ..., b_k\\)\n\ngroup as a random effect (y ~ 1 + (1|group)) = groups vary in \\(y\\) with a standard deviation of \\(\\sigma_0\\)\n\nOne way to think about whether a group is best in the random effects part or in the fixed part of our model is to think about “what would happen if I repeated the experiment?”\n\n\n\n\n\n\n\n\nCriterion:\nRepetition:  If the experiment were repeated:\nDesired inference:  The conclusions refer to:\n\n\n\n\nFixed effects\n\n\n\n\nRandom effects\n\n\n\n\n\nPractical points:\n- Sometimes there isn’t enough variability between groups to model as random effects (i.e. the variance gets estimated as too close to zero). - Sometimes you might not have sufficient number of groups to model as random effects (e.g. for groups of fewer than c8 things, estimates of the variance are often not a reliable reflection of the population).\n\n\n\n\n\n\n\n\n4 - random effects\n\n\n\nWhat slopes could vary by participant?\nQ: Could participants vary in their performance over time?\nA: Yes, (poly1 + poly2 | Subject)\nQ: Could participants vary in how performance differs between Tasks?\nA: Yes, (poly1 + poly2 + Task | Subject). E.g., Some participants might be much better at the memory task than other tasks, some might be better at the other tasks.\nQ: Could participants vary in how tasks differ in their performance over time?\nA: Yes, ((poly1 + poly2)*Task | Subject). E.g., For some participants, memory could decline more than cADL, for other participants it could decline less.\nlmer(Performance ~ (poly1 + poly2)*Task ... +\n                   (1 + (poly1 + poly2)*Task | Subject)\n\n\n\n\n\n5 - the model\n\n\n\nsmall note: I’m using the bobyqa optimizer here because it tends to be slightly quicker than the default.\n\nm1 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + (poly1 + poly2) * Task | Subject),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\nboundary (singular) fit: see help(‘isSingular’) Warning messages: 1: In commonArgs(par, fn, control, environment()) : maxfun &lt; 10 * length(par)^2 is not recommended. 2: In optwrap(optimizer, devfun, getStart(start, rho$pp), lower = rho$lower, : convergence code 1 from bobyqa: bobyqa – maximum number of function evaluations exceeded\n\n\n\n\n\nQuestion 5\n\n\nOkay, so the model didn’t converge. It’s trying to estimate a lot of things in the random effects (even though it didn’t converge, try looking at VarCorr(model) to see all the covariances it is trying to estimate).\n\n\n\n\n\n\nCategorical random effects on the RHS\n\n\n\n\n\nWhen we have a categorical random effect (i.e. where the x in (1 + x | g) is a categorical variable), then model estimation can often get tricky, because “the effect of x” for a categorical variable with \\(k\\) levels is identified via \\(k-1\\) parameters, meaning we have a lot of variances and covariances to estimate when we include x|g.\n\n\nWhen x is numeric:\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         x           ...      ...\nResidual             ...     \n\n\n\nWhen x is categorical with \\(k\\) levels:\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n\n\nHowever, we can use an alternative formation of the random effects by putting a categorical x into the right-hand side:\nInstead of (1 + x | g) we can fit (1 | g) + (1 | g:x).\nThe symbol : in g:x is used to refer to the combination of g and x.\n\n\n      g        x     g:x\n1    p1        a    p1.a\n2    p1        a    p1.a\n3    p1        b    p1.b\n4   ...      ...     ...\n5    p2        a    p2.a\n6    p2        b    p2.b\n7   ...      ...     ...\n\n\nIt’s a bit weird to think about it, but these two formulations of the random effects can kind of represent the same idea:\n\n(1 + x | g): each group of g can have a different intercept and a different effect of x\n\n(1 | g) + (1 | g:x): each group of g can have a different intercept, and each level of x within each g can have a different intercept.\n\nBoth of these allow the outcome y to change across x differently for each group in g (i.e. both of them result in y being different for each level of x in each group g).\nThe first does so explicitly by estimating the group level variance of the y~x effect.\nThe second one estimates the variance of \\(y\\) between groups, and also the variance of \\(y\\) between ‘levels of x within groups’. In doing so, it achieves more or less the same thing, but by capturing these as intercept variances between levels of x, we don’t have to worry about lots of covariances:\n\n\n(1 + x | g)\nGroups   Name        Std.Dev. Corr  \ng        (Intercept) ...        \n         xlevel2     ...      ...\n         xlevel3     ...      ...     ...\n         ...         ...      ...     ...     ...\n         xlevelk     ...      ...     ...     ...   ...\nResidual             ...     \n\n\n\n(1 | g) + (1 | g:x)\nGroups   Name        Std.Dev. \ng        (Intercept) ...        \ng.x      (Intercept) ...        \nResidual             ...     \n\n\n\n\n\nTry adjusting your model by first moving Task to the right hand side of the random effects, and from there starting to simplify things (remove random slopes one-by-one)\nThis is our first experience of our random effect structures becoming more complex than simply (.... | group). This is going to feel confusing, but don’t worry, we’ll see more structures like this next week.\n\n\n\n\n\n\nHints\n\n\n\n\n\n... + (1 + poly1 + poly2 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nTo then start simplifying (if this model doesn’t converge), it can be helpful to look at the VarCorr() of the non-converging model to see if anything looks awry. Look for small variances, perfect (or near perfect) correlations. These might be sensible things to remove.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s our model with subject-task effects on the right hand side.\nAgain we have problems, as we have a singular fit:\n\nm2 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 + poly2 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\nboundary (singular) fit: see help(‘isSingular’)\n\nLooking at the random effects of our model, note that the poly2|Subject random effect has very little variance (and high correlations).\nNote that it makes sense that by including the random effects for Subject:Task, there might not be much above that leftover in Subject random effects.\n\nVarCorr(m2)\n\n Groups       Name        Std.Dev.  Corr         \n Subject:Task (Intercept) 3.1008437              \n              poly1       0.9234884  0.275       \n              poly2       0.0526444 -0.329  0.021\n Subject      (Intercept) 3.4612332              \n              poly1       1.5659246 -0.177       \n              poly2       0.0076382  0.075 -0.995\n Residual                 1.0195745              \n\n\nWhen we remove it, our model converges!!\n\nm3 = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\nQuestion 6\n\n\nConduct a series of model comparisons investigating whether\n\nTasks differ only in their linear change\n\nTasks differ in their quadratic change\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nRemember, these sorts of model comparisons are being used to isolate and test part of the fixed effects (we’re interested in the how the average participant performs over the study). So our models want to have the same random effect structure, but different fixed effects.\nSee the end of 3A #example-in-mlm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAs I’m comparing these with a likelihood ratio test, I’ll fit them with REML=FALSE\n\nm3int = lmer(Performance ~ poly1 + poly2 + Task + \n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3lin = lmer(Performance ~ poly1*Task + poly2 +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nm3full = lmer(Performance ~ (poly1 + poly2) * Task +\n            (1 + poly1 | Subject) +\n            (1 + poly1 + poly2 | Subject:Task),\n            REML = FALSE,\n          data=Az, control=lmerControl(optimizer = \"bobyqa\"))\n\nanova(m3int, m3lin, m3full)\n\nData: Az\nModels:\nm3int: Performance ~ poly1 + poly2 + Task + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nm3lin: Performance ~ poly1 * Task + poly2 + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\nm3full: Performance ~ (poly1 + poly2) * Task + (1 + poly1 | Subject) + (1 + poly1 + poly2 | Subject:Task)\n       npar    AIC    BIC  logLik deviance   Chisq Df Pr(&gt;Chisq)    \nm3int    15 3801.8 3873.8 -1885.9   3771.8                          \nm3lin    17 3775.4 3857.0 -1870.7   3741.4  30.413  2  2.488e-07 ***\nm3full   19 3607.8 3699.1 -1784.9   3569.8 171.532  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe linear change over time differs between Tasks (\\(\\chi^2(2) = 30.41, p &lt; 0.001). The quadratic change over time differs between Tasks (\\)^2(2) = 171.53, p &lt; 0.001).\n\n\n\n\n\nQuestion 7\n\n\nGet some confidence intervals and provide an interpretation of each coefficient from the full model.\n\n\n\n\n\nSolution\n\n\n\nAs we’ve used likelihood ratio tests above, we’ll get some profile likelihood confidence intervals for our parameters.\nnote this took me about 4 mins to run\n\nconfint(m3full, method=\"profile\", parm=\"beta_\")\n\n\n\n\n\n\n\n  \n    \n      term\n      est\n      CI\n      interpretation\n    \n  \n  \n    (Intercept)\n63.88\n[62.19, 65.57]\nestimated score on the cADL task at baseline\n    poly1\n-3.27\n[-3.93, -2.61]\nestimated linear change in cADL scores from baseline\n    poly2\n0.01\n[-0.02, 0.03]\nno significant curvature to the cADL trajectory\n    TasksADL\n1.44\n[-0.18, 3.06]\nno significant difference between sADL and cADL tasks at baseline\n    TaskMemory\n-2.40\n[-4.02, -0.78]\nat baseline, scores on memory task are 2.4 lower than cADL\n    poly1:TasksADL\n1.34\n[0.82, 1.85]\nperformance on sADL task is not decreasing from baseline as much as performance on cADL\n    poly1:TaskMemory\n-3.30\n[-3.81, -2.78]\nperformance on Memory task is decreasing (linearly) more than performance on cADL\n    poly2:TasksADL\n-0.01\n[-0.05, 0.02]\nno significant difference between quadratic change of sADL from that of cADL\n    poly2:TaskMemory\n0.34\n[0.3, 0.37]\nsignificant difference in quadratic change between performance on Memory vs performance on cADL\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nsome tables of predictions (in case they help)\n\n\n\n\n\nTo get a sense of the quadratic term ‘in action’, think about the predictions across time for each task:\nFor cADL, this is just the linear change. Every timepoint, performance decreases by 3.27.\n\n\n\n\n\n\n\ntimepoint\ncADL\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) = 63.88\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) = 60.61\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2) = 57.34\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3) = 54.07\\)\n\n\n\nFor sADL, the additional change is +1.34, so at every timepoint performance decreases by -1.93 (this is -3.27+1.34)\n\n\n\n\n\n\n\ntimepoint\nsADL\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +\\) \\((1.44) + (1.34 \\times time) + (-0.01 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time) + (1.34 \\times time)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) + (1.34 \\times 0) = 63.88\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) + (1.34 \\times 1) = 61.95\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2)+ (1.34 \\times 2) = 60.02\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3)+ (1.34 \\times 3) = 58.09\\)\n\n\n\nFor the Memory task, the quadratic term is in play. at every timepoint performance decreases by \\(-3.27-3.30 +(0.34 \\times time^2)\\). So for low timepoints, the quadratic term doesn’t make much of a difference as it does for bigger time points.\n\n\n\n\n\n\n\ntimepoint\nMemory\n\n\n\n\nprediction formula\n\\(63.88 + (-3.27 \\times time) + (0.01 \\times time^2) +\\) \\((-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)\\)\n\n\nprediction formula(with non-sig terms removed)\n\\(63.88 + (-3.27 \\times time) +\\) \\((-2.40) + (-3.30 \\times time) + (0.34 \\times time^2)\\)\n\n\n0\n\\(63.88 + (-3.27 \\times 0) +\\) \\((-2.40) + (-3.30 \\times 0) + (0.34 \\times 0^2) = 61.48\\)\n\n\n1\n\\(63.88 + (-3.27 \\times 1) +\\) \\((-2.40) + (-3.30 \\times 1) + (0.34 \\times 1^2) = 55.25\\)\n\n\n2\n\\(63.88 + (-3.27 \\times 2) +\\) \\((-2.40) + (-3.30 \\times 2) + (0.34 \\times 2^2) = 49.70\\)\n\n\n3\n\\(63.88 + (-3.27 \\times 3) +\\) \\((-2.40) + (-3.30 \\times 3) + (0.34 \\times 3^2) = 44.83\\)\n\n\n…\n…\n\n\n9\n\\(63.88 + (-3.27 \\times 9) +\\) \\((-2.40) + (-3.30 \\times 9) + (0.34 \\times 9^2) = 29.89\\)\n\n\n10\n\\(63.88 + (-3.27 \\times 10) +\\) \\((-2.40) + (-3.30 \\times 10) + (0.34 \\times 10^2) = 29.78\\)\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nTake a piece of paper, and based on your interpretation for the previous question, sketch out the model estimated trajectories for each task.\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nMake a plot showing both the average performance and the average model predicted performance across time.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(broom.mixed)\naugment(m3) |&gt;\n  ggplot(aes(x=poly1,col=Task))+\n  stat_summary(aes(y=Performance), geom=\"pointrange\") + \n  stat_summary(aes(y=.fitted), geom=\"line\")"
  },
  {
    "objectID": "04a_ranef.html",
    "href": "04a_ranef.html",
    "title": "4A: Random Effect Structures",
    "section": "",
    "text": "This reading:\n\nextending the multilevel model to encompass more complex random effect structures\nmodel building and common issues"
  },
  {
    "objectID": "04a_ranef.html#example-1-two-levels",
    "href": "04a_ranef.html#example-1-two-levels",
    "title": "4A: Random Effect Structures",
    "section": "Example 1: Two levels",
    "text": "Example 1: Two levels\nBelow is an example of a study that has a similar structure to those that we’ve seen thus far, in which we have just two levels (observations that are grouped in some way).\n\n\nStudy Design\nSuppose, for instance, that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nWe can see our data here:\n\npsychstress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\nhead(psychstress)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Holly N     Self-report     1 -0.417\n2 Psych Holly N     Self-report     2  0.924\n3 Psych Holly N     Self-report     3  0.634\n4 Psych Holly N     Self-report     4  1.21 \n5 Psych Holly N     Self-report     5  0.506\n6 Psych Tom   Y     Self-report     1 -0.557\n\n\n\n\nPlot\n\n\nCode\n# take the dataset, and make the x axis of our plot the 'day' variable, \n# and the y axis the 'stress' variable: \n# color everything by the CBD groups\nggplot(psychstress, aes(x = day, y = stress, col=CBD)) + \n  geom_point() + # add points to the plot\n  geom_line() + # add lines to the plot\n  facet_wrap(~pid) # split it by participant\n\n\n\n\n\n\n\n\n\n\n\nModel\nWe might fit a model that looks something like this:\n\n\nCode\nlibrary(lme4)\n# re-center 'day' so the intercept is day 1\npsychstress$day &lt;- psychstress$day-1 \n\n# fit a model of stress over time: stress~day\n# estimate differences between the groups in their stress change: day*CBD\n# people vary in their overall stress levels: 1|pid\n# people vary in their how stress changes over the week: day|pid\nm2level &lt;- lmer(stress ~ 1 + day * CBD + \n                  (1 + day | pid), data = psychstress)\n\n\nNote that there is a line in the model summary output just below the random effects that shows us the information about the groups, telling us that we have 100 observations that are grouped into 20 different participants’.\n\nsummary(m2level)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day | pid)\n   Data: psychstress\n\nREML criterion at convergence: 127.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.17535 -0.65204 -0.02667  0.64622  1.81574 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n pid      (Intercept) 0.199441 0.44659      \n          day         0.004328 0.06579  0.02\n Residual             0.112462 0.33535      \nNumber of obs: 100, groups:  pid, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.13178    0.14329   0.920\nday          0.07567    0.03461   2.186\nCBDY        -0.08516    0.24221  -0.352\nday:CBDY    -0.19128    0.05851  -3.270\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.339              \nCBDY     -0.592  0.201       \nday:CBDY  0.201 -0.592 -0.339"
  },
  {
    "objectID": "04a_ranef.html#example-2-three-level-nested",
    "href": "04a_ranef.html#example-2-three-level-nested",
    "title": "4A: Random Effect Structures",
    "section": "Example 2: Three level Nested",
    "text": "Example 2: Three level Nested\nLet’s suppose that instead of simply sampling 20 staff members from the Psychology department, we instead went out and sampled lots of people from different departments across the University. The dataset below contains not just our 20 Psychology staff members, but also data from 220 other people from departments such as History, Philosophy, Art, etc..\n\nneststress &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\nhead(neststress)\n\n# A tibble: 6 × 6\n  dept  pid      CBD   measure       day stress\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 CMVM  Ryan     Y     Self-report     1  0.933\n2 CMVM  Ryan     Y     Self-report     2  0.997\n3 CMVM  Ryan     Y     Self-report     3  0.408\n4 CMVM  Ryan     Y     Self-report     4  0.581\n5 CMVM  Ryan     Y     Self-report     5  0.442\n6 CMVM  Nicholas Y     Self-report     1  0.138\n\n\nIn this case, we have observations that are grouped by participants, and those participants can be grouped into the department in which they work. Three levels of nesting!\nYou can see in the Figure 6 below that there is variation between departments (i.e. people working in Art are a bit more relaxed, Political Science and CMVM is stressful, etc), and then within each of those, there is variation between participants (i.e. some people working in Art are more stressed than other people in Art).\n\n\nCode\nggplot(neststress, aes(x=day, y=stress,col=CBD))+\n  # plot points\n  geom_point()+\n  # split by departments\n  facet_wrap(~dept)+\n  # make a line for each participant\n  geom_line(aes(group=pid),alpha=.3)+ \n  # plot the mean and SE for each day.\n  stat_summary(geom=\"pointrange\",col=\"black\")\n\n\n\n\n\nFigure 6: A longitudinal study in which participants are nested within department\n\n\n\n\nTo account for these multiple sources of variation, we can fit a model that says both ( ... | dept) (“things vary by department”) and ( ... | dept:pid) (“things vary by participants within departments”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nneststress$day &lt;- neststress$day-1\n\nmnest &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nNote that we can have different random slopes for departments vs those for participants. Our model above includes all random slopes that are feasible given the study design.\n\n\n\n\n\n\nexplanations of each random slope\n\n\n\n\n\n\nparticipants can vary in their baseline stress levels.\n\n(1 | dept:pid)\n\nparticipants can vary in how stress changes over the week. e.g., some participants might get more stressed over the week, some might get less stressed\n\n(days | dept:pid)\n\n\nparticipants cannot vary in how CBD changes their stress level. because each participant is either CBD or control, “the effect of CBD on stress” doesn’t exist for a single participant (and so can’t very between participants)\n\n(CBD | dept:pid)\n\n\nparticipants cannot vary in how CBD affects their changes in stress over the week. For the same reason as above.\n\n( day*CBD | dept:pid)\n\ndepartments can vary in their baseline stress levels.\n\n(1 | dept)\n\n\ndepartments can vary in how stress changes over the week.\n\n(days | dept)\n\ndepartments can vary in how CBD changes stress levels. because each department contains some participants in the CBD group and some in the control group, “the effect of CBD on stress” does exist for a given department, and so could vary between departments. e.g. Philosophers taking CBD get really relaxed, but CBD doesn’t affect Mathematicians that much.\n\n(CBD | dept)\n\n\ndepartments can vary in how CBD affects changes in stress over the week\n\n( day*CBD | dept)\n\n\n\n\n\nNote that the above model is a singular fit, but it gives us a better place to start simplifying from. If we remove the day*CBD interaction in the by-department random effects, we get a model that converges:\n\nmnest2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | dept) +\n                (1 + day | dept:pid), data = neststress)\n\nAnd plot our fitted values\n\n\nCode\nlibrary(broom.mixed)\naugment(mnest2) |&gt; \n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    # split by departments\n    facet_wrap(~dept) + \n    # make a line for each participant\n    geom_line(aes(group=pid),alpha=.3)+\n    # average fitted value for CBD vs control:  \n    stat_summary(geom=\"line\",aes(col=CBD),lwd=1)\n\n\n\n\n\nFigure 7: Plot of fitted values of the model. Individual lines for each participant, facetted by department. Thicker lines represent the department average fitted values split by CBD group\n\n\n\n\nAnd we can see in our summary that there is a lot of by-department variation - departments vary in their baseline stress levels with a standard deviation of 0.81, and within departments, participants vary in baseline stress scores with a standard deviation of 0.38.\n\nsummary(mnest2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n dept:pid (Intercept) 0.147661 0.38427             \n          day         0.012142 0.11019  -0.03      \n dept     (Intercept) 0.648410 0.80524             \n          day         0.001979 0.04449  -0.18      \n          CBDY        0.055388 0.23535   0.40 -0.22\n Residual             0.129765 0.36023             \nNumber of obs: 1200, groups:  dept:pid, 240; dept, 12\n...\nExamining ranef(mnest2) now gives us a list of dept:pid random effects, and then of dept random effects. We can plot them using dotplot.ranef.mer(), as seen below. From these, we can see for instance, that the effect of CBD is more negative for Theology, and Sociology and Maths have higher slopes of day. These map with the plot of fitted values we saw in Figure 7 - the department lines are going up more Math and Sociology than in other departments, and in Theology the blue CBD line is much lower relative to the red control line than in other departments.\n\ndotplot.ranef.mer(ranef(mnest2))$dept"
  },
  {
    "objectID": "04a_ranef.html#example-3-crossed",
    "href": "04a_ranef.html#example-3-crossed",
    "title": "4A: Random Effect Structures",
    "section": "Example 3: Crossed",
    "text": "Example 3: Crossed\nForgetting about participants nested in departments, let’s return to our sample of 20 staff members from the Psychology department. In our initial study design, we had just one self report measure of stress each day for each person.\nHowever, we might just as easily have taken more measurements. i.e. on Day 1, we could have recorded Martin’s stress levels 10 times. Furthermore, we could have used 10 different measurements of stress, rather than just a self-report measure. We could measure his cortisol levels, blood pressure, heart rate variability, give him different questionnaires, ask an informant like his son to report his stress, and so on. And we could have done the same for everybody.\n\nstresscross &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\nhead(stresscross)\n\n# A tibble: 6 × 6\n  dept  pid   CBD   measure          day stress\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Psych Aja   N     Alpha-Amylase      1  0.269\n2 Psych Aja   N     Blood Pressure     1  0.855\n3 Psych Aja   N     Cortisol           1  0.278\n4 Psych Aja   N     EEQ                1  0.470\n5 Psych Aja   N     HRV                1 -0.404\n6 Psych Aja   N     Informant          1  0.774\n\n\nIn this case, we can group our participants in two different ways. For each participant we have 5 datapoints for each of 10 different measures of stress. So we have 5x10 = 50 observations for each participant. But if we group them by measure instead, then we have each measure 5 times for 20 participants, so 5x20 = 100 observations of each measure. And there is no hierarchy here - the “blood pressure” measure is the same measure for Martin as it is for Dan and Aja etc. It makes sense to think of by-measure variability as not being ‘within-participants’.\nThis means we can choose when plotting whether to split the plots by participants, with a different line for each measure (Figure 8), or split by measure with a different line for each participant (Figure 9)\n\n\nfacet = participant, line = measure\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  #make a line for each measure\n  geom_line(aes(group=measure))+\n  facet_wrap(~pid)\n\n\n\n\n\nFigure 8: crossed designs with participants and measures. we can facet by participant and plot a line for each measure\n\n\n\n\n\n\nfacet = measure, line = participant\n\n\nCode\nggplot(stresscross, aes(x=day, y=stress, col=CBD))+\n  geom_point()+\n  # make a line for each ppt\n  geom_line(aes(group=pid))+\n  facet_wrap(~measure)\n\n\n\n\n\nFigure 9: crossed designs with participants and measures. we can facet by measure and plot a line for each participant\n\n\n\n\n\n\nWe can fit a model that therefore accounts for the by-participant variation (“things vary between participants”) and the by-measure variation (“things vary between measures”).\nSo a model might look something like this:\n\n# re-center 'day' so the intercept is day 1\nstresscross$day &lt;- stresscross$day-1\n\nmcross &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day * CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nNote that just as with the nested example above, we can have different random slopes for measures vs those for participants, depending upon what effects can vary given the study design.\nAs before, removing the interaction in the random effects achieves model convergence:\n\nmcross2 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), data = stresscross)\n\nAnd again we might plot our fitted values either of the ways we plotted our initial data in Figure 8 above, only with the .fitted values obtained from the augment() function:\n\n\nCode\naugment(mcross2) |&gt;\n  ggplot(aes(x=day, y=.fitted, col=CBD))+\n    geom_point()+\n    geom_line(aes(group=pid))+\n    facet_wrap(~measure)\n\n\n\n\n\n\n\n\n\nOur random effect variances show the estimated variance in different terms (the intercept, slopes of day, effect of CBD) between participants, and between measures.\nFrom the below it is possible to see, for instance, that there is considerable variability between how measures respond to CBD (they vary in the effect of CBD on stress with a standard deviation of 0.53)\n\nsummary(mcross2)\n\n...\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n pid      (Intercept) 0.316578 0.56265             \n          day         0.014693 0.12121  -0.51      \n measure  (Intercept) 0.087111 0.29515             \n          day         0.008542 0.09242   0.88      \n          CBDY        0.283635 0.53257  -0.10  0.11\n Residual             0.088073 0.29677             \nNumber of obs: 1000, groups:  pid, 20; measure, 10\n...\nAgain, our dotplots of random effects help to also show this picture. We can see that the measures of “blood pressure”, “alpha-amylase”, “cortisol”, and “HRV” all have more effects of CBD that are more negative. We can see this in our plot of fitted values - these measures look like CBD vs control differnce is greater than in other measures.\n\ndotplot.ranef.mer(ranef(mcross2))$measure"
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 4 and 30 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/msmr_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: msmr_gadeduc.csv\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nHere’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"../../data/msmr_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient        visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 OT_Control_1        26      27      27      27      27\n2 OT_Control_2        25      24      27      26      25\n3 OT_Treatment_3      26      27      24      23      22\n4 OT_Treatment_4      26      26      26      25      25\n5 ND_Control_1        25      25      25      25      25\n6 ND_Control_2        26      24      25      24      23\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 and shoving their values into one variable, and keeping the name of the column they come from as another variable:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,600 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 OT_Control_1 visit_0    26\n 2 OT_Control_1 visit_1    27\n 3 OT_Control_1 visit_2    27\n 4 OT_Control_1 visit_3    27\n 5 OT_Control_1 visit_4    27\n 6 OT_Control_2 visit_0    25\n 7 OT_Control_2 visit_1    24\n 8 OT_Control_2 visit_2    27\n 9 OT_Control_2 visit_3    26\n10 OT_Control_2 visit_4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\n\n2 - time is numeric\n\n\n\nNow we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" with nothingness \"\", and then convert them to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,600 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 OT_Control_1     0    26\n 2 OT_Control_1     1    27\n 3 OT_Control_1     2    27\n 4 OT_Control_1     3    27\n 5 OT_Control_1     4    27\n 6 OT_Control_2     0    25\n 7 OT_Control_2     1    24\n 8 OT_Control_2     2    27\n 9 OT_Control_2     3    26\n10 OT_Control_2     4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nFinally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,600 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 OT        Control 1           0    26\n 2 OT        Control 1           1    27\n 3 OT        Control 1           2    27\n 4 OT        Control 1           3    27\n 5 OT        Control 1           4    27\n 6 OT        Control 2           0    25\n 7 OT        Control 2           1    24\n 8 OT        Control 2           2    27\n 9 OT        Control 2           3    26\n10 OT        Control 2           4    25\n# ℹ 2,590 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect?\nDoes it look like it worked for every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists PT and GI don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\n\n\n\n\n\n1 - fixed effects\n\n\n\nWe want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nWe have multiple observations for each of the 520 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “OT” is a different person from patient “1” from therapist “ND”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nNote that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can’t vary by patient, so this doesn’t work. hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndotplot.ranef.mer() might help here!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIt would be best to go to one of the therapists WG, EQ, or EI…\nWhy? These therapists all have the most negative slope of visit:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using the effects package.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient)\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")"
  },
  {
    "objectID": "04ex.html#footnotes",
    "href": "04ex.html#footnotes",
    "title": "Week 4 Exercises: Nested and Crossed",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎"
  },
  {
    "objectID": "05a_assump.html",
    "href": "05a_assump.html",
    "title": "5A: Model Assumptions",
    "section": "",
    "text": "This reading:\n\nMultilevel model assumptions: random effects can be thought of as another level of residual!\nInfluence in multilevel models: influential observations and influential groups."
  },
  {
    "objectID": "05a_assump.html#level-1-residuals",
    "href": "05a_assump.html#level-1-residuals",
    "title": "5A: Model Assumptions",
    "section": "Level 1 residuals",
    "text": "Level 1 residuals\nWe can get the level 1 (observation-level) residuals the same way we used to do for lm() - by just using resid() or residuals(). Additionally, there are a few useful techniques for plotting these which we have listed below:\n\n\nresid vs fitted\nWe can plot the residuals vs fitted model (just like we used to for lm()), and assess the extend to which the assumption holds that the residuals are zero mean. (we want the blue smoothed line to be fairly close to zero across the plot)\n\n# \"p\" below is for points and \"smooth\" for the smoothed line\nplot(jsmod, type=c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nscale-location\nAgain, like we can for lm(), we can also look at a scale-location plot. This is where the square-root of the absolute value of the residuals is plotted against the fitted values, and allows us to more easily assess the assumption of constant variance.\n(we want the blue smoothed line to be close to horizontal across the plot)\n\nplot(jsmod,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\n\n\nfacetted plots\nWe can also plot these “resid v fitted” and “scale-location” plots for each cluster, to check that our residual mean and variance is not related to the clusters:\n\nplot(jsmod,\n         form = resid(.) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\nplot(jsmod,\n         form = sqrt(abs(resid(.))) ~ fitted(.) | dept,\n         type = c(\"p\"))\n\n\n\n\n\n\n\n\n\n\nresidual normality\nWe can also examine the normality the level 1 residuals, using things such as histograms and QQplots:\n(we want the datapoints to follow close to the diagonal line)\n\nqqnorm(resid(jsmod)); qqline(resid(jsmod))\n\n\n\n\n\n\n\n\n\nhist(resid(jsmod))"
  },
  {
    "objectID": "05a_assump.html#level-2-residuals",
    "href": "05a_assump.html#level-2-residuals",
    "title": "5A: Model Assumptions",
    "section": "Level 2+ residuals",
    "text": "Level 2+ residuals\nThe second level of residuals in the multilevel model are actually just our random effects! We’ve seen them already whenever we use ranef()!\nTo get out these we often need to do a bit of indexing. ranef(model) will give us a list with an item for each grouping. In each item we have a set of columns, one for each thing which is varying by that grouping.\nBelow, we see that ranef(jsmod) gives us something with one entry, $dept, which contains 2 columns (the random intercepts and random slopes of payscale):\n\nranef(jsmod)\n\n$dept\n                                        (Intercept)    payscale\nAccounting                              -0.03045458 -0.19259376\nArchitecture and Landscape Architecture  0.29419381 -0.35855884\nArt                                     -0.29094345  0.15293285\nBusiness Studies                        -0.27858102  0.18008149\n...                                      ...         ... \nSo we can extract the random intercepts using ranef(jsmod)$dept[,1].\nAgain, we want normality of the random effects, so we can make more histograms or qqplots, for both the random intercepts and the random slopes:\ne.g., for the random intercepts:\n\nqqnorm(ranef(jsmod)$dept[,1]);qqline(ranef(jsmod)$dept[,1])\n\n\n\n\n\n\n\n\nand for the random slopes:\n\nqqnorm(ranef(jsmod)$dept[,2]);qqline(ranef(jsmod)$dept[,2])"
  },
  {
    "objectID": "05a_assump.html#model-simulations",
    "href": "05a_assump.html#model-simulations",
    "title": "5A: Model Assumptions",
    "section": "model simulations",
    "text": "model simulations\nSometimes, a good global assessment of your model comes from how good a representation of the observed data it is. We can look at this in a cool way by simulating from our model a new set of values for the outcome. If we do this a few times over, and plot each ‘draw’ (i.e. set of simulated values), we can look at how well it maps to the observed set of values:\nOne quick way to do this is with the check_predictions() function from the performance package:\n\nlibrary(performance)\ncheck_predictions(jsmod, iterations = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptional: doing it yourself\n\n\n\n\n\nDoing this ourself gives us a lot more scope to query differences between our observed vs model-predicted data.\nThe simulate() function will simulate response variable values for us.\nThe re.form = NULL bit is saying to include the random effects when making simulations (i.e. use the information about the specific clusters we have in our data). If we said re.form = NA it would base simulations on a randomly generated set of clusters with the associated intercept and slope variances estimated by our model.\n\nmodsim &lt;- simulate(jsmod, nsim = 200, re.form=NULL)\n\nTo get this plotted, we’ll have to do a bit of reworking, because it gives us a separate column for each draw. So if we pivot them longer we can make a density plot for each draw, and then add on top of that our observed scores:\n\n# take the simulations\nmodsim |&gt; \n  # pivot \"everything()\" (useful function to capture all columns),\n  # put column names into \"sim\", and the values into \"value\"\n  pivot_longer(everything(), names_to=\"sim\",values_to=\"value\") |&gt;\n  # plot them! \n  ggplot(aes(x=value))+\n  # plot a different line for each sim. \n  # to make the alpha transparency work, i need to use\n  # geom_line(stat=\"density\") rather than \n  # geom_density() (for some reason alpha applies to fill here)\n  geom_line(aes(group=sim), stat=\"density\", alpha=.1,\n            col=\"darkorange\") +\n  # finally, add the observed scores!  \n  geom_density(data = jsuni, aes(x=jobsat), lwd=1)\n\n\n\n\n\n\n\n\nHowever, we can also go further! We can pick a statistic, let’s use the IQR, and see how different our observed IQR is from the IQRs of a series of simulated draws.\nHere are 1000 simulations. This time I don’t care about simulating for these specific clusters, I just want to compare to random draws of clusters:\n\nsims &lt;- simulate(jsmod, nsim=1000, re.form=NA)\n\nThe apply() function (see also lapply, sapply ,vapply, tapply) is a really nice way to take an object, and apply a function to it. The number 2 here is to say “do it on each column”. If we had 1 it would be saying “do it on each row”.\nThis gives us the IQR of each simulation:\n\nsimsIQR &lt;- apply(sims, 2, IQR)\n\nWe can then ask what proportion of our simulated draws have an IQR smaller than our observed IQR? If the answer is very big or very small it indicates our model does not very represent this part of reality very well.\n\nmean(IQR(jsuni$jobsat)&gt;simsIQR)\n\n[1] 0.451"
  },
  {
    "objectID": "05b_writing.html",
    "href": "05b_writing.html",
    "title": "5B: Writing",
    "section": "",
    "text": "This reading:\n\nA (non-exhaustive) checklist of things to think about/include when writing up analyses with multilevel models"
  },
  {
    "objectID": "05b_writing.html#the-sample-data",
    "href": "05b_writing.html#the-sample-data",
    "title": "5B: Writing",
    "section": "The sample data",
    "text": "The sample data\nDescriptives of hierarchical data are sometimes a bit more difficult than when we don’t have any ‘levels’. Typically, what we are wanting to do is provide our readers with a picture of the characteristics of our sample. “Our sample” now refers to multiple levels, so we want to describe each of these. More often than not, one of these levels will be a bit more interesting to us as a population we are hoping to generalise to. In psychology we are usually interested in “people”, so if we have data that is multiple trials per participant, we would probably want to focus on describing the participants (the clusters) as the individual trials are something we exert control over as the experimenter. If each datapoint was a child and they were nested in schools, we would probably want to describe both the children and the schools that are in our sample.\nThe aim here is to provide a picture of our sample so that a reader can get a sense of how ‘transportable’ the findings are to different contexts. For instance, if participants in our study are all university students, then we want to be careful about thinking that the findings will apply in other populations (see e.g. “most people aren’t WEIRD”).\n\nA checklist\n\n\nwhat is the hierarchical data structure (how many levels, what is each level?)\nDescribe any data cleaning outlier/data removal prior to calculating descriptive statistics (these tend to be the impossible values - i.e. observations that you would never want in your data anyway)\nsample sizes: how many at each level?\n\nhow many lower-level within each higher level unit? (if this varies, provide an average, and possibly a min and a max)\n\nscales of measured variables\ndescriptive statistics of relevant variables that characterise your sample.\n\nthese should be computed at the level at which they were measured. For instance, if you have observations grouped by participant, mean(data$age) would give the average age of your observations (which isn’t meaningful, and would differ from the average age of your participants if you have a different number of observations for each participant).\n\nHow much of the variability in the outcome variable is attributable to the clustering? (i.e. ICC)"
  },
  {
    "objectID": "05b_writing.html#the-methods",
    "href": "05b_writing.html#the-methods",
    "title": "5B: Writing",
    "section": "The methods",
    "text": "The methods\nWhen writing up any statistical analysis, one important thing to keep in mind is transparency in the decisions and actions taken in the analysis process. The aim is to avoid a reader wondering “how did they end up with these results?”. Ideally, another researcher would be able to reproduce your analysis based on your explanation of what you have done.\nWith multilevel models, there’s a lot of choices that we make - the scaling and centering of variables, models being fitted with ML vs REML, the method used to conduct inference, and so on. In addition, in the event that we arrived at our final model after a series of non-converging models that were then simplified, we would ideally explain this process.\n\nA checklist\n\n\nDescribe any transformations to the data that are made prior to conducting the analysis (e.g., you’ll often re-center a time variable)\nDescribe the process that led to your final model(s)\n\nClearly explain the structure of your initial model (e.g. this might be the ‘maximal model’), and if this failed to converge, explain what random effects were removed and in what order? if possible, explain why.\n\nState the software packages and versions used to fit models, along with the estimation method (ML/REML) and optimiser used.\n\nWhat is the structure of your final model(s)?\n\nYou don’t need to write a complicated mathematical equation for your model. Describing it in words is fine provided you’re clear. e.g. “the outcome variable Y was modelled using mixed effects regression with afixed effects including a main effect of A and B as well as their interaction. The random effects include a random intercept by participant”\nLinear/binomial/poisson/… - if not linear, what link function (e.g., logit, log) was used?\nSpecify all fixed effects.\nSpecify all random effects according to the sampling units (e.g. schools/children etc) with which they interact. Be careful to make sure it’s clear what slopes are for which groupings!.\n\n\nIt’s often useful to state clearly the relevant test/comparison/parameter estimate of interest, and link this explicitly to the research questions/hypotheses.\n\nAny model comparisons should be clearly stated so that the reader understands the structure of both models being compared.\nSpecify the methods used to conduct inference (e.g. LRT, bootstrap), and if relevant, explain why (e.g. Kenward Rogers might be used due to a small number of level 2 units)."
  },
  {
    "objectID": "05b_writing.html#the-results",
    "href": "05b_writing.html#the-results",
    "title": "5B: Writing",
    "section": "The results",
    "text": "The results\nWriting up results will vary depending on the strategies employed. The important part is to highlight the relevant test/comparison that addresses the research aims, and explain what the result means with respect to the question at hand.\nAdditionally, be sure to take some time to understand what the estimate actually means (\\(p&lt;.05\\) is just a small part of the story). With models like these we are almost always just looking at outcome “differences” between levels of a categorical predictor or “change” across some continuous predictor. Does the estimated difference/change, and its direction, make sense to you? What does it mean practically? Asking yourself questions like this is also a good way of sense checking your analysis (i.e. a strong counter-intuitive finding could mean you have a variable coded back to front!).\nFor reporting parameter estimates, ideally we would include both the estimate and the precision (i.e. the standard error or a confidence interval). When reporting statistical tests, make sure to include the test statistic (\\(t\\), \\(F\\), \\(\\chi^2\\), etc.), the relevant degrees of freedom, and the p-value.\n\nA checklist\n\n\nresults of model comparisons and what they mean in the context of the research question\n\nparameter estimates and precision for relevant fixed effects.\n\nvariance components\n\nhow does the effect of interest vary between groups?\n\nis it related to other group level variance (i.e. the random effect correlations if modelled)\n\nif relevant - sensitivity to influential observations and clusters."
  },
  {
    "objectID": "05ex.html",
    "href": "05ex.html",
    "title": "Week 5 Exercises: Assumptions, Diagnostics, Writing up",
    "section": "",
    "text": "Dataset: NGV.csv\nThese data are from an experiment designed to investigate how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.\nThe experiment involved playing 10 levels of a game in which the objective was to escape a maze. Various obstacles and other characters were present throughout the maze, and players could interact with these by side-stepping or jumping over them, or by pushing or shooting at them. All of these actions took the same amount of effort to complete (pressing a button), and each one achieved the same end (moving beyond the obstacle and being able to continue through the maze).\nEach participant completed all 10 levels twice, once in which all characters were presented as cartoons, and once in which all characters were presented as realistic humans and animals. The layout of the level was identical in both, the only difference being the depiction of objects and characters. For each participant, these 20 levels (\\(2 \\times 10\\) mazes) were presented in a random order. Half of the participants played via a screen, and the other half played via a VR headset. For each level played, we have a record of “needless game violence” (NGV) which was calculated via the number of aggressive (pushing/shooting) actions taken (+0.5 for every action that missed an object, +1 for every action aimed at an inanimate object, and +2 for every action aimed at an animate character).\nPrior to the experiment, each participant completed the Short Dark Triad 3 (SD-3), which measures the three traits of machiavellianism, narcissism, and psychopathy.\nDataset: https://uoepsy.github.io/data/NGV.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant number\n    age\nParticipant age (years)\n    level\nMaze level (1 to 20)\n    character\nWhether the objects and characters in the level were presented as 'cartoon' or as 'realistic'\n    mode\nWhether the participant played via a screen or with a VR headset\n    P\nPsycopathy Trait from SD-3 (score 1-5)\n    N\nNarcissism Trait from SD-3 (score 1-5)\n    M\nMachiavellianism Trait from SD-3 (score 1-5)\n    NGV\nNeedless Game Violence metric\n  \n  \n  \n\n\n\n\n\n\nQuestion 1\n\n\nConduct an analysis to address the research aims!\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThere’s a lot to unpack in the research aim: “how the realism of video games is associated with more/less unnecessarily aggressive gameplay, and whether this differs depending upon a) the playing mode (playing on a screen vs VR headset), and b) individual differences in the ‘dark triad’ personality traits.”\n\n\n\n\n\n\n\n\n\n1 - some quick checks\n\n\n\n\nngv &lt;- read_csv(\"https://uoepsy.github.io/data/NGV.csv\")\n\nLet’s make things factors and see where we stand:\n\nngv &lt;- ngv |&gt; \n  mutate(\n    PID = factor(PID),\n    level = factor(level),\n    character = factor(character),\n    mode = factor(mode)\n  )\n\nsummary(ngv)\n\n      PID            age            level         character       mode    \n ppt_1  :  20   Min.   :18.00   level1 :152   cartoon  :760   Screen:740  \n ppt_10 :  20   1st Qu.:28.00   level10:152   realistic:760   VR    :780  \n ppt_11 :  20   Median :36.50   level2 :152                               \n ppt_12 :  20   Mean   :34.99   level3 :152                               \n ppt_13 :  20   3rd Qu.:42.25   level4 :152                               \n ppt_14 :  20   Max.   :48.00   level5 :152                               \n (Other):1400                   (Other):608                               \n       P               N               M              NGV        \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 0.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 8.875  \n Median :2.000   Median :3.000   Median :3.000   Median :11.000  \n Mean   :2.171   Mean   :2.763   Mean   :2.842   Mean   :10.788  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:13.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :29.000  \n                                                                 \n\n\neverything looks good to me! All ranges looks fine.\nHow many people do we have?\n\ndim(table(ngv$PID, ngv$character))\n\n[1] 76  2\n\n\n76!\nDoes everyone have 10 datapoints for cartoon and 10 for realistic?\n\nany(table(ngv$PID, ngv$character)!=10)\n\n[1] FALSE\n\n\nyes, they do!\n\n\n\n\n\n2 - some exploratory plots\n\n\n\nOkay. Let’s plot.. The question asks about “how the realism of video games is associated with more/less unnecessarily aggressive gameplay”.\nSo we’ll put the character on the x-axis and our outcome NGV on the y:\nI like jitters, but you could put boxplots or violin plots too!\n\nggplot(ngv, aes(x = character, y = NGV)) +\n  geom_jitter(height=0, width=.2, alpha=.2)\n\n\n\n\n\n\n\n\nWe’re also interested in whether this differs depending on the mode of gameplay (screen vs VR headset). So we could facet_wrap perhaps? or colour?\nLet’s also plot the means - i’ll put those to the side of all the jittered points with a “nudge”..\n\nggplot(ngv, aes(x = character, y = NGV, col = mode)) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\n\nAs well as looking at whether the NGV~character relationship differs between modes, we’re also interested in the differences in this relationship due to individual differences in the dark triad personality traits. We have these measured for each person, so we can just use a similar idea:\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(P))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(M))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\nggplot(ngv, aes(x = character, y = NGV, col = factor(N))) +\n  geom_jitter(height=0, width=.2, alpha=.2) +\n  stat_summary(geom=\"pointrange\", position = position_nudge(x=.25))\n\n\n\n\n\n\n\n\nWhat do we get from all these plots? Well, it looks like mode might be changing the relationship between character and violence. It also looks like there’s a considerable effect of the dark triad on the amount of violence people use! Of course, in these individual plots, it’s hard to ascertain the extent to which plots showing differences between levels of Narcissism are due to Narcissism or due to differences in Psychopathy (all the dark triad traits are fairly correlated)\n\nngv |&gt; select(P,N,M) |&gt;\n  cor() |&gt; round(2)\n\n     P    N    M\nP 1.00 0.43 0.51\nN 0.43 1.00 0.58\nM 0.51 0.58 1.00\n\n\nSo we need to do some modelling!\n\n\n\n\n\n3 - fitting a model\n\n\n\nNOTE: This is how I might approach this question. There are lots of other things that we could quite sensibly do!\n\nWe know that we’re interested in NGV ~ character.\n\nWe also have the additional question of whether this is different between modes - NGV ~ character * mode.\n\nAnd whether the NGV ~ character association is modulated by Psychopathy NGV ~ character * P, and by Narcissism NGV ~ character * N, and by Machiavellianism NGV ~ character * M.\n\nWe could fit these all in one:\nNGV ~ character * (mode + P + M + N)\nWe have multiple observations per participant PID, and we also have multiple observations for each level level. All participants see every level, and every level is seen by all participants. It’s not the case that a level is unique to a single participant, so these are crossed.\n( ?? | PID) + ( ?? | level)\nParticipants plays both the cartoon and the realistic versions, so we could have variation between participants in how realism affects violence - (1 + character | PID). Beyond this, all variables are measured at the participant level, so we can’t have anything else.\nFor the levels, each level is played by some participants in VR headsets and some participants on a screen, so we could have some levels for which VR feels very different (and makes you play more violently?) - (mode | level). We could also have some levels for which the realism has a bigger effect - (character | level), and also have some levels for which people high on the dark triad play differently - i.e. the dark triad could result in lots of violence in level 2, but not in level 3 - (P + M + N | level).\n(you could also try to fit the interactions in the random effects here but i’m not going to even try!)\n\nm0 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + character + mode + P + M + N | level), data = ngv)\n\nafter some simplification, I end up at the model below. You might end up at a slightly different random effect structure, and that is completely okay! The important thing is to be transparent in your decisions.\n\nm1 = lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), data = ngv)\n\n\n\n\n\nQuestion 2\n\n\nCheck the assumptions of your model\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe have a multilevel model, so we have assumptions at multiple levels! See 5A#mlm-assumptions-diagnostics.\nBe careful - QQplots with few datapoints can make things look weirder than they are - try a histogram too\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere are some assumption plots:\n\nplot(m1)\n\n\n\n\n\n\n\nplot(m1,\n     form = sqrt(abs(resid(.))) ~ fitted(.),\n     type = c(\"p\",\"smooth\"))\n\n\n\n\n\n\n\n\nThere’s something weird going on in the left hand side with a bunch of points looking funny! My guess is that this may well come out when examining influence.\nIn the QQplots of the random effects below we can see a couple of participants are a bit off - this may well be what we are seeing above.\n\nqqnorm(ranef(m1)$PID[,1],main=\"1|PID\");qqline(ranef(m1)$PID[,1])\nqqnorm(ranef(m1)$PID[,2],main=\"character|PID\");qqline(ranef(m1)$PID[,2])\nqqnorm(ranef(m1)$level[,1],main=\"1|level\");qqline(ranef(m1)$level[,1])\nqqnorm(ranef(m1)$level[,2],main=\"mode|level\");qqline(ranef(m1)$level[,2])\n\n\n\n\n\n\n\n\nThe QQplots for the level random effects are hard to evaluate in part because there aren’t many levels (only 10). Let’s do some histograms too. They don’t look terrible..\n\nhist(ranef(m1)$PID[,1],main=\"1|PID\")\nhist(ranef(m1)$PID[,2],main=\"character|PID\")\nhist(ranef(m1)$level[,1],main=\"1|level\")\nhist(ranef(m1)$level[,2],main=\"mode|level\")\n\n\n\n\n\n\n\n\nOur model predictions don’t look great either. Something is happening at values of 0?\n\nlibrary(performance)\ncheck_predictions(m1)\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nCheck the extent to which your results may be sensitive to certain influential observations, or participants, or levels!\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 5A #influence for two packages that can assess influence.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nlet’s check for influential participants first:\n\nlibrary(HLMdiag)\ninf2 &lt;- hlm_influence(m1,level=\"PID\")\ndotplot_diag(inf2$cooksd, index=inf2$PID, cutoff=\"internal\")\n\n\n\n\n\n\n\n\nand let’s re-fit without those two weird people..\n\nm1a &lt;- lmer(NGV ~ character * (mode + P + M + N) + \n            (1 + character | PID) + \n            (1 + mode | level), \n            data = ngv |&gt; filter(!(PID %in% c(\"ppt_59\",\"ppt_53\"))))\n\nOur conclusions change!\nThe significance of P (which is the association between psychopathy and needless violence in the cartoon condition) depends upon exclusion of these two participants. I’m showing the table with Satterthwaite p-values as it’s a bit quicker, but given that we have only 10 groups for the level random effect, it might be worth switching to KR\n\nlibrary(sjPlot)\ntab_model(m1,m1a, df.method=\"satterthwaite\")\n\n\n\n\n \nNGV\nNGV\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n6.25\n3.14 – 9.35\n&lt;0.001\n5.52\n3.10 – 7.93\n&lt;0.001\n\n\ncharacter [realistic]\n-2.98\n-4.48 – -1.49\n&lt;0.001\n-2.98\n-4.50 – -1.45\n&lt;0.001\n\n\nmode [VR]\n-1.00\n-2.34 – 0.34\n0.140\n-0.36\n-1.41 – 0.70\n0.500\n\n\nP\n0.90\n-0.29 – 2.08\n0.135\n1.19\n0.25 – 2.13\n0.014\n\n\nM\n0.89\n-0.20 – 1.97\n0.107\n0.81\n-0.03 – 1.65\n0.060\n\n\nN\n0.22\n-0.90 – 1.34\n0.693\n0.33\n-0.58 – 1.24\n0.470\n\n\ncharacter [realistic] ×mode [VR]\n-0.41\n-1.05 – 0.23\n0.206\n-0.40\n-1.06 – 0.26\n0.228\n\n\ncharacter [realistic] × P\n0.76\n0.19 – 1.33\n0.010\n0.79\n0.19 – 1.38\n0.010\n\n\ncharacter [realistic] × M\n0.52\n-0.00 – 1.04\n0.051\n0.53\n-0.01 – 1.06\n0.053\n\n\ncharacter [realistic] × N\n-0.00\n-0.54 – 0.54\n0.988\n-0.03\n-0.61 – 0.55\n0.920\n\n\nRandom Effects\n\n\n\nσ2\n3.33\n3.41\n\n\n\nτ00\n7.91 PID\n4.58 PID\n\n\n\n0.06 level\n0.06 level\n\n\nτ11\n1.26 PID.characterrealistic\n1.29 PID.characterrealistic\n\n\n\n0.05 level.modeVR\n0.05 level.modeVR\n\n\nρ01\n-0.11 PID\n-0.18 PID\n\n\n\n-0.51 level\n-0.45 level\n\n\nICC\n0.71\n0.59\n\n\nN\n76 PID\n74 PID\n\n\n\n10 level\n10 level\n\nObservations\n1520\n1480\n\n\nMarginal R2 / Conditional R2\n0.226 / 0.777\n0.306 / 0.714\n\n\n\n\n\n\nNote that our model predictions look much better\n\ncheck_predictions(m1a)\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\nIf we look at those two people - they just didn’t do much (or any) “needless game violence”.\n\nngv |&gt; filter(PID %in% c(\"ppt_59\",\"ppt_53\")) |&gt;\n  ggplot(aes(x=character, y=NGV)) +\n  geom_jitter(height=0, width=.2) + \n  facet_wrap(~PID)\n\n\n\n\n\n\n\n\nLet’s check for influential levels now:\n\ninf3 &lt;- hlm_influence(m1a,level=\"level\")\ndotplot_diag(inf3$cooksd)\n\n\n\n\n\n\n\n\nand for influential observations\n\ninf1 &lt;- hlm_influence(m1a,level=1)\ndotplot_diag(inf1$cooksd)"
  },
  {
    "objectID": "05ex.html#readings",
    "href": "05ex.html#readings",
    "title": "Week 5 Exercises: Assumptions, Diagnostics, Writing up",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nmotivation and grades in school children\n\n\n\n\n\nThis dataset contains information on 900 children from 30 different schools across Scotland. The data was collected as part of a study looking at whether education-related motivation is associated with school grades. This is expected to be different for state vs privately funded schools.\nAll children completed an ‘education motivation’ questionnaire, and their end-of-year grade average has been recorded.\nIt is available at https://uoepsy.github.io/data/schoolmot.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    motiv\nChild's Education Motivation Score (range 0 - 10)\n    funding\nFunding ('state' or 'private')\n    schoolid\nName of School that the child attends\n    grade\nChild's end-of-year grade average (0-100)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\nmod1 &lt;- lmer(grade ~ motiv * funding + \n               (1 + motiv | schoolid), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: grade ~ motiv * funding + (1 + motiv | schoolid)\n   Data: df\n\nREML criterion at convergence: 7083.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08250 -0.67269  0.03043  0.63562  3.13012 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 105.124  10.253        \n          motiv         2.595   1.611   -0.48\n Residual             139.030  11.791        \nNumber of obs: 900, groups:  schoolid, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         40.3143     4.6414   8.686\nmotiv                2.6294     0.8652   3.039\nfundingstate       -17.2531     5.7346  -3.009\nmotiv:fundingstate   2.8485     1.0591   2.689\n\nCorrelation of Fixed Effects:\n            (Intr) motiv  fndngs\nmotiv       -0.782              \nfundingstat -0.809  0.633       \nmtv:fndngst  0.639 -0.817 -0.773\n\n\n\n\n\n\n\n\n\n\n\nmonkey social status and problem solving ability\n\n\n\n\n\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nThe data is available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    status\nSocial Status of monkey (adolescent, subordinate adult, or dominant adult)\n    difficulty\nProblem difficulty ('easy' vs 'difficult')\n    monkeyID\nMonkey Name\n    solved\nWhether or not the problem was successfully solved by the monkey\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\n\n# relevel difficulty\ndf$difficulty &lt;- factor(df$difficulty, \n                        levels=c(\"easy\",\"difficult\"))\n\nmod1 &lt;- glmer(solved ~ difficulty + status +\n                (1 + difficulty | monkeyID), \n              family=binomial,\n              data = df)\n\nsummary(mod1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n   503.7    531.6   -244.8    489.7      390 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9358 -0.6325 -0.3975  0.6748  2.5161 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n monkeyID (Intercept)         1.552    1.246         \n          difficultydifficult 1.371    1.171    -0.44\nNumber of obs: 397, groups:  monkeyID, 50\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          -0.3945     0.3867  -1.020  0.30770   \ndifficultydifficult  -0.8586     0.3053  -2.812  0.00492 **\nstatusdominant        0.6682     0.4714   1.417  0.15637   \nstatussubordinate     1.4596     0.5692   2.564  0.01034 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) dffclt sttsdm\ndffcltydffc -0.333              \nstatusdmnnt -0.721 -0.031       \nstatssbrdnt -0.594 -0.033  0.497\n\n\n\n\n\n\n\n\n\n\n\nmindfulness and cognitive decline\n\n\n\n\n\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nThe data are available at: https://uoepsy.github.io/data/msmr_mindfuldecline.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    study_visit\nStudy Visit Number (1 - 10)\n    age\nAge (in years) at study visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\n\n#recenter age\ndf$ageC &lt;- df$age-60\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ACE ~ 1 + ageC * condition + (1 + ageC | ppt)\n   Data: df\n\nREML criterion at convergence: 365\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.29607 -0.68577 -0.04105  0.68630  2.42029 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n ppt      (Intercept) 0.11982  0.3462       \n          ageC        0.01923  0.1387   0.26\n Residual             0.24453  0.4945       \nNumber of obs: 177, groups:  ppt, 20\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)               85.20343    0.14927 570.796\nageC                      -0.26595    0.04483  -5.933\nconditionmindfulness       0.04803    0.20871   0.230\nageC:conditionmindfulness  0.17294    0.06343   2.726\n\nCorrelation of Fixed Effects:\n            (Intr) ageC   cndtnm\nageC         0.068              \ncndtnmndfln -0.715 -0.048       \nagC:cndtnmn -0.048 -0.707  0.073\n\n\n\n\n\n\n\n\n\n\n\nmid-life happiness slump in great apes\n\n\n\n\n\nPrevious research has evidenced a notable dip in happiness for middle-aged humans. Interestingly, this phenomenon has even been observed in other primates, such as chimpanzees.\nThe present study is interested in examining whether the ‘middle-age slump’ happens to a similar extent for Orangutans as it does for Chimpanzees.\n0 apes (0 Chimps and 0 Orangutans) were included in the study. All apes were studied from early adulthood (10-12 years old for most great apes), and researchers administered the Happiness in Primates (HiP) scale to each participant every 3 years, up until the age of 40.\nThe data are available at https://uoepsy.github.io/data/midlife_ape.csv.\nThe dataset has already been cleaned, and the researchers have confirmed that it includes 0 Chimps and 0 Orangutans, and every ape has complete data (i.e. 10 rows for each ape).\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    apeID\nApe's Name (all names are chosen to be unique)\n    age\nAge (in years) at assessment\n    species\nSpecies (chimp v orangutan)\n    HiP\nHappiness in Primate Scale (range 1 to 18)\n    timepoint\nStudy visit (1 to 10)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/midlife_ape.csv\")\n\n# add polynomials\ndf &lt;- df |&gt; \n  mutate(\n    poly1 = poly(timepoint,2,raw=F)[,1],\n    poly2 = poly(timepoint,2,raw=F)[,2]\n  )\n\nmod1 = lmer(HiP ~ 1 + (poly1 + poly2) * species +\n            (1 + poly1 + poly2 | apeID), \n            data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: HiP ~ 1 + (poly1 + poly2) * species + (1 + poly1 + poly2 | apeID)\n   Data: df\n\nREML criterion at convergence: 6908.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1222 -0.6175  0.0017  0.6340  3.2437 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr     \n apeID    (Intercept)   4.346   2.085            \n          poly1        27.684   5.262   0.11     \n          poly2       106.925  10.340   0.10 0.52\n Residual               1.247   1.117            \nNumber of obs: 2000, groups:  apeID, 200\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)              9.1155     0.1955  46.630\npoly1                    1.8796     1.5391   1.221\npoly2                    9.4004     1.7453   5.386\nspeciesorangutan         0.6853     0.3035   2.258\npoly1:speciesorangutan   0.1104     2.3892   0.046\npoly2:speciesorangutan  -6.2718     2.7093  -2.315\n\nCorrelation of Fixed Effects:\n            (Intr) poly1  poly2  spcsrn ply1:s\npoly1        0.033                            \npoly2        0.054  0.090                     \nspecisrngtn -0.644 -0.021 -0.035              \nply1:spcsrn -0.021 -0.644 -0.058  0.033       \nply2:spcsrn -0.035 -0.058 -0.644  0.054  0.090\n\n\n\n\n\n\n\n\n\n\n\nCBD drinks and stress levels: Version 1\n\n\n\n\n\nSuppose that we conducted an experiment on a sample of 20 staff members from the Psychology department to investigate effects of CBD consumption on stress over the course of the working week. Participants were randomly allocated to one of two conditions: the control group continued as normal, and the CBD group were given one CBD drink every day. Over the course of the working week (5 days) participants stress levels were measured using a self-report questionnaire.\nDataset: https://uoepsy.github.io/data/stressweek1.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    dept\nDepartment\n    pid\nParticipant Name\n    CBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n    measure\nMeasure used to assess stress levels\n    day\nDay of the working week (1 to 5)\n    stress\nStress Level (standardised)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek1.csv\")\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day | pid)\n   Data: df\n\nREML criterion at convergence: 127.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.17535 -0.65204 -0.02667  0.64622  1.81574 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n pid      (Intercept) 0.199441 0.44659      \n          day         0.004328 0.06579  0.02\n Residual             0.112462 0.33535      \nNumber of obs: 100, groups:  pid, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.13178    0.14329   0.920\nday          0.07567    0.03461   2.186\nCBDY        -0.08516    0.24221  -0.352\nday:CBDY    -0.19128    0.05851  -3.270\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.339              \nCBDY     -0.592  0.201       \nday:CBDY  0.201 -0.592 -0.339\n\n\n\n\n\n\n\n\n\n\n\nCBD drinks and stress levels: Version 2\n\n\n\n\n\nAs for previous study, but instead of a sample of 20 participants from the psychology staff, we have 240 people from various departments such as History, Philosophy, Art, etc..\nDataset: https://uoepsy.github.io/data/stressweek_nested.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    dept\nDepartment\n    pid\nParticipant Name\n    CBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n    measure\nMeasure used to assess stress levels\n    day\nDay of the working week (1 to 5)\n    stress\nStress Level (standardised)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_nested.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n               (1 + day + CBD | dept) +\n               (1 + day | dept:pid), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day + CBD | dept) + (1 + day |  \n    dept:pid)\n   Data: df\n\nREML criterion at convergence: 1681.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.99635 -0.57005  0.00139  0.57906  2.76094 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n dept:pid (Intercept) 0.147661 0.38427             \n          day         0.012142 0.11019  -0.03      \n dept     (Intercept) 0.648410 0.80524             \n          day         0.001979 0.04449  -0.18      \n          CBDY        0.055388 0.23535   0.40 -0.22\n Residual             0.129765 0.36023             \nNumber of obs: 1200, groups:  dept:pid, 240; dept, 12\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.03887    0.23720   0.164\nday          0.07651    0.02002   3.822\nCBDY        -0.10608    0.09382  -1.131\nday:CBDY    -0.13900    0.02115  -6.572\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday      -0.166              \nCBDY      0.182  0.040       \nday:CBDY  0.052 -0.572 -0.245\n\n\n\n\n\n\n\n\n\n\n\nCBD drinks and stress levels: Version 3\n\n\n\n\n\nAs for version 1 of this study (20 staff members from the Psychology department), but instead of taking a measurement only on a self-report scale, we took 10 different measures every time point (cortisol levels, blood pressure, heart rate variability, various questionnaires etc).\nDataset: https://uoepsy.github.io/data/stressweek_crossed.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    dept\nDepartment\n    pid\nParticipant Name\n    CBD\nWhether or not they were allocated to the control group (N) or the CBD group (Y)\n    measure\nMeasure used to assess stress levels\n    day\nDay of the working week (1 to 5)\n    stress\nStress Level (standardised)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressweek_crossed.csv\")\n\n# re-center 'day' so the intercept is day 1\ndf$day &lt;- df$day-1 \n\n# removed day*CBD|dept to obtain convergence\nmod1 &lt;- lmer(stress ~ 1 + day * CBD + \n                (1 + day + CBD | measure) +\n                (1 + day | pid), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ 1 + day * CBD + (1 + day + CBD | measure) + (1 + day |  \n    pid)\n   Data: df\n\nREML criterion at convergence: 670.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86172 -0.66229  0.09595  0.68404  2.42690 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n pid      (Intercept) 0.26163  0.51150             \n          day         0.01469  0.12122  -0.32      \n measure  (Intercept) 0.14390  0.37935             \n          day         0.00854  0.09241   0.93      \n          CBDY        0.28360  0.53254  -0.05  0.11\n Residual             0.08807  0.29677             \nNumber of obs: 1000, groups:  pid, 20; measure, 10\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.08096    0.18687  -0.433\nday          0.12651    0.04530   2.793\nCBDY        -0.15808    0.29499  -0.536\nday:CBDY    -0.25830    0.05851  -4.415\n\nCorrelation of Fixed Effects:\n         (Intr) day    CBDY  \nday       0.187              \nCBDY     -0.390  0.167       \nday:CBDY  0.154 -0.452 -0.279"
  },
  {
    "objectID": "05ex.html#labs",
    "href": "05ex.html#labs",
    "title": "Week 5 Exercises: Assumptions, Diagnostics, Writing up",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\nNew Toys!\n\n\n\n\n\nRecall the example from last semesters’ USMR course, where the lectures explored linear regression with a toy dataset of how practice influences the reading age of toy characters (see USMR Week 7 Lecture). We’re going to now broaden our scope to the investigation of how practice affects reading age for all toys (not just Martin’s Playmobil characters).\nYou can find a dataset at https://uoepsy.github.io/data/toy2.csv containing information on 129 different toy characters that come from a selection of different families/types of toy.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    toy_type\nType of Toy\n    year\nYear Released\n    toy\nCharacter\n    hrs_week\nHours of practice per week\n    R_AGE\nReading Age\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio interference in executive functioning\n\n\n\n\n\nThis data is from a simulated study that aims to investigate the following research question:\n\nHow do different types of audio interfere with executive functioning, and does this interference differ depending upon whether or not noise-cancelling headphones are used?\n\n30 healthy volunteers each completed the Symbol Digit Modalities Test (SDMT) - a commonly used test to assess processing speed and motor speed - a total of 15 times. During the tests, participants listened to either no audio (5 tests), white noise (5 tests) or classical music (5 tests). Half the participants listened via active-noise-cancelling headphones, and the other half listened via speakers in the room. Unfortunately, lots of the tests were not administered correctly, and so not every participant has the full 15 trials worth of data.\nThe data is available at https://uoepsy.github.io/data/efsdmt.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    audio\nAudio heard during the test ('no_audio', 'white_noise','music')\n    headphones\nWhether the participant listened via speakers in the room or via noise cancelling headphones\n    SDMT\nSymbol Digit Modalities Test (SDMT) score\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\ndominance in adolescence of great apes\n\n\n\n\n\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/msmr_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/msmr_apespecies.csv.\n\n\n\n\n\n\n\n\nTable 1:  Data Dictionary: msmr_apespecies.csv \n  \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    species\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2:  Data Dictionary: msmr_apeage.csv \n  \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    age\nAge at assessment (years)\n    dominance\nDominance (Z-scored)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrolley problems\n\n\n\n\n\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n  \n    \n      positive frame\n      neutral frame\n      negative frame\n    \n  \n  \n    5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One people will be saved if you do not pull the lever, but 5 people will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n  \n  \n  \n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nThe data are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    frame\nframing of the thought experiment (positive/neutral/negative\n    lives\nlives at stake in the thought experiment (5 or 15)\n    lever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nnovel word learning\n\n\n\n\n\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test.  Data were also collect from healthy controls.  Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    group\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n    lesion_location\nLocation of brain lesion: anterior vs posterior\n    block\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n    PropCorrect\nProportion of 30 responses in a given block that the participant got correct\n    NumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n    NumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n    ID\nParticipant Identifier\n    Phase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAz.rda\n\n\n\n\n\nThese data are available at https://uoepsy.github.io/data/Az.rda. You can load the dataset using:\n\nload(url(\"https://uoepsy.github.io/data/Az.rda\"))\n\nand you will find the Az object in your environment.\nThe Az object contains information on 30 Participants with probable Alzheimer’s Disease, who completed 3 tasks over 10 time points: A memory task, and two scales investigating ability to undertake complex activities of daily living (cADL) and simple activities of daily living (sADL). Performance on all of tasks was calculated as a percentage of total possible score, thereby ranging from 0 to 100.\nWe’re interested in whether performance on these tasks differed at the outset of the study, and if they differed in their subsequent change in performance.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    Subject\nUnique Subject Identifier\n    Time\nTime point of the study (1 to 10)\n    Task\nTask type (Memory, cADL, sADL)\n    Performance\nScore on test (range 0 to 100)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPsychoeducation treatment effects\n\n\n\n\n\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 4 and 30 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/msmr_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    patient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n    visit_0\nScore on the GAD7 at baseline\n    visit_1\nGAD7 at 1 month assessment\n    visit_2\nGAD7 at 2 month assessment\n    visit_3\nGAD7 at 3 month assessment\n    visit_4\nGAD7 at 4 month assessment\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTest-enhanced learning\n\n\n\n\n\nAn experiment was run to conceptually replicate “test-enhanced learning” (Roediger & Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word).\nThe critical (replication) prediction is that the StudyStudy group recall more items on the immediate test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test.\nThe following code loads the data into your R environment by creating a variable called tel:\n\nload(url(\"https://uoepsy.github.io/data/testenhancedlearning.RData\"))\n\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    Subject_ID\nUnique Participant Identifier\n    Group\nGroup denoting whether the participant studied the material twice (StudyStudy), or studied it once then did a test (StudyTest)\n    Delay\nTime of recall test ('min' = Immediate, 'week' = One week later)\n    Test_word\nWord being recalled (175 different test words)\n    Correct\nWhether or not the word was correctly recalled\n    Rtime\nTime to recall word (milliseconds)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nVocabulary development in monolingual and bilingual children\n\n\n\n\n\n488 children from 30 schools were included in the study. Children were assessed on a yearly basis for 7 years throughout primary school on a measure of vocabulary administered in English, the Picture Vocab Test (PVT). 295 were monolingual English speakers, and 193 were bilingual (english + another language).\nPrevious research conducted on monolingual children has suggested that that scores on the PVT increase steadily up until the age of approximately 7 or 8 at which point they begin to plateau. The aim of the present study is to investigate differences in the development of vocabulary between monolingual and bilingual children.\nThe data are available at https://uoepsy.github.io/data/pvt_bilingual.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    child\nChild's name\n    school\nSchool Identifier\n    isBilingual\nBinary variable indicating whether the child is monolingual (0) or bilingual (1)\n    age\nAge (years)\n    PVT\nScore on the Picture Vocab Test (PVT). Scores range 0 to 60"
  },
  {
    "objectID": "05ex.html#new",
    "href": "05ex.html#new",
    "title": "Week 5 Exercises: Assumptions, Diagnostics, Writing up",
    "section": "New",
    "text": "New\n\n\n\n\n\n\nroutine and emotion dysregulation in children\n\n\n\n\n\nAre children with more day-to-day routine better at regulating their emotions? A study of 200 children from 20 schools (9 private schools and 11 state schools) completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).\nDATASET: https://uoepsy.github.io/data/crqeds.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    schoolid\nSchool Identifier\n    EDS\nEmotion Dysregulation Score (range 1-6, higher values indicate more *dys*regulation of emotions)\n    CRQ\nChildhood Routine Questionnaire Score (range 0-7, higher values indicate more day-to-day routine)\n    schooltype\nSchool type (private / state)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/crqeds.csv\")\n\nmod1 &lt;- lmer(EDS ~ schooltype + CRQ + \n       (1 + CRQ | schoolid), \n       data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: EDS ~ schooltype + CRQ + (1 + CRQ | schoolid)\n   Data: df\n\nREML criterion at convergence: 289.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.85256 -0.84640  0.02332  0.66027  2.04265 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 0.20734  0.4553        \n          CRQ         0.01891  0.1375   -0.76\n Residual             0.23942  0.4893        \nNumber of obs: 174, groups:  schoolid, 20\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)      4.53262    0.17583  25.778\nschooltypeState -0.16708    0.15415  -1.084\nCRQ             -0.05306    0.05067  -1.047\n\nCorrelation of Fixed Effects:\n            (Intr) schltS\nscholtypStt -0.510       \nCRQ         -0.762  0.042\n\n\n\n\n\n\n\n\n\n\n\nthe role of mannequins in clothing purchases\n\n\n\n\n\nDoes clothing seem more attractive to shoppers when it is viewed on a model, and is this dependent on item price? 30 participants were presented with a set of pictures of items of clothing, and rated each item how likely they were to buy it. Each participant saw 20 items, ranging in price from £5 to £100. 15 participants saw these items worn by a model, while the other 15 saw the items against a white background.\nDATASET: https://uoepsy.github.io/data/dapr3_mannequin.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    purch_rating\nPurchase Rating (sliding scale 0 to 100, with higher ratings indicating greater perceived likelihood of purchase)\n    price\nPrice presented with item (range £5 to £100)\n    ppt\nParticipant Identifier\n    condition\nWhether items are seen on a model or on a white background\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_mannequin.csv\")\n\n#scale price to help convergence\n#change 1 in price is now change of £10\ndf$price &lt;- df$price/10\n\nmod1 &lt;- lmer(purch_rating ~ price*condition + \n       (1+price|ppt), \n       data = df)\n  \nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: purch_rating ~ price * condition + (1 + price | ppt)\n   Data: df\n\nREML criterion at convergence: 4754.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7237 -0.6389  0.0491  0.6836  3.2354 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept)  59.3610  7.7046       \n          price         0.7131  0.8444  -0.78\n Residual             148.0384 12.1671       \nNumber of obs: 600, groups:  ppt, 30\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)           41.2807     2.4672  16.732\nprice                  2.4767     0.3270   7.575\nconditionmodel        -1.8533     3.4891  -0.531\nprice:conditionmodel   1.1600     0.4624   2.509\n\nCorrelation of Fixed Effects:\n            (Intr) price  cndtnm\nprice       -0.804              \nconditinmdl -0.707  0.568       \nprc:cndtnmd  0.568 -0.707 -0.804\n\n\n\n\n\n\n\n\n\n\n\nmental wellbeing across Scotland\n\n\n\n\n\nResearchers want to study the relationship between time spent outdoors and mental wellbeing, across all of Scotland. They contact all the Local Authority Areas (LAAs) and ask them to collect data for them, with participants completing the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being, and being asked to estimate the average number of hours they spend outdoors each week. Twenty of the Local Authority Areas provided data.\nDATASET: https://uoepsy.github.io/data/LAAwellbeing.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    name\nParticipant Name\n    laa\nLocal Authority Area\n    outdoor_time\nNumber of hours spent outdoors per week\n    wellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n    density\nPopulation density of local authority area (number of people per square km)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/LAAwellbeing.csv\")\n\nmod1 &lt;- lmer(wellbeing ~ density + outdoor_time + \n       (1 + outdoor_time | laa), \n       data = df) \n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: wellbeing ~ density + outdoor_time + (1 + outdoor_time | laa)\n   Data: df\n\nREML criterion at convergence: 863.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.25154 -0.55979  0.00415  0.62341  1.95652 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr\n laa      (Intercept)  53.1785  7.2924       \n          outdoor_time  0.1095  0.3309   0.14\n Residual              21.3433  4.6199       \nNumber of obs: 132, groups:  laa, 20\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)  39.123896   2.289384  17.089\ndensity      -0.002121   0.002234  -0.949\noutdoor_time  0.216197   0.102358   2.112\n\nCorrelation of Fixed Effects:\n            (Intr) densty\ndensity     -0.445       \noutdoor_tim -0.291  0.025\n\n\neaster egg: check for influential people!\n\n\n\n\n\n\n\n\n\nwork patterns and mental wellbeing\n\n\n\n\n\nThe “Wellbeing in Work” dataset contains information on employee wellbeing, assessed at baseline (start of study), 12 months post, 24 months post, and 36 months post. over the course of 36 months. Participants were randomly assigned to one of three employment conditions:\n\ncontrol: No change to employment. Employees continue at 5 days a week, with standard allocated annual leave quota.\n\nunlimited_leave : Employees were given no limit to their annual leave, but were still expected to meet required targets as specified in their job description.\nfourday_week: Employees worked a 4 day week for no decrease in pay, and were still expected to meet required targets as specified in their job description.\n\nThe researchers have two main questions: Overall, did the participants’ wellbeing stay the same or did it change? Did the employment condition groups differ in the how wellbeing changed over the assessment period?\nDATASET: https://uoepsy.github.io/data/wellbeingwork3.rda\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ID\nParticipant ID\n    TimePoint\nTimepoint (0 = baseline, 1 = 12 months, 2 = 24 months, 3 = 36 months)\n    Condition\nEmployment Condition ('control' = 5 day week, 28 days of leave. 'unlimited_leave' = 5 days a week, unlimited leave. 'fourday_week' = 4 day week, 28 days of leave)\n    Wellbeing\nWellbeing score (Warwick Edinburgh Mental Wellbeing Scale). Range 15 - 75, with higher scores indicating better mental wellbeing\n  \n  \n  \n\n\n\n\n\n\n\n\n\nload(url(\"https://uoepsy.github.io/data/wellbeingwork3.rda\"))\n\nmod1 &lt;- lmer(Wellbeing~TimePoint+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod2 &lt;- lmer(Wellbeing~TimePoint+Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nmod3 &lt;- lmer(Wellbeing~TimePoint*Condition+(1+TimePoint|ID), \n             data = wellbeingwork3)\nanova(mod1,mod2,mod3)\n\nData: wellbeingwork3\nModels:\nmod1: Wellbeing ~ TimePoint + (1 + TimePoint | ID)\nmod2: Wellbeing ~ TimePoint + Condition + (1 + TimePoint | ID)\nmod3: Wellbeing ~ TimePoint * Condition + (1 + TimePoint | ID)\n     npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod1    6 4171.7 4199.2 -2079.8   4159.7                         \nmod2    8 4164.3 4200.9 -2074.2   4148.3 11.393  2   0.003358 ** \nmod3   10 4144.6 4190.4 -2062.3   4124.6 23.711  2  7.098e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Wellbeing ~ TimePoint * Condition + (1 + TimePoint | ID)\n   Data: wellbeingwork3\n\nREML criterion at convergence: 4127.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02242 -0.56063 -0.01029  0.59140  3.09484 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept)  0.8077  0.8987        \n          TimePoint    3.8622  1.9653   -0.71\n Residual             12.3969  3.5209        \nNumber of obs: 720, groups:  ID, 180\n\nFixed effects:\n                                   Estimate Std. Error t value\n(Intercept)                        38.35167    0.39761  96.456\nTimePoint                          -0.02333    0.32511  -0.072\nConditionunlimited_leave           -0.01833    0.56230  -0.033\nConditionfourday_week              -0.26000    0.56230  -0.462\nTimePoint:Conditionunlimited_leave  1.35667    0.45977   2.951\nTimePoint:Conditionfourday_week     2.28167    0.45977   4.963\n\nCorrelation of Fixed Effects:\n              (Intr) TimPnt Cndtnn_ Cndtnf_ TmPnt:Cndtnn_\nTimePoint     -0.642                                     \nCndtnnlmtd_   -0.707  0.454                              \nCndtnfrdy_w   -0.707  0.454  0.500                       \nTmPnt:Cndtnn_  0.454 -0.707 -0.642  -0.321               \nTmPnt:Cndtnf_  0.454 -0.707 -0.321  -0.642   0.500       \n\n\n\n\n\n\n\n\n\n\n\nevaluating an intervention to reduce adolescent aggressive behaviours\n\n\n\n\n\nIn 2010 A US state’s commissioner for education was faced with growing community concern about rising levels of adolescent antisocial behaviours.\nAfter a series of focus groups, the commissioner approved the trialing of an intervention in which yearly Parent Management Training (PMT) group sessions were offered to the parents of a cohort of students entering 10 different high schools. Every year, the parents were asked to fill out an informant-based version of the Aggressive Behaviour Scale (ABS), measuring verbal and physical abuse, socially inappropriate behavior, and resisting care. Where possible, the same parents were followed up throughout the child’s progression through high school. Alongside this, parents from a cohort of students entering 10 further high schools in the state were recruited to also complete the same informant-based ABS, but were not offered the PMT group sessions.\nThe commissioner has two main questions: Does the presentation of aggressive behaviours increase as children enter the secondary school system? If so, is there any evidence for the effectiveness of Parent Management Training (PMT) group sessions in curbing the rise of aggressive behaviors during a child’s transition into the secondary school system?\nDATASET: https://uoepsy.github.io/data/abs_intervention.csv\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    schoolid\nSchool Name\n    ppt\nParticipant Identifier\n    age\nAge (years)\n    interv\nWhether or not parents attended Parent Management Training (PMT) group sessions (0 = No, 1 = Yes)\n    ABS\nAggressive Behaviours Scale. Measures verbal and physical abuse, socially inappropriate behavior, and resisting care. Scores range from 0 to 100, with higher scores indicating more aggressive behaviours.\n  \n  \n  \n\n\n\n\n\n\n\n\nCheck here: https://uoepsy.github.io/dapr3/2324/lectures/dapr3_2324_05b_rq2mod.html#4\n\n\n\n\n\n\n\n\n\nthe influence of music on driving speeds\n\n\n\n\n\nThese data are simulated to represent data from a fake experiment, in which participants were asked to drive around a route in a 30mph zone. Each participant completed the route 3 times (i.e. “repeated measures”), but each time they were listening to different audio (either speech, classical music or rap music). Their average speed across the route was recorded. This is a fairly simple design, that we might use to ask “how is the type of audio being listened to associated with driving speeds?”\nThe data are available at https://uoepsy.github.io/data/drivingmusicwithin.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    pid\nParticipant Identifier\n    speed\nAvg Speed Driven on Route (mph)\n    music\nMusic listened to while driving (classical music / rap music / spoken word)\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/drivingmusicwithin.csv\")\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each music, so we’re overfitting\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 + music | pid), \n             data = df)\n\nError: number of observations (=150) &lt;= number of random effects (=150) for term (1 + music | pid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable\n\n\n\nmod1 &lt;- lmer(speed ~ music + \n               (1 | pid), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: speed ~ music + (1 | pid)\n   Data: df\n\nREML criterion at convergence: 895.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.47567 -0.56300 -0.01549  0.54691  2.42387 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 20.32    4.508   \n Residual             13.54    3.679   \nNumber of obs: 150, groups:  pid, 50\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  34.2456     0.8229  41.615\nmusicrap     -5.7905     0.7358  -7.869\nmusicspeech  -5.2048     0.7358  -7.073\n\nCorrelation of Fixed Effects:\n            (Intr) muscrp\nmusicrap    -0.447       \nmusicspeech -0.447  0.500\n\n\n\n\n\n\n\n\n\n\n\nCBT and stress levels\n\n\n\n\n\nThese data are simulated to represent data from 50 participants, each measured at 3 different time-points (pre, during, and post) on a measure of stress. Participants were randomly allocated such that half received some cognitive behavioural therapy (CBT) treatment, and half did not. This study is interested in assessing whether the two groups (control vs treatment) differ in how stress changes across the 3 time points.\nThe data are available at https://uoepsy.github.io/data/stressint.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    stress\nStress (range 0 to 100)\n    time\nTime (pre/post/during)\n    group\nWhether participant is in the CBT group or control group\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/stressint.csv\")\ndf$time &lt;- factor(df$time, levels=c(\"Pre\",\"During\",\"Post\"))\n\nTemptation is to fit the below, but it won’t work, because each pid has only 1 obs for each music, so we’re overfitting\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 + time|ppt), \n             data = df)\n\nError: number of observations (=150) &lt;= number of random effects (=150) for term (1 + time | ppt); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable\n\n\n\nmod1 &lt;- lmer(stress ~ time*group + \n               (1 |ppt), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: stress ~ time * group + (1 | ppt)\n   Data: df\n\nREML criterion at convergence: 1096.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.83946 -0.53326 -0.05639  0.53766  2.31546 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ppt      (Intercept) 184.82   13.595  \n Residual              43.26    6.577  \nNumber of obs: 150, groups:  ppt, 50\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)                 62.840      3.020  20.805\ntimeDuring                  -3.200      1.860  -1.720\ntimePost                    -6.760      1.860  -3.634\ngroupTreatment               1.920      4.272   0.449\ntimeDuring:groupTreatment  -21.120      2.631  -8.028\ntimePost:groupTreatment    -28.440      2.631 -10.810\n\nCorrelation of Fixed Effects:\n            (Intr) tmDrng timPst grpTrt tmDr:T\ntimeDuring  -0.308                            \ntimePost    -0.308  0.500                     \ngroupTrtmnt -0.707  0.218  0.218              \ntmDrng:grpT  0.218 -0.707 -0.354 -0.308       \ntmPst:grpTr  0.218 -0.354 -0.707 -0.308  0.500\n\n\n\n\n\n\n\n\n\n\n\nerm.. I don’t believe you\n\n\n\n\n\nThese data are simulated to represent data from 30 participants who took part in an experiment designed to investigate whether fluency of speech influences how believable an utterance is perceived to be.\nEach participant listened to the same 20 statements, with 10 being presented in fluent speech, and 10 being presented with a disfluency (an “erm, …”). Fluency of the statements was counterbalanced such that 15 participants heard statements 1 to 10 as fluent and 11 to 20 as disfluent, and the remaining 15 participants heard statements 1 to 10 as disfluent, and 11 to 20 as fluent. The order of the statements presented to each participant was random. Participants rated each statement on how believable it is on a scale of 0 to 100.\nThe data are available at https://uoepsy.github.io/data/erm_belief.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    trial_n\nTrial number\n    sentence\nStatement identifier\n    condition\nCondition (fluent v disfluent)\n    belief\nbelief rating (0-100)\n    statement\nStatement\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/erm_belief.csv\")\n# make trial_n numeric\ndf$trial_n &lt;- as.numeric(gsub(\"trial_\",\"\",df$trial_n))\n# relevel condition:\ndf$condition &lt;- factor(df$condition, levels=c(\"fluent\",\"disfluent\"))\n\n# having (trial_n | ppt) doesn't converge, so simplify to:\nmod1 &lt;- lmer(belief ~ trial_n + condition + \n               (1 | ppt) + \n               (1 + condition | statement), \n             data = df)\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: belief ~ trial_n + condition + (1 | ppt) + (1 + condition | statement)\n   Data: df\n\nREML criterion at convergence: 3732.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7869 -0.6440 -0.0320  0.6498  3.4210 \n\nRandom effects:\n Groups    Name               Variance Std.Dev. Corr \n ppt       (Intercept)         0.7144  0.8452        \n statement (Intercept)         6.0925  2.4683        \n           conditiondisfluent  2.8453  1.6868   -0.09\n Residual                     26.3688  5.1351        \nNumber of obs: 600, groups:  ppt, 30; statement, 20\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)        49.80050    0.74668  66.696\ntrial_n            -0.04966    0.03716  -1.337\nconditiondisfluent -2.02893    0.56469  -3.593\n\nCorrelation of Fixed Effects:\n            (Intr) tril_n\ntrial_n     -0.503       \ncndtndsflnt -0.227 -0.051\n\n\neaster egg: plot your ranefs\n\ndotplot.ranef.mer(ranef(mod1))$statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndomain differences in cognitive aging\n\n\n\n\n\nThese data are simulated to represent a large scale international study of cognitive aging, for which data from 17 research centers has been combined. The study team are interested in whether different cognitive domains have different trajectories as people age. Do all cognitive domains decline at the same rate? Do some decline more steeply, and some less? The literature suggests that scores on cognitive ability are predicted by educational attainment, so they would like to control for this.\nEach of the 17 research centers recruited a minimum of 14 participants (Median = 21, Range 14-29) at age 48, and recorded their level of education (in years). Participants were then tested on 5 cognitive domains: processing speed, spatial visualisation, memory, reasoning, and vocabulary. Participants were contacted for follow-up on a further 9 occasions (resulting in 10 datapoints for each participant), and at every follow-up they were tested on the same 5 cognitive domains. Follow-ups were on average 3 years apart (Mean = 3, SD = 0.8).\nThe data are available at https://uoepsy.github.io/data/cogdecline.csv.\n\n\n\n\n\n\n  \n    \n      variable\n      description\n    \n  \n  \n    cID\nCenter ID\n    pptID\nParticipant Identifier\n    educ\nEducational attainment (years of education)\n    age\nAge at visit (years)\n    processing_speed\nScore on Processing Speed domain task\n    spatial_visualisation\nScore on Spatial Visualisation domain task\n    memory\nScore on Memory domain task\n    reasoning\nScore on Reasoning domain task\n    vocabulary\nScore on Vocabulary domain task\n  \n  \n  \n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://uoepsy.github.io/data/cogdecline.csv\")\n\n# reshape\ndf &lt;- df |&gt; pivot_longer(processing_speed:vocabulary,\n                   names_to = \"domain\",\n                   values_to = \"score\")\n\n# recenter age and educ\ndf$age &lt;- df$age - 48\ndf$educ &lt;- df$educ - min(df$educ)\n\nthis won’t converge:\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 + educ + age * domain | cID) + \n               (1 + age * domain | cID:pptID),\n             data = df)\n\n\nmod1 &lt;- lmer(score ~ educ + age * domain + \n               (1 | cID) + \n               (1 + age | cID:pptID) +\n               (1 | cID:pptID:domain),\n             data = df,\n             control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: score ~ educ + age * domain + (1 | cID) + (1 + age | cID:pptID) +  \n    (1 | cID:pptID:domain)\n   Data: df\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 82497\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7903 -0.6569  0.0085  0.6598  3.9712 \n\nRandom effects:\n Groups           Name        Variance Std.Dev. Corr\n cID:pptID:domain (Intercept) 0.06134  0.2477       \n cID:pptID        (Intercept) 1.74032  1.3192       \n                  age         0.01047  0.1023   0.26\n cID              (Intercept) 0.03628  0.1905       \n Residual                     4.01099  2.0027       \nNumber of obs: 18950, groups:  cID:pptID:domain, 1895; cID:pptID, 379; cID, 17\n\nFixed effects:\n                                 Estimate Std. Error t value\n(Intercept)                     -1.203378   0.240409  -5.006\neduc                             0.293357   0.036099   8.126\nage                             -0.024486   0.006466  -3.787\ndomainprocessing_speed           0.080018   0.087180   0.918\ndomainreasoning                  0.063089   0.087180   0.724\ndomainspatial_visualisation      0.020739   0.087180   0.238\ndomainvocabulary                -0.779985   0.087180  -8.947\nage:domainprocessing_speed      -0.005106   0.005326  -0.959\nage:domainreasoning             -0.001689   0.005326  -0.317\nage:domainspatial_visualisation -0.001290   0.005326  -0.242\nage:domainvocabulary             0.064308   0.005326  12.074\n\nCorrelation of Fixed Effects:\n            (Intr) educ   age    dmnpr_ dmnrsn dmnsp_ dmnvcb ag:dmnp_ ag:dmnr\neduc        -0.904                                                           \nage         -0.063  0.000                                                    \ndmnprcssng_ -0.181  0.000  0.339                                             \ndomainrsnng -0.181  0.000  0.339  0.500                                      \ndmnsptl_vsl -0.181  0.000  0.339  0.500  0.500                               \ndomanvcblry -0.181  0.000  0.339  0.500  0.500  0.500                        \nag:dmnprcs_  0.149  0.000 -0.412 -0.824 -0.412 -0.412 -0.412                 \nag:dmnrsnng  0.149  0.000 -0.412 -0.412 -0.824 -0.412 -0.412  0.500          \nag:dmnsptl_  0.149  0.000 -0.412 -0.412 -0.412 -0.824 -0.412  0.500    0.500 \nag:dmnvcblr  0.149  0.000 -0.412 -0.412 -0.412 -0.412 -0.824  0.500    0.500 \n            ag:dmns_\neduc                \nage                 \ndmnprcssng_         \ndomainrsnng         \ndmnsptl_vsl         \ndomanvcblry         \nag:dmnprcs_         \nag:dmnrsnng         \nag:dmnsptl_         \nag:dmnvcblr  0.500"
  },
  {
    "objectID": "07ex.html",
    "href": "07ex.html",
    "title": "Week 7 Exercises: PCA & EFA",
    "section": "",
    "text": "New packages\nWe’re going to be needing some different packages this week (no more lme4!).\nMake sure you have these packages installed:\n\npsych\n\nGPArotation\n\ncar"
  },
  {
    "objectID": "07ex.html#footnotes",
    "href": "07ex.html#footnotes",
    "title": "Week 7 Exercises: PCA & EFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎"
  },
  {
    "objectID": "08ex.html",
    "href": "08ex.html",
    "title": "Week 8 Exercises: CFA",
    "section": "",
    "text": "New packages\nMake sure you have these packages installed:\n\nlavaan\nsemPlot"
  },
  {
    "objectID": "08ex.html#footnotes",
    "href": "08ex.html#footnotes",
    "title": "Week 8 Exercises: CFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nor in logistic regression, “what is the likelihood of observing the outcome \\(y\\) given our model parameters?”↩︎"
  },
  {
    "objectID": "09ex.html",
    "href": "09ex.html",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "How does Path Analysis work?\n\n\n\n\n\nThe logic behind path analysis is to estimate a system of equations that best reproduce the covariance structure that we see in the data.\n\nWe specify our theoretical model of the world as a system of paths between variables that represent which variables influence which other variables. This is simply a diagramatic way of specifying a set of equations.\n\nA single headed arrow from \\(X \\rightarrow Y\\) indicates that Y “listens to” X - if X changes, then Y will change accordingly (but not vice versa)\nA double headed arrow between \\(X \\leftrightarrow Y\\) indicates that these two variables are related, not because one causes the other, but because there is some mechanism outside of our model that results in these two variables being associated.\n\nWe collect data on the relevant variables and we observe a covariance matrix (i.e. how each variable covaries with every other variable).\nWe fit our model to the data, and evaluate how well our theoretical model (the estimated values for our path coefficients) can reproduce the observed covariance matrix.\n\n\nTerminology\n\nExogenous variables are a bit like what we have been describing with words like “independent variable” or “predictor”. In a path diagram, they have no arrows coming into them from other variables in the system, but have paths going out to other variables.\n\nEndogenous variables are more like the “outcome”/“dependent”/“response” variables we are used to. They have some arrow coming into them from another variable in the system (and may also - but not necessarily - have paths going out from them)."
  },
  {
    "objectID": "09ex.html#section",
    "href": "09ex.html#section",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "Option A\n\n\n\n\n\n\n\n\n\n\n\nOption B"
  },
  {
    "objectID": "09ex.html#section-1",
    "href": "09ex.html#section-1",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "We now have two predictors, one mediator and one outcome (and two indirect effects, one for each predictor). We can represent this in two lines: one where we specify academic performance as the outcome variable and one where we specify teacher relationships (the mediator) as the outcome variable.\n\nmodel2 &lt;- '\n    Acad ~ c_A*Agg + c_NA*Non_agg + b*Teach_r\n    Teach_r ~ a_A*Agg + a_NA*Non_agg\n    \n    #indirect effect for aggressive conduct problems\n    ind_agg := a_A*b \n    #indirect effect for non-aggressive conduct problems\n    ind_nonagg := a_NA*b \n    \n    # total effects\n    tot_agg := a_A*b + c_A\n    tot_nonagg := a_NA*b + c_NA\n'"
  },
  {
    "objectID": "09ex.html#footnotes",
    "href": "09ex.html#footnotes",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif we were wanting covariances, and we didn’t have standardised variables, then these would have to be multiplied by the corresponding variance estimates for the variables↩︎\nYou can’t go out of one arrow-head into another arrow-head. We can go heads-to-tails, or tails-to-heads, not heads-heads↩︎\nthe lavaan project↩︎\nthere are methods that attempt to “block” a mediator, or to manipulate X in multiple ways in order to increase or decrease the mediators’ effect. If you’re interested see Design approaches to experimental mediation, Pirlott & MacKinnon 2016↩︎\nNote that the model fitted with sem() provides \\(Z\\) values instead of the \\(t\\)-values in regression models. This is because sem() fits models with maximum likelihood thereby assuming a reasonably large sample size.↩︎\nread y~m|x as y~m controlling for x↩︎\nsometimes referred to as “steps approach”↩︎"
  },
  {
    "objectID": "09exWRIGHT.html",
    "href": "09exWRIGHT.html",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "How does Path Analysis work?\n\n\n\n\n\nThe logic behind path analysis is to estimate a system of equations that best reproduce the covariance structure that we see in the data.\n\nWe specify our theoretical model of the world as a system of paths between variables that represent which variables influence which other variables. This is simply a diagramatic way of specifying a set of equations.\n\nA single headed arrow from \\(X \\rightarrow Y\\) indicates that Y “listens to” X - if X changes, then Y will change accordingly (but not vice versa)\nA double headed arrow between \\(X \\leftrightarrow Y\\) indicates that these two variables are related, not because one causes the other, but because there is some mechanism outside of our model that results in these two variables being associated.\n\nWe collect data on the relevant variables and we observe a covariance matrix (i.e. how each variable covaries with every other variable).\nWe fit our model to the data, and evaluate how well our theoretical model (the estimated values for our path coefficients) can reproduce the observed covariance matrix.\n\n\nTerminology\n\nExogenous variables are a bit like what we have been describing with words like “independent variable” or “predictor”. In a path diagram, they have no arrows coming into them from other variables in the system, but have paths going out to other variables.\n\nEndogenous variables are more like the “outcome”/“dependent”/“response” variables we are used to. They have some arrow coming into them from another variable in the system (and may also - but not necessarily - have paths going out from them)."
  },
  {
    "objectID": "09exWRIGHT.html#section",
    "href": "09exWRIGHT.html#section",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "Option A\n\n\n\n\n\n\n\n\n\n\n\nOption B"
  },
  {
    "objectID": "09exWRIGHT.html#section-1",
    "href": "09exWRIGHT.html#section-1",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "",
    "text": "We now have two predictors, one mediator and one outcome (and two indirect effects, one for each predictor). We can represent this in two lines: one where we specify academic performance as the outcome variable and one where we specify teacher relationships (the mediator) as the outcome variable.\n\nmodel2 &lt;- '\n    Acad ~ c_A*Agg + c_NA*Non_agg + b*Teach_r\n    Teach_r ~ a_A*Agg + a_NA*Non_agg\n    \n    #indirect effect for aggressive conduct problems\n    ind_agg := a_A*b \n    #indirect effect for non-aggressive conduct problems\n    ind_nonagg := a_NA*b \n    \n    # total effects\n    tot_agg := a_A*b + c_A\n    tot_nonagg := a_NA*b + c_NA\n'"
  },
  {
    "objectID": "09exWRIGHT.html#footnotes",
    "href": "09exWRIGHT.html#footnotes",
    "title": "Week 9 Exercises: Path Analysis & Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif we were wanting covariances, and we didn’t have standardised variables, then these would have to be multiplied by the corresponding variance estimates for the variables↩︎\nYou can’t go out of one arrow-head into another arrow-head. We can go heads-to-tails, or tails-to-heads, not heads-heads↩︎\nthe lavaan project↩︎\nthere are methods that attempt to “block” a mediator, or to manipulate X in multiple ways in order to increase or decrease the mediators’ effect. If you’re interested see Design approaches to experimental mediation, Pirlott & MacKinnon 2016↩︎\nNote that the model fitted with sem() provides \\(Z\\) values instead of the \\(t\\)-values in regression models. This is because sem() fits models with maximum likelihood thereby assuming a reasonably large sample size.↩︎\nread y~m|x as y~m controlling for x↩︎\nsometimes referred to as “steps approach”↩︎"
  },
  {
    "objectID": "10ex.html",
    "href": "10ex.html",
    "title": "Week 10 Exercises: Structural Equation Modelling (SEM)",
    "section": "",
    "text": "Measurement Error and the need for SEM\n\n\n\n\n\nYou have probably heard the term “Structural Equation Modelling (SEM)” for a few weeks now, but we haven’t been very clear on what exactly it is. Is it CFA? Is it Path Analysis? In fact it is both - it is the overarching framework of which CFA and Path Analysis are just particular cases. The beauty comes in when we put the CFA and Path Analysis approaches together.\nPath analysis, as we saw last week, offers a way of specifying and evaluating a structural model, in which variables relate to one another in various ways, via different (sometimes indirect) paths. Common models like our old friend multiple regression can be expressed in a Path Analysis framework.\nFactor Analysis, on the other hand, brings something absolutely crucial to the table - it allows us to mitigate some of the problems which are associated with measurement error by specifying the existence of some latent variable which is measured via some observed variables. No question can perfectly measure someone’s level of “anxiety”, but if we take a set of 10 carefully chosen questions, we can consider the shared covariance between those 10 questions to represent the construct that is common between all of them (they all ask, in different ways, about “anxiety”), also modeling the unique error with which each individual question fails to perfectly represent the entire construct.\nCombine them and we can reap the rewards of having both a structural model and a measurement model. The measurement model is our specification between the items we directly observed, and the latent variables of which we consider these items to be manifestations. The structural model is our specified model of the relationships between the latent variables.\n\n\n\n\n\nFigure 1: SEM diagram. Measurement model in orange, Structural model in purple\n\n\n\n\n\nYou can’t test the structural model if the measurement model is bad\nIf you test the relationships between a set of latent factors, and they are not reliably measured by the observed items, then this error propagates up to influence the fit of the structural model.\nTo test the measurement model, it is typical to saturate the structural model (i.e., allow all the latent variables to correlate with one another). This way any misfit is due to the measurement model only.\nAlternatively, we can fit individual CFA models for each construct and assess their fit (making any reasonable adjustments if necessary) prior to then fitting the full SEM.\n\n\n\n\n\nExercising Exercises\n\nDataset: tpb2\nThe “Theory of Planned Behaviour” is a theory about why people engage in physical activity (i.e. why people exercise).\nThe theory is represented in the diagram in Figure 2 (only the latent variables and not the measured items are shown). Attitudes refer to the extent to which a person has a favourable view of exercising; subjective norms refer to whether they believe others whose opinions they care about believe exercise to be a good thing; and perceived behavioural control refers to the extent to which they believe exercising is under their control. Intentions refer to whether a person intends to exercise and behaviour is a measure of the extent to which they exercised. Each construct is measured using four items.\n\n\n\n\n\nFigure 2: Theory of planned behaviour (latent variables only)\n\n\n\n\nThe data are available either:\n\nas a .RData file: https://uoepsy.github.io/data/tpb2.Rdata\nas a .txt file: https://uoepsy.github.io/data/tpb2.txt\n\n\n\n\n\n\n\nTable 1:  Data Dictionary for TPB data \n  \n    \n      variable\n      question\n    \n  \n  \n    SN1\nWhen I think about people whose opinions matter to me, I believe they value and support regular exercise\n    SN2\nI feel pressure from those I care about to exercise regularly\n    SN3\nMost people who are important to me approve of my exercising\n    SN4\nMost people like me exercise regularly\n    PBC1\nMy exercise routine is up to me and only me\n    PBC2\nI am confident that if I want to then I can exercise regularly\n    PBC3\nI believe I have the ability to overcome any obstacles that may prevent me from exercising regularly.\n    PBC4\nI feel capable of sticking to a consistent exercise schedule, even when faced with challenges or distractions\n    attitude1\nI see exercising as an enjoyable and rewarding activity.\n    attitude2\nI believe that exercising contributes positively to my overall well-being and health.\n    attitude3\nI view exercising as an important part of maintaining a healthy lifestyle.\n    attitude4\nI feel energized and invigorated after engaging in physical exercise.\n    int1\nI am determined to take concrete steps towards establishing a consistent exercise habit\n    int2\nI intend to exercise for at least 20 minutes, three times per week for the next three months.\n    int3\nI have made a firm decision to prioritize exercise and allocate time for it in my schedule\n    int4\nI intend to be in shape within the next three months.\n    int5\nI am committed to incorporating regular exercise into my weekly routine.\n    beh1\nI currently engage in physical activity for at least 20 minutes, three times per week, as recommended.\n    beh2\nI already allocate time for exercise in my weekly schedule and adhere to it regularly.\n    beh3\nI track my exercise sessions and ensure I meet my weekly goals\n    beh4\nI do not currently exercise enough\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nLoad in the various packages you will probably need (tidyverse, lavaan), and read in the data using the appropriate function.\nWe’ve given you .csv files for a long time now, but it’s good to be prepared to encounter all sorts of weird filetypes. Can you successfully read in from both types of data?\n\n\n\n\nEither one or the other of:\n\nlibrary(tidyverse)\nlibrary(lavaan)\nload(url(\"https://uoepsy.github.io/data/tpb2.Rdata\"))\n\nTPB_data &lt;- read.table(\"https://uoepsy.github.io/data/tpb2.txt\", header = TRUE, sep = \"\\t\")\n\n\n\n\n\nQuestion 2\n\n\nTest separate one-factor models for each construct.\nAre the measurement models satisfactory? (check their fit measures).\n\n\n\n\nHere we specify our one factor CFA model for attitudes:\n\natt_mod &lt;- \"\n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  \"\n\nAnd we estimate the model using cfa()\n\natt_mod.est &lt;- cfa(att_mod, data=TPB_data, std.lv = TRUE)\n\nLet’s first inspect the fit measures:\n\nfitmeasures(att_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n      rmsea        srmr         tli         cfi \n0.007237593 0.010669452 0.999299418 0.999766473 \n\n\nOur fit is good: RMSEA&lt;.05, SRMR&lt;.05, TLI&gt;0.95 and CFI&gt;.95.\nWe should also check that all loadings are significant and \\(&gt;|.30|\\).\nTo save space I am going to not show the entire summary output here, but just pull out the parameter estimates:\n\nparameterestimates(att_mod.est)\n\n        lhs op       rhs   est    se      z pvalue ci.lower ci.upper\n1       att =~ attitude1 0.682 0.051 13.355      0    0.582    0.782\n2       att =~ attitude2 0.617 0.045 13.656      0    0.528    0.705\n3       att =~ attitude3 0.681 0.049 13.928      0    0.585    0.777\n4       att =~ attitude4 0.644 0.048 13.415      0    0.550    0.738\n5 attitude1 ~~ attitude1 1.097 0.069 15.883      0    0.961    1.232\n6 attitude2 ~~ attitude2 0.837 0.054 15.498      0    0.731    0.943\n7 attitude3 ~~ attitude3 0.959 0.063 15.121      0    0.835    1.084\n8 attitude4 ~~ attitude4 0.966 0.061 15.809      0    0.847    1.086\n9       att ~~       att 1.000 0.000     NA     NA    1.000    1.000\n\n\nThey all look good!\n\n\n\n\nFollowing the same logic as for the Attitudes, let’s fit the CFA for Subjective norms. Again, all fit measures are very good, and loadings are all significant at greater than 0.3.\n\nsn_mod &lt;- \"\n  SubjN =~ SN1 + SN2 + SN3 + SN4\n  \"\n\nsn_mod.est &lt;- cfa(sn_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(sn_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.03058032 0.01444391 0.98605752 0.99535251 \n\nparameterestimates(sn_mod.est)\n\n    lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1 SubjN =~   SN1 0.644 0.048 13.524      0    0.550    0.737\n2 SubjN =~   SN2 0.585 0.049 11.923      0    0.489    0.681\n3 SubjN =~   SN3 0.584 0.044 13.266      0    0.498    0.670\n4 SubjN =~   SN4 0.615 0.049 12.578      0    0.519    0.711\n5   SN1 ~~   SN1 0.850 0.058 14.560      0    0.735    0.964\n6   SN2 ~~   SN2 1.041 0.062 16.736      0    0.919    1.163\n7   SN3 ~~   SN3 0.749 0.050 14.985      0    0.651    0.847\n8   SN4 ~~   SN4 0.985 0.062 15.974      0    0.864    1.106\n9 SubjN ~~ SubjN 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\nAll good with Perceived Behavioural Control!\nAlmost too good (TLI&gt;1, and RMSEA is coming out at exactly 0!), but this is most probably because of this being fake data.\nWhen data is simulated based on a specific model, then fitting that same model structure to the data will obviously fit extremely well! s\n\npbc_mod &lt;- \"\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  \"\n\npbc_mod.est &lt;- cfa(pbc_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(pbc_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n      rmsea        srmr         tli         cfi \n0.000000000 0.003369084 1.010079110 1.000000000 \n\nparameterestimates(pbc_mod.est)\n\n   lhs op  rhs   est    se      z pvalue ci.lower ci.upper\n1  PBC =~ PBC1 0.696 0.043 16.258      0    0.612    0.780\n2  PBC =~ PBC2 0.627 0.038 16.346      0    0.551    0.702\n3  PBC =~ PBC3 0.592 0.041 14.619      0    0.513    0.672\n4  PBC =~ PBC4 0.676 0.045 15.058      0    0.588    0.764\n5 PBC1 ~~ PBC1 0.765 0.052 14.794      0    0.663    0.866\n6 PBC2 ~~ PBC2 0.609 0.041 14.677      0    0.527    0.690\n7 PBC3 ~~ PBC3 0.768 0.046 16.601      0    0.678    0.859\n8 PBC4 ~~ PBC4 0.919 0.057 16.184      0    0.808    1.030\n9  PBC ~~  PBC 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\nUh-oh, it’s looking less good with Intentions.\nThe loadings all look okay, but the fit indices aren’t great\n\nint_mod &lt;- \"\n  intent =~ int1 + int2 + int3 + int4 + int5\n  \"\n\nint_mod.est &lt;- cfa(int_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(int_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.14128950 0.05266866 0.84561107 0.92280553 \n\nparameterestimates(int_mod.est)\n\n      lhs op    rhs   est    se      z pvalue ci.lower ci.upper\n1  intent =~   int1 0.698 0.043 16.363      0    0.614    0.781\n2  intent =~   int2 0.801 0.035 23.173      0    0.733    0.869\n3  intent =~   int3 0.684 0.039 17.407      0    0.607    0.761\n4  intent =~   int4 0.868 0.038 22.746      0    0.793    0.943\n5  intent =~   int5 0.581 0.037 15.518      0    0.508    0.655\n6    int1 ~~   int1 1.046 0.056 18.555      0    0.936    1.157\n7    int2 ~~   int2 0.487 0.036 13.665      0    0.417    0.557\n8    int3 ~~   int3 0.858 0.047 18.108      0    0.765    0.951\n9    int4 ~~   int4 0.614 0.043 14.145      0    0.529    0.699\n10   int5 ~~   int5 0.827 0.044 18.874      0    0.741    0.913\n11 intent ~~ intent 1.000 0.000     NA     NA    1.000    1.000\n\n\nLet’s examine the modification indices:\n\nmodindices(int_mod.est, sort = TRUE)\n\n    lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n17 int2 ~~ int4 97.630  0.414   0.414    0.757    0.757\n13 int1 ~~ int3 50.107  0.270   0.270    0.285    0.285\n16 int2 ~~ int3 21.423 -0.159  -0.159   -0.246   -0.246\n20 int3 ~~ int5 18.787  0.145   0.145    0.172    0.172\n19 int3 ~~ int4 17.657 -0.158  -0.158   -0.217   -0.217\n12 int1 ~~ int2 16.578 -0.148  -0.148   -0.207   -0.207\n14 int1 ~~ int4 10.596 -0.129  -0.129   -0.161   -0.161\n21 int4 ~~ int5 10.532 -0.111  -0.111   -0.156   -0.156\n15 int1 ~~ int5  4.521  0.077   0.077    0.083    0.083\n18 int2 ~~ int5  3.438 -0.058  -0.058   -0.091   -0.091\n\n\nIt looks like correlating the residuals for items int2 and int4 would improve our model. The expected correlation is 0.757, which is fairly large (remember correlations are between -1 and 1).\nNote that the items have a possible theoretical link too, beyond just “intention to exercise”. It looks like both int2 and int4 are specifically about intentions in the next three months. It might make sense that responses to these two items are related more than just representing general ‘intention’.\nWhen we include this covariance, our model fit looks much better!\n\nint_mod &lt;- \"\n  intent =~ int1 + int2 + int3 + int4 + int5\n  int2 ~~ int4\n  \"\n\nint_mod.est &lt;- cfa(int_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(int_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.02753383 0.01475947 0.99413687 0.99765475 \n\nparameterestimates(int_mod.est)\n\n      lhs op    rhs   est    se      z pvalue ci.lower ci.upper\n1  intent =~   int1 0.795 0.044 18.018      0    0.708    0.881\n2  intent =~   int2 0.633 0.039 16.341      0    0.557    0.709\n3  intent =~   int3 0.800 0.041 19.607      0    0.720    0.880\n4  intent =~   int4 0.682 0.043 15.916      0    0.598    0.766\n5  intent =~   int5 0.629 0.039 16.205      0    0.553    0.705\n6    int2 ~~   int4 0.343 0.039  8.748      0    0.266    0.419\n7    int1 ~~   int1 0.902 0.057 15.734      0    0.789    1.014\n8    int2 ~~   int2 0.727 0.044 16.589      0    0.641    0.813\n9    int3 ~~   int3 0.686 0.049 13.927      0    0.589    0.782\n10   int4 ~~   int4 0.902 0.054 16.827      0    0.797    1.007\n11   int5 ~~   int5 0.769 0.045 17.201      0    0.681    0.856\n12 intent ~~ intent 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\nFinally, the behaviour model looks absolutely fine.\nNote that bey4 has a negative loading, which is perfectly okay. In fact, if you look at the items, you’ll notice that this is the only item that is reversed (higher scores on the item reflect less exercising)\n\nbeh_mod &lt;- \"\n  behav =~ beh1 + beh2 + beh3 + beh4\n  \"\n\nbeh_mod.est &lt;- cfa(beh_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(beh_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.02896260 0.01285583 0.99191922 0.99730641 \n\nparameterestimates(beh_mod.est)\n\n    lhs op   rhs    est    se       z pvalue ci.lower ci.upper\n1 behav =~  beh1  0.659 0.045  14.593      0    0.571    0.748\n2 behav =~  beh2  0.735 0.045  16.275      0    0.647    0.824\n3 behav =~  beh3  0.787 0.045  17.331      0    0.698    0.875\n4 behav =~  beh4 -0.724 0.046 -15.626      0   -0.815   -0.633\n5  beh1 ~~  beh1  0.987 0.058  16.962      0    0.873    1.101\n6  beh2 ~~  beh2  0.889 0.058  15.312      0    0.775    1.003\n7  beh3 ~~  beh3  0.819 0.059  13.908      0    0.703    0.934\n8  beh4 ~~  beh4  0.980 0.061  16.029      0    0.860    1.099\n9 behav ~~ behav  1.000 0.000      NA     NA    1.000    1.000\n\n\n\n\n\n\nQuestion 3\n\n\nUsing lavaan syntax, specify the full structural equation model that corresponds to the model in Figure 2. For each construct use the measurement models from the previous question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis involves specifying the measurement models for all the latent variables, and then also specifying the relationships between those latent variables. All in the same model!\n\n\n\n\n\n\n\n\nTPB_model&lt;-'\n  # measurement models  \n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  SN =~ SN1 + SN2 + SN3 + SN4\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  intent =~ int1 + int2 + int3 + int4 + int5\n  beh =~ beh1 + beh2 + beh3 + beh4\n  \n  # covariances between items\n  int2 ~~ int4\n\n  # regressions  \n  beh ~ intent + PBC\n  intent ~ att + SN + PBC\n\n  # covariances between attitudes, SN, and PBC\n  att ~~ SN    \n  att ~~ PBC\n  SN ~~ PBC\n'\n\n\n\n\n\nQuestion 4\n\n\nEstimate and evaluate the model\n\nDoes the model fit well?\n\nAre the hypothesised paths significant?\n\n\n\n\n\nWe can estimate the model using the sem() function.\nAs with cfa(), by default the sem() function will scale the latent variables by fixing the loading of the first item for each latent variable to 1.\n\nTPB_model.est&lt;-sem(TPB_model, data=TPB_data, std.lv=TRUE)\n\nfitmeasures(TPB_model.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n    rmsea      srmr       tli       cfi \n0.0108428 0.0268991 0.9935594 0.9944795 \n\n\nWe can see that the model fits well according to RMSEA, SRMR, TLI and CFI.\nFrom the output below, all of the hypothesised paths in the theory of planned behaviour are statistically significant.\n\nsummary(TPB_model.est, standardized=TRUE)\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           890\n\nModel Test User Model:\n                                                      \n  Test statistic                               198.834\n  Degrees of freedom                               180\n  P-value (Chi-square)                           0.160\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  att =~                                                                \n    attitude1         0.687    0.050   13.854    0.000    0.687    0.550\n    attitude2         0.614    0.044   14.018    0.000    0.614    0.557\n    attitude3         0.662    0.047   13.977    0.000    0.662    0.555\n    attitude4         0.660    0.047   14.140    0.000    0.660    0.561\n  SN =~                                                                 \n    SN1               0.644    0.045   14.213    0.000    0.644    0.572\n    SN2               0.595    0.047   12.573    0.000    0.595    0.505\n    SN3               0.573    0.042   13.637    0.000    0.573    0.548\n    SN4               0.620    0.047   13.202    0.000    0.620    0.531\n  PBC =~                                                                \n    PBC1              0.687    0.041   16.555    0.000    0.687    0.615\n    PBC2              0.617    0.037   16.606    0.000    0.617    0.616\n    PBC3              0.608    0.039   15.412    0.000    0.608    0.575\n    PBC4              0.681    0.044   15.581    0.000    0.681    0.581\n  intent =~                                                             \n    int1              0.648    0.039   16.793    0.000    0.777    0.628\n    int2              0.554    0.034   16.519    0.000    0.664    0.626\n    int3              0.643    0.036   17.876    0.000    0.771    0.670\n    int4              0.586    0.037   15.818    0.000    0.703    0.601\n    int5              0.534    0.034   15.879    0.000    0.641    0.594\n  beh =~                                                                \n    beh1              0.556    0.038   14.461    0.000    0.678    0.568\n    beh2              0.588    0.039   15.203    0.000    0.718    0.600\n    beh3              0.635    0.039   16.208    0.000    0.775    0.646\n    beh4             -0.604    0.040  -15.220    0.000   -0.737   -0.601\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  beh ~                                                                 \n    intent            0.465    0.057    8.166    0.000    0.457    0.457\n    PBC               0.251    0.063    3.967    0.000    0.206    0.206\n  intent ~                                                              \n    att               0.242    0.061    3.986    0.000    0.202    0.202\n    SN                0.335    0.064    5.233    0.000    0.279    0.279\n    PBC               0.338    0.059    5.726    0.000    0.282    0.282\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .int2 ~~                                                               \n   .int4              0.307    0.037    8.403    0.000    0.307    0.397\n  att ~~                                                                \n    SN                0.316    0.050    6.300    0.000    0.316    0.316\n    PBC               0.245    0.049    5.056    0.000    0.245    0.245\n  SN ~~                                                                 \n    PBC               0.275    0.049    5.628    0.000    0.275    0.275\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .attitude1         1.089    0.067   16.231    0.000    1.089    0.697\n   .attitude2         0.840    0.052   16.057    0.000    0.840    0.690\n   .attitude3         0.985    0.061   16.101    0.000    0.985    0.692\n   .attitude4         0.946    0.059   15.925    0.000    0.946    0.685\n   .SN1               0.850    0.055   15.385    0.000    0.850    0.672\n   .SN2               1.030    0.060   17.099    0.000    1.030    0.745\n   .SN3               0.762    0.047   16.061    0.000    0.762    0.699\n   .SN4               0.979    0.059   16.516    0.000    0.979    0.718\n   .PBC1              0.777    0.050   15.682    0.000    0.777    0.622\n   .PBC2              0.621    0.040   15.630    0.000    0.621    0.620\n   .PBC3              0.750    0.045   16.715    0.000    0.750    0.670\n   .PBC4              0.912    0.055   16.576    0.000    0.912    0.663\n   .int1              0.929    0.055   16.918    0.000    0.929    0.606\n   .int2              0.687    0.041   16.675    0.000    0.687    0.609\n   .int3              0.730    0.046   15.843    0.000    0.730    0.551\n   .int4              0.873    0.051   17.115    0.000    0.873    0.639\n   .int5              0.754    0.043   17.618    0.000    0.754    0.648\n   .beh1              0.962    0.056   17.226    0.000    0.962    0.677\n   .beh2              0.915    0.055   16.517    0.000    0.915    0.640\n   .beh3              0.837    0.055   15.238    0.000    0.837    0.582\n   .beh4              0.961    0.058   16.498    0.000    0.961    0.639\n    att               1.000                               1.000    1.000\n    SN                1.000                               1.000    1.000\n    PBC               1.000                               1.000    1.000\n   .intent            1.000                               0.695    0.695\n   .beh               1.000                               0.672    0.672\n\n\n\n\n\n\nQuestion 5\n\n\nExamine the modification indices and expected parameter changes - are there any additional parameters you would consider including?\n\n\n\n\n\nmodindices(TPB_model.est, sort = TRUE) |&gt; head()\n\n     lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n316 int1 ~~ int3 12.167  0.143   0.143    0.174    0.174\n103  PBC =~ int5 11.028  0.151   0.151    0.140    0.140\n303 PBC3 ~~ beh2  8.351  0.095   0.095    0.114    0.114\n336 int4 ~~ beh1  7.886  0.087   0.087    0.095    0.095\n291 PBC2 ~~ int5  7.847  0.076   0.076    0.111    0.111\n263  SN4 ~~ PBC4  6.978 -0.098  -0.098   -0.104   -0.104\n\n\nIn this case, none of the expected parameter changes are large enough that we would consider including any additional parameters\n\n\n\n\nQuestion 6\n\n\nTest the indirect effect of attitudes, subjective norms, and perceived behavioural control on behaviour via intentions.\nRemember, when you fit the model with sem(), use se='bootstrap' to get boostrapped standard errors (it may take a few minutes). When you inspect the model using summary(), get the 95% confidence intervals for parameters with ci = TRUE.\n\n\n\n\nFirst, let’s name the paths in the structural equation model:\n\n\n\n\n\n\n\n\n\nTo test these indirect effects we create new a parameter for each indirect effect:\n\nTPB_model2 &lt;- '\n  # measurement models  \n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  SN =~ SN1 + SN2 + SN3 + SN4\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  intent =~ int1 + int2 + int3 + int4 + int5\n  beh =~ beh1 + beh2 + beh3 + beh4\n  \n  # covariances between items\n  int2 ~~ int4\n\n  # regressions  \n  beh ~ b*intent + PBC\n  intent ~ a1*att + a2*SN + a3*PBC\n\n  # covariances between attitudes, SN, and PBC\n  att ~~ SN    \n  att ~~ PBC\n  SN ~~ PBC\n\n  # indirect effects:  \n  ind1 := a1*b  #indirect effect of attitudes via intentions\n  ind2 := a2*b  #indirect effect of SN via intentions\n  ind3 := a3*b  #indirect effect of PBC via intentions\n'\n\nWhen we estimate the model, we request bootstrapped standard errors:\n\nTPB_model2.est&lt;-sem(TPB_model2, std.lv=TRUE, se='bootstrap', data=TPB_data)\n\nWhen we inspect the model, we request the 95% confidence intervals for parameters:\n\nsummary(TPB_model2.est, ci=TRUE)\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           890\n\nModel Test User Model:\n                                                      \n  Test statistic                               198.834\n  Degrees of freedom                               180\n  P-value (Chi-square)                           0.160\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  att =~                                                                \n    attitude1         0.687    0.049   14.147    0.000    0.585    0.780\n    attitude2         0.614    0.044   13.899    0.000    0.528    0.700\n    attitude3         0.662    0.046   14.327    0.000    0.567    0.745\n    attitude4         0.660    0.045   14.639    0.000    0.566    0.745\n  SN =~                                                                 \n    SN1               0.644    0.046   13.946    0.000    0.552    0.734\n    SN2               0.595    0.046   12.962    0.000    0.501    0.687\n    SN3               0.573    0.044   12.955    0.000    0.487    0.657\n    SN4               0.620    0.048   13.001    0.000    0.524    0.717\n  PBC =~                                                                \n    PBC1              0.687    0.039   17.604    0.000    0.614    0.765\n    PBC2              0.617    0.036   17.152    0.000    0.540    0.683\n    PBC3              0.608    0.040   15.052    0.000    0.524    0.687\n    PBC4              0.681    0.043   15.872    0.000    0.594    0.761\n  intent =~                                                             \n    int1              0.648    0.040   16.116    0.000    0.571    0.728\n    int2              0.554    0.033   16.651    0.000    0.485    0.615\n    int3              0.643    0.038   16.874    0.000    0.566    0.715\n    int4              0.586    0.037   15.918    0.000    0.515    0.655\n    int5              0.534    0.031   17.381    0.000    0.472    0.594\n  beh =~                                                                \n    beh1              0.556    0.039   14.358    0.000    0.477    0.628\n    beh2              0.588    0.039   15.130    0.000    0.512    0.662\n    beh3              0.635    0.039   16.230    0.000    0.558    0.709\n    beh4             -0.604    0.040  -15.181    0.000   -0.684   -0.528\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  beh ~                                                                 \n    intent     (b)    0.465    0.056    8.242    0.000    0.354    0.576\n    PBC               0.251    0.066    3.794    0.000    0.126    0.383\n  intent ~                                                              \n    att       (a1)    0.242    0.066    3.672    0.000    0.116    0.369\n    SN        (a2)    0.335    0.064    5.196    0.000    0.213    0.476\n    PBC       (a3)    0.338    0.062    5.483    0.000    0.228    0.468\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n .int2 ~~                                                               \n   .int4              0.307    0.038    8.153    0.000    0.233    0.383\n  att ~~                                                                \n    SN                0.316    0.050    6.313    0.000    0.216    0.412\n    PBC               0.245    0.050    4.942    0.000    0.148    0.342\n  SN ~~                                                                 \n    PBC               0.275    0.049    5.563    0.000    0.175    0.370\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .attitude1         1.089    0.062   17.481    0.000    0.966    1.204\n   .attitude2         0.840    0.052   16.198    0.000    0.741    0.940\n   .attitude3         0.985    0.060   16.344    0.000    0.862    1.098\n   .attitude4         0.946    0.062   15.353    0.000    0.817    1.061\n   .SN1               0.850    0.055   15.489    0.000    0.742    0.954\n   .SN2               1.030    0.059   17.516    0.000    0.914    1.148\n   .SN3               0.762    0.051   15.082    0.000    0.671    0.858\n   .SN4               0.979    0.060   16.353    0.000    0.860    1.100\n   .PBC1              0.777    0.049   15.821    0.000    0.679    0.873\n   .PBC2              0.621    0.037   16.567    0.000    0.547    0.693\n   .PBC3              0.750    0.042   17.901    0.000    0.671    0.831\n   .PBC4              0.912    0.052   17.498    0.000    0.811    1.015\n   .int1              0.929    0.053   17.564    0.000    0.822    1.030\n   .int2              0.687    0.042   16.534    0.000    0.600    0.771\n   .int3              0.730    0.050   14.687    0.000    0.628    0.830\n   .int4              0.873    0.050   17.561    0.000    0.772    0.970\n   .int5              0.754    0.042   18.084    0.000    0.670    0.836\n   .beh1              0.962    0.053   18.042    0.000    0.857    1.071\n   .beh2              0.915    0.057   16.080    0.000    0.802    1.035\n   .beh3              0.837    0.057   14.574    0.000    0.717    0.947\n   .beh4              0.961    0.061   15.767    0.000    0.837    1.085\n    att               1.000                               1.000    1.000\n    SN                1.000                               1.000    1.000\n    PBC               1.000                               1.000    1.000\n   .intent            1.000                               1.000    1.000\n   .beh               1.000                               1.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind1              0.113    0.032    3.478    0.001    0.055    0.185\n    ind2              0.156    0.035    4.470    0.000    0.091    0.229\n    ind3              0.157    0.034    4.685    0.000    0.099    0.231\n\n\nWe can see that all of the indirect effects are statistically significant at p&lt;.05 as none of the 95% confidence intervals for the coefficients include zero.\n\n\n\n\nQuestion 7\n\n\nWrite up your analysis as if you were presenting the work in academic paper, with brief separate ‘Method’ and ‘Results’ sections\n\n\n\n\nMethod\nWe tested a theory of planned behaviour model of physical activity by fitting a structural equation model in which attitudes, subjective norms, perceived behavioural control, intentions and behaviour were latent variables defined by four items. We first tested the measurement models for each construct by fitting a one-factor CFA model. Latent variable scaling was by fixing the loading of the first item for each construct to 1.\nWithin the SEM, behaviour was regressed on intentions and perceived behavioural control and intentions were regressed on attitudes, subjective norms, and perceived behavioiural control. In addition, attitudes, subjective norms, and perceived behavioural control were allowed to covary. The indirect effects of attitudes, subjective norms and perceived behavioural control on behaviour were calculated as the product of the effect of the relevant predictor on the mediator (intentions) and the effect of the mediator on the outcome. The statistical significance of the indirect effects were evaluated using bootstrapped 95% confidence intervals with 1000 resamples.\nIn all cases models were fit using maximum likelihood estimation and model fit was judged to be good if CFI and TLI were \\(&gt;.95\\) and RMSEA and SRMR were \\(&lt;.05\\). Modification indices and expected parameter changes were inspected to identify any areas of local mis-fit but model modifications were only made if they could be justified on substantive grounds.\nResults\nAll measurement models fit well (CFI and TLI \\(&gt;.95\\) and RMSEA and SRMR \\(&lt;.05\\)) with the exception of the measurement model for intentions. Modification indices suggested the inclusion of residual covariance between two items on the intentions scale (int2 and int4) that both made specific reference to short term intentions. The addition of this parameter resulted in a good fit. The full structural equation model fit well (CFI = 0.99, TLI = 0.99, RMSEA = 0.01, SRMR = 0.03). Unstandardised parameter estimates are provided in Table 2. All of the hypothesised paths were statistically significant at \\(p&lt;.05\\). The significant indirect effects suggested that intentions mediate the effects of attitudes, subjective norms, and perceived behavioural control on behaviour whilst perceived behavioural control also has a direct effect on behaviour. Results thus provide support for a theory of planned behaviour model of physical activity.\n\n\n\n\n\n\nTable 2:  Unstandardised parameter estimates for structural equation model for\na theory of planned behaviour model of physical activity. Note: PBC =\nPerceived Behavioural Control, CI = Confidence Interval \n  \n    \n       \n      Parameter\n      Estimate\n      SE\n      z\n      p\n      95% CI\n    \n  \n  \n    \n      Loadings\n    \n    Attitudes\nattitude1\n0.69\n0.05\n14.15\n&lt;0.001\n[0.59, 0.78]\n    \nattitude2\n0.61\n0.04\n13.90\n&lt;0.001\n[0.53, 0.7]\n    \nattitude3\n0.66\n0.05\n14.33\n&lt;0.001\n[0.57, 0.75]\n    \nattitude4\n0.66\n0.05\n14.64\n&lt;0.001\n[0.57, 0.75]\n    Subjective Norms\nSN1\n0.64\n0.05\n13.95\n&lt;0.001\n[0.55, 0.73]\n    \nSN2\n0.59\n0.05\n12.96\n&lt;0.001\n[0.5, 0.69]\n    \nSN3\n0.57\n0.04\n12.95\n&lt;0.001\n[0.49, 0.66]\n    \nSN4\n0.62\n0.05\n13.00\n&lt;0.001\n[0.52, 0.72]\n    PBC\nPBC1\n0.69\n0.04\n17.60\n&lt;0.001\n[0.61, 0.77]\n    \nPBC2\n0.62\n0.04\n17.15\n&lt;0.001\n[0.54, 0.68]\n    \nPBC3\n0.61\n0.04\n15.05\n&lt;0.001\n[0.52, 0.69]\n    \nPBC4\n0.68\n0.04\n15.87\n&lt;0.001\n[0.59, 0.76]\n    Intentions\nint1\n0.65\n0.04\n16.12\n&lt;0.001\n[0.57, 0.73]\n    \nint2\n0.55\n0.03\n16.65\n&lt;0.001\n[0.49, 0.62]\n    \nint3\n0.64\n0.04\n16.87\n&lt;0.001\n[0.57, 0.72]\n    \nint4\n0.59\n0.04\n15.92\n&lt;0.001\n[0.51, 0.66]\n    \nint5\n0.53\n0.03\n17.38\n&lt;0.001\n[0.47, 0.59]\n    Behaviours\nbeh1\n0.56\n0.04\n14.36\n&lt;0.001\n[0.48, 0.63]\n    \nbeh2\n0.59\n0.04\n15.13\n&lt;0.001\n[0.51, 0.66]\n    \nbeh3\n0.64\n0.04\n16.23\n&lt;0.001\n[0.56, 0.71]\n    \nbeh4\n-0.60\n0.04\n-15.18\n&lt;0.001\n[-0.68, -0.53]\n    \n      Covariances\n    \n    \nint2 with int4\n0.31\n0.04\n8.15\n&lt;0.001\n[0.23, 0.38]\n    \nAttitudes with Subjective Norms\n0.32\n0.05\n6.31\n&lt;0.001\n[0.22, 0.41]\n    \nAttitudes with PBC\n0.25\n0.05\n4.94\n&lt;0.001\n[0.15, 0.34]\n    \nSubjective Norms with PBC\n0.27\n0.05\n5.56\n&lt;0.001\n[0.18, 0.37]\n    \n      Regressions\n    \n    \nBehaviours on Intentions\n0.47\n0.06\n8.24\n&lt;0.001\n[0.35, 0.58]\n    \nBehaviours on PBC\n0.25\n0.07\n3.79\n&lt;0.001\n[0.13, 0.38]\n    \nIntentions on Attitudes\n0.24\n0.07\n3.67\n&lt;0.001\n[0.12, 0.37]\n    \nIntentions on Subjective Norms\n0.33\n0.06\n5.20\n&lt;0.001\n[0.21, 0.48]\n    \nIntentions on PBC\n0.34\n0.06\n5.48\n&lt;0.001\n[0.23, 0.47]\n    \n      Indirect effects\n    \n    \nAttitudes via Intentions\n0.11\n0.03\n3.48\n&lt;0.001\n[0.05, 0.19]\n    \nSubjective Norms via Intentions\n0.16\n0.03\n4.47\n&lt;0.001\n[0.09, 0.23]\n    \nPBC via Intentions\n0.16\n0.03\n4.68\n&lt;0.001\n[0.1, 0.23]"
  },
  {
    "objectID": "10exDUNCAN.html",
    "href": "10exDUNCAN.html",
    "title": "Week 10 Exercises: Structural Equation Modelling (SEM)",
    "section": "",
    "text": "Measurement Error and the need for SEM\n\n\n\n\n\nYou have probably heard the term “Structural Equation Modelling (SEM)” for a few weeks now, but we haven’t been very clear on what exactly it is. Is it CFA? Is it Path Analysis? In fact it is both - it is the overarching framework of which CFA and Path Analysis are just particular cases. The beauty comes in when we put the CFA and Path Analysis approaches together.\nPath analysis, as we saw last week, offers a way of specifying and evaluating a structural model, in which variables relate to one another in various ways, via different (sometimes indirect) paths. Common models like our old friend multiple regression can be expressed in a Path Analysis framework.\nFactor Analysis, on the other hand, brings something absolutely crucial to the table - it allows us to mitigate some of the problems which are associated with measurement error by specifying the existence of some latent variable which is measured via some observed variables. No question can perfectly measure someone’s level of “anxiety”, but if we take a set of 10 carefully chosen questions, we can consider the shared covariance between those 10 questions to represent the construct that is common between all of them (they all ask, in different ways, about “anxiety”), also modeling the unique error with which each individual question fails to perfectly represent the entire construct.\nCombine them and we can reap the rewards of having both a structural model and a measurement model. The measurement model is our specification between the items we directly observed, and the latent variables of which we consider these items to be manifestations. The structural model is our specified model of the relationships between the latent variables.\n\n\n\n\n\nFigure 1: SEM diagram. Measurement model in orange, Structural model in purple\n\n\n\n\n\nYou can’t test the structural model if the measurement model is bad\nIf you test the relationships between a set of latent factors, and they are not reliably measured by the observed items, then this error propagates up to influence the fit of the structural model.\nTo test the measurement model, it is typical to saturate the structural model (i.e., allow all the latent variables to correlate with one another). This way any misfit is due to the measurement model only.\nAlternatively, we can fit individual CFA models for each construct and assess their fit (making any reasonable adjustments if necessary) prior to then fitting the full SEM.\n\n\n\n\n\nExercising Exercises\n\nDataset: tpb2\nThe “Theory of Planned Behaviour” is a theory about why people engage in physical activity (i.e. why people exercise).\nThe theory is represented in the diagram in Figure 2 (only the latent variables and not the measured items are shown). Attitudes refer to the extent to which a person has a favourable view of exercising; subjective norms refer to whether they believe others whose opinions they care about believe exercise to be a good thing; and perceived behavioural control refers to the extent to which they believe exercising is under their control. Intentions refer to whether a person intends to exercise and behaviour is a measure of the extent to which they exercised. Each construct is measured using four items.\n\n\n\n\n\nFigure 2: Theory of planned behaviour (latent variables only)\n\n\n\n\nThe data are available either:\n\nas a .RData file: https://uoepsy.github.io/data/tpb2.Rdata\nas a .txt file: https://uoepsy.github.io/data/tpb2.txt\n\n\n\n\n\n\n\nTable 1:  Data Dictionary for TPB data \n  \n    \n      variable\n      question\n    \n  \n  \n    SN1\nWhen I think about people whose opinions matter to me, I believe they value and support regular exercise\n    SN2\nI feel pressure from those I care about to exercise regularly\n    SN3\nMost people who are important to me approve of my exercising\n    SN4\nMost people like me exercise regularly\n    PBC1\nMy exercise routine is up to me and only me\n    PBC2\nI am confident that if I want to then I can exercise regularly\n    PBC3\nI believe I have the ability to overcome any obstacles that may prevent me from exercising regularly.\n    PBC4\nI feel capable of sticking to a consistent exercise schedule, even when faced with challenges or distractions\n    attitude1\nI see exercising as an enjoyable and rewarding activity.\n    attitude2\nI believe that exercising contributes positively to my overall well-being and health.\n    attitude3\nI view exercising as an important part of maintaining a healthy lifestyle.\n    attitude4\nI feel energized and invigorated after engaging in physical exercise.\n    int1\nI am determined to take concrete steps towards establishing a consistent exercise habit\n    int2\nI intend to exercise for at least 20 minutes, three times per week for the next three months.\n    int3\nI have made a firm decision to prioritize exercise and allocate time for it in my schedule\n    int4\nI intend to be in shape within the next three months.\n    int5\nI am committed to incorporating regular exercise into my weekly routine.\n    beh1\nI currently engage in physical activity for at least 20 minutes, three times per week, as recommended.\n    beh2\nI already allocate time for exercise in my weekly schedule and adhere to it regularly.\n    beh3\nI track my exercise sessions and ensure I meet my weekly goals\n    beh4\nI do not currently exercise enough\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 1\n\n\nLoad in the various packages you will probably need (tidyverse, lavaan), and read in the data using the appropriate function.\nWe’ve given you .csv files for a long time now, but it’s good to be prepared to encounter all sorts of weird filetypes. Can you successfully read in from both types of data?\n\n\n\n\n\nSolution\n\n\n\nEither one or the other of:\n\nlibrary(tidyverse)\nlibrary(lavaan)\nload(url(\"https://uoepsy.github.io/data/tpb2.Rdata\"))\n\nTPB_data &lt;- read.table(\"https://uoepsy.github.io/data/tpb2.txt\", header = TRUE, sep = \"\\t\")\n\n\n\n\n\nQuestion 2\n\n\nTest separate one-factor models for each construct.\nAre the measurement models satisfactory? (check their fit measures).\n\n\n\n\n\nAttitudes\n\n\n\nHere we specify our one factor CFA model for attitudes:\n\natt_mod &lt;- \"\n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  \"\n\nAnd we estimate the model using cfa()\n\natt_mod.est &lt;- cfa(att_mod, data=TPB_data, std.lv = TRUE)\n\nLet’s first inspect the fit measures:\n\nfitmeasures(att_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n      rmsea        srmr         tli         cfi \n0.007237593 0.010669452 0.999299418 0.999766473 \n\n\nOur fit is good: RMSEA&lt;.05, SRMR&lt;.05, TLI&gt;0.95 and CFI&gt;.95.\nWe should also check that all loadings are significant and \\(&gt;|.30|\\).\nTo save space I am going to not show the entire summary output here, but just pull out the parameter estimates:\n\nparameterestimates(att_mod.est)\n\n        lhs op       rhs   est    se      z pvalue ci.lower ci.upper\n1       att =~ attitude1 0.682 0.051 13.355      0    0.582    0.782\n2       att =~ attitude2 0.617 0.045 13.656      0    0.528    0.705\n3       att =~ attitude3 0.681 0.049 13.928      0    0.585    0.777\n4       att =~ attitude4 0.644 0.048 13.415      0    0.550    0.738\n5 attitude1 ~~ attitude1 1.097 0.069 15.883      0    0.961    1.232\n6 attitude2 ~~ attitude2 0.837 0.054 15.498      0    0.731    0.943\n7 attitude3 ~~ attitude3 0.959 0.063 15.121      0    0.835    1.084\n8 attitude4 ~~ attitude4 0.966 0.061 15.809      0    0.847    1.086\n9       att ~~       att 1.000 0.000     NA     NA    1.000    1.000\n\n\nThey all look good!\n\n\n\n\n\nSubjective Norms\n\n\n\nFollowing the same logic as for the Attitudes, let’s fit the CFA for Subjective norms. Again, all fit measures are very good, and loadings are all significant at greater than 0.3.\n\nsn_mod &lt;- \"\n  SubjN =~ SN1 + SN2 + SN3 + SN4\n  \"\n\nsn_mod.est &lt;- cfa(sn_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(sn_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.03058032 0.01444391 0.98605752 0.99535251 \n\nparameterestimates(sn_mod.est)\n\n    lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1 SubjN =~   SN1 0.644 0.048 13.524      0    0.550    0.737\n2 SubjN =~   SN2 0.585 0.049 11.923      0    0.489    0.681\n3 SubjN =~   SN3 0.584 0.044 13.266      0    0.498    0.670\n4 SubjN =~   SN4 0.615 0.049 12.578      0    0.519    0.711\n5   SN1 ~~   SN1 0.850 0.058 14.560      0    0.735    0.964\n6   SN2 ~~   SN2 1.041 0.062 16.736      0    0.919    1.163\n7   SN3 ~~   SN3 0.749 0.050 14.985      0    0.651    0.847\n8   SN4 ~~   SN4 0.985 0.062 15.974      0    0.864    1.106\n9 SubjN ~~ SubjN 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\n\nPerceived Behavioural Control\n\n\n\nAll good with Perceived Behavioural Control!\nAlmost too good (TLI&gt;1, and RMSEA is coming out at exactly 0!), but this is most probably because of this being fake data.\nWhen data is simulated based on a specific model, then fitting that same model structure to the data will obviously fit extremely well! s\n\npbc_mod &lt;- \"\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  \"\n\npbc_mod.est &lt;- cfa(pbc_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(pbc_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n      rmsea        srmr         tli         cfi \n0.000000000 0.003369084 1.010079110 1.000000000 \n\nparameterestimates(pbc_mod.est)\n\n   lhs op  rhs   est    se      z pvalue ci.lower ci.upper\n1  PBC =~ PBC1 0.696 0.043 16.258      0    0.612    0.780\n2  PBC =~ PBC2 0.627 0.038 16.346      0    0.551    0.702\n3  PBC =~ PBC3 0.592 0.041 14.619      0    0.513    0.672\n4  PBC =~ PBC4 0.676 0.045 15.058      0    0.588    0.764\n5 PBC1 ~~ PBC1 0.765 0.052 14.794      0    0.663    0.866\n6 PBC2 ~~ PBC2 0.609 0.041 14.677      0    0.527    0.690\n7 PBC3 ~~ PBC3 0.768 0.046 16.601      0    0.678    0.859\n8 PBC4 ~~ PBC4 0.919 0.057 16.184      0    0.808    1.030\n9  PBC ~~  PBC 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\n\nIntentions\n\n\n\nUh-oh, it’s looking less good with Intentions.\nThe loadings all look okay, but the fit indices aren’t great\n\nint_mod &lt;- \"\n  intent =~ int1 + int2 + int3 + int4 + int5\n  \"\n\nint_mod.est &lt;- cfa(int_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(int_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.14128950 0.05266866 0.84561107 0.92280553 \n\nparameterestimates(int_mod.est)\n\n      lhs op    rhs   est    se      z pvalue ci.lower ci.upper\n1  intent =~   int1 0.698 0.043 16.363      0    0.614    0.781\n2  intent =~   int2 0.801 0.035 23.173      0    0.733    0.869\n3  intent =~   int3 0.684 0.039 17.407      0    0.607    0.761\n4  intent =~   int4 0.868 0.038 22.746      0    0.793    0.943\n5  intent =~   int5 0.581 0.037 15.518      0    0.508    0.655\n6    int1 ~~   int1 1.046 0.056 18.555      0    0.936    1.157\n7    int2 ~~   int2 0.487 0.036 13.665      0    0.417    0.557\n8    int3 ~~   int3 0.858 0.047 18.108      0    0.765    0.951\n9    int4 ~~   int4 0.614 0.043 14.145      0    0.529    0.699\n10   int5 ~~   int5 0.827 0.044 18.874      0    0.741    0.913\n11 intent ~~ intent 1.000 0.000     NA     NA    1.000    1.000\n\n\nLet’s examine the modification indices:\n\nmodindices(int_mod.est, sort = TRUE)\n\n    lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n17 int2 ~~ int4 97.630  0.414   0.414    0.757    0.757\n13 int1 ~~ int3 50.107  0.270   0.270    0.285    0.285\n16 int2 ~~ int3 21.423 -0.159  -0.159   -0.246   -0.246\n20 int3 ~~ int5 18.787  0.145   0.145    0.172    0.172\n19 int3 ~~ int4 17.657 -0.158  -0.158   -0.217   -0.217\n12 int1 ~~ int2 16.578 -0.148  -0.148   -0.207   -0.207\n14 int1 ~~ int4 10.596 -0.129  -0.129   -0.161   -0.161\n21 int4 ~~ int5 10.532 -0.111  -0.111   -0.156   -0.156\n15 int1 ~~ int5  4.521  0.077   0.077    0.083    0.083\n18 int2 ~~ int5  3.438 -0.058  -0.058   -0.091   -0.091\n\n\nIt looks like correlating the residuals for items int2 and int4 would improve our model. The expected correlation is 0.757, which is fairly large (remember correlations are between -1 and 1).\nNote that the items have a possible theoretical link too, beyond just “intention to exercise”. It looks like both int2 and int4 are specifically about intentions in the next three months. It might make sense that responses to these two items are related more than just representing general ‘intention’.\nWhen we include this covariance, our model fit looks much better!\n\nint_mod &lt;- \"\n  intent =~ int1 + int2 + int3 + int4 + int5\n  int2 ~~ int4\n  \"\n\nint_mod.est &lt;- cfa(int_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(int_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.02753383 0.01475947 0.99413687 0.99765475 \n\nparameterestimates(int_mod.est)\n\n      lhs op    rhs   est    se      z pvalue ci.lower ci.upper\n1  intent =~   int1 0.795 0.044 18.018      0    0.708    0.881\n2  intent =~   int2 0.633 0.039 16.341      0    0.557    0.709\n3  intent =~   int3 0.800 0.041 19.607      0    0.720    0.880\n4  intent =~   int4 0.682 0.043 15.916      0    0.598    0.766\n5  intent =~   int5 0.629 0.039 16.205      0    0.553    0.705\n6    int2 ~~   int4 0.343 0.039  8.748      0    0.266    0.419\n7    int1 ~~   int1 0.902 0.057 15.734      0    0.789    1.014\n8    int2 ~~   int2 0.727 0.044 16.589      0    0.641    0.813\n9    int3 ~~   int3 0.686 0.049 13.927      0    0.589    0.782\n10   int4 ~~   int4 0.902 0.054 16.827      0    0.797    1.007\n11   int5 ~~   int5 0.769 0.045 17.201      0    0.681    0.856\n12 intent ~~ intent 1.000 0.000     NA     NA    1.000    1.000\n\n\n\n\n\n\n\nBehaviour\n\n\n\nFinally, the behaviour model looks absolutely fine.\nNote that bey4 has a negative loading, which is perfectly okay. In fact, if you look at the items, you’ll notice that this is the only item that is reversed (higher scores on the item reflect less exercising)\n\nbeh_mod &lt;- \"\n  behav =~ beh1 + beh2 + beh3 + beh4\n  \"\n\nbeh_mod.est &lt;- cfa(beh_mod, data=TPB_data, std.lv = TRUE)\n\nfitmeasures(beh_mod.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n     rmsea       srmr        tli        cfi \n0.02896260 0.01285583 0.99191922 0.99730641 \n\nparameterestimates(beh_mod.est)\n\n    lhs op   rhs    est    se       z pvalue ci.lower ci.upper\n1 behav =~  beh1  0.659 0.045  14.593      0    0.571    0.748\n2 behav =~  beh2  0.735 0.045  16.275      0    0.647    0.824\n3 behav =~  beh3  0.787 0.045  17.331      0    0.698    0.875\n4 behav =~  beh4 -0.724 0.046 -15.626      0   -0.815   -0.633\n5  beh1 ~~  beh1  0.987 0.058  16.962      0    0.873    1.101\n6  beh2 ~~  beh2  0.889 0.058  15.312      0    0.775    1.003\n7  beh3 ~~  beh3  0.819 0.059  13.908      0    0.703    0.934\n8  beh4 ~~  beh4  0.980 0.061  16.029      0    0.860    1.099\n9 behav ~~ behav  1.000 0.000      NA     NA    1.000    1.000\n\n\n\n\n\n\nQuestion 3\n\n\nUsing lavaan syntax, specify the full structural equation model that corresponds to the model in Figure 2. For each construct use the measurement models from the previous question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis involves specifying the measurement models for all the latent variables, and then also specifying the relationships between those latent variables. All in the same model!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nTPB_model&lt;-'\n  # measurement models  \n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  SN =~ SN1 + SN2 + SN3 + SN4\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  intent =~ int1 + int2 + int3 + int4 + int5\n  beh =~ beh1 + beh2 + beh3 + beh4\n  \n  # covariances between items\n  int2 ~~ int4\n\n  # regressions  \n  beh ~ intent + PBC\n  intent ~ att + SN + PBC\n\n  # covariances between attitudes, SN, and PBC\n  att ~~ SN    \n  att ~~ PBC\n  SN ~~ PBC\n'\n\n\n\n\n\nQuestion 4\n\n\nEstimate and evaluate the model\n\nDoes the model fit well?\n\nAre the hypothesised paths significant?\n\n\n\n\n\n\nSolution\n\n\n\nWe can estimate the model using the sem() function.\nAs with cfa(), by default the sem() function will scale the latent variables by fixing the loading of the first item for each latent variable to 1.\n\nTPB_model.est&lt;-sem(TPB_model, data=TPB_data, std.lv=TRUE)\n\nfitmeasures(TPB_model.est)[c(\"rmsea\",\"srmr\",\"tli\",\"cfi\")]\n\n    rmsea      srmr       tli       cfi \n0.0108428 0.0268991 0.9935594 0.9944795 \n\n\nWe can see that the model fits well according to RMSEA, SRMR, TLI and CFI.\nFrom the output below, all of the hypothesised paths in the theory of planned behaviour are statistically significant.\n\nsummary(TPB_model.est, standardized=TRUE)\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           890\n\nModel Test User Model:\n                                                      \n  Test statistic                               198.834\n  Degrees of freedom                               180\n  P-value (Chi-square)                           0.160\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  att =~                                                                \n    attitude1         0.687    0.050   13.854    0.000    0.687    0.550\n    attitude2         0.614    0.044   14.018    0.000    0.614    0.557\n    attitude3         0.662    0.047   13.977    0.000    0.662    0.555\n    attitude4         0.660    0.047   14.140    0.000    0.660    0.561\n  SN =~                                                                 \n    SN1               0.644    0.045   14.213    0.000    0.644    0.572\n    SN2               0.595    0.047   12.573    0.000    0.595    0.505\n    SN3               0.573    0.042   13.637    0.000    0.573    0.548\n    SN4               0.620    0.047   13.202    0.000    0.620    0.531\n  PBC =~                                                                \n    PBC1              0.687    0.041   16.555    0.000    0.687    0.615\n    PBC2              0.617    0.037   16.606    0.000    0.617    0.616\n    PBC3              0.608    0.039   15.412    0.000    0.608    0.575\n    PBC4              0.681    0.044   15.581    0.000    0.681    0.581\n  intent =~                                                             \n    int1              0.648    0.039   16.793    0.000    0.777    0.628\n    int2              0.554    0.034   16.519    0.000    0.664    0.626\n    int3              0.643    0.036   17.876    0.000    0.771    0.670\n    int4              0.586    0.037   15.818    0.000    0.703    0.601\n    int5              0.534    0.034   15.879    0.000    0.641    0.594\n  beh =~                                                                \n    beh1              0.556    0.038   14.461    0.000    0.678    0.568\n    beh2              0.588    0.039   15.203    0.000    0.718    0.600\n    beh3              0.635    0.039   16.208    0.000    0.775    0.646\n    beh4             -0.604    0.040  -15.220    0.000   -0.737   -0.601\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  beh ~                                                                 \n    intent            0.465    0.057    8.166    0.000    0.457    0.457\n    PBC               0.251    0.063    3.967    0.000    0.206    0.206\n  intent ~                                                              \n    att               0.242    0.061    3.986    0.000    0.202    0.202\n    SN                0.335    0.064    5.233    0.000    0.279    0.279\n    PBC               0.338    0.059    5.726    0.000    0.282    0.282\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .int2 ~~                                                               \n   .int4              0.307    0.037    8.403    0.000    0.307    0.397\n  att ~~                                                                \n    SN                0.316    0.050    6.300    0.000    0.316    0.316\n    PBC               0.245    0.049    5.056    0.000    0.245    0.245\n  SN ~~                                                                 \n    PBC               0.275    0.049    5.628    0.000    0.275    0.275\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .attitude1         1.089    0.067   16.231    0.000    1.089    0.697\n   .attitude2         0.840    0.052   16.057    0.000    0.840    0.690\n   .attitude3         0.985    0.061   16.101    0.000    0.985    0.692\n   .attitude4         0.946    0.059   15.925    0.000    0.946    0.685\n   .SN1               0.850    0.055   15.385    0.000    0.850    0.672\n   .SN2               1.030    0.060   17.099    0.000    1.030    0.745\n   .SN3               0.762    0.047   16.061    0.000    0.762    0.699\n   .SN4               0.979    0.059   16.516    0.000    0.979    0.718\n   .PBC1              0.777    0.050   15.682    0.000    0.777    0.622\n   .PBC2              0.621    0.040   15.630    0.000    0.621    0.620\n   .PBC3              0.750    0.045   16.715    0.000    0.750    0.670\n   .PBC4              0.912    0.055   16.576    0.000    0.912    0.663\n   .int1              0.929    0.055   16.918    0.000    0.929    0.606\n   .int2              0.687    0.041   16.675    0.000    0.687    0.609\n   .int3              0.730    0.046   15.843    0.000    0.730    0.551\n   .int4              0.873    0.051   17.115    0.000    0.873    0.639\n   .int5              0.754    0.043   17.618    0.000    0.754    0.648\n   .beh1              0.962    0.056   17.226    0.000    0.962    0.677\n   .beh2              0.915    0.055   16.517    0.000    0.915    0.640\n   .beh3              0.837    0.055   15.238    0.000    0.837    0.582\n   .beh4              0.961    0.058   16.498    0.000    0.961    0.639\n    att               1.000                               1.000    1.000\n    SN                1.000                               1.000    1.000\n    PBC               1.000                               1.000    1.000\n   .intent            1.000                               0.695    0.695\n   .beh               1.000                               0.672    0.672\n\n\n\n\n\n\nQuestion 5\n\n\nExamine the modification indices and expected parameter changes - are there any additional parameters you would consider including?\n\n\n\n\n\nSolution\n\n\n\n\nmodindices(TPB_model.est, sort = TRUE) |&gt; head()\n\n     lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n316 int1 ~~ int3 12.167  0.143   0.143    0.174    0.174\n103  PBC =~ int5 11.028  0.151   0.151    0.140    0.140\n303 PBC3 ~~ beh2  8.351  0.095   0.095    0.114    0.114\n336 int4 ~~ beh1  7.886  0.087   0.087    0.095    0.095\n291 PBC2 ~~ int5  7.847  0.076   0.076    0.111    0.111\n263  SN4 ~~ PBC4  6.978 -0.098  -0.098   -0.104   -0.104\n\n\nIn this case, none of the expected parameter changes are large enough that we would consider including any additional parameters\n\n\n\n\nQuestion 6\n\n\nTest the indirect effect of attitudes, subjective norms, and perceived behavioural control on behaviour via intentions.\nRemember, when you fit the model with sem(), use se='bootstrap' to get boostrapped standard errors (it may take a few minutes). When you inspect the model using summary(), get the 95% confidence intervals for parameters with ci = TRUE.\n\n\n\n\n\nSolution\n\n\n\nFirst, let’s name the paths in the structural equation model:\n\n\n\n\n\n\n\n\n\nTo test these indirect effects we create new a parameter for each indirect effect:\n\nTPB_model2 &lt;- '\n  # measurement models  \n  att =~ attitude1 + attitude2 + attitude3 + attitude4\n  SN =~ SN1 + SN2 + SN3 + SN4\n  PBC =~ PBC1 + PBC2 + PBC3 + PBC4\n  intent =~ int1 + int2 + int3 + int4 + int5\n  beh =~ beh1 + beh2 + beh3 + beh4\n  \n  # covariances between items\n  int2 ~~ int4\n\n  # regressions  \n  beh ~ b*intent + PBC\n  intent ~ a1*att + a2*SN + a3*PBC\n\n  # covariances between attitudes, SN, and PBC\n  att ~~ SN    \n  att ~~ PBC\n  SN ~~ PBC\n\n  # indirect effects:  \n  ind1 := a1*b  #indirect effect of attitudes via intentions\n  ind2 := a2*b  #indirect effect of SN via intentions\n  ind3 := a3*b  #indirect effect of PBC via intentions\n'\n\nWhen we estimate the model, we request bootstrapped standard errors:\n\nTPB_model2.est&lt;-sem(TPB_model2, std.lv=TRUE, se='bootstrap', data=TPB_data)\n\nWhen we inspect the model, we request the 95% confidence intervals for parameters:\n\nsummary(TPB_model2.est, ci=TRUE)\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           890\n\nModel Test User Model:\n                                                      \n  Test statistic                               198.834\n  Degrees of freedom                               180\n  P-value (Chi-square)                           0.160\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  att =~                                                                \n    attitude1         0.687    0.049   14.147    0.000    0.585    0.780\n    attitude2         0.614    0.044   13.899    0.000    0.528    0.700\n    attitude3         0.662    0.046   14.327    0.000    0.567    0.745\n    attitude4         0.660    0.045   14.639    0.000    0.566    0.745\n  SN =~                                                                 \n    SN1               0.644    0.046   13.946    0.000    0.552    0.734\n    SN2               0.595    0.046   12.962    0.000    0.501    0.687\n    SN3               0.573    0.044   12.955    0.000    0.487    0.657\n    SN4               0.620    0.048   13.001    0.000    0.524    0.717\n  PBC =~                                                                \n    PBC1              0.687    0.039   17.604    0.000    0.614    0.765\n    PBC2              0.617    0.036   17.152    0.000    0.540    0.683\n    PBC3              0.608    0.040   15.052    0.000    0.524    0.687\n    PBC4              0.681    0.043   15.872    0.000    0.594    0.761\n  intent =~                                                             \n    int1              0.648    0.040   16.116    0.000    0.571    0.728\n    int2              0.554    0.033   16.651    0.000    0.485    0.615\n    int3              0.643    0.038   16.874    0.000    0.566    0.715\n    int4              0.586    0.037   15.918    0.000    0.515    0.655\n    int5              0.534    0.031   17.381    0.000    0.472    0.594\n  beh =~                                                                \n    beh1              0.556    0.039   14.358    0.000    0.477    0.628\n    beh2              0.588    0.039   15.130    0.000    0.512    0.662\n    beh3              0.635    0.039   16.230    0.000    0.558    0.709\n    beh4             -0.604    0.040  -15.181    0.000   -0.684   -0.528\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  beh ~                                                                 \n    intent     (b)    0.465    0.056    8.242    0.000    0.354    0.576\n    PBC               0.251    0.066    3.794    0.000    0.126    0.383\n  intent ~                                                              \n    att       (a1)    0.242    0.066    3.672    0.000    0.116    0.369\n    SN        (a2)    0.335    0.064    5.196    0.000    0.213    0.476\n    PBC       (a3)    0.338    0.062    5.483    0.000    0.228    0.468\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n .int2 ~~                                                               \n   .int4              0.307    0.038    8.153    0.000    0.233    0.383\n  att ~~                                                                \n    SN                0.316    0.050    6.313    0.000    0.216    0.412\n    PBC               0.245    0.050    4.942    0.000    0.148    0.342\n  SN ~~                                                                 \n    PBC               0.275    0.049    5.563    0.000    0.175    0.370\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .attitude1         1.089    0.062   17.481    0.000    0.966    1.204\n   .attitude2         0.840    0.052   16.198    0.000    0.741    0.940\n   .attitude3         0.985    0.060   16.344    0.000    0.862    1.098\n   .attitude4         0.946    0.062   15.353    0.000    0.817    1.061\n   .SN1               0.850    0.055   15.489    0.000    0.742    0.954\n   .SN2               1.030    0.059   17.516    0.000    0.914    1.148\n   .SN3               0.762    0.051   15.082    0.000    0.671    0.858\n   .SN4               0.979    0.060   16.353    0.000    0.860    1.100\n   .PBC1              0.777    0.049   15.821    0.000    0.679    0.873\n   .PBC2              0.621    0.037   16.567    0.000    0.547    0.693\n   .PBC3              0.750    0.042   17.901    0.000    0.671    0.831\n   .PBC4              0.912    0.052   17.498    0.000    0.811    1.015\n   .int1              0.929    0.053   17.564    0.000    0.822    1.030\n   .int2              0.687    0.042   16.534    0.000    0.600    0.771\n   .int3              0.730    0.050   14.687    0.000    0.628    0.830\n   .int4              0.873    0.050   17.561    0.000    0.772    0.970\n   .int5              0.754    0.042   18.084    0.000    0.670    0.836\n   .beh1              0.962    0.053   18.042    0.000    0.857    1.071\n   .beh2              0.915    0.057   16.080    0.000    0.802    1.035\n   .beh3              0.837    0.057   14.574    0.000    0.717    0.947\n   .beh4              0.961    0.061   15.767    0.000    0.837    1.085\n    att               1.000                               1.000    1.000\n    SN                1.000                               1.000    1.000\n    PBC               1.000                               1.000    1.000\n   .intent            1.000                               1.000    1.000\n   .beh               1.000                               1.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind1              0.113    0.032    3.478    0.001    0.055    0.185\n    ind2              0.156    0.035    4.470    0.000    0.091    0.229\n    ind3              0.157    0.034    4.685    0.000    0.099    0.231\n\n\nWe can see that all of the indirect effects are statistically significant at p&lt;.05 as none of the 95% confidence intervals for the coefficients include zero.\n\n\n\n\nQuestion 7\n\n\nWrite up your analysis as if you were presenting the work in academic paper, with brief separate ‘Method’ and ‘Results’ sections\n\n\n\n\n\nSolution\n\n\n\nMethod\nWe tested a theory of planned behaviour model of physical activity by fitting a structural equation model in which attitudes, subjective norms, perceived behavioural control, intentions and behaviour were latent variables defined by four items. We first tested the measurement models for each construct by fitting a one-factor CFA model. Latent variable scaling was by fixing the loading of the first item for each construct to 1.\nWithin the SEM, behaviour was regressed on intentions and perceived behavioural control and intentions were regressed on attitudes, subjective norms, and perceived behavioiural control. In addition, attitudes, subjective norms, and perceived behavioural control were allowed to covary. The indirect effects of attitudes, subjective norms and perceived behavioural control on behaviour were calculated as the product of the effect of the relevant predictor on the mediator (intentions) and the effect of the mediator on the outcome. The statistical significance of the indirect effects were evaluated using bootstrapped 95% confidence intervals with 1000 resamples.\nIn all cases models were fit using maximum likelihood estimation and model fit was judged to be good if CFI and TLI were \\(&gt;.95\\) and RMSEA and SRMR were \\(&lt;.05\\). Modification indices and expected parameter changes were inspected to identify any areas of local mis-fit but model modifications were only made if they could be justified on substantive grounds.\nResults\nAll measurement models fit well (CFI and TLI \\(&gt;.95\\) and RMSEA and SRMR \\(&lt;.05\\)) with the exception of the measurement model for intentions. Modification indices suggested the inclusion of residual covariance between two items on the intentions scale (int2 and int4) that both made specific reference to short term intentions. The addition of this parameter resulted in a good fit. The full structural equation model fit well (CFI = 0.99, TLI = 0.99, RMSEA = 0.01, SRMR = 0.03). Unstandardised parameter estimates are provided in Table 2. All of the hypothesised paths were statistically significant at \\(p&lt;.05\\). The significant indirect effects suggested that intentions mediate the effects of attitudes, subjective norms, and perceived behavioural control on behaviour whilst perceived behavioural control also has a direct effect on behaviour. Results thus provide support for a theory of planned behaviour model of physical activity.\n\n\n\n\n\n\nTable 2:  Unstandardised parameter estimates for structural equation model for\na theory of planned behaviour model of physical activity. Note: PBC =\nPerceived Behavioural Control, CI = Confidence Interval \n  \n    \n       \n      Parameter\n      Estimate\n      SE\n      z\n      p\n      95% CI\n    \n  \n  \n    \n      Loadings\n    \n    Attitudes\nattitude1\n0.69\n0.05\n14.15\n&lt;0.001\n[0.59, 0.78]\n    \nattitude2\n0.61\n0.04\n13.90\n&lt;0.001\n[0.53, 0.7]\n    \nattitude3\n0.66\n0.05\n14.33\n&lt;0.001\n[0.57, 0.75]\n    \nattitude4\n0.66\n0.05\n14.64\n&lt;0.001\n[0.57, 0.75]\n    Subjective Norms\nSN1\n0.64\n0.05\n13.95\n&lt;0.001\n[0.55, 0.73]\n    \nSN2\n0.59\n0.05\n12.96\n&lt;0.001\n[0.5, 0.69]\n    \nSN3\n0.57\n0.04\n12.95\n&lt;0.001\n[0.49, 0.66]\n    \nSN4\n0.62\n0.05\n13.00\n&lt;0.001\n[0.52, 0.72]\n    PBC\nPBC1\n0.69\n0.04\n17.60\n&lt;0.001\n[0.61, 0.77]\n    \nPBC2\n0.62\n0.04\n17.15\n&lt;0.001\n[0.54, 0.68]\n    \nPBC3\n0.61\n0.04\n15.05\n&lt;0.001\n[0.52, 0.69]\n    \nPBC4\n0.68\n0.04\n15.87\n&lt;0.001\n[0.59, 0.76]\n    Intentions\nint1\n0.65\n0.04\n16.12\n&lt;0.001\n[0.57, 0.73]\n    \nint2\n0.55\n0.03\n16.65\n&lt;0.001\n[0.49, 0.62]\n    \nint3\n0.64\n0.04\n16.87\n&lt;0.001\n[0.57, 0.72]\n    \nint4\n0.59\n0.04\n15.92\n&lt;0.001\n[0.51, 0.66]\n    \nint5\n0.53\n0.03\n17.38\n&lt;0.001\n[0.47, 0.59]\n    Behaviours\nbeh1\n0.56\n0.04\n14.36\n&lt;0.001\n[0.48, 0.63]\n    \nbeh2\n0.59\n0.04\n15.13\n&lt;0.001\n[0.51, 0.66]\n    \nbeh3\n0.64\n0.04\n16.23\n&lt;0.001\n[0.56, 0.71]\n    \nbeh4\n-0.60\n0.04\n-15.18\n&lt;0.001\n[-0.68, -0.53]\n    \n      Covariances\n    \n    \nint2 with int4\n0.31\n0.04\n8.15\n&lt;0.001\n[0.23, 0.38]\n    \nAttitudes with Subjective Norms\n0.32\n0.05\n6.31\n&lt;0.001\n[0.22, 0.41]\n    \nAttitudes with PBC\n0.25\n0.05\n4.94\n&lt;0.001\n[0.15, 0.34]\n    \nSubjective Norms with PBC\n0.27\n0.05\n5.56\n&lt;0.001\n[0.18, 0.37]\n    \n      Regressions\n    \n    \nBehaviours on Intentions\n0.47\n0.06\n8.24\n&lt;0.001\n[0.35, 0.58]\n    \nBehaviours on PBC\n0.25\n0.07\n3.79\n&lt;0.001\n[0.13, 0.38]\n    \nIntentions on Attitudes\n0.24\n0.07\n3.67\n&lt;0.001\n[0.12, 0.37]\n    \nIntentions on Subjective Norms\n0.33\n0.06\n5.20\n&lt;0.001\n[0.21, 0.48]\n    \nIntentions on PBC\n0.34\n0.06\n5.48\n&lt;0.001\n[0.23, 0.47]\n    \n      Indirect effects\n    \n    \nAttitudes via Intentions\n0.11\n0.03\n3.48\n&lt;0.001\n[0.05, 0.19]\n    \nSubjective Norms via Intentions\n0.16\n0.03\n4.47\n&lt;0.001\n[0.09, 0.23]\n    \nPBC via Intentions\n0.16\n0.03\n4.68\n&lt;0.001\n[0.1, 0.23]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n\nSolution\n\n\n\nsolution\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariate Statistics and Methodology in R",
    "section": "",
    "text": "Multivariate Statistics and Methodology in R (MSMR) is an advanced semester-long course designed for Masters students in psychology seeking a deeper understanding of statistical techniques to analyze complex data sets with multiple sources of variation. Building on the foundation laid by the Univariate Statistics and Methodology in R (USMR) course, MSMR extends students’ analytical repertoire to encompass multilevel models, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM).\nThe initial half of the course introduces students to the intricacies of multilevel models, providing a solid theoretical framework for understanding hierarchical data structures. Students will gain practical insights into applying these models to address research questions involving nested data and varying sources of variation.\nThe second half of the course delves into methods such as PCA and EFA for reducing dimensionality of data, before moving to Confirmatory Factor models and subsequently Structural Equation Models as a means of modeling and testing our theories about psychological constructs."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 5\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  28 108 212 291 218 105  32   3"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n  \n    \n      Ways to get 2 heads in 8 flips\n    \n  \n  \n    TTTHHTTT\n    THTHTTTT\n    THTTTTTH\n    HTHTTTTT\n    THHTTTTT\n    TTTTHTHT\n    HHTTTTTT\n    TTTTHTTH\n    ...\n  \n  \n  \n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta)\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]