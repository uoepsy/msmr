[
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below."
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596"
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\nWe can get out some bootstrapped confidence intervals for our coefficients using the car package:\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965"
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA"
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable"
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎"
  },
  {
    "objectID": "01a_clustered.html",
    "href": "01a_clustered.html",
    "title": "1A: Clustered Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to clustered data\nWorking with clustered data (sample sizes, ICC, visualisations)"
  },
  {
    "objectID": "01a_clustered.html#clusters-clusters-everywhere",
    "href": "01a_clustered.html#clusters-clusters-everywhere",
    "title": "1A: Clustered Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\nTable 1:  Various different study designs will give rise to clustered data. \n  \n    \n    \n       \n      Cross Sectional\n      Repeated Measures\n      Longitudinal\n    \n  \n  \n    Level n\n...\n...\n...\n    ...\n...\n...\n...\n    Level 3\nSchool\n...\nFamilies\n    Level 2\nClassroom\nParticipants\nPeople\n    Level 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n  \n  \n  \n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person)."
  },
  {
    "objectID": "01a_clustered.html#what-are-clusters",
    "href": "01a_clustered.html#what-are-clusters",
    "title": "1A: Clustered Data",
    "section": "What are ‘clusters’?",
    "text": "What are ‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we will probably start using the terms “clusters” and “groups” interchangeably, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing can be thought of as a random sample of higher level units.\n\nMore often than not, the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how do drugs placebo/aspirin/beta-blockers influence people’s heart rate?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01a_clustered.html#determining-sample-sizes",
    "href": "01a_clustered.html#determining-sample-sizes",
    "title": "1A: Clustered Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n  \n    \n    \n       \n      Unit\n      Sample Size\n    \n  \n  \n    Level 2\nSchool\n30\n    Level 1 (Observations)\nChildren\n900\n  \n  \n  \n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30"
  },
  {
    "objectID": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1A: Clustered Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context."
  },
  {
    "objectID": "01a_clustered.html#visualisations",
    "href": "01a_clustered.html#visualisations",
    "title": "1A: Clustered Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01a_clustered.html#footnotes",
    "href": "01a_clustered.html#footnotes",
    "title": "1A: Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nDepending on the research question and design of the study, we may be only interested in things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎"
  },
  {
    "objectID": "01b_lmm.html",
    "href": "01b_lmm.html",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel model (MLM)\nHow the MLM achieves partial pooling\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)"
  },
  {
    "objectID": "01b_lmm.html#random-intercepts",
    "href": "01b_lmm.html#random-intercepts",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)."
  },
  {
    "objectID": "01b_lmm.html#random-slopes",
    "href": "01b_lmm.html#random-slopes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho \\sigma_0 \\sigma_1\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho \\sigma_0 \\sigma_1\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values"
  },
  {
    "objectID": "01b_lmm.html#extracting-model-parameters",
    "href": "01b_lmm.html#extracting-model-parameters",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod2)\n\n(Intercept)       motiv \n  29.233320    4.475717 \n\n\n\n\nrandom effect variances\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\nVarCorr(smod2)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 12.5745        \n          motiv        2.0708  -0.698\n Residual             11.8036"
  },
  {
    "objectID": "01b_lmm.html#making-model-predictions",
    "href": "01b_lmm.html#making-model-predictions",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 7.07 higher than average, and an estimate slope of motivation that is 0.47 lower than average.\n\nranef(smod2)\n\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod2))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slope for each cluster.\nFor example, if we are estimating that “Anderson High School” has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.\nWe can get these out using coef()\n\ncoef(smod2)\n\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n\n\nfixef() + ranef() = coef()"
  },
  {
    "objectID": "01b_lmm.html#a-more-complex-model",
    "href": "01b_lmm.html#a-more-complex-model",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "A more complex model",
    "text": "A more complex model\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child’s education motivation, motiv), and our observations (children) happen to be clustered into groups (schools).\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).\nFor instance, we might have questions that take the form:\n\n“does [Level-2 variable] predict [Level-1 outcome]?”\n\n“does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?\n\n(in our example, Level-1 = children, Level-2 = Schools).\nConsider, for example, if we want to investigate whether the relationship between children’s motivation levels and their grades is different depending upon the source of school funding (private vs state).\nAddressing such questions simply requires a more complex fixed effect structure (specifically the interaction between motiv and funding (private vs state).\n\nsmod3 &lt;- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n\nNote, we cannot include funding in the random effects part of our model, because “the effect of funding on school grades” is something we assess by comparing between schools. We cannot think of that effect varying by-school because every school is either “private” or “state” funded. We never observe “Ardnamurchan High School” as anything other than “state” funded, so “the effect on grades of being state/private funded” does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the funding coefficient and the motiv:funding interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in Figure 7, to having two lines (one for “private” schools and one for “state” schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.\n\n\n\n\n\n\nModel equation\n\n\n\n\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome.\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the lmer() syntax in R, rather than in terms of the mathematical expressions.\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!\nBecause the funding variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n\\]\nIt is sometimes easier to think of this in the “mixed effects notation” we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:\n\\[\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\noptional: an attempted visual explanation\n\n\n\n\n\nFigure 12 shows an attempted visual intuition of how the different parts of the model work:\n\n\n\n\n\nFigure 12: visual explanation of a model with a cross-level interaction\n\n\n\n\n\n\n\n\n\n\n\n\nmodel summary\n\nsummary(smod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: grade ~ motiv * funding + (1 + motiv | schoolid)\n   Data: schoolmot\n\nREML criterion at convergence: 7083.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08250 -0.67269  0.03043  0.63562  3.13012 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 105.124  10.253        \n          motiv         2.595   1.611   -0.48\n Residual             139.030  11.791        \nNumber of obs: 900, groups:  schoolid, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         40.3143     4.6414   8.686\nmotiv                2.6294     0.8652   3.039\nfundingstate       -17.2531     5.7346  -3.009\nmotiv:fundingstate   2.8485     1.0591   2.689\n\nCorrelation of Fixed Effects:\n            (Intr) motiv  fndngs\nmotiv       -0.782              \nfundingstat -0.809  0.633       \nmtv:fndngst  0.639 -0.817 -0.773\n\n\n\n\nplot\nFor plotting the fixed effect estimates (which are often the bit we’re most interested in) from multilevel models, we can’t rely on using predict(), fitted() or augment(), as these return to us the cluster-specific predicted values.\nInstead, we need to use tools like the effects package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:\n\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)"
  },
  {
    "objectID": "01b_lmm.html#convergence-warnings-singular-fits",
    "href": "01b_lmm.html#convergence-warnings-singular-fits",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))"
  },
  {
    "objectID": "01b_lmm.html#footnotes",
    "href": "01b_lmm.html#footnotes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "01ex.html#footnotes",
    "href": "01ex.html#footnotes",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "01exMARKOV.html",
    "href": "01exMARKOV.html",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "01exMARKOV.html#footnotes",
    "href": "01exMARKOV.html#footnotes",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "02a_inference.html",
    "href": "02a_inference.html",
    "title": "Inference for LMM",
    "section": "",
    "text": "The term “inference” is used to refer to the process of moving from beyond a description of our specific sample to being able to make statements about the broader population from which we have sampled.\nIn the framework for statistics that we have been learning, this centers on the idea of samples statistics that we might see in the long run (i.e. if we did the experiment again and again and again with a new sample each time).\n\n\n\n\n\nFigure 1: The standard error is the standard deviation of the ‘sampling distribution’ - the distribution of sample statistics that we could see. We use this to ask how likely our observed sample is in a universe where the null hypothesis is true. This probability gives us reason to reject (or not) said null hypothesis.\n\n\n\n\nIn USMR, we saw various ways in which this logic was applied, combining the observed sample statistic with the standard error to create a test statistic (\\(z\\), \\(t\\), \\(\\chi^2\\), \\(F\\), then compared to the appropriate standard distribution).\nIn the linear models we were making with lm(), these were \\(t\\) (for the coefficient estimate) and \\(F\\) (the reduction in residual sums of squares) tests, and accordingly they had an associated degrees of freedom. If we fit a linear model lm(y~x) to 10 datapoints, then our tests would have \\(10-2=8\\)1 degrees of freedom, and test statistics would be compared against, e.g. a \\(t\\) distribution with 8 degrees of freedom. Alternatively, if we fit the same model to 100 datapoints, we would be working with a distribution with 98 degrees of freedom. The degrees of freedom reflects the fact that there is more variability in statistics from smaller samples.\nAnother way of thinking of degrees of freedom is that they are the number of independent datapoints that are left “free to vary” around our model parameters.\nBut we are now working with multilevel data, and in the scenario where we have, e.g. \\(n_p\\) pupils clustered into \\(n_s\\) schools, how many independent bits of information do we have to begin with? \\(n_p\\)? \\(n_s\\)? somewhere in between? Our random effects are not “free to vary” in the sense that they are estimated under certain constraints (such as following a normal distribution).\nIn very specific situations that correspond to classical experimental designs (in which, e.g., we have perfectly balanced numbers across experimental factors and equal sizes within groups) it is possible to conduct similar \\(F\\) tests (and so \\(t\\) too) with a calculable degrees of freedom. Unfortunately, transferring this to more general scenarios (any missing data, unbalanced designs, more complex random effect structures) is problematic. Partly because defining the degrees of freedom is much more tricky, and partly because the test statistic may not even follow an \\(F\\) distribution with any degrees of freedom.\nHowever, there are various strategies that we can use to conduct inferences that either attempt to approximate the degrees of freedom, or use an alternative method based on, e.g., likelihoods or bootstrapping.\nBelow, we’ll go through each method in R, applying it to the following model (recall this is the model we ended with in reading 1B).\n\nlibrary(tidyverse)\nlibrary(lme4)\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n\nsmod3 &lt;- lmer(grade ~ motiv * funding + \n                (1 + motiv | schoolid), \n              data = schoolmot)\n\n\n\n\n\n\n\ndf approximations (Satterthwaite & KR)\n\n\n\n\n\nTwo methods have been suggested as approximations for the denominator degrees of freedom for multilevel models.\nsatterthwaite\nkr\n\n# ddf ----\nd3=slice_sample(d3,prop=.78)\nm0 =lmer(ACE ~ visit + condition + (1+visit|ppt), d3,REML=F)\nm0r =lmer(ACE ~ visit + condition + (1+visit|ppt), d3,REML=T)\nm1 =lmer(ACE ~ visit * condition + (1+visit|ppt), d3,REML=F)\nm1r =lmer(ACE ~ visit * condition + (1+visit|ppt), d3,REML=T)\nSATmodcomp(m1,m0) # refits with REML\nSATmodcomp(m1r,m0r) # refits with REML\nKRmodcomp(m1,m0) # refits with REML\nKRmodcomp(m1r,m0r) # refits with REML\n\n\nSATmodcomp(m1,m0) # refits with REML\nKRmodcomp(m1,m0) # refits with REML\n\nparameters::model_parameters(m1, ci_method=\"kr\")\n# CIs computed via REML, estimates, t, p not unless initial model fitted with REML\nparameters::model_parameters(m1r,ci_method=\"sat\")\n# CIs computed via REML, estimates, t, p not unless initial model fitted with REML\n\n\n\n\n\n\n\n\n\n\nlikelihood based methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparametric bootstrap\n\n\n\n\n\n\n\n\n\ntable!"
  },
  {
    "objectID": "02a_inference.html#footnotes",
    "href": "02a_inference.html#footnotes",
    "title": "Inference for LMM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(n\\) observations minus \\(k\\) parameters (slope of x) minus 1 intercept↩︎"
  },
  {
    "objectID": "02b_long.html",
    "href": "02b_long.html",
    "title": "Multilevel Models for Longitudinal Data",
    "section": "",
    "text": "multilevel models are perfectly geared towards dealing with longitudinal data, which can be framed as having essentially the same hierarchical structure as “children in schools”.\n\n\n\n\n\n\nMindful Cognitive Aging\n\n\n\n\n\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants undertake to complete daily mindfulness sessions, while the remaining participants did not.\nDATASET: https://uoepsy.github.io/data/dapr3_mindfuldecline.csv\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    sitename\nSite Identifier\n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    visit\nVisit number (1 - 10)\n    age\nAge (years) at visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)"
  },
  {
    "objectID": "02c_log.html",
    "href": "02c_log.html",
    "title": "Logistic Multilevel Models",
    "section": "",
    "text": "d3 &lt;- read_csv(\"https://uoepsy.github.io/data/dapr3_mindfuldecline.csv\")\nd3$isImp &lt;- ifelse(d3$imp==\"imp\",1,0)\n\nlibrary(lme4)\n\nglmer(isImp ~ visit + (1+visit|ppt), d3, family=binomial) |&gt; summary()\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: isImp ~ visit + (1 + visit | ppt)\n   Data: d3\n\n     AIC      BIC   logLik deviance df.resid \n   122.7    138.5    -56.3    112.7      172 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.41073  0.00504  0.05123  0.26999  1.43076 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n ppt    (Intercept) 3.501    1.871         \n        visit       1.218    1.104    -0.90\nNumber of obs: 177, groups:  ppt, 20\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -1.0187     0.7344  -1.387  0.16539   \nvisit         1.0066     0.3729   2.699  0.00695 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nvisit -0.839"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n\nSolution\n\n\n\nsolution\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariate Statistics and Methodology in R",
    "section": "",
    "text": "Multivariate Statistics and Methodology in R (MSMR) is an advanced semester-long course designed for Masters students in psychology seeking a deeper understanding of statistical techniques to analyze complex data sets with multiple sources of variation. Building on the foundation laid by the Univariate Statistics and Methodology in R (USMR) course, MSMR extends students’ analytical repertoire to encompass multilevel models, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM).\nThe initial half of the course introduces students to the intricacies of multilevel models, providing a solid theoretical framework for understanding hierarchical data structures. Students will gain practical insights into applying these models to address research questions involving nested data and varying sources of variation.\nThe second half of the course delves into methods such as PCA and EFA for reducing dimensionality of data, before moving to Confirmatory Factor models and subsequently Structural Equation Models as a means of modeling and testing our theories about psychological constructs."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 2\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 6\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  3  40 114 222 261 214 113  28   5"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n  \n    \n    \n      Ways to get 2 heads in 8 flips\n    \n  \n  \n    TTHTHTTT\n    TTTTTHHT\n    TTTTTHTH\n    THHTTTTT\n    TTTHTTHT\n    TTHHTTTT\n    TTTTHHTT\n    HHTTTTTT\n    HTTHTTTT\n    ...\n  \n  \n  \n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta)\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]