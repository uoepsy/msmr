[
  {
    "objectID": "00_lm_assumpt.html",
    "href": "00_lm_assumpt.html",
    "title": "LM Troubleshooting",
    "section": "",
    "text": "In the face of plots (or tests) that appear to show violations of the distributional assumptions of linear regression (i.e. our residuals appear non-normal, or variance changes across the range of the fitted model), we should always take care to ensure our model is correctly specified (interactions or other non-linear effects, if present in the data but omitted from our model, can result in assumption violations). Following this, if we continue to have problems satisfying our assumptions, there are various options that give us more flexibility. Brief introductions to some of these methods are detailed below."
  },
  {
    "objectID": "00_lm_assumpt.html#tests-of-the-coefficients",
    "href": "00_lm_assumpt.html#tests-of-the-coefficients",
    "title": "LM Troubleshooting",
    "section": "Tests of the coefficients",
    "text": "Tests of the coefficients\n\nlibrary(lmtest)\nlibrary(sandwich)\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC0\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.0383561  0.8635215 -0.0444  0.96466  \nx            0.4924743  0.2631998  1.8711  0.06438 .\nx2b          1.2305743  0.7625359  1.6138  0.10985  \nx2c         -0.0010129  0.9210642 -0.0011  0.99912  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "00_lm_assumpt.html#model-comparisons",
    "href": "00_lm_assumpt.html#model-comparisons",
    "title": "LM Troubleshooting",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nmod_res &lt;- lm(y ~ 1 + x, data = troubledf2)\nmod_unres &lt;- lm(y ~ 1 + x + x2, data = troubledf2)\nwaldtest(mod_res, mod_unres, vcov = vcovHC(mod_unres, type = \"HC0\"))\n\nWald test\n\nModel 1: y ~ 1 + x\nModel 2: y ~ 1 + x + x2\n  Res.Df Df      F Pr(&gt;F)\n1     98                 \n2     96  2 1.8704 0.1596"
  },
  {
    "objectID": "00_lm_assumpt.html#boostrapped-coefficients",
    "href": "00_lm_assumpt.html#boostrapped-coefficients",
    "title": "LM Troubleshooting",
    "section": "Boostrapped Coefficients",
    "text": "Boostrapped Coefficients\nWe can get out some bootstrapped confidence intervals for our coefficients using the car package:\n\nlibrary(car)\n# bootstrap our model coefficients\nboot_mod &lt;- Boot(mod)\n# compute confidence intervals\nConfint(boot_mod)\n\nBootstrap bca confidence intervals\n\n              Estimate        2.5 %    97.5 %\n(Intercept)  1.5156272  0.269082523 3.0150279\nx            0.3769504  0.005839124 0.7201455\nx2b          0.2497345 -0.718176725 1.3009887\nx2c         -0.1305828 -1.015342466 0.6681926\nx2d          1.1534433  0.031319608 2.4027965"
  },
  {
    "objectID": "00_lm_assumpt.html#bootstrapped-anova",
    "href": "00_lm_assumpt.html#bootstrapped-anova",
    "title": "LM Troubleshooting",
    "section": "Bootstrapped ANOVA",
    "text": "Bootstrapped ANOVA\nIf we want to conduct a more traditional ANOVA, using Type I sums of squares to test the reduction in residual variance with the incremental addition of each predictor, we can get bootstrapped p-values from the ANOVA.boot function in the lmboot package.\nOur original ANOVA:\n\nanova( lm(y~x+x2, data = df) )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nx          1  20.64 20.6427  5.4098 0.02215 *\nx2         3  25.60  8.5331  2.2363 0.08902 .\nResiduals 95 362.50  3.8158                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd our bootstrapped p-values:\n\nlibrary(lmboot)\nmy_anova &lt;- ANOVA.boot(y~x+x2, data = df, \n                       B = 1000)\n# these are our bootstrapped p-values:\nmy_anova$`p-values`\n\n[1] 0.023 0.100\n\n#let's put them alongside our original ANOVA table:\ncbind(\n  anova( lm(y~x+x2, data = df) ),\n  p_bootstrap = c(my_anova$`p-values`,NA)\n)\n\n          Df    Sum Sq   Mean Sq  F value     Pr(&gt;F) p_bootstrap\nx          1  20.64273 20.642727 5.409835 0.02215056       0.023\nx2         3  25.59936  8.533122 2.236273 0.08902175       0.100\nResiduals 95 362.49886  3.815777       NA         NA          NA"
  },
  {
    "objectID": "00_lm_assumpt.html#other-things",
    "href": "00_lm_assumpt.html#other-things",
    "title": "LM Troubleshooting",
    "section": "Other things",
    "text": "Other things\nWe can actually bootstrap almost anything, we just need to get a bit more advanced into the coding, and create a little function that takes a) a dataframe and b) an index that defines the bootstrap sample.\nFor example, to bootstrap the \\(R^2\\) for the model lm(y~x+x2), we would create a little function called rsq:\n\nrsq &lt;- function(data, indices){\n  # this is the bootstrap resample\n  bdata &lt;- data[indices,]\n  # this is the model, fitted to the resample\n  fit &lt;- lm(y ~ x + x2, data = bdata)\n  # this returns the R squared\n  return(summary(fit)$r.square)\n}\n\nWe then use the boot package, giving 1) our original data and 2) our custom function to the boot() function, and compute some confidence intervals:\n\nlibrary(boot)\nbootrsq_results &lt;- boot(data = df, statistic = rsq, R = 1000)\nboot.ci(bootrsq_results, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootrsq_results, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.0174,  0.2196 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable"
  },
  {
    "objectID": "00_lm_assumpt.html#footnotes",
    "href": "00_lm_assumpt.html#footnotes",
    "title": "LM Troubleshooting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎\nThis is a special formulation of something called a ‘Sandwich’ estimator!↩︎\nor \\(sign( rank(|y|) )\\)↩︎"
  },
  {
    "objectID": "01a_clustered.html",
    "href": "01a_clustered.html",
    "title": "1A: Clustered Data",
    "section": "",
    "text": "This reading:\n\nA refresher on the linear regression model\n\nAn introduction to clustered data\nWorking with clustered data (sample sizes, ICC, visualisations)"
  },
  {
    "objectID": "01a_clustered.html#clusters-clusters-everywhere",
    "href": "01a_clustered.html#clusters-clusters-everywhere",
    "title": "1A: Clustered Data",
    "section": "Clusters clusters everywhere",
    "text": "Clusters clusters everywhere\nThe idea of observing “children in schools” is just one such example of clustering that we might come across. This same hierarchical data structure can be found in other settings, such as patients within medical practices, employees within departments, people within towns etc. These sort of groups are higher level observations that we might sample (i.e. I randomly sample 20 schools, and then from each school randomly sample 30 children). However, there are also lots of cases where clustered data might arise as the result of our study design. For instance, in a Repeated Measures study we have individual experimental trials clustered within participants. Longitudinal studies exhibit the same data structure but have time-ordered observations clustered within people.\nIn addition, we can extend this logic to think about having clusters of clusters, and clusters of cluster of clusters4. Table 1 shows just a few examples of different levels of clustering that may arise from different types of study.\n\n\n\n\n\n\n\nTable 1:  Various different study designs will give rise to clustered data. \n  \n    \n    \n       \n      Cross Sectional\n      Repeated Measures\n      Longitudinal\n    \n  \n  \n    Level n\n...\n...\n...\n    ...\n...\n...\n...\n    Level 3\nSchool\n...\nFamilies\n    Level 2\nClassroom\nParticipants\nPeople\n    Level 1 (Observations)\nChildren\nExperimental Stimuli\nTime\n  \n  \n  \n\n\n\n\n\nThe common thread throughout all these designs is the hierarchy. At the lowest level of our hierarchy is the individual observed thing. For some designs, individual people might be the lowest observation level, for others, people might be the clusters (i.e. we have multiple data points per person)."
  },
  {
    "objectID": "01a_clustered.html#what-are-clusters",
    "href": "01a_clustered.html#what-are-clusters",
    "title": "1A: Clustered Data",
    "section": "What are ‘clusters’?",
    "text": "What are ‘clusters’?\nAt the fundamental level, we are using the term ‘cluster’ here to refer to a grouping of observations. In fact, we will probably start using the terms “clusters” and “groups” interchangeably, so it’s worth taking a bit of time to try and understand the kind of groupings that we’re talking about (and how we think about them).\n\n\n“Clusters” are just “groups”.\n\nWhen we talk about clustered data, the groups we are discussing can be thought of as a random sample of higher level units.\n\nMore often than not, the specific group-differences are not of interest.\n\n\nContrast the idea of ‘clusters’ with how we think about other sorts of groupings. In a study that looks at “how do drugs placebo/aspirin/beta-blockers influence people’s heart rate?” (Figure 7 LH plot), we can group participants into which drug they have received. But these groupings are the very groups of interest to us, and we are interested in comparing placebo with aspirin with beta-blockers. If we were to run the study again, we’ll use the same drugs (they’re not just a random sample of drugs - the x-axis of our LH plot in Figure 7 will be the same).\nIf we are interested in “what is the average grade at GCSE?”, and we have children grouped into different schools (Figure 7 RH plot), we are probably not interested in all the specific differences between grades in Broughton High School vs Gryffe High School etc. If we were to run our study again, we don’t collect data from the same set of schools. We can view these schools as ‘clusters’ - they are another source of random variation (i.e. not systematic variation such as the effect of a drug, but variation we see just because schools are different from one another).\n\n\n\n\n\nFigure 7: Groupings of observations may be of specific interest - e.g. comparing two different drugs - or may be a groupings that we have no specific interest in (e.g. school A is just a random school)\n\n\n\n\nOften, while the specific clusters are not of interest, we may have research questions that are about features of those clusters, and how they relate to things at other levels. For example, we might be interested in if the type of school funding (a school-level variable) influences the grade performance (a child-level variable). The focus of this course is multilevel modelling (also known as “mixed effects modelling”), which is a regression modelling technique that allows us to explore questions such as these (and many more).5\n\n\n\n\n\n\noptional “univariate”and “multivariate”\n\n\n\n\n\nIn “univariate” statistics there is just one source of variation we are looking at explaining, which is the observation level. In psychology, our observations are often individual people, and we have variation because people are different from one another. Our studies are looking to explain this variation.\nIn “multivariate” statistics, there are more sources of variation. For the “children in schools” example: individual children are different from another, and schools are also different from one another. We also have multiple sources of variation from questionnaire scales (e.g. 9 survey questions about anxiety), because both there is variation in scores due to both a) people varying from one another and b) the 9 questions tending to illicit different responses from one another.\n\n\n\n\n\n\n\n\n\noptional: “Panel data”\n\n\n\n\n\nIn some fields (e.g. economics), clustering sometimes gets referred to as ‘panel data’. This can be a nice intuitive way of thinking about it, because we think of a plot of our data being split into different panels for each cluster:\n\n\n\n\n\nFigure 8: Panels of data\n\n\n\n\n\n\n\n\n\nFigure 9: Panels of panels of data"
  },
  {
    "objectID": "01a_clustered.html#determining-sample-sizes",
    "href": "01a_clustered.html#determining-sample-sizes",
    "title": "1A: Clustered Data",
    "section": "Determining Sample Sizes",
    "text": "Determining Sample Sizes\nOne thing we are going to want to know is our sample size. Only we now have a few more questions to keep on top of. We need to know the different sample sizes at different levels.\nIn the description of the SchoolMot data above we are told the relevant numbers:\n\n\n\n\n\n\n  \n    \n    \n       \n      Unit\n      Sample Size\n    \n  \n  \n    Level 2\nSchool\n30\n    Level 1 (Observations)\nChildren\n900\n  \n  \n  \n\n\n\n\nWe can check this in our data:\n\nschoolmot &lt;- read_csv(\"https://uoepsy.github.io/data/schoolmot.csv\")\n# how many children? (how many rows in the data?)\nnrow(schoolmot)\n\n[1] 900\n\n# how many schools? (how many distinct values in the schoolid column?)\nn_distinct(schoolmot$schoolid)\n\n[1] 30\n\n\nAnother important thing to examine when you first get hierarchical data is the number of level 1 units that belong to each level 2 unit - i.e., do we have 100 children from Calderglen High School and only 10 from Broughton High School, or do we have the same number in each?\nWe can easily count how many children are in each school by counting the number of rows for each distinct value in the school identifier column. We could then pass this to the summary() function to see the minimum, median, mean, maximum etc. As we can see below, in this dataset every school has data from exactly 30 children (min is the same as max):\n\nschoolmot |&gt;\n  count(schoolid) |&gt;\n  summary()\n\n   schoolid               n     \n Length:30          Min.   :30  \n Class :character   1st Qu.:30  \n Mode  :character   Median :30  \n                    Mean   :30  \n                    3rd Qu.:30  \n                    Max.   :30"
  },
  {
    "objectID": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "href": "01a_clustered.html#icc---quantifying-clustering-in-an-outcome-variable",
    "title": "1A: Clustered Data",
    "section": "ICC - Quantifying clustering in an outcome variable",
    "text": "ICC - Quantifying clustering in an outcome variable\nThe IntraClass Correlation Coefficient (ICC) is a measure of how much variation in a variable is attributable to the clustering. It is the ratio of the variance between the clusters/groups to the total variance in the variable, and is often denoted by the symbol \\(\\rho\\):7\n\\[\n\\begin{align}\nICC \\; (\\rho) &= \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\text{Where} & \\\\\n& \\sigma^2_b: \\text{between-group variance} \\\\\n& \\sigma^2_e: \\text{within-group variance} \\\\  \n\\end{align}\n\\]\nThis is illustrated in the Figure 10 below, in which our continuous outcome variable (children’s grades) is on the y-axis, and we have the different groups (our set of 30 schools) across the x-axis. We can think of the “between-group variance” as the variance of the group means around the overall mean (the black dots around the horizontal black line), and the “within-group variance” as the variance of the individual observations around each group mean (each set of coloured points around their respective larger black dot):\n\n\nCode\nggplot(schoolmot, aes(x=schoolid, y=grade))+\n  geom_point(aes(col=schoolid),alpha=.3)+\n  stat_summary(geom = \"pointrange\")+\n  geom_hline(yintercept = mean(schoolmot$grade))+\n  scale_x_discrete(labels=abbreviate) + \n  theme(axis.text.x=element_text(angle=90))+\n  guides(col=\"none\")\n\n\n\n\n\nFigure 10: Variance in grades between schools. Data from https://uoepsy.github.io/data/schoolmot.csv\n\n\n\n\nThere are various packages that allow us to calculate the ICC, and when we get to fitting multilevel models we will see how we can extract it from a fitted model.\nIn the school motivation data (visualised above), it’s estimated that 22% of the variance in grades is due to school-related differences:\n\nlibrary(ICC)\nICCbare(schoolid, grade, data = schoolmot)\n\n[1] 0.2191859\n\n\n\n\n\n\n\n\noptional: calculating ICC manually\n\n\n\n\n\nWe have equal group sizes here (there are 30 schools, each with 30 observations), which makes calculating ICC by hand a lot easier, but it’s still a bit tricky.\nLet’s take a look at the formula for ICC:\n\\[\n\\begin{align}\nICC \\; (\\rho) = & \\frac{\\sigma^2_{b}}{\\sigma^2_{b} + \\sigma^2_e} \\\\\n\\qquad \\\\\n= & \\frac{\\frac{MS_b - MS_e}{k}}{\\frac{MS_b - MS_e}{k} + MS_e} \\\\\n\\qquad \\\\\n= & \\frac{MS_b - MS_e}{MS_b + (k-1)MS_e} \\\\\n\\qquad \\\\\n\\qquad \\\\\n\\text{Where:} & \\\\\nk = & \\textrm{number of observations in each group} \\\\\n\\qquad \\\\\nMS_b = & \\textrm{Mean Squares between groups} \\\\\n= & \\frac{\\text{Sums Squares between groups}}{df_\\text{groups}}\n= \\frac{\\sum\\limits_{i=1}(\\bar{y}_i - \\bar{y})^2}{\\textrm{n groups}-1}\\\\\n\\qquad \\\\\nMS_e = & \\textrm{Mean Squares within groups} \\\\\n= & \\frac{\\text{Sums Squares within groups}}{df_\\text{within groups}}\n= \\frac{\\sum\\limits_{i=1}\\sum\\limits_{j=1}(y_{ij} - \\bar{y_i})^2}{\\textrm{n obs}-\\textrm{n groups}}\\\\\n\\end{align}\n\\]\nSo we’re going to need to calculate the grand mean of \\(y\\), the group means of \\(y\\), and then the various squared differences between group means and grand mean, and between observations and their respective group means.\nThe code below will give us a couple of new columns. The first is the overall mean of \\(y\\), and the second is the mean of \\(y\\) for each group. Note that we calculate this by first using group_by to make the subsequent operation (the mutate) be applied to each group. To ensure that the grouping does not persist after this, we’ve passed it to ungroup at the end.\n\nschoolmot &lt;- \n  schoolmot |&gt; \n  mutate(\n    grand_mean = mean(grade)\n  ) |&gt;\n  group_by(schoolid) |&gt;\n  mutate(\n    group_mean = mean(grade)\n  ) |&gt;\n  ungroup()\n\nNow we need to create a column which is the squared differences between the observations \\(y_{ij}\\) and the group means \\(\\bar{y_i}\\).\nWe also want a column which is the squared differences between the group means \\(\\bar{y_i}\\) and the overall mean \\(\\bar{y}\\).\n\nschoolmot &lt;- schoolmot |&gt; \n  mutate(\n    within = (grade-group_mean)^2,\n    between = (group_mean-grand_mean)^2\n  )\n\nAnd then we want to sum them:\n\nssbetween = sum(schoolmot$between)\nsswithin = sum(schoolmot$within)\n\nFinally, we divide them by the degrees of freedom. Our degrees of freedom for our between group variance \\(30 \\text{ groups} - 1 \\text{ grand mean}=29\\)\nOur degrees of freedom for our within group variance is \\(900 \\text{ observations} - 30 \\text{ groups}=870\\)\n\n# Mean Squares between\nmsb = ssbetween / (30-1)\n# Mean Squares within \nmse = sswithin / (900-30)\n\nAnd calculate the ICC!!!\nThe 29 here is the \\(k-1\\) in the formula above, where \\(k\\) is the number of observations within each group.\n\n# ICC\n(msb-mse) /(msb + (29*mse))\n\n[1] 0.2191859\n\n\n\n\n\nAnother way of thinking about the ICC is that it is the correlation between two randomly drawn observations from the same group. This is a bit of a tricky thing to get your head round if you try to relate it to the type of “correlation” that you are familiar with. Pearson’s correlation (e.g think about a typical scatterplot) operates on pairs of observations (a set of values on the x-axis and their corresponding values on the y-axis), whereas ICC operates on data which is structured in groups.\nWe can think of it as the average correlation between all possible pairs of observations from the same group. Suppose I pick a school, and within that pick 2 children and plot their grades against each other. I randomly pick another school, and another two children from it, and add them to the plot, and then keep doing this (Figure 11). The ICC is the correlation between such pairs.\n\n\n\n\n\nFigure 11: ICC is the correlation of randomly drawn pairs from the same group\n\n\n\n\n\n\n\n\n\n\noptional: a little simulation\n\n\n\n\n\nWe can actually do the “randomly drawn pair of observations from the same group” via simulation.\nThe code below creates a function for us to use. Can you figure out how it works?\n\nget_random_pair &lt;- function(){\n  my_school = sample(unique(schoolmot$schoolid), 1)\n  my_obs = sample(schoolmot$grade[schoolmot$schoolid == my_school], size=2)\n  my_obs\n}\n\nTry it out, by running it several times.\n\nget_random_pair()\n\n[1] 28.35 50.84\n\n\nNow let’s make our computer do it loads and loads of times:\n\n# replicate is a way of making R execute the same code repeatedly, n times.\nsims &lt;- replicate(10000, get_random_pair())\n# t() is short for \"transpose\" and simple rotates the object 90 degrees (so rows become columns and columns become rows)\nsims &lt;- t(sims)\ncor(sims[,1], sims[,2])\n\n[1] 0.2097805\n\n\n\n\n\n\n\n\n\n\n\noptional: correlations from group-structured data\n\n\n\n\n\nLet’s suppose we had only 2 observations in each group.\n\n\n# A tibble: 7 × 3\n  cluster observation y    \n* &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 group_1 1           4    \n2 group_1 2           2    \n3 group_2 1           4    \n4 group_2 2           2    \n5 group_3 1           7    \n6 group_3 2           5    \n7 ...     ...         ...  \n\n\nThe ICC for this data is 0.18.\nNow suppose we reshape our data so that we have one row per group, and one column for each observation to look like this:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nCalculating Pearson’s correlation on those two columns yields 0.2, which isn’t quite right. It’s close, but not quite..\n\nThe crucial thing here is that it is completely arbitrary which observations get called “obs1” and which get called “obs2”.\nThe data aren’t paired, they’re just random draws from a group.\n\nEssentially, there are lots of different combinations of “pairs” here. There are the ones we have shown above:\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 4     2    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\nBut we might have equally chosen any of these:\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 4     2    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 7     5    \n4 group_4 2     7    \n5 group_5 8     3    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\n…\n\n\n# A tibble: 7 × 3\n  cluster obs1  obs2 \n* &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 group_1 2     4    \n2 group_2 2     4    \n3 group_3 5     7    \n4 group_4 2     7    \n5 group_5 3     8    \n6 group_6 6     7    \n7 ...     ...   ...  \n\n\n\n\nIf we take the correlation of all these combinations of pairings, then we get our ICC of 0.18!\nICC = the expected correlation of a randomly drawn pair of observations from the same group.\n\n\n\n\nWhy ICC?\nThe ICC tells us the proportion of the total variability in an outcome variable that is attributable to the differences between groups/clusters. It ranges from 0 to 1.\nThis helps us to assess the appropriateness of using a multilevel approach. If the ICC is high, it suggests that a large amount of the variance is at the cluster level (justifying the use of multilevel modeling to account for this structure).\nThere are no cut-offs - the interpretation of ICC values is inherently field-specific, as what constitutes a high or low ICC depends on the nature of the outcome variable, and the hierarchical structure within a particular research context."
  },
  {
    "objectID": "01a_clustered.html#visualisations",
    "href": "01a_clustered.html#visualisations",
    "title": "1A: Clustered Data",
    "section": "Visualisations",
    "text": "Visualisations\nWhen we’re visualising data that has a hierarchical structure such as this (i.e. observations grouped into clusters), we need to be careful to think about what exactly we want to show. For instance, as we are interested in how motivation is associated with grades, we might make a little plot of the two variables, but this could hide the association that happens within a given school (see e.g. Figure 5 from earlier).\nSome useful ggplot tools here are:\n\nfacet_wrap() - make a separate little plot for each level of a grouping variable\nthe group aesthetic - add separate geoms (shapes) for each level of a grouping variable\n\n\n\nfacets\n\nggplot(schoolmot, aes(x=motiv,y=grade))+\n  geom_point() +\n  facet_wrap(~schoolid)\n\n\n\n\n\n\n\n\n\n\ngroup\n\nggplot(schoolmot, aes(x=motiv,y=grade,group=schoolid))+\n  geom_point(alpha=.2) +\n  geom_smooth(method=lm, se=FALSE)"
  },
  {
    "objectID": "01a_clustered.html#footnotes",
    "href": "01a_clustered.html#footnotes",
    "title": "1A: Clustered Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy is this? It’s because the formula to calculate the standard error involves \\(\\sigma^2\\) - the variance of the residuals. If this standard deviation is not accurate (because the residuals are non-normally distributed, or because it changes across the fitted model), then this in turn affects the accuracy of the standard error of the coefficient↩︎\nWith the exception of Generalized Least Squares (an extension of Weighted Least Squares), for which we can actually specify a correlational structure of the residuals. As this course focuses on multilevel models, we will not cover GLS here. However, it can often be a useful method if our the nature of the dependency in our residuals is simply a nuisance thing (i.e. not something that has any properties which are of interest to us).↩︎\nor “mean squares residual”↩︎\nIt’s “turtles all the way down”↩︎\nDepending on the research question and design of the study, we may be only interested in things that occur at “level 1” (the lowest observation level). While not the focus of this course, there are alternative methods (survey weighting tools, cluster robust standard errors, or generalised estimating equations) that we may use to simply “account for the nuisance clustering”.↩︎\nNote, this is not true for a set of analytical methods called “cluster analysis”, which attempts to identify clusters that haven’t been measured/observed (or may not even ‘exist’ in any real sense of the word).↩︎\nalthough this symbol get used for lots of other correlation-y things too!↩︎"
  },
  {
    "objectID": "01b_lmm.html",
    "href": "01b_lmm.html",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "",
    "text": "This reading:\n\nIntroducing the multilevel model (MLM)\nHow the MLM achieves partial pooling\nFitting multilevel models in R\nModel estimation and convergence\n\n\n\n\n\n\n\ndifferent names for the same thing\n\n\n\n\n\nThe methods we’re going to start to look at are known by lots of different names (see Figure 1). The core idea is that model parameters vary at more than one level..\n\n\n\n\n\nFigure 1: size weighted by hits on google scholar search (sept 2020)"
  },
  {
    "objectID": "01b_lmm.html#random-intercepts",
    "href": "01b_lmm.html#random-intercepts",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random intercepts",
    "text": "random intercepts\nTo extend the single-level regression model to the multi-level regression model, we add in an extra suffix to our equation to indicate which cluster an observation belongs to.1 Then, we can take a coefficient \\(b_?\\) and allow it to be different for each cluster \\(i\\) by adding the suffix \\(b_{?i}\\). Below, we have done this for our intercept \\(b_0\\), which has become \\(b_{0i}\\).\nHowever, we also need to define these differences in some way, and the multilevel model does this by expressing each cluster’s intercept as a deviation (\\(\\zeta_{0i}\\) for cluster \\(i\\), below) from a fixed number (\\(\\gamma_{00}\\), below). Because these differences are to do with the clusters (and not the individual observations within them), we often write these as a “level 2 equation”:\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\n\\color{red}y_{ij} &\\color{black}= \\color{green}b_{0i} \\color{blue} + b_1 \\cdot x_{ij} \\color{black}+ \\epsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\n\\color{green}b_{0i} &\\color{black}= \\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nmixed-effects notation\n\n\n\n\n\nInstead of writing several equations at multiple levels, we substitute the Level 2 terms into the Level 1 equation to get something that is longer, but all in one:\n\\[\n\\color{red}y_{ij} \\color{black}= \\underbrace{(\\color{blue}\\gamma_{00} \\color{black}+ \\color{orange}\\zeta_{0i}\\color{black})}_{\\color{green}b_{0i}} \\cdot 1 + \\color{blue}b_{1} \\cdot x_{ij} \\color{black}+  \\varepsilon_{ij}\n\\]\nThis notation typically corresponds with the “mixed effects” terminology because parameters can now be a combination of both a fixed number and a random deviation, as in the intercept below:\n\\[\ny_{ij} = \\underbrace{(\\underbrace{\\gamma_{00}}_{\\textrm{fixed}} + \\underbrace{\\zeta_{0i}}_{\\textrm{random}})}_{\\text{intercept, }b_{0i}} \\cdot 1 + \\underbrace{b_1}_{\\textrm{fixed}} \\cdot x_{ij} +  \\varepsilon_{ij}\n\\]\n\n\n\nReturning to our school children’s grade example, we can fit a model with “random intercepts for schools”, which would account for some schools having higher grades, some having lower grades, etc.\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\end{align}\n\\] If we consider one of our schools (e.g. “Beeslack Community High School”) we can see that our model predicts that this school has higher grades than most other schools (Figure 3). We can see how this is modelled as a deviation \\(\\zeta_{0\\text{B}}\\) (B for Beeslack) from some fixed value \\(\\gamma_{00}\\).\n\n\n\n\n\nFigure 3: Fitted values from a multilevel model with random intercepts for schools\n\n\n\n\nAt this point, you might be wondering how this is any different from simply fitting clusters as an additional predictor in a single level regression (i.e. a clusters-as-fixed-effect approach of lm(grade ~ motiv + schoolid)), which would also estimate a difference for each cluster?\n\nThe key to the multilevel model is that we are not actually estimating the cluster-specific lines themselves (although we can get these out). We are estimating a distribution of deviations.\n\nSpecifically, the parameters of the multilevel model that are estimated are the mean and the variance of a normal distribution of clusters.\nSo the parameters that are estimated from our model with a random intercept by-schools, are:\n\n\n\n\na fixed intercept \\(\\gamma_{00}\\)\n\nthe variance with which schools deviate from the fixed intercept \\(\\sigma^2_0\\)\n\na fixed slope for motiv \\(b_1\\)\n\nand we also need the residual variance too \\(\\sigma^2_\\varepsilon\\)\n\n\n\n\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_1 \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{where: }& \\\\\n&\\zeta_{0i} \\sim N(0,\\sigma_0) \\\\\n&\\varepsilon_{ij} \\sim N(0,\\sigma_\\varepsilon) \\\\\n\\end{align}\n\\]\n\n\nRemember, \\(\\sim N(m,s)\\) is a way of writing “are normally distributed with a mean of \\(m\\) and a standard deviation of \\(s\\)”. So the \\(\\zeta_{0i} \\sim N(0,\\sigma_0)\\) bit is saying that the school deviations from the fixed intercept are modelled as a normal distribution, with a mean of 0, and a standard deviation of \\(\\sigma_0\\) (which gets estimated by our model).\nThis can be seen in Figure 4 - the model is actually estimating a fixed intercept; a fixed slope; and the spread of a normal distribution of school-level deviations from the fixed intercept.\n\n\n\n\n\nFigure 4: grade predicted by motivation, with a by-school random intercept. The school-level intercepts are modelled as a normal distribution. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)."
  },
  {
    "objectID": "01b_lmm.html#random-slopes",
    "href": "01b_lmm.html#random-slopes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "random slopes",
    "text": "random slopes\nIt is not just the intercept that we can allow to vary by-schools. We can also model cluster-level deviations from other coefficients (i.e. slopes). For instance, we can allow the slope of \\(x\\) on \\(y\\) to be different for each cluster, by specifying in our model that \\(b_{1i}\\) is a distribution of cluster deviations \\(\\zeta_{1i}\\) around the fixed slope \\(\\gamma_{10}\\).\n\\[\n\\begin{align}\n\\text{For observation }j&\\text{ in cluster }i \\\\\n\\text{Level 1:}& \\\\\ny_{ij} &= b_{0i} + b_{1i} \\cdot x_{ij} + \\varepsilon_{ij} \\\\\n\\text{Level 2:}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} \\\\\n& \\qquad \\\\\n\\text{Where:}& \\\\\n& \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\end{align}\n\\]\nWhen we have random intercepts and random slopes, our assumption is that both of intercepts and slopes are normally distributed. However, we also typically allow these to be correlated, so the complicated looking bit at the bottom of the equation above is really just saying “random intercepts and slopes are normally distributed with mean of 0 and standard deviations of \\(\\sigma_0\\) and \\(\\sigma_1\\) respectively, and with a correlation of \\(\\rho \\sigma_0 \\sigma_1\\)”. We’ll see more on this in future weeks, so don’t worry too much right now.\nIn Figure 5, we can see now that both the intercept and the slope of grades across motivation are varying by-school.\n\n\n\n\n\nFigure 5: predicted values from the multilevel model that includes by-school random intercepts and by-school random slopes of motivation.\n\n\n\n\nMuch like for the random intercepts, we are modelling the random slopes as the distribution of school-level deviations \\(\\zeta_{1i}\\) around a fixed estimate \\(\\gamma_{10}\\).\nSo each group (school) now has, as visualised in Figure 6:\n\na deviation from the fixed intercept\na deviation from the fixed slope\n\n\n\n\n\n\nFigure 6: random intercepts and random slopes\n\n\n\n\nWhile it’s possible to show the distribution of intercepts on the left hand side of our grade ~ motiv plot, it’s hard to put the distribution of slopes on the same plot, so I have placed these in the bottom panel in Figure 7. We can see, for instance, that “Hutcheson’s Grammar School” has a higher intercept, but a lower slope.\n\n\n\n\n\nFigure 7: grade predicted by motivation, with by-school random intercepts and by-school random slopes of motivation. Parameters estimated by the model are shown in purple (fixed effects) and orange (variance components)\n\n\n\n\n\n\n\n\n\n\noptional: joint distribution of intercept and slopes\n\n\n\n\n\nWhen we have random intercepts and slopes in our model, we don’t just estimate two separate distributions of intercept deviations and slope deviations. We estimate them as related. This comes back to the part of the equation we mentioned briefly above, where we used:\n\n\\(\\sigma_0\\) to represent the standard deviation of intercept deviations\n\\(\\sigma_1\\) to represent the standard deviation of slope deviations\n\\(\\rho \\sigma_0 \\sigma_1\\) to represent the correlation between intercept deviations and slope deviations\n\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0 & \\rho \\sigma_0 \\sigma_1 \\\\\n        \\rho \\sigma_0 \\sigma_1 & \\sigma_1\n    \\end{bmatrix}\n\\right)\n\\] For a visual intuition about this, see Figure 8, in which the x-axis is the intercept deviations, and the y-axis is the slope deviations. We can see that these are each distributed normally, but are negatively related (schools with higher intercepts tend to have slightly lower slopes).\n\n\n\n\n\nFigure 8: Intercept deviations (x axis) and slope deviations (y axis). One school is highlighted for comparison with previous plot of fitted values"
  },
  {
    "objectID": "01b_lmm.html#extracting-model-parameters",
    "href": "01b_lmm.html#extracting-model-parameters",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Extracting model parameters",
    "text": "Extracting model parameters\nAlongside summary(), there are some useful functions in R that allow us to extract the parameters estimated by the model:\n\nfixed effects\nThe fixed effects represent the estimated average relationship within the entire sample of clusters.\n\nfixef(smod2)\n\n(Intercept)       motiv \n  29.233320    4.475717 \n\n\n\n\nrandom effect variances\nThe random effect variances represent the estimated spread with which clusters vary around the fixed effects\n\nVarCorr(smod2)\n\n Groups   Name        Std.Dev. Corr  \n schoolid (Intercept) 12.5745        \n          motiv        2.0708  -0.698\n Residual             11.8036"
  },
  {
    "objectID": "01b_lmm.html#making-model-predictions",
    "href": "01b_lmm.html#making-model-predictions",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Making model predictions",
    "text": "Making model predictions\nWhile they are not computed directly in the estimation of the model, the cluster-specific deviations from fixed effects can be extracted from our models\n\nrandom effects\nOften referred to as the “random effects”, the deviations for each cluster from the fixed effects can be obtained using ranef().\nNote that each row is a cluster (a school, in this example), and the columns show the distance from the fixed effects. We can see that “Anderson High School” has an estimated intercept that is 7.07 higher than average, and an estimate slope of motivation that is 0.47 lower than average.\n\nranef(smod2)\n\n$schoolid\n                                        (Intercept)       motiv\nAnderson High School                     7.07164826 -0.46505592\nArdnamurchan High School                -7.26417838  0.70012536\nBalwearie High School                  -20.53626558  2.31397177\nBeeslack Community High School          18.63574795 -1.45057126\n...                                     ...          ...\nWe can also visualise all these using a handy function. This sort of visualisation is great for checking for peculiar clusters.\n\ndotplot.ranef.mer(ranef(smod2))\n\n$schoolid\n\n\n\n\n\n\n\n\n\n\n\ncluster coefficients\nRather than looking at deviations from fixed effects, we can calculate the intercept and slope for each cluster.\nFor example, if we are estimating that “Anderson High School” has an intercept that is 7.07 higher than average, and the average is 29.23, then we know that this has an intercept of 29.23 + 7.07 = 36.3.\nWe can get these out using coef()\n\ncoef(smod2)\n\n$schoolid\n                                    (Intercept)  motiv\nAnderson High School                36.304968    4.010661\nArdnamurchan High School            21.969141    5.175842\nBalwearie High School                8.697054    6.789689\nBeeslack Community High School      47.869068    3.025146\n...                                 ...          ...\n\n\nfixef() + ranef() = coef()"
  },
  {
    "objectID": "01b_lmm.html#a-more-complex-model",
    "href": "01b_lmm.html#a-more-complex-model",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "A more complex model",
    "text": "A more complex model\nThe models fitted in the reading thus far are fairly simple in that they only really have one predictor (a measure of a child’s education motivation, motiv), and our observations (children) happen to be clustered into groups (schools).\nHowever, the multilevel model can also allow us to study questions that we might have about features of those groups (i.e., things about the schools) and how those relate to observation-level variables (things about the children).\nFor instance, we might have questions that take the form:\n\n“does [Level-2 variable] predict [Level-1 outcome]?”\n\n“does [Level-2 variable] influence the relationship between [Level-1 predictor] and [Level-1 outcome]?\n\n(in our example, Level-1 = children, Level-2 = Schools).\nConsider, for example, if we want to investigate whether the relationship between children’s motivation levels and their grades is different depending upon the source of school funding (private vs state).\nAddressing such questions simply requires a more complex fixed effect structure (specifically the interaction between motiv and funding (private vs state).\n\nsmod3 &lt;- lmer(grade ~ motiv * funding + (1 + motiv | schoolid), \n              data = schoolmot)\n\nNote, we cannot include funding in the random effects part of our model, because “the effect of funding on school grades” is something we assess by comparing between schools. We cannot think of that effect varying by-school because every school is either “private” or “state” funded. We never observe “Ardnamurchan High School” as anything other than “state” funded, so “the effect on grades of being state/private funded” does not exist for Ardnamurchan High School (and hence it is illogical to try and say that this effect varies between schools).\nOur additions to the fixed effects part here simply add in a couple of fixed terms to our model (the funding coefficient and the motiv:funding interaction coefficient). This means that in terms of our model structure, it is simply moving from the single line we had in Figure 7, to having two lines (one for “private” schools and one for “state” schools). The random effects are, as before, the variance in deviations of individual schools around these fixed estimates.\n\n\n\n\n\n\nModel equation\n\n\n\n\n\nThis model is not too much of an extension on our previous equation, but when we move to models with more than 2 levels (e.g., children in schools in districts), these equations can become very cumbersome.\nAdditionally, as you become more practiced at fitting multilevel models, you may well begin to think of these models in terms of the lmer() syntax in R, rather than in terms of the mathematical expressions.\nThis is absolutely fine, and you should feel free to ignore these equations if they are of no help to your understanding!\nBecause the funding variable is something we measure at Level 2 (schools), in most notations it gets placed in the level 2 equations:\n\\[\n\\begin{align}\n\\text{For Child }j\\text{ in School }i& \\\\\n\\text{Level 1 (child):}& \\\\\n\\text{grade}_{ij} &= b_{0i} + b_{1i} \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (school):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} + \\gamma_{01} \\cdot \\text{Funding}_i\\\\\nb_{1i} &= \\gamma_{10} + \\zeta_{1i} + \\gamma_{11} \\cdot \\text{Funding}_i\\\\\n\\end{align}\n\\]\nIt is sometimes easier to think of this in the “mixed effects notation” we saw above, where we substitute the level 2 equations into the level 1 equation, and rearrange to get:\n\\[\n\\begin{align}\n&\\text{For Child }j\\text{ in School }i \\\\\n&\\text{grade}_{ij} = (\\gamma_{00} + \\zeta_{0i}) + \\gamma_{01} \\cdot \\text{Funding}_i + (\\gamma_{10} + \\zeta_{1i})\\cdot \\text{motiv}_{ij} + \\gamma_{11} \\cdot \\text{Funding}_i \\cdot \\text{motiv}_{ij} + \\epsilon_{ij} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\noptional: an attempted visual explanation\n\n\n\n\n\nFigure 12 shows an attempted visual intuition of how the different parts of the model work:\n\n\n\n\n\nFigure 12: visual explanation of a model with a cross-level interaction\n\n\n\n\n\n\n\n\n\n\n\n\nmodel summary\n\nsummary(smod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: grade ~ motiv * funding + (1 + motiv | schoolid)\n   Data: schoolmot\n\nREML criterion at convergence: 7083.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08250 -0.67269  0.03043  0.63562  3.13012 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolid (Intercept) 105.124  10.253        \n          motiv         2.595   1.611   -0.48\n Residual             139.030  11.791        \nNumber of obs: 900, groups:  schoolid, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         40.3143     4.6414   8.686\nmotiv                2.6294     0.8652   3.039\nfundingstate       -17.2531     5.7346  -3.009\nmotiv:fundingstate   2.8485     1.0591   2.689\n\nCorrelation of Fixed Effects:\n            (Intr) motiv  fndngs\nmotiv       -0.782              \nfundingstat -0.809  0.633       \nmtv:fndngst  0.639 -0.817 -0.773\n\n\n\n\nplot\nFor plotting the fixed effect estimates (which are often the bit we’re most interested in) from multilevel models, we can’t rely on using predict(), fitted() or augment(), as these return to us the cluster-specific predicted values.\nInstead, we need to use tools like the effects package that we saw at the end of the USMR course, that takes a fixed effect and averages over the other terms in the model:\n\nlibrary(effects)\neffect(term=\"motiv*funding\",mod=smod3,xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=motiv,y=fit,col=funding,fill=funding))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=.3)"
  },
  {
    "objectID": "01b_lmm.html#convergence-warnings-singular-fits",
    "href": "01b_lmm.html#convergence-warnings-singular-fits",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "convergence warnings & singular fits",
    "text": "convergence warnings & singular fits\nThere are different algorithms that we can use to actually undertake the iterative estimation procedure, which we can apply by using different ‘optimisers’.\n\n\nlmer(formula,         data = dataframe,          REML = logical,          control = lmerControl(options)          )\n\n\nTechnical problems to do with model convergence and ‘singular fit’ come into play when the optimiser we are using either can’t find a suitable maximum, or gets stuck in a plateau, or gets stuck trying to move towards a number that we know isn’t possible.\nFor large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning when trying to fit a model, and in the coming weeks you will see plenty of warnings such as:\n\nA typical convergence warning:\n\n\nwarning(s): Model failed to converge with max|grad| = 0.0071877 (tol = 0.002, component 1)\n\nA singular fit:\n\n\nboundary (singular) fit: see ?isSingular\n\n\n\nDo not trust the results of a model that does not converge\n\nThere are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them), but for the time being, if lmer() gives you convergence errors or singular fits, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model.\n\nlmer(y ~ 1 + x1 + ... + (1 + .... | g), data = df, \n     control = lmerControl(optimizer = \"bobyqa\"))"
  },
  {
    "objectID": "01b_lmm.html#footnotes",
    "href": "01b_lmm.html#footnotes",
    "title": "1B: Linear Mixed Models/Multi-level Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome books use “cluster \\(j\\) &gt;&gt; observation \\(i\\)”, others use “cluster \\(i\\) &gt;&gt; observation \\(j\\)”. We use the latter here↩︎\nthis exact formula applies to the model with random intercepts, but the logic scales up when random slopes are added↩︎\nremember, variance = standard deviation squared↩︎\nit’s a bit like n-1 being in the denominator of the formula for standard deviation↩︎"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "01ex.html#footnotes",
    "href": "01ex.html#footnotes",
    "title": "Week 1 Exercises: Intro to MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "02a_inference.html",
    "href": "02a_inference.html",
    "title": "2A: Inference for MLM",
    "section": "",
    "text": "This reading:\nConducting inference (i.e. getting confidence intervals or p-values, model comparisons) for MLMs can be tricky partly because there are a variety of different methods that have been developed.\nThis reading briefly explains why getting p-values from lmer() is not as easy as it was for lm(), before giving an outline of some of the main approaches people tend to take. Don’t feel like you have to remember all of these, just be aware that they exist, and refer back to this page whenever you need to."
  },
  {
    "objectID": "02a_inference.html#summary",
    "href": "02a_inference.html#summary",
    "title": "2A: Inference for MLM",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\ndf approximations\nlikelihood based\nparametric bootstrap\n\n\n\n\ntests/CIs of individual parameters\nTests of individual parameters can be done by refitting with lmerTest::lmer(...) for the Satterthwaite (S) method, or using parameters::model_parameters(model, ci_method=\"kr\") for Kenward Rogers (KR).\nProfile likelihood CIs for individual parameters can be obtained via confint(m, method=\"profile\"), but this can be computationally demanding.\nParametric Bootstrapped CIs for individual parameters can be obtained via confint(m, method=\"boot\")\n\n\nmodel comparisons(different fixed effects, same random effects)\nComparisons of models that differ only in their fixed effects can be done via \\(F\\) tests in the pbkrtest package:SATmodcomp(m2, m1) for S and KRmodcomp(m2, m1) for KR.\nComparisons of models that differ only in their fixed effects can be done via LRT using anova(m1, m2)\nComparisons of models that differ only in their fixed effects can be done via a bootstrapped LRT using PBmodcomp(m2, m1) from the pbkrtest package.\n\n\n\nFor KR, models must be fitted with REML=TRUE (a good option for small samples). For S, models can be fitted with either.\nFor likelihood based methods for fixed effects, models must be fitted with REML=FALSE.Likelihood based methods are asymptotic (i.e. hold when \\(n \\rightarrow \\infty\\)). Best avoided with smaller sample sizes (i.e. a small number of clusters)\nTime consuming, but considered best available method (can be problematic with unstable models)\n\n\n\n\n\n\n\n\n\noptional: testing random effects?\n\n\n\n\n\nTests of random effects are difficult because the null hypothesis (the random effect variance is zero) lies on a boundary (you can’t have a negative variance). Comparisons of models that differ only in their random effects can be done by comparing ratio of likelihoods when fitted with REML=TRUE (this has to be done manually), but these tests should be treated with caution.\nWe can obtain confidence intervals for our random effect variances using both the profile likelihood and the parametric boostrap methods discussed above.\nAs random effects are typically part of the experimental design, there is often little need to test their significance. In most cases, the maximal random effect structure can be conceptualised without reference to the data or any tests, and the inclusion/exclusion of specific random effects is more a matter of what simplifications are required for the model to converge. Inclusion/exclusion of parameters based on significance testing is rarely, if ever a sensible approach."
  },
  {
    "objectID": "02a_inference.html#footnotes",
    "href": "02a_inference.html#footnotes",
    "title": "2A: Inference for MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(n\\) observations minus \\(k\\) parameters (slope of x) minus 1 intercept↩︎"
  },
  {
    "objectID": "02b_loglong.html",
    "href": "02b_loglong.html",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "",
    "text": "This reading:\nTwo examples!\n\nLogistic multilevel models (lm() is to glm() as lmer() is to glmer())\n\nMuch of the heavy lifting in understanding the transition from linear &gt;&gt; logistic models is just the same as USMR Week 10, so it might be worth looking back over that for a refresher.\n\n“Change over time” - Fitting multilevel models to longitudinal data.\n\nThe application of multilevel models to longitudinal data is very much just that - we are taking the same sort of models we learned last week and simply applying them to a different context in which “time” is a predictor."
  },
  {
    "objectID": "02b_loglong.html#example",
    "href": "02b_loglong.html#example",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: monkeystatus.csv\nOur primate researchers have been busy collecting more data. They have given a sample of Rhesus Macaques various problems to solve in order to receive treats. Troops of Macaques have a complex social structure, but adult monkeys tend can be loosely categorised as having either a “dominant” or “subordinate” status. The monkeys in our sample are either adolescent monkeys, subordinate adults, or dominant adults. Each monkey attempted various problems before they got bored/distracted/full of treats. Each problems were classed as either “easy” or “difficult”, and the researchers recorded whether or not the monkey solved each problem.\nWe’re interested in how the social status of monkeys is associated with the ability to solve problems.\nThe data is available at https://uoepsy.github.io/data/msmr_monkeystatus.csv.\n\n\ngetting to know my monkeys\nWe know from the study background that we have a series group of monkeys who have each attempted to solve some problems. If we look at our data, we can see that it is already in long format, in that each row represents the lowest unit of observation (a single problem attempted). We also have the variable monkeyID which indicates what monkey each problem has been attempted by. We can see the status of each monkey, and the difficulty of each task, along with whether it was solved:\n\nlibrary(tidyverse)\nlibrary(lme4)\nmstat &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_monkeystatus.csv\")\nhead(mstat)\n\n# A tibble: 6 × 4\n  status      difficulty monkeyID solved\n  &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;\n1 subordinate easy       Seunghoo      1\n2 subordinate easy       Seunghoo      0\n3 subordinate difficult  Seunghoo      0\n4 subordinate easy       Seunghoo      1\n5 subordinate difficult  Seunghoo      0\n6 subordinate easy       Seunghoo      1\n\n\nWe can do some quick exploring to see how many monkeys we have (50), and how many problems each one attempted (min = 3, max = 11:\n\nmstat |&gt; \n  count(monkeyID) |&gt; # count the monkeys!  \n  summary()\n\n   monkeyID               n        \n Length:50          Min.   : 3.00  \n Class :character   1st Qu.: 6.25  \n Mode  :character   Median : 8.00  \n                    Mean   : 7.94  \n                    3rd Qu.:10.00  \n                    Max.   :11.00  \n\n\nLet’s also see how many monkeys of different statuses we have in our sample:\n\nmstat |&gt; \n  group_by(status) |&gt; # group statuses\n  summarise(\n    # count the distinct monkeys\n    nmonkey = n_distinct(monkeyID)\n  ) \n\n# A tibble: 3 × 2\n  status      nmonkey\n  &lt;chr&gt;         &lt;int&gt;\n1 adolescent       16\n2 dominant         23\n3 subordinate      11\n\n\nIt’s often worth plotting as much as you can to get to a sense of what we’re working with. Here are the counts of easy/difficult problems that each monkey attempted. We can see that Richard only did difficult problems, and Nadheera only did easy ones, but most of the monkeys did both types of problem.\n\n# which monkeys did what type of problems? \nmstat |&gt; count(status, monkeyID, difficulty) |&gt;\n  ggplot(aes(x=difficulty,y=n, fill=status))+\n  geom_col()+\n  facet_wrap(~monkeyID) +\n  scale_x_discrete(labels=abbreviate) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen working with binary outcomes, it’s often useful to calculate and plot proportions. In this case, the proportions of problems solved for each status of monkey. At first glance it looks like “subordinate” monkeys solve more problems, and adolescents solve fewer (makes sense - they’re still learning!).\n\n# a quick look at proportions of problems solved:\nggplot(mstat, aes(x=difficulty, y=solved,\n                       col=status))+\n  stat_summary(geom=\"pointrange\",size=1)+\n  facet_wrap(~status)\n\n\n\n\n\n\n\n\n\n\nmodels of monkeys\nNow we come to fitting our model.\nRecall that we are interested in how the ability to solve problems differs between monkeys of different statuses. It’s very likely that difficulty of a problem is going to influence that it is solved, so we’ll control for difficulty.\nglmer(solved ~ difficulty + status + \n      ...\n      data = mstat, family = binomial)\nWe know that we have multiple datapoints for each monkey, and it also makes sense that there will be monkey-to-monkey variability in the ability to solve problems (e.g. Brianna may be more likely to solve problems than Jonathan).\nglmer(solved ~ difficulty + status + \n      (1 + ... | monkeyID),\n      data = mstat, family = binomial)\nFinally, it also makes sense that effects of problem-difficulty might vary by monkey (e.g., if Brianna is just really good at solving problems, problem-difficulty might not make much difference. Whereas if Jonathan is struggling with the easy problems, he’s likely to really really struggle with the difficult ones!).\nFirst, we’ll relevel the difficulty variable so that the reference level is “easy”:\n\nmstat &lt;- mstat |&gt; mutate(\n  difficulty = fct_relevel(factor(difficulty), \"easy\")\n)\n\nand fit our model:\n\nmmod &lt;- glmer(solved ~ difficulty + status + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nsummary(mmod)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n   Data: mstat\n\n     AIC      BIC   logLik deviance df.resid \n   503.7    531.6   -244.8    489.7      390 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9358 -0.6325 -0.3975  0.6748  2.5161 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n monkeyID (Intercept)         1.552    1.246         \n          difficultydifficult 1.371    1.171    -0.44\nNumber of obs: 397, groups:  monkeyID, 50\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          -0.3945     0.3867  -1.020  0.30770   \ndifficultydifficult  -0.8586     0.3053  -2.812  0.00492 **\nstatusdominant        0.6682     0.4714   1.417  0.15637   \nstatussubordinate     1.4596     0.5692   2.564  0.01034 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) dffclt sttsdm\ndffcltydffc -0.333              \nstatusdmnnt -0.721 -0.031       \nstatssbrdnt -0.594 -0.033  0.497\n\n\n\n\ntest and visualisations of monkey status\nTo examine if monkey status has an effect, we can compare with the model without status:\n\n\nCode\nmmod0 &lt;- glmer(solved ~ difficulty + \n      (1 + difficulty | monkeyID),\n      data = mstat, family = binomial)\nanova(mmod0, mmod)\n\n\nData: mstat\nModels:\nmmod0: solved ~ difficulty + (1 + difficulty | monkeyID)\nmmod: solved ~ difficulty + status + (1 + difficulty | monkeyID)\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmmod0    5 506.13 526.05 -248.07   496.13                       \nmmod     7 503.70 531.58 -244.85   489.70 6.4367  2    0.04002 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd we can see that the status of monkeys is associated with differences in the probability of successful problem solving (\\(\\chi^2(2)\\) = 6.44, p &lt; 0.05).\nAnd if we want to visualise the relevant effect, we can (as we did with glm()) plot on the predicted probability scale, which is much easier to interpret:\n\n\nCode\nlibrary(effects)\neffect(term=c(\"status\",\"difficulty\"), mod=mmod) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=difficulty, y=fit))+\n  geom_pointrange(aes(ymin=lower,ymax=upper, col=status),\n                  size=1, lwd=1,\n                  position=position_dodge(width=.3)) +\n  labs(x = \"problem difficulty\", y = \"predicted probability\")\n\n\n\n\n\n\n\n\n\n\n\ninterpretation\nAnd just with the single level logistic models, our fixed effects can be converted to odds ratios (OR), by exponentiation:\n\ncbind(\n  fixef(mmod), # the fixed effects\n  confint(mmod, method=\"Wald\", parm=\"beta_\") # Wald CIs for fixed effects\n) |&gt;\n  exp()\n\n                                  2.5 %     97.5 %\n(Intercept)         0.6740333 0.3158677  1.4383264\ndifficultydifficult 0.4237470 0.2329198  0.7709156\nstatusdominant      1.9506801 0.7743073  4.9142675\nstatussubordinate   4.3043097 1.4106363 13.1338476\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      est\n      OR\n      OR interpretation\n    \n  \n  \n    (Intercept)\n-0.39\n0.67\nestimated odds of an adolescent monkey solving an easy problem\n    difficultydifficult\n-0.86\n0.42\nodds of successful problem solving are more than halved (0.42 times the odds) when a given monkey moves from an easy to a difficult problem\n    statusdominant\n0.67\n1.95\nodds of success would be almost doubled (1.95 times the odds) if a monkey were to change from adolescent to dominant status (NB this is non-significant)\n    statussubordinate\n1.46\n4.30\nodds of success would quadruple (4.3 times the odds) if a monkey were to change from adolescent to subordinate status\n  \n  \n  \n\n\n\n\n\n\nSide note\nContrast this with what we would get from a linear multilevel model. If we were instead modelling a “problem score” with lmer(), rather than “solved yes/no” with glmer(), our coefficients would be interpreted as the estimated difference in scores between adolescent and subordinate monkeys.\nNote that estimating differences between groups is not quite the same idea as estimating the effect “if a particular monkey changed from adolescent to subordinate”. In the linear world, these two things are the same, but our odds ratios give us only the latter."
  },
  {
    "objectID": "02b_loglong.html#example-1",
    "href": "02b_loglong.html#example-1",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Example",
    "text": "Example\n\nData: mindfuldecline.csv\nA study is interested in examining whether engaging in mindfulness can prevent cognitive decline in older adults. They recruit a sample of 20 participants at age 60, and administer the Addenbrooke’s Cognitive Examination (ACE) every 2 years (until participants were aged 78). Half of the participants complete weekly mindfulness sessions, while the remaining participants did not.\nThe data are available at: https://uoepsy.github.io/data/msmr_mindfuldecline.csv.\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ppt\nParticipant Identifier\n    condition\nWhether the participant engages in mindfulness or not (control/mindfulness)\n    study_visit\nStudy Visit Number (1 - 10)\n    age\nAge (in years) at study visit\n    ACE\nAddenbrooke's Cognitive Examination Score. Scores can range from 0 to 100\n    imp\nClinical diagnosis of cognitive impairment ('imp' = impaired, 'unimp' = unimpaired)\n  \n  \n  \n\n\n\n\n\n\nexploring the data\n\nlibrary(tidyverse)\nlibrary(lme4)\nmmd &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_mindfuldecline.csv\")\nhead(mmd)\n\n# A tibble: 6 × 6\n  ppt   condition study_visit   age   ACE imp  \n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 PPT_1 control             1    60  84.5 unimp\n2 PPT_1 control             2    62  85.6 imp  \n3 PPT_1 control             3    64  84.5 imp  \n4 PPT_1 control             4    66  83.1 imp  \n5 PPT_1 control             5    68  82.3 imp  \n6 PPT_1 control             6    70  83.3 imp  \n\n\nHow many participants in each condition? We know from the description there should be 10 in each, but lets check!\n\nmmd |&gt; \n  group_by(condition) |&gt;\n  summarise(\n    n_ppt = n_distinct(ppt)\n  )\n\n# A tibble: 2 × 2\n  condition   n_ppt\n  &lt;chr&gt;       &lt;int&gt;\n1 control        10\n2 mindfulness    10\n\n\nHow many observations does each participant have? With only 20 participants, we could go straight to plotting as a way of getting lots of information all at once. From the plot below, we can see that on the whole participants’ cognitive scores tend to decrease. Most participants have data at every time point, but 4 or 5 people are missing a few. The control participants look (to me) like they have a slightly steeper decline than the mindfulness group:\n\nggplot(mmd, aes(x = age, y = ACE, col = condition)) + \n  geom_point() +\n  geom_line(aes(group=ppt), alpha=.4)+\n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\n\n\nmodelling change over time\nInitially, we’ll just model how cognition changes over time across our entire sample (i.e. ignoring the condition the participants are in). Note that both the variables study_visit and age represent exactly the same information (time), so we have a choice of which one to use.\n\n\n\n\n\n\nWhy the age variable (currently) causes problems\n\n\n\n\n\nAs it is, the age variable we have starts at 60 and goes up to 78 or so.\nIf we try and use this in a model, we get an error!\n\nmod1 &lt;- lmer(ACE ~ 1 + age + \n               (1 + age | ppt), \n             data = mmd)\n\nModel failed to converge with max|grad| = 0.366837 (tol = 0.002, component 1)\nThis is because of the fact that intercepts and slopes are inherently dependent upon one another. Remember that the intercept is “when all predictors are zero”. So in this case it is the estimate cognition of new-born babies. But all our data comes from people who are 65+ years old!\nThis means that trying to fit (1 + age | ppt) will try to estimate the variability in people’s change in cognition over time, and the variability in cognition at age zero. As we can see in Figure 2, because the intercept is so far away from the data, the angle of each persons’ slope has a huge influence over where their intercept is. The more upwards a persons’ slope is, the lower down their intercept is.\n\n\n\n\n\nFigure 2: lines indicate predicted values from the model with random intercepts and random slopes of age, where age. Due to how age is coded, the ‘intercept’ is estimated back at age 0\n\n\n\n\nThis results in issues for estimating our model, because the intercepts and slopes are perfectly correlated! The estimation process has hit a boundary (a perfect correlation):\n\nVarCorr(mod1)\n\n Groups   Name        Std.Dev. Corr  \n ppt      (Intercept) 7.51567        \n          age         0.12696  -0.999\n Residual             0.51536        \n\n\nSo what we can do is either center age on 60 (so that the random intercept is the estimated variability in cognition at aged 60, i.e. the start of the study), or use the study_visit variable.\nEither will do, we just need to remember the units they are measured in!\n\n\n\nLet’s center age on 60:\n\nmmd$ageC &lt;- mmd$age-60\n\nAnd fit our model:\n\nmod1 &lt;- lmer(ACE ~ 1 + ageC + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom our fixed effects, we can see that scores on the ACE tend to decrease by about 0.18 for every 1 year older people get (as a very rough rule of thumb, \\(t\\) statistics that are \\(&gt;|2\\text{-ish}|\\) are probably going to be significant when assessed properly).\n\nsummary(mod1)\n\n...\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 85.22558    0.10198 835.735\nageC        -0.17938    0.03666  -4.893\nWe’re now ready to add in group differences in their trajectories of cognition:\n\nmod2 &lt;- lmer(ACE ~ 1 + ageC * condition + \n               (1 + ageC | ppt), \n             data = mmd)\n\nFrom this model, we can see that for the control group the estimated score on the ACE at age 60 is 85 (that’s the (Intercept)). For these participants, scores are estimated to decrease by -0.27 points every year (that’s the slope of ageC). For the participants in the mindfulness condition, they do not score significantly differently from the control group at age 60 (the condition [mindfulness] coefficient). For the mindfulness group, there is a reduction in the decline of cognition compared to the control group, such that this group decline 0.17 less than the control group every year.\n(note, there are always lots of ways to frame interactions. A “reduction in decline” feels most appropriate to me here)\nGiven that we have a fairly small number of clusters here (20 participants), Kenward Rogers is a good method of inference as it allows us to use REML (meaning unbiased estimates of the random effect variances) and it includes a small sample adjustment to our standard errors.\n\nlibrary(parameters)\nmodel_parameters(mod2, ci_method=\"kr\", ci_random=FALSE)\n\n# Fixed Effects\n\nParameter                      | Coefficient |   SE |         95% CI |      t |    df |      p\n----------------------------------------------------------------------------------------------\n(Intercept)                    |       85.20 | 0.15 | [84.89, 85.52] | 568.00 | 17.75 | &lt; .001\nageC                           |       -0.27 | 0.04 | [-0.36, -0.17] |  -5.93 | 17.95 | &lt; .001\ncondition [mindfulness]        |        0.05 | 0.21 | [-0.39,  0.49] |   0.23 | 17.49 | 0.821 \nageC × condition [mindfulness] |        0.17 | 0.06 | [ 0.04,  0.31] |   2.73 | 17.99 | 0.014 \n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: ppt)       |        0.35\nSD (ageC: ppt)            |        0.14\nCor (Intercept~ageC: ppt) |        0.26\nSD (Residual)             |        0.49\n\n\nFrom those parameters and our interpretation above, we are able to start putting a picture together - two groups that start at the same point, one goes less steeply down over time than the other.\nAnd that’s exactly what we see when we visualise those fixed effects:\n\n\nCode\nlibrary(effects)\neffect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=ageC+60,y=fit,\n             ymin=lower,ymax=upper,\n             col=condition, fill = condition))+\n  geom_line(lwd=1)+\n  geom_ribbon(alpha=.2, col=NA) +\n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nSometimes it is more helpful for a reader if we add in the actual observed trajectories to these plots. To do so, we need to combine two data sources - the fixed effects estimation from effect(), and the data itself:\n\n\nCode\nploteff &lt;- effect(term=\"ageC*condition\", mod=mod2, xlevels=10) |&gt;\n  as.data.frame()\n\nmmd |&gt;\n  ggplot(aes(x=ageC+60,col=condition,fill=condition))+\n  geom_line(aes(y=ACE,group=ppt), alpha=.4) +\n  geom_line(data = ploteff, aes(y=fit), lwd=1)+\n  geom_ribbon(data = ploteff, aes(y=fit,ymin=lower,ymax=upper),\n              alpha=.2, col=NA) + \n  scale_color_manual(values=c(\"#a64bb0\",\"#82b69b\"))+\n  scale_fill_manual(values=c(\"#a64bb0\",\"#82b69b\"))\n\n\n\n\n\n\n\n\n\nThis plot gives us more a lot more context. To a lay reader, our initial plot potentially could be interpreted as if we would expect every person’s cognitive trajectories to fall in the blue and red bands. But those bands are representing the uncertainty in the fixed effects - i.e. the uncertainty in the average persons’ trajectory. When we add in the observed trajectories, we see the variability in people’s trajectories (one person even goes up over time!).\nOur model represents this variability in the random effects part. While the estimated average slope is -0.27 for the control group (and -0.27+0.17=-0.09 for the mindfulness group), people are estimated to vary in their slopes with a standard deviation of 0.14 (remember we can extract this info using VarCorr(), or just look in the output of summary(model)).\n\nVarCorr(mod2)\n\n Groups   Name        Std.Dev. Corr \n ppt      (Intercept) 0.34615       \n          ageC        0.13866  0.260\n Residual             0.49450       \n\n\n\n\n\n\n\n\n\nFigure 3: Two normal distributions with mean of -0.27 (purple) and -.09 (green) and a standard deviation of 0.14\n\n\n\n\nIf you think about what this means - it means that some participants we would expect to actually increase in their slopes. If we have a normal distribution with a mean of -0.3 or -0.09 and a standard distribution of 0.14, then we would expect some values to to positive (see e.g., Figure 3)."
  },
  {
    "objectID": "02b_loglong.html#footnotes",
    "href": "02b_loglong.html#footnotes",
    "title": "2B: Logistic MLM | Longitudinal MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that binary outcomes are just a special case of the binomial↩︎\nassuming that it is people we are studying!↩︎"
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "Week 2 Exercises: Logistic and Longitudinal",
    "section": "",
    "text": "Great Apes!\n\nData: msmr_apespecies.csv & msmr_apeage.csv\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/msmr_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/msmr_apespecies.csv.\n\n\n\n\n\n\n\n\nTable 1:  Data Dictionary: msmr_apespecies.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    species\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2:  Data Dictionary: msmr_apeage.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    age\nAge at assessment (years)\n    dominance\nDominance (Z-scored)\n  \n  \n  \n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and check over it. Do any relevant cleaning/wrangling that might be necessary.\n\n\n\n\n\n1 - reading and joining\n\n\n\nWe’ll read in both datasets, and then join them together.\n\nlibrary(tidyverse)\nlibrary(lme4)\nape_species &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apespecies.csv\")\nape_age &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apeage.csv\")\n\nSometimes is handy to check that all our participants are in both datasets:\n\n# are all the apes in ape_age also in ape_species?\nall(ape_age$ape %in% ape_species$ape)\n\n[1] TRUE\n\n# and vice versa?\nall(ape_species$ape %in% ape_age$ape)\n\n[1] TRUE\n\n\nLet’s join them:\n\napedat &lt;- full_join(ape_age, ape_species)\nhead(apedat)\n\n# A tibble: 6 × 4\n  ape     age dominance species   \n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 Joel      7       0.6 chimpanzee\n2 Joel      5       1.2 chimpanzee\n3 Joel      8       1.1 chimpanzee\n4 Joel      1       0.2 chimpanzee\n5 Joel      2       0.5 chimpanzee\n6 Joel      6       1   chimpanzee\n\n\n\n\n\n\n\n2 - identifying issues\n\n\n\nFirst off, we can see that we’ve got some weird typos. Some apes have been identified as “gorrila” but it is actually spelled “gorilla”.\nAlso, we’ve got people using two alternatives for the chimps: “chimp” and “chimpanzee”. We’ll need to combine those.\n\ntable(apedat$species)\n\n\n    bonobo      chimp chimpanzee    gorilla    gorrila  orangutan \n       187        146        127        211          2        157 \n\n\nAge looks like it has some weird values (possibly “-99”?), and there are possibly a few outliers in the dominance variable. Given that dominance is standardised, it is extremely unlikely that we would see values around 20.. They’re not “impossible”, but they’re so incredibly unlikely that I’d be more comfortable assuming they are typos:\n\nhist(apedat$age, breaks=20)\nhist(apedat$dominance, breaks=20)\n\n\n\n\n\n\n\n\nJust to see what the most extreme values of dominance are:\n\n# show the biggest 5 absolute values in dominance variable\nsort(abs(df$dominance), decreasing = TRUE)[1:5]\n\n[1] 21.2 19.4  3.9  2.9  2.9\n\n\n\n\n\n\n\n3 - cleaning up\n\n\n\n\napedat &lt;- apedat |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\n\n\n\n\nQuestion 2\n\n\nHow is this data structure “hierarchical” (or “clustered”)? What are our level 1 units, and what are our level 2 units?\n\n\n\n\nWe have a random sample of \\(\\underbrace{\\text{timepoints}}_{\\text{level 1}}\\) from a random sample of \\(\\underbrace{\\text{apes}}_{\\text{level 2}}\\).\n\n\n\n\nQuestion 3\n\n\nFor how many apes do we have data? How many of each species?\nHow many datapoints does each ape have?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’ve seen this last week too - counting the different levels in our data. See 2B #getting-to-know-my-monkeys for an example (with another monkey example!)\n\n\n\n\n\n\n\nWe have 168 apes in our dataset:\n\nlength(unique(apedat$ape))\n\n[1] 168\n\n\nHere’s how many of each species:\n\napedat |&gt; \n  group_by(species) |&gt;\n  summarise(\n   n_apes = n_distinct(ape) \n  )\n\n# A tibble: 4 × 2\n  species   n_apes\n  &lt;chr&gt;      &lt;int&gt;\n1 bonobo        36\n2 chimp         56\n3 gorilla       46\n4 orangutan     30\n\n\nLet’s create a table of how many observations for each ape, and then we can create a table from that table, to show how many apes have 2 datapoints, how many have 3, 4, and so on:\n\ntable(apedat$ape) |&gt;\n  table() |&gt;\n  barplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nMake a plot to show how dominance changes as apes get older.\n\n\n\n\n\n\nHints\n\n\n\n\n\nIn 2B #exploring-the-data we made a facet for each cluster (each participant). That was fine because we had only 20 people. In this dataset we have 168! That’s too many to facet. The group aesthetic will probably help instead!\n\n\n\n\n\n\n\nHere’s a line for each ape, and a facet for each species:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_line(aes(group = ape)) + \n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nIt’s kind of hard to see the trend for each ape, so let’s also make a separate little linear model for each ape:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_point(alpha=.1) +\n  stat_smooth(aes(group=ape),geom=\"line\",method=lm,se=F,alpha=.5) +\n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nRecenter the age variable on 1, which is the youngest ages that we’ve got data on for any of our species.\nThen fit a model that estimates the differences between primate species in how dominance changes over time.\n\n\n\n\n\napedat$age = apedat$age-1 \n\nm.full &lt;- lmer(dominance ~ 1 + age * species + (1 + age | ape), data = apedat)\n\n\n\n\n\nQuestion 6\n\n\nDo primate species differ in the growth of dominance?\nPerform an appropriate test/comparison.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is asking about the age*species interaction, which in our model is represented by 3 parameters. To assess the overall question, it might make more sense to do a model comparison.\n\n\n\n\n\n\n\n\nm.int &lt;- lmer(dominance ~ 1 + age + species + (1 + age | ape), data = apedat)\n\nanova(m.int, m.full)\n\nData: apedat\nModels:\nm.int: dominance ~ 1 + age + species + (1 + age | ape)\nm.full: dominance ~ 1 + age * species + (1 + age | ape)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nm.int     9 806.67 849.11 -394.34   788.67                        \nm.full   12 801.16 857.74 -388.58   777.16 11.517  3   0.009237 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies differ in how dominance changes over adolescence (\\(\\chi^2(3) = 11.52, p = 0.009\\)).\n\n\n\n\n\nQuestion 7\n\n\nPlot the average model predicted values for each age.\nBefore you plot.. do you expect to see straight lines? (remember, not every ape is measured at age 2, or age 3, etc).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot (see the lecture).\n\n\n\n\n\n\n\nAveraging fitted values would give us straight lines if every ape had data at all ages, but in our study we have some apes with only 2 data points, and each ape has different set of ages (e.g., one ape might be measured at age 3, 6, and 10, another ape might be at ages 2 and 16).\n\nlibrary(broom.mixed)\n\naugment(m.full) |&gt;\nggplot(aes(age,dominance, color=species)) +\n  # the point ranges are our observations\n  stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n  # the lines are our average predictions  \n  stat_summary(aes(y=.fitted, linetype=species), fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the model based fixed effects:\n\n\n\n\n\neffects::effect(\"age*species\", m.full, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=age+1,y=fit,col=species))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=species),col=NA,alpha=.3) +  \n  scale_color_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  scale_fill_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  facet_wrap(~species) + \n  guides(col=\"none\",fill=\"none\") +\n  labs(x=\"Age (years)\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nInterpret each of the fixed effects from the model (you might also want to get some p-values or confidence intervals).\n\n\n\n\n\n\nHints\n\n\n\n\n\nEach of the estimates should correspond to part of our plot from the previous question.\n\n\n\n\n\n\n\nLet’s get some confidence intervals:\n\nconfint(m.full, method=\"profile\",\n        parm = \"beta_\")\n\n                           2.5 %      97.5 %\n(Intercept)          -0.67066925 -0.17299177\nage                   0.02361398  0.08142209\nspecieschimp          0.13383485  0.77009884\nspeciesgorilla        0.28124162  0.94933844\nspeciesorangutan     -0.38909919  0.34257548\nage:specieschimp     -0.03973125  0.03392308\nage:speciesgorilla   -0.05012759  0.02799394\nage:speciesorangutan -0.10625760 -0.02167806\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      est\n      CI\n      interpretation\n    \n  \n  \n    (Intercept)\n-0.42\n[-0.67, -0.17]*\nestimated dominance of 1 year old bonobos (at left hand side of plot, bonobo line is lower than 0)\n    age\n0.05\n[0.02, 0.08]*\nestimated change in dominance score for every year older a bonobo gets (slope of bonobo line)\n    specieschimp\n0.45\n[0.13, 0.77]*\nestimated difference in dominance scores at age 1 between bonobos and chimps (at left hand side of plot, chimp line is higher than bonobo line)\n    speciesgorilla\n0.62\n[0.28, 0.95]*\nestimated difference in dominance scores at age 1 between bonobos and gorillas (at left hand side of plot, gorilla line is higher than bonobo line)\n    speciesorangutan\n-0.02\n[-0.39, 0.34]\nno significant difference in dominance scores at age 1 between bonobos and orangutans (at the left hand side of our plot, orangutan line is similar height to bonobo line)\n    age:specieschimp\n0.00\n[-0.04, 0.03]\nno significant difference between chimps and bonobos in the change in dominance for every year older (slope of chimp line is similar to slope of bonobo line)\n    age:speciesgorilla\n-0.01\n[-0.05, 0.03]\nno significant difference between gorillas and bonobos in the change in dominance for every year older (slope of gorilla line is similar to slope of bonobo line)\n    age:speciesorangutan\n-0.06\n[-0.11, -0.02]*\nestimated difference between orangutans and bonobos in the change in dominance for every year older (slope of orangutan line is less steep than slope of bonobo line)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTrolley problems\n\nData: msmr_trolley.csv\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n  \n    \n    \n      positive frame\n      neutral frame\n      negative frame\n    \n  \n  \n    5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One worker will be saved if you do not pull the lever, but 5 workers will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n  \n  \n  \n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nThe data are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\nTable 3:  Data Dictionary: trolley.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    frame\nframing of the thought experiment (positive/neutral/negative\n    lives\nlives at stake in the thought experiment (5 or 15)\n    lever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 10\n\n\nRead in the data and check over how many people we have, and whether we have complete data for each participant.\n\n\n\n\n\n\nHints\n\n\n\n\n\nI would maybe try data |&gt; group_by(participant) |&gt; summarise(), and then use the n_distinct() function to count how many “things” each person sees (e.g., 2B #example).\n\n\n\n\n\n\n\n\ntrolley &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_trolley.csv\")\nhead(trolley)\n\n# A tibble: 6 × 4\n  PID   frame    lives lever\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 PPT_1 positive     5     1\n2 PPT_1 positive    15     1\n3 PPT_1 positive     5     1\n4 PPT_1 positive    15     1\n5 PPT_1 neutral      5     1\n6 PPT_1 neutral     15     0\n\n\nHow many participants?\n\nlength(unique(trolley$PID))\n\n[1] 120\n\n\nHow many trials for each participant in each condition.\nWe can, for each participant, count how many trials they have in total, how many “frames” they see, how many “lives” they see, and how many “frame x lives” combinations they see:\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  )\n\n# A tibble: 120 × 5\n   PID     n_trials n_frame n_lives n_combn\n   &lt;chr&gt;      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 PPT_1         12       3       2       6\n 2 PPT_10        12       3       2       6\n 3 PPT_100       12       3       2       6\n 4 PPT_101       12       3       2       6\n 5 PPT_102       12       3       2       6\n 6 PPT_103       12       3       2       6\n 7 PPT_104       12       3       2       6\n 8 PPT_105       12       3       2       6\n 9 PPT_106       12       3       2       6\n10 PPT_107       12       3       2       6\n# ℹ 110 more rows\n\n\nIf everybody gets the same here (as we can see they do below), then everyone has complete data!\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  ) |&gt;\n  summary()\n\n     PID               n_trials     n_frame     n_lives     n_combn \n Length:120         Min.   :12   Min.   :3   Min.   :2   Min.   :6  \n Class :character   1st Qu.:12   1st Qu.:3   1st Qu.:2   1st Qu.:6  \n Mode  :character   Median :12   Median :3   Median :2   Median :6  \n                    Mean   :12   Mean   :3   Mean   :2   Mean   :6  \n                    3rd Qu.:12   3rd Qu.:3   3rd Qu.:2   3rd Qu.:6  \n                    Max.   :12   Max.   :3   Max.   :2   Max.   :6  \n\n\n\n\n\n\nQuestion 11\n\n\nConstruct an appropriate plot to summarise the data in a suitable way to illustrate the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSomething making use of stat_summary() to give proportions, a bit like the plot in 2B #getting-to-know-my-monkeys?\n\n\n\n\n\n\n\nHere is a plot of proportions of trials in which the lever was pulled, split by how the problem was framed, and the number of lives saved:\n\nggplot(trolley, aes(x=frame, y=lever, col=lives)) +\n  stat_summary(geom=\"pointrange\", size=1, \n               position=position_dodge(width=.2)) \n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nFit a model to assess the research aims.\nDon’t worry if it gives you an error, we’ll deal with that in a second.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nRemember, a good way to start is to split this up into 3 parts: 1) the outcome and fixed effects, 2) the grouping structure, and 3) the random slopes.\n\nfitting (or attempting to fit!) glmer models might take time!\n\n\n\n\n\n\n\n\n\n1 - fixed\n\n\n\nThe researchers are “interested in whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved)”.\n“the framing effect on moral judgements” here is operationalised as\n\nlever ~ frame\n\nand the wording “depends upon the stakes” means that we want to know if that effect of frame “is different for” the situations when lives = 5, vs lives = 15 - i.e. we need the interaction!\n\nlever ~ frame * lives\n\nThe outcome here is lever pulled (yes v no), so it’s a binary variable!\n\nglmer(lever ~ frame * lives + ....\n      ....,  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n2 - grouping\n\n\n\nWe know that we have multiple observations for each participant, and those participants are just a random sample (it’s not something we’re interested in testing, we would like to model participant differences as random variation).\n\nglmer(lever ~ frame * lives + ....\n      (1 + .... | PID),  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n3 - random\n\n\n\nFinally, what effects could theoretically vary between our participants?\nEvery participant saw everything (i.e. both frame and lives are “within participant” variables).\nIn theory, all of these are possible given our design:\n\nthe effect of frame on probability of pulling the lever could vary between participants\nthe effect of number of lives on probability of pulling the lever could vary between participants\nthe amount by which number of lives influences the effect of frame on pulling the lever could vary between participants\n\nSo we could theoretically try and fit this model:\n\nmod1 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame * lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning messages: 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, : failure to converge in 10000 evaluations 2: In optwrap(optimizer, devfun, start, rho$lower, control = control, : convergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations 3: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.0795145 (tol = 0.002, component 1)\n\n\n\n\n\nQuestion 13\n\n\nThis is probably the first time we’ve had to deal with a model not converging.\nWhile sometimes changing the optimizer can help, more often than not, the model we are trying to fit is just too complex. Often, the groups in our sample just don’t vary enough for us to estimate a random slope.\nThe aim here is to simplify our random effect structure in order to obtain a converging model, but be careful not to over simplify.\nTry it now. What model do you end up with? (You might not end up with the same model as each other, which is fine. These methods don’t have “cookbook recipes”!)\n\n\n\n\n\n\nHints\n\n\n\n\n\nyou could think of the interaction as the ‘most complex’ part of our random effects, so you might want to remove that first.\n\n\n\n\n\n\n\nThis model still does not converge:\n\nmod2 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame + lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning message: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.00734804 (tol = 0.002, component 1)\n\nWe have a choice here - do we remove frame|PID or lives|PID? One practical point is that each participant has only 4 observations for each frame type, but they have 6 observations for each lives type, which might make it easier to fit.\n\nmod3 &lt;- glmer(lever ~ frame * lives + \n      (1 + lives | PID),  \n      data = trolley, family = binomial)\n\nHooray! it converges!\n\n\n\n\nQuestion 14\n\n\nPlot the predicted probabilities from your model for each combination of frame and lives.\n\n\n\n\n\nlibrary(effects)\neffect(\"frame*lives\", mod3) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x = frame, y = fit, col = lives)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  position=position_dodge(width=.2),\n                  size=1)+\n  labs(y=\"probability of pulling the lever\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional extra: Novel Word Learning\n\nData: nwl.Rdata\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test.  Data were also collect from healthy controls.  Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    group\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n    lesion_location\nLocation of brain lesion: anterior vs posterior\n    block\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n    PropCorrect\nProportion of 30 responses in a given block that the participant got correct\n    NumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n    NumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n    ID\nParticipant Identifier\n    Phase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n  \n  \n  \n\n\n\n\n\n\nQuestion 16\n\n\nLoad the data. Take a look around. Any missing values? Can you think of why?\n\n\n\n\n\nSolution\n\n\n\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\nsummary(nwl)\n\n     group      lesion_location     block    PropCorrect       NumCorrect   \n control:126   anterior : 45    Min.   :1   Min.   :0.2000   Min.   : 6.00  \n patient:117   posterior: 63    1st Qu.:3   1st Qu.:0.5333   1st Qu.:16.00  \n               NA's     :135    Median :5   Median :0.7000   Median :21.00  \n                                Mean   :5   Mean   :0.6822   Mean   :20.47  \n                                3rd Qu.:7   3rd Qu.:0.8333   3rd Qu.:25.00  \n                                Max.   :9   Max.   :1.0000   Max.   :30.00  \n                                                                            \n    NumError              ID         Phase          \n Min.   : 0.000   control1 :  9   Length:243        \n 1st Qu.: 5.000   control10:  9   Class :character  \n Median : 9.000   control11:  9   Mode  :character  \n Mean   : 9.535   control12:  9                     \n 3rd Qu.:14.000   control13:  9                     \n Max.   :24.000   control14:  9                     \n                  (Other)  :189                     \n\n\nThe only missing vales are in the lesion location, and it’s probably because the healthy controls don’t have any lesions. There may also be a few patients for which the lesion_location is missing, but this should be comparatively fewer values compared to controls.\nThe following command creates a two-way frequency table showing the number of controls or patients by lesion location, confirming that controls only have missing values (NAs) and only 9 patients have missing values:\n\ntable(nwl$group, nwl$lesion_location, useNA = \"ifany\")\n\n         \n          anterior posterior &lt;NA&gt;\n  control        0         0  126\n  patient       45        63    9\n\n\n\n\n\n\nQuestion 17\n\n\nOur broader research aim today is to compare the two lesion location groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study.\n\nWhat is the outcome variable?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink carefully: there might be several variables which either fully or partly express the information we are considering the “outcome” here. We saw this back in USMR with the glm()!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe outcome here is (in words) the proportion of correct answers or, equivalently, the probability of answering correctly. A proportion/probability can only vary between 0 and 1 and, as such, we cannot use traditional linear regression or we could end up with predictions outside of the [0, 1] range.\nAs said, the outcome is the proportion of correct answers in each block. This makes it tempting to look at the variable called PropCorrect, but this is encoded as a proportion. We have learned to use logistic models, but these require either:\n\na binary outcome variable, where the values are 0s or 1s\na binomial outcome variable, where the values are aggregated counts of 1s and 0s\n\nBinary data. In the case below you would use the specification correct ~ ...:\n\n\n\n\n\n\n  \n    \n    \n      participant\n      question\n      correct\n    \n  \n  \n    1\n1\n1\n    1\n2\n0\n    1\n3\n1\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nBinomial data. You would use the specification cbind(num_successes, num_failures) which, in the case below, would be:\ncbind(questions_correct, questions_incorrect) ~ ...\n\n\n\n\n\n\n  \n    \n    \n      participant\n      questions_correct\n      questions_incorrect\n    \n  \n  \n    1\n2\n1\n    2\n1\n2\n    3\n3\n0\n    ...\n...\n...\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 18\n\n\n\nResearch Question 1:\nIs the learning rate (training blocks) different between the two lesion location groups?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\nEnsure you are running models on only the data we are actually interested in.\n\nAre the healthy controls included in the research question under investigation?\nAre the testing blocks included in the research question, or only the learning blocks?\n\nWe could use model comparison via likelihood ratio tests (using anova(model1, model2, model3, ...). For this question, we could compare:\n\nA model with just the change over the sequence of blocks\nA model with the change over the sequence of blocks and an overall difference between groups\nA model with groups differing with respect to their change over the sequence of blocks\n\nWhat about the random effects part?\n\nWhat are our observations grouped by?\nWhat variables can vary within these groups?\nWhat do you want your model to allow to vary within these groups?\n\n\n\n\n\n\n\n\n\n\n1 - answers to the hints\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\n\nYes, we don’t a binary variable with correct/incorrect questions but the binomial variables NumCorrect and NumError representing, respectively, the aggregated count (out of 30) of correct and incorrect questions. As such, we will need the following: cbind(NumCorrect, NumError)\n\nEnsure you are running models on only the data we are actually interested in.\n\nThe healthy controls are not included in the research question under investigation, so we will exclude them.\nWe are only interested in the learning blocks, and we will exclude the testing blocks (block &gt; 7)\nYou might want to store this data in a separate object, but in the code for the solution we will just use filter() inside the glmer().\n\nA model with just the change over the sequence of blocks:\n\noutcome ~ block\n\nA model with the change over the sequence of blocks and an overall difference between groups:\n\noutcome ~ block + lesion_location\n\nA model with groups differing with respect to their change *over the sequence of blocks:\n\noutcome ~ block * lesion_location\n\nWhat are our observations grouped by?\n\nrepeated measures by-participant. i.e., the ID variable\n\nWhat variables can vary within these groups?\n\nBlock and Phase. Be careful though - you can create the Phase variable out of the Block variable, so really this is just one piece of information, encoded differently in two variables.\nThe other variables (lesion_location and group) do not vary for each ID. Lesions don’t suddenly change where they are located, nor do participants swap between being a patient vs a control (we don’t need the group variable anyway as we are excluding the controls).\nWhat do you want your model to allow to vary within these groups?\nDo you think the change over the course of the blocks is the same for everybody? Or do you think it varies? Is this variation important to think about in terms of your research question?\n\n\n\n\n\n\n\n2 - modelling\n\n\n\n\nm.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), \n                data = filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\n\nanova(m.base, m.loc0, m.loc1, test=\"Chisq\")\n\nData: filter(nwl, block &lt; 8, !is.na(lesion_location))\nModels:\nm.base: cbind(NumCorrect, NumError) ~ block + (block | ID)\nm.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID)\nm.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nm.base    5 454.12 466.27 -222.06   444.12                     \nm.loc0    6 454.66 469.25 -221.33   442.66 1.4572  1     0.2274\nm.loc1    7 454.47 471.48 -220.23   440.47 2.1974  1     0.1382\n\n\n\nNo significant difference in learning rate between groups (\\(\\chi^2(1)=2.2, p = 0.138\\)).\n\n\n\n\n\nQuestion 19\n\n\n\nResearch Question 2\nIn the testing phase, does performance on the immediate test differ between lesion location groups, and does the retention from immediate to follow-up test differ between the two lesion location groups?\n\nLet’s try a different approach to this. Instead of fitting various models and comparing them via likelihood ratio tests, just fit the one model which could answer both parts of the question above.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThis might required a bit more data-wrangling beforehand. Think about the order of your factor levels (alphabetically speaking, “Follow-up” comes before “Immediate”)!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nnwl_test &lt;- filter(nwl, block &gt; 7, !is.na(lesion_location)) %&gt;%\n    mutate(\n        Phase = factor(Phase), \n        Phase = fct_relevel(Phase, \"Immediate\")\n    )\n\nm.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 | ID), \n                      nwl_test, family=\"binomial\")\n\nsummary(m.recall.loc)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 |      ID)\n   Data: nwl_test\n\n     AIC      BIC   logLik deviance df.resid \n   139.3    145.2    -64.6    129.3       19 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1556 -0.3352  0.0039  0.4963  1.3506 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.3626   0.6021  \nNumber of obs: 24, groups:  ID, 12\n\nFixed effects:\n                                        Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                              -0.1124     0.3167  -0.355   0.7226  \nPhaseFollow-up                           -0.0278     0.2357  -0.118   0.9061  \nlesion_locationposterior                  0.9672     0.4211   2.297   0.0216 *\nPhaseFollow-up:lesion_locationposterior  -0.2035     0.3191  -0.638   0.5236  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PhsFl- lsn_lc\nPhaseFllw-p -0.372              \nlsn_lctnpst -0.752  0.280       \nPhsFllw-p:_  0.275 -0.739 -0.385\n\n\nNote 1:\nIn the above, we have made sure to select the patients by specifying !is.na(lesion_location), meaning that we want those rows where the lesion location is not missing. As a reminder ! is the negation function (not). As we saw in Question A1, this excludes the 126 healthy controls, as well as the 9 patients for which we have missing values (NAs).\nNote 2:\nWe didn’t specify (Phase | ID) as the random effect because each participant only has 2 data points for Phase, and there is only one line that fits two data points. In other words, there is only one possible way to fit those two data points. As such, as each group of 2 points will have a perfect line fit, and the residuals \\(\\varepsilon_{ij}\\) will all be 0. As a consequence of this, the residuals will have no variability as they are all 0, so \\(\\sigma_{\\epsilon}\\) is 0 which in turn leads to problem with estimating the model coefficients.\n\nsubset(nwl_test, ID == 'patient15')\n\n     group lesion_location block PropCorrect NumCorrect NumError        ID\n1  patient        anterior     8   0.5333333         16       14 patient15\n13 patient        anterior     9   0.5333333         16       14 patient15\n       Phase\n1  Immediate\n13 Follow-up\n\n\nIf you try using (Phase | ID) as random effect, you will see the following message:\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nQuestion 20\n\n\n\nIn family = binomial(link='logit'). What function is used to relate the linear predictors in the model to the expected value of the response variable?\n\nHow do we convert this into something more interpretable?\n\n\n\n\n\n\nSolution\n\n\n\n\nThe link function is the logit, or log-odds (other link functions are available).\nTo convert log-odds to odds, we can use exp(), to get odds and odds ratios.\n\n\n\n\n\nQuestion 21\n\n\nMake sure you pay attention to trying to interpret each fixed effect from your models.\nThese can be difficult, especially when it’s logistic, and especially when there are interactions.\n\nWhat is the increase in the odds of answering correctly in the immediate test if you were to have a posterior legion instead of an anterior legion?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept) ==&gt; Anterior lesion group performance in immediate test. This is the log-odds of them answering correctly in the immediate test.\nPhaseFollow-up ==&gt; Change in performance (for someone with an anterior lesion) from immediate to follow-up test.\nlesion_locationposterior ==&gt; Change in performance in immediate test were a patient to have a posterior lesion instead of an anterior lesion.\nPhaseFollow-up:lesion_locationposterior ==&gt; How change in performance from immediate to follow-up test would differ were a patient to have a posterior lesion instead of an anterior lesion.\n\n\n\nlesion_locationposterior \n                2.630634 \n\n\n\nHaving a posterior lesions is associated with 2.63 times the odds of answering correctly in the immediate test compared to having an anterior lesion.\n\n\n\n\n\nQuestion 22\n\n\nRecreate the visualisation in Figure 2.\n\n\n\n\n\nFigure 2: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, \n                                                 color=lesion_location, \n                                                 shape=lesion_location)) +\n    #geom_line(aes(group=ID),alpha=.2) + \n    stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n    stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), \n                 fun=mean, geom=\"line\") + \n    geom_hline(yintercept=0.5, linetype=\"dashed\") + \n    geom_vline(xintercept=c(7.5, 8.5), linetype=\"dashed\") + \n    scale_x_continuous(breaks=1:9, \n                       labels=c(1:7, \"Test\", \"Follow-Up\")) + \n    theme_bw(base_size=10) + \n    labs(x=\"Block\", y=\"Proportion Correct\", \n         shape=\"Lesion\\nLocation\", color=\"Lesion\\nLocation\")"
  },
  {
    "objectID": "02exHIGHAM.html",
    "href": "02exHIGHAM.html",
    "title": "Week 2 Exercises: Logistic and Longitudinal",
    "section": "",
    "text": "Great Apes!\n\nData: msmr_apespecies.csv & msmr_apeage.csv\nWe have data from a large sample of great apes who have been studied between the ages of 1 to 10 years old (i.e. during adolescence). Our data includes 4 species of great apes: Chimpanzees, Bonobos, Gorillas and Orangutans. Each ape has been assessed on a primate dominance scale at various ages. Data collection was not very rigorous, so apes do not have consistent assessment schedules (i.e., one may have been assessed at ages 1, 3 and 6, whereas another at ages 2 and 8).\nThe researchers are interested in examining how the adolescent development of dominance in great apes differs between species.\nData on the dominance scores of the apes are available at https://uoepsy.github.io/data/msmr_apeage.csv and the information about which species each ape is are in https://uoepsy.github.io/data/msmr_apespecies.csv.\n\n\n\n\n\n\n\n\nTable 1:  Data Dictionary: msmr_apespecies.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    species\nSpecies (Bonobo, Chimpanzee, Gorilla, Orangutan)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2:  Data Dictionary: msmr_apeage.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    ape\nApe Name\n    age\nAge at assessment (years)\n    dominance\nDominance (Z-scored)\n  \n  \n  \n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and check over it. Do any relevant cleaning/wrangling that might be necessary.\n\n\n\n\n\n1 - reading and joining\n\n\n\nWe’ll read in both datasets, and then join them together.\n\nlibrary(tidyverse)\nlibrary(lme4)\nape_species &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apespecies.csv\")\nape_age &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_apeage.csv\")\n\nSometimes is handy to check that all our participants are in both datasets:\n\n# are all the apes in ape_age also in ape_species?\nall(ape_age$ape %in% ape_species$ape)\n\n[1] TRUE\n\n# and vice versa?\nall(ape_species$ape %in% ape_age$ape)\n\n[1] TRUE\n\n\nLet’s join them:\n\napedat &lt;- full_join(ape_age, ape_species)\nhead(apedat)\n\n# A tibble: 6 × 4\n  ape     age dominance species   \n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 Joel      7       0.6 chimpanzee\n2 Joel      5       1.2 chimpanzee\n3 Joel      8       1.1 chimpanzee\n4 Joel      1       0.2 chimpanzee\n5 Joel      2       0.5 chimpanzee\n6 Joel      6       1   chimpanzee\n\n\n\n\n\n\n\n2 - identifying issues\n\n\n\nFirst off, we can see that we’ve got some weird typos. Some apes have been identified as “gorrila” but it is actually spelled “gorilla”.\nAlso, we’ve got people using two alternatives for the chimps: “chimp” and “chimpanzee”. We’ll need to combine those.\n\ntable(apedat$species)\n\n\n    bonobo      chimp chimpanzee    gorilla    gorrila  orangutan \n       187        146        127        211          2        157 \n\n\nAge looks like it has some weird values (possibly “-99”?), and there are possibly a few outliers in the dominance variable. Given that dominance is standardised, it is extremely unlikely that we would see values around 20.. They’re not “impossible”, but they’re so incredibly unlikely that I’d be more comfortable assuming they are typos:\n\nhist(apedat$age, breaks=20)\nhist(apedat$dominance, breaks=20)\n\n\n\n\n\n\n\n\nJust to see what the most extreme values of dominance are:\n\n# show the biggest 5 absolute values in dominance variable\nsort(abs(df$dominance), decreasing = TRUE)[1:5]\n\n[1] 21.2 19.4  3.9  2.9  2.9\n\n\n\n\n\n\n\n3 - cleaning up\n\n\n\n\napedat &lt;- apedat |&gt; \n  mutate(\n    # fix species typos\n    species = case_when(\n      species %in% c(\"chimp\",\"chimpanzee\") ~ \"chimp\",\n      species %in% c(\"gorilla\",\"gorrila\") ~ \"gorilla\",\n      TRUE ~ species\n    )\n  ) |&gt;\n    filter(\n      # get rid of ages -99\n      age &gt; 0, \n      # keep when dominance is between -5 and 5 \n      # (5 here is a slightly arbitrary choice, but you can see from\n      # our checks that this will only exclude the two extreme datapoints\n      # that are 21.2 and 19.4\n      (dominance &lt; 5 & dominance &gt; -5) \n    )\n\n\n\n\n\nQuestion 2\n\n\nHow is this data structure “hierarchical” (or “clustered”)? What are our level 1 units, and what are our level 2 units?\n\n\n\n\n\nSolution\n\n\n\nWe have a random sample of \\(\\underbrace{\\text{timepoints}}_{\\text{level 1}}\\) from a random sample of \\(\\underbrace{\\text{apes}}_{\\text{level 2}}\\).\n\n\n\n\nQuestion 3\n\n\nFor how many apes do we have data? How many of each species?\nHow many datapoints does each ape have?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’ve seen this last week too - counting the different levels in our data. See 2B #getting-to-know-my-monkeys for an example (with another monkey example!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWe have 168 apes in our dataset:\n\nlength(unique(apedat$ape))\n\n[1] 168\n\n\nHere’s how many of each species:\n\napedat |&gt; \n  group_by(species) |&gt;\n  summarise(\n   n_apes = n_distinct(ape) \n  )\n\n# A tibble: 4 × 2\n  species   n_apes\n  &lt;chr&gt;      &lt;int&gt;\n1 bonobo        36\n2 chimp         56\n3 gorilla       46\n4 orangutan     30\n\n\nLet’s create a table of how many observations for each ape, and then we can create a table from that table, to show how many apes have 2 datapoints, how many have 3, 4, and so on:\n\ntable(apedat$ape) |&gt;\n  table() |&gt;\n  barplot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nMake a plot to show how dominance changes as apes get older.\n\n\n\n\n\n\nHints\n\n\n\n\n\nIn 2B #exploring-the-data we made a facet for each cluster (each participant). That was fine because we had only 20 people. In this dataset we have 168! That’s too many to facet. The group aesthetic will probably help instead!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere’s a line for each ape, and a facet for each species:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_line(aes(group = ape)) + \n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\nIt’s kind of hard to see the trend for each ape, so let’s also make a separate little linear model for each ape:\n\nggplot(apedat, aes(x = age, y = dominance, col = species))+\n  geom_point(alpha=.1) +\n  stat_smooth(aes(group=ape),geom=\"line\",method=lm,se=F,alpha=.5) +\n  facet_wrap(~species) + \n  guides(col=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nRecenter the age variable on 1, which is the youngest ages that we’ve got data on for any of our species.\nThen fit a model that estimates the differences between primate species in how dominance changes over time.\n\n\n\n\n\nSolution\n\n\n\n\napedat$age = apedat$age-1 \n\nm.full &lt;- lmer(dominance ~ 1 + age * species + (1 + age | ape), data = apedat)\n\n\n\n\n\nQuestion 6\n\n\nDo primate species differ in the growth of dominance?\nPerform an appropriate test/comparison.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is asking about the age*species interaction, which in our model is represented by 3 parameters. To assess the overall question, it might make more sense to do a model comparison.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nm.int &lt;- lmer(dominance ~ 1 + age + species + (1 + age | ape), data = apedat)\n\nanova(m.int, m.full)\n\nData: apedat\nModels:\nm.int: dominance ~ 1 + age + species + (1 + age | ape)\nm.full: dominance ~ 1 + age * species + (1 + age | ape)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nm.int     9 806.67 849.11 -394.34   788.67                        \nm.full   12 801.16 857.74 -388.58   777.16 11.517  3   0.009237 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSpecies differ in how dominance changes over adolescence (\\(\\chi^2(3) = 11.52, p = 0.009\\)).\n\n\n\n\n\nQuestion 7\n\n\nPlot the average model predicted values for each age.\nBefore you plot.. do you expect to see straight lines? (remember, not every ape is measured at age 2, or age 3, etc).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot (see the lecture).\n\n\n\n\n\n\n\n\nSolution\n\n\n\nAveraging fitted values would give us straight lines if every ape had data at all ages, but in our study we have some apes with only 2 data points, and each ape has different set of ages (e.g., one ape might be measured at age 3, 6, and 10, another ape might be at ages 2 and 16).\n\nlibrary(broom.mixed)\n\naugment(m.full) |&gt;\nggplot(aes(age,dominance, color=species)) +\n  # the point ranges are our observations\n  stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n  # the lines are our average predictions  \n  stat_summary(aes(y=.fitted, linetype=species), fun=mean, geom=\"line\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the model based fixed effects:\n\n\n\n\n\nSolution\n\n\n\n\neffects::effect(\"age*species\", m.full, xlevels=10) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=age+1,y=fit,col=species))+\n  geom_line(lwd=1)+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=species),col=NA,alpha=.3) +  \n  scale_color_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  scale_fill_manual(values=c(\"grey30\",\"black\",\"grey50\",\"darkorange\")) +\n  facet_wrap(~species) + \n  guides(col=\"none\",fill=\"none\") +\n  labs(x=\"Age (years)\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nInterpret each of the fixed effects from the model (you might also want to get some p-values or confidence intervals).\n\n\n\n\n\n\nHints\n\n\n\n\n\nEach of the estimates should correspond to part of our plot from the previous question.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nLet’s get some confidence intervals:\n\nconfint(m.full, method=\"profile\",\n        parm = \"beta_\")\n\n                           2.5 %      97.5 %\n(Intercept)          -0.67066925 -0.17299177\nage                   0.02361398  0.08142209\nspecieschimp          0.13383485  0.77009884\nspeciesgorilla        0.28124162  0.94933844\nspeciesorangutan     -0.38909919  0.34257548\nage:specieschimp     -0.03973125  0.03392308\nage:speciesgorilla   -0.05012759  0.02799394\nage:speciesorangutan -0.10625760 -0.02167806\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      est\n      CI\n      interpretation\n    \n  \n  \n    (Intercept)\n-0.42\n[-0.67, -0.17]*\nestimated dominance of 1 year old bonobos (at left hand side of plot, bonobo line is lower than 0)\n    age\n0.05\n[0.02, 0.08]*\nestimated change in dominance score for every year older a bonobo gets (slope of bonobo line)\n    specieschimp\n0.45\n[0.13, 0.77]*\nestimated difference in dominance scores at age 1 between bonobos and chimps (at left hand side of plot, chimp line is higher than bonobo line)\n    speciesgorilla\n0.62\n[0.28, 0.95]*\nestimated difference in dominance scores at age 1 between bonobos and gorillas (at left hand side of plot, gorilla line is higher than bonobo line)\n    speciesorangutan\n-0.02\n[-0.39, 0.34]\nno significant difference in dominance scores at age 1 between bonobos and orangutans (at the left hand side of our plot, orangutan line is similar height to bonobo line)\n    age:specieschimp\n0.00\n[-0.04, 0.03]\nno significant difference between chimps and bonobos in the change in dominance for every year older (slope of chimp line is similar to slope of bonobo line)\n    age:speciesgorilla\n-0.01\n[-0.05, 0.03]\nno significant difference between gorillas and bonobos in the change in dominance for every year older (slope of gorilla line is similar to slope of bonobo line)\n    age:speciesorangutan\n-0.06\n[-0.11, -0.02]*\nestimated difference between orangutans and bonobos in the change in dominance for every year older (slope of orangutan line is less steep than slope of bonobo line)\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTrolley problems\n\nData: msmr_trolley.csv\nThe “Trolley Problem” is a thought experiment in moral philosophy that asks you to decide whether or not to pull a lever to divert a trolley. Pulling the lever changes the trolley direction from hitting 5 people to a track on which it will hit one person.\n\n\n\n\n\n\n\n\n\nPrevious research has found that the “framing” of the problem will influence the decisions people make:\n\n\n\n\n\n\n  \n    \n    \n      positive frame\n      neutral frame\n      negative frame\n    \n  \n  \n    5 people will be saved if you pull the lever; one person on another track will be saved if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n5 people will be saved if you pull the lever, but another person will die. One worker will be saved if you do not pull the lever, but 5 workers will die. All your actions are legal and understandable. Will you pull the lever?\nOne person will die if you pull the lever. 5 people will die if you do not pull the lever. All your actions are legal and understandable. Will you pull the lever?\n  \n  \n  \n\n\n\n\nWe conducted a study to investigate whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved).\n120 participants were recruited, and each gave answers to 12 versions of the thought experiment. For each participant, four versions followed each of the positive/neutral/negative framings described above, and for each framing, 2 would save 5 people and 2 would save 15 people.\nThe data are available at https://uoepsy.github.io/data/msmr_trolley.csv.\n\n\n\n\n\n\nTable 3:  Data Dictionary: trolley.csv \n  \n    \n    \n      variable\n      description\n    \n  \n  \n    PID\nParticipant ID\n    frame\nframing of the thought experiment (positive/neutral/negative\n    lives\nlives at stake in the thought experiment (5 or 15)\n    lever\nWhether or not the participant chose to pull the lever (1 = yes, 0 = no)\n  \n  \n  \n\n\n\n\n\n\n\nQuestion 10\n\n\nRead in the data and check over how many people we have, and whether we have complete data for each participant.\n\n\n\n\n\n\nHints\n\n\n\n\n\nI would maybe try data |&gt; group_by(participant) |&gt; summarise(), and then use the n_distinct() function to count how many “things” each person sees (e.g., 2B #example).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\ntrolley &lt;- read_csv(\"https://uoepsy.github.io/data/msmr_trolley.csv\")\nhead(trolley)\n\n# A tibble: 6 × 4\n  PID   frame    lives lever\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 PPT_1 positive     5     1\n2 PPT_1 positive    15     1\n3 PPT_1 positive     5     1\n4 PPT_1 positive    15     1\n5 PPT_1 neutral      5     1\n6 PPT_1 neutral     15     0\n\n\nHow many participants?\n\nlength(unique(trolley$PID))\n\n[1] 120\n\n\nHow many trials for each participant in each condition.\nWe can, for each participant, count how many trials they have in total, how many “frames” they see, how many “lives” they see, and how many “frame x lives” combinations they see:\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  )\n\n# A tibble: 120 × 5\n   PID     n_trials n_frame n_lives n_combn\n   &lt;chr&gt;      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 PPT_1         12       3       2       6\n 2 PPT_10        12       3       2       6\n 3 PPT_100       12       3       2       6\n 4 PPT_101       12       3       2       6\n 5 PPT_102       12       3       2       6\n 6 PPT_103       12       3       2       6\n 7 PPT_104       12       3       2       6\n 8 PPT_105       12       3       2       6\n 9 PPT_106       12       3       2       6\n10 PPT_107       12       3       2       6\n# ℹ 110 more rows\n\n\nIf everybody gets the same here (as we can see they do below), then everyone has complete data!\n\ntrolley |&gt;\n  group_by(PID) |&gt;\n  summarise(\n    n_trials = n(),\n    n_frame = n_distinct(frame),\n    n_lives = n_distinct(lives),\n    n_combn = n_distinct(frame,lives)\n  ) |&gt;\n  summary()\n\n     PID               n_trials     n_frame     n_lives     n_combn \n Length:120         Min.   :12   Min.   :3   Min.   :2   Min.   :6  \n Class :character   1st Qu.:12   1st Qu.:3   1st Qu.:2   1st Qu.:6  \n Mode  :character   Median :12   Median :3   Median :2   Median :6  \n                    Mean   :12   Mean   :3   Mean   :2   Mean   :6  \n                    3rd Qu.:12   3rd Qu.:3   3rd Qu.:2   3rd Qu.:6  \n                    Max.   :12   Max.   :3   Max.   :2   Max.   :6  \n\n\n\n\n\n\nQuestion 11\n\n\nConstruct an appropriate plot to summarise the data in a suitable way to illustrate the research question.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSomething making use of stat_summary() to give proportions, a bit like the plot in 2B #getting-to-know-my-monkeys?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nHere is a plot of proportions of trials in which the lever was pulled, split by how the problem was framed, and the number of lives saved:\n\nggplot(trolley, aes(x=frame, y=lever, col=lives)) +\n  stat_summary(geom=\"pointrange\", size=1, \n               position=position_dodge(width=.2)) \n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\nFit a model to assess the research aims.\nDon’t worry if it gives you an error, we’ll deal with that in a second.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nRemember, a good way to start is to split this up into 3 parts: 1) the outcome and fixed effects, 2) the grouping structure, and 3) the random slopes.\n\nfitting (or attempting to fit!) glmer models might take time!\n\n\n\n\n\n\n\n\n\n1 - fixed\n\n\n\nThe researchers are “interested in whether the framing effects on moral judgements depends upon the stakes (i.e. the number of lives saved)”.\n“the framing effect on moral judgements” here is operationalised as\n\nlever ~ frame\n\nand the wording “depends upon the stakes” means that we want to know if that effect of frame “is different for” the situations when lives = 5, vs lives = 15 - i.e. we need the interaction!\n\nlever ~ frame * lives\n\nThe outcome here is lever pulled (yes v no), so it’s a binary variable!\n\nglmer(lever ~ frame * lives + ....\n      ....,  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n2 - grouping\n\n\n\nWe know that we have multiple observations for each participant, and those participants are just a random sample (it’s not something we’re interested in testing, we would like to model participant differences as random variation).\n\nglmer(lever ~ frame * lives + ....\n      (1 + .... | PID),  \n      data = trolley, family = binomial)\n\n\n\n\n\n\n3 - random\n\n\n\nFinally, what effects could theoretically vary between our participants?\nEvery participant saw everything (i.e. both frame and lives are “within participant” variables).\nIn theory, all of these are possible given our design:\n\nthe effect of frame on probability of pulling the lever could vary between participants\nthe effect of number of lives on probability of pulling the lever could vary between participants\nthe amount by which number of lives influences the effect of frame on pulling the lever could vary between participants\n\nSo we could theoretically try and fit this model:\n\nmod1 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame * lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning messages: 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, : failure to converge in 10000 evaluations 2: In optwrap(optimizer, devfun, start, rho$lower, control = control, : convergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations 3: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.0795145 (tol = 0.002, component 1)\n\n\n\n\n\nQuestion 13\n\n\nThis is probably the first time we’ve had to deal with a model not converging.\nWhile sometimes changing the optimizer can help, more often than not, the model we are trying to fit is just too complex. Often, the groups in our sample just don’t vary enough for us to estimate a random slope.\nThe aim here is to simplify our random effect structure in order to obtain a converging model, but be careful not to over simplify.\nTry it now. What model do you end up with? (You might not end up with the same model as each other, which is fine. These methods don’t have “cookbook recipes”!)\n\n\n\n\n\n\nHints\n\n\n\n\n\nyou could think of the interaction as the ‘most complex’ part of our random effects, so you might want to remove that first.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThis model still does not converge:\n\nmod2 &lt;- glmer(lever ~ frame * lives + \n      (1 + frame + lives | PID),  \n      data = trolley, family = binomial)\n\n\nWarning message: In checkConv(attr(opt, “derivs”), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| = 0.00734804 (tol = 0.002, component 1)\n\nWe have a choice here - do we remove frame|PID or lives|PID? One practical point is that each participant has only 4 observations for each frame type, but they have 6 observations for each lives type, which might make it easier to fit.\n\nmod3 &lt;- glmer(lever ~ frame * lives + \n      (1 + lives | PID),  \n      data = trolley, family = binomial)\n\nHooray! it converges!\n\n\n\n\nQuestion 14\n\n\nPlot the predicted probabilities from your model for each combination of frame and lives.\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(effects)\neffect(\"frame*lives\", mod3) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x = frame, y = fit, col = lives)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  position=position_dodge(width=.2),\n                  size=1)+\n  labs(y=\"probability of pulling the lever\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional extra: Novel Word Learning\n\nData: nwl.Rdata\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\n\nIn the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test.  Data were also collect from healthy controls.  Figure 1 shows the differences between lesion location groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)\n\n\n\n\n\nFigure 1: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      variable\n      description\n    \n  \n  \n    group\nWhether participant is a stroke patient ('patient') or a healthy control ('control')\n    lesion_location\nLocation of brain lesion: anterior vs posterior\n    block\nExperimental block (1-9). Blocks 1-7 were learning blocks, immediately followed by a test in block 8. Block 9 was a follow-up test at a later point\n    PropCorrect\nProportion of 30 responses in a given block that the participant got correct\n    NumCorrect\nNumber of responses (out of 30) in a given block that the participant got correct\n    NumError\nNumber of responses (out of 30) in a given block that the participant got incorrect\n    ID\nParticipant Identifier\n    Phase\nExperimental phase, corresponding to experimental block(s): 'Learning', 'Immediate','Follow-up'\n  \n  \n  \n\n\n\n\n\n\nQuestion 16\n\n\nLoad the data. Take a look around. Any missing values? Can you think of why?\n\n\n\n\n\nSolution\n\n\n\n\nload(url(\"https://uoepsy.github.io/msmr/data/nwl.RData\"))\nsummary(nwl)\n\n     group      lesion_location     block    PropCorrect       NumCorrect   \n control:126   anterior : 45    Min.   :1   Min.   :0.2000   Min.   : 6.00  \n patient:117   posterior: 63    1st Qu.:3   1st Qu.:0.5333   1st Qu.:16.00  \n               NA's     :135    Median :5   Median :0.7000   Median :21.00  \n                                Mean   :5   Mean   :0.6822   Mean   :20.47  \n                                3rd Qu.:7   3rd Qu.:0.8333   3rd Qu.:25.00  \n                                Max.   :9   Max.   :1.0000   Max.   :30.00  \n                                                                            \n    NumError              ID         Phase          \n Min.   : 0.000   control1 :  9   Length:243        \n 1st Qu.: 5.000   control10:  9   Class :character  \n Median : 9.000   control11:  9   Mode  :character  \n Mean   : 9.535   control12:  9                     \n 3rd Qu.:14.000   control13:  9                     \n Max.   :24.000   control14:  9                     \n                  (Other)  :189                     \n\n\nThe only missing vales are in the lesion location, and it’s probably because the healthy controls don’t have any lesions. There may also be a few patients for which the lesion_location is missing, but this should be comparatively fewer values compared to controls.\nThe following command creates a two-way frequency table showing the number of controls or patients by lesion location, confirming that controls only have missing values (NAs) and only 9 patients have missing values:\n\ntable(nwl$group, nwl$lesion_location, useNA = \"ifany\")\n\n         \n          anterior posterior &lt;NA&gt;\n  control        0         0  126\n  patient       45        63    9\n\n\n\n\n\n\nQuestion 17\n\n\nOur broader research aim today is to compare the two lesion location groups (those with anterior vs. posterior lesions) with respect to their accuracy of responses over the course of the study.\n\nWhat is the outcome variable?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink carefully: there might be several variables which either fully or partly express the information we are considering the “outcome” here. We saw this back in USMR with the glm()!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThe outcome here is (in words) the proportion of correct answers or, equivalently, the probability of answering correctly. A proportion/probability can only vary between 0 and 1 and, as such, we cannot use traditional linear regression or we could end up with predictions outside of the [0, 1] range.\nAs said, the outcome is the proportion of correct answers in each block. This makes it tempting to look at the variable called PropCorrect, but this is encoded as a proportion. We have learned to use logistic models, but these require either:\n\na binary outcome variable, where the values are 0s or 1s\na binomial outcome variable, where the values are aggregated counts of 1s and 0s\n\nBinary data. In the case below you would use the specification correct ~ ...:\n\n\n\n\n\n\n  \n    \n    \n      participant\n      question\n      correct\n    \n  \n  \n    1\n1\n1\n    1\n2\n0\n    1\n3\n1\n    ...\n...\n...\n  \n  \n  \n\n\n\n\nBinomial data. You would use the specification cbind(num_successes, num_failures) which, in the case below, would be:\ncbind(questions_correct, questions_incorrect) ~ ...\n\n\n\n\n\n\n  \n    \n    \n      participant\n      questions_correct\n      questions_incorrect\n    \n  \n  \n    1\n2\n1\n    2\n1\n2\n    3\n3\n0\n    ...\n...\n...\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 18\n\n\n\nResearch Question 1:\nIs the learning rate (training blocks) different between the two lesion location groups?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\nEnsure you are running models on only the data we are actually interested in.\n\nAre the healthy controls included in the research question under investigation?\nAre the testing blocks included in the research question, or only the learning blocks?\n\nWe could use model comparison via likelihood ratio tests (using anova(model1, model2, model3, ...). For this question, we could compare:\n\nA model with just the change over the sequence of blocks\nA model with the change over the sequence of blocks and an overall difference between groups\nA model with groups differing with respect to their change over the sequence of blocks\n\nWhat about the random effects part?\n\nWhat are our observations grouped by?\nWhat variables can vary within these groups?\nWhat do you want your model to allow to vary within these groups?\n\n\n\n\n\n\n\n\n\n\n1 - answers to the hints\n\n\n\n\nDo we want cbind(num_successes, num_failures)?\n\nYes, we don’t a binary variable with correct/incorrect questions but the binomial variables NumCorrect and NumError representing, respectively, the aggregated count (out of 30) of correct and incorrect questions. As such, we will need the following: cbind(NumCorrect, NumError)\n\nEnsure you are running models on only the data we are actually interested in.\n\nThe healthy controls are not included in the research question under investigation, so we will exclude them.\nWe are only interested in the learning blocks, and we will exclude the testing blocks (block &gt; 7)\nYou might want to store this data in a separate object, but in the code for the solution we will just use filter() inside the glmer().\n\nA model with just the change over the sequence of blocks:\n\noutcome ~ block\n\nA model with the change over the sequence of blocks and an overall difference between groups:\n\noutcome ~ block + lesion_location\n\nA model with groups differing with respect to their change *over the sequence of blocks:\n\noutcome ~ block * lesion_location\n\nWhat are our observations grouped by?\n\nrepeated measures by-participant. i.e., the ID variable\n\nWhat variables can vary within these groups?\n\nBlock and Phase. Be careful though - you can create the Phase variable out of the Block variable, so really this is just one piece of information, encoded differently in two variables.\nThe other variables (lesion_location and group) do not vary for each ID. Lesions don’t suddenly change where they are located, nor do participants swap between being a patient vs a control (we don’t need the group variable anyway as we are excluding the controls).\nWhat do you want your model to allow to vary within these groups?\nDo you think the change over the course of the blocks is the same for everybody? Or do you think it varies? Is this variation important to think about in terms of your research question?\n\n\n\n\n\n\n\n2 - modelling\n\n\n\n\nm.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), \n                data = filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\nm.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), \n                data=filter(nwl, block &lt; 8, !is.na(lesion_location)),\n                family=binomial)\n\n\nanova(m.base, m.loc0, m.loc1, test=\"Chisq\")\n\nData: filter(nwl, block &lt; 8, !is.na(lesion_location))\nModels:\nm.base: cbind(NumCorrect, NumError) ~ block + (block | ID)\nm.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID)\nm.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nm.base    5 454.12 466.27 -222.06   444.12                     \nm.loc0    6 454.66 469.25 -221.33   442.66 1.4572  1     0.2274\nm.loc1    7 454.47 471.48 -220.23   440.47 2.1974  1     0.1382\n\n\n\nNo significant difference in learning rate between groups (\\(\\chi^2(1)=2.2, p = 0.138\\)).\n\n\n\n\n\nQuestion 19\n\n\n\nResearch Question 2\nIn the testing phase, does performance on the immediate test differ between lesion location groups, and does the retention from immediate to follow-up test differ between the two lesion location groups?\n\nLet’s try a different approach to this. Instead of fitting various models and comparing them via likelihood ratio tests, just fit the one model which could answer both parts of the question above.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThis might required a bit more data-wrangling beforehand. Think about the order of your factor levels (alphabetically speaking, “Follow-up” comes before “Immediate”)!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nnwl_test &lt;- filter(nwl, block &gt; 7, !is.na(lesion_location)) %&gt;%\n    mutate(\n        Phase = factor(Phase), \n        Phase = fct_relevel(Phase, \"Immediate\")\n    )\n\nm.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 | ID), \n                      nwl_test, family=\"binomial\")\n\nsummary(m.recall.loc)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (1 |      ID)\n   Data: nwl_test\n\n     AIC      BIC   logLik deviance df.resid \n   139.3    145.2    -64.6    129.3       19 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1556 -0.3352  0.0039  0.4963  1.3506 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.3626   0.6021  \nNumber of obs: 24, groups:  ID, 12\n\nFixed effects:\n                                        Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                              -0.1124     0.3167  -0.355   0.7226  \nPhaseFollow-up                           -0.0278     0.2357  -0.118   0.9061  \nlesion_locationposterior                  0.9672     0.4211   2.297   0.0216 *\nPhaseFollow-up:lesion_locationposterior  -0.2035     0.3191  -0.638   0.5236  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PhsFl- lsn_lc\nPhaseFllw-p -0.372              \nlsn_lctnpst -0.752  0.280       \nPhsFllw-p:_  0.275 -0.739 -0.385\n\n\nNote 1:\nIn the above, we have made sure to select the patients by specifying !is.na(lesion_location), meaning that we want those rows where the lesion location is not missing. As a reminder ! is the negation function (not). As we saw in Question A1, this excludes the 126 healthy controls, as well as the 9 patients for which we have missing values (NAs).\nNote 2:\nWe didn’t specify (Phase | ID) as the random effect because each participant only has 2 data points for Phase, and there is only one line that fits two data points. In other words, there is only one possible way to fit those two data points. As such, as each group of 2 points will have a perfect line fit, and the residuals \\(\\varepsilon_{ij}\\) will all be 0. As a consequence of this, the residuals will have no variability as they are all 0, so \\(\\sigma_{\\epsilon}\\) is 0 which in turn leads to problem with estimating the model coefficients.\n\nsubset(nwl_test, ID == 'patient15')\n\n     group lesion_location block PropCorrect NumCorrect NumError        ID\n1  patient        anterior     8   0.5333333         16       14 patient15\n13 patient        anterior     9   0.5333333         16       14 patient15\n       Phase\n1  Immediate\n13 Follow-up\n\n\nIf you try using (Phase | ID) as random effect, you will see the following message:\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nQuestion 20\n\n\n\nIn family = binomial(link='logit'). What function is used to relate the linear predictors in the model to the expected value of the response variable?\n\nHow do we convert this into something more interpretable?\n\n\n\n\n\n\nSolution\n\n\n\n\nThe link function is the logit, or log-odds (other link functions are available).\nTo convert log-odds to odds, we can use exp(), to get odds and odds ratios.\n\n\n\n\n\nQuestion 21\n\n\nMake sure you pay attention to trying to interpret each fixed effect from your models.\nThese can be difficult, especially when it’s logistic, and especially when there are interactions.\n\nWhat is the increase in the odds of answering correctly in the immediate test if you were to have a posterior legion instead of an anterior legion?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n(Intercept) ==&gt; Anterior lesion group performance in immediate test. This is the log-odds of them answering correctly in the immediate test.\nPhaseFollow-up ==&gt; Change in performance (for someone with an anterior lesion) from immediate to follow-up test.\nlesion_locationposterior ==&gt; Change in performance in immediate test were a patient to have a posterior lesion instead of an anterior lesion.\nPhaseFollow-up:lesion_locationposterior ==&gt; How change in performance from immediate to follow-up test would differ were a patient to have a posterior lesion instead of an anterior lesion.\n\n\n\nlesion_locationposterior \n                2.630634 \n\n\n\nHaving a posterior lesions is associated with 2.63 times the odds of answering correctly in the immediate test compared to having an anterior lesion.\n\n\n\n\n\nQuestion 22\n\n\nRecreate the visualisation in Figure 2.\n\n\n\n\n\nFigure 2: Differences between groups in the average proportion of correct responses at each block\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, \n                                                 color=lesion_location, \n                                                 shape=lesion_location)) +\n    #geom_line(aes(group=ID),alpha=.2) + \n    stat_summary(fun.data=mean_se, geom=\"pointrange\") + \n    stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), \n                 fun=mean, geom=\"line\") + \n    geom_hline(yintercept=0.5, linetype=\"dashed\") + \n    geom_vline(xintercept=c(7.5, 8.5), linetype=\"dashed\") + \n    scale_x_continuous(breaks=1:9, \n                       labels=c(1:7, \"Test\", \"Follow-Up\")) + \n    theme_bw(base_size=10) + \n    labs(x=\"Block\", y=\"Proportion Correct\", \n         shape=\"Lesion\\nLocation\", color=\"Lesion\\nLocation\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\n\n\n\n\n\nSolution\n\n\n\nsolution\n\n\n\n\n\nOptional hello my optional friend\n\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multivariate Statistics and Methodology in R",
    "section": "",
    "text": "Multivariate Statistics and Methodology in R (MSMR) is an advanced semester-long course designed for Masters students in psychology seeking a deeper understanding of statistical techniques to analyze complex data sets with multiple sources of variation. Building on the foundation laid by the Univariate Statistics and Methodology in R (USMR) course, MSMR extends students’ analytical repertoire to encompass multilevel models, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM).\nThe initial half of the course introduces students to the intricacies of multilevel models, providing a solid theoretical framework for understanding hierarchical data structures. Students will gain practical insights into applying these models to address research questions involving nested data and varying sources of variation.\nThe second half of the course delves into methods such as PCA and EFA for reducing dimensionality of data, before moving to Confirmatory Factor models and subsequently Structural Equation Models as a means of modeling and testing our theories about psychological constructs."
  },
  {
    "objectID": "lvp.html",
    "href": "lvp.html",
    "title": "Likelihood vs Probability",
    "section": "",
    "text": "Upon hearing the terms “probability” and “likelihood”, people will often tend to interpret them as synonymous. In statistics, however, the distinction between these two concepts is very important (and often misunderstood)."
  },
  {
    "objectID": "lvp.html#setup",
    "href": "lvp.html#setup",
    "title": "Likelihood vs Probability",
    "section": "Setup",
    "text": "Setup\nLet’s consider a coin flip. For a fair coin, the chance of getting a heads/tails for any given flip is 0.5.\nWe can simulate the number of “heads” in a single fair coin flip with the following code (because it is a single flip, it’s just going to return 0 or 1):\n\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 1\n\n\nWe can simulate the number of “heads” in 8 fair coin flips with the following code:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 8\n\n\nAs the coin is fair, what number of heads would we expect to see out of 8 coin flips? Answer: 4! Doing another 8 flips:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 4\n\n\nand another 8:\n\nrbinom(n = 1, size = 8, prob = 0.5)\n\n[1] 3\n\n\nWe see that they tend to be around our intuition expected number of 4 heads. We can change n = 1 to ask rbinom() to not just do 1 set of 8 coin flips, but to do 1000 sets of 8 flips:\n\ntable(rbinom(n = 1000, size = 8, prob = 0.5))\n\n\n  0   1   2   3   4   5   6   7   8 \n  6  46  98 228 277 217 101  24   3"
  },
  {
    "objectID": "lvp.html#probability",
    "href": "lvp.html#probability",
    "title": "Likelihood vs Probability",
    "section": "Probability",
    "text": "Probability\nSo what is the probability of observing \\(k\\) heads in \\(n\\) flips of a fair coin?\nAs coin flips are independent, we can calculate probability using the product rule (\\(P(AB) = P(A)\\cdot P(B)\\) where \\(A\\) and \\(B\\) are independent).\nSo the probability of observing 2 heads in 2 flips is \\(0.5 \\cdot 0.5 = 0.25\\)\nWe can get to this probability using dbinom():\n\ndbinom(2, size=2, prob=0.5)\n\n[1] 0.25\n\n\nIn 8 flips, those two heads could occur in various ways:\n\n\n\n\n\n\n  \n    \n    \n      Ways to get 2 heads in 8 flips\n    \n  \n  \n    TTTTTHHT\n    TTHTTHTT\n    TTTHTTTH\n    HTTTTTTH\n    HTTHTTTT\n    TTTHTTHT\n    TTTTHTHT\n    TTHTTTHT\n    ...\n  \n  \n  \n\n\n\n\nAs it happens, there are 28 different ways this could happen.2\nThe probability of getting 2 heads in 8 flips of a fair coin is, therefore:\n\n28 * (0.5^8)\n\n[1] 0.109375\n\n\nOr, using dbinom()\n\ndbinom(2, size = 8, prob = 0.5)\n\n[1] 0.109375\n\n\n\nThe important thing here is that when we are computing the probability, two things are fixed:\n\nthe number of coin flips (8)\nthe value(s) that govern the coin’s behaviour (0.5 chance of landing on heads for any given flip)\n\nWe can then can compute the probabilities for observing various numbers of heads:\n\ndbinom(0:8, 8, prob = 0.5)\n\n[1] 0.00390625 0.03125000 0.10937500 0.21875000 0.27343750 0.21875000 0.10937500\n[8] 0.03125000 0.00390625\n\n\n\n\n\n\n\n\n\n\n\nNote that the probability of observing 10 heads in 8 coin flips is 0, as we would hope!\n\ndbinom(10, 8, prob = 0.5)\n\n[1] 0"
  },
  {
    "objectID": "lvp.html#likelihood",
    "href": "lvp.html#likelihood",
    "title": "Likelihood vs Probability",
    "section": "Likelihood",
    "text": "Likelihood\nSo how does likelihood differ?\nFor likelihood, we are interested in hypotheses about or models of our coin. Do we think it is a fair coin (for which the probability of heads is 0.5?). Do we think it is biased to land on heads 60% of the time? or 30% of the time? All of these are different ‘models’.\nTo consider these hypotheses, we need to observe some data - we need to have a given number of flips, and the resulting number of heads.\nWhereas when discussing probability, we varied the number of heads, and fixed the parameter that designates the true chance of landing on heads for any given flip, for the likelihood we are fixing the number of heads observed, and can make statements about different possible parameters that might govern the coin’s behaviour.\nFor example, let’s suppose we did observe 2 heads in 8 flips, what is the probability of seeing this data given various parameters?\nHere, our parameter (the probability that we think the coin lands on heads) can take any real number between from 0 to 1, but let’s do it for a selection:\n\npossible_parameters = seq(from = 0, to = 1, by = 0.05)\ndbinom(2, 8, possible_parameters)\n\n [1] 0.000000e+00 5.145643e-02 1.488035e-01 2.376042e-01 2.936013e-01\n [6] 3.114624e-01 2.964755e-01 2.586868e-01 2.090189e-01 1.569492e-01\n[11] 1.093750e-01 7.033289e-02 4.128768e-02 2.174668e-02 1.000188e-02\n[16] 3.845215e-03 1.146880e-03 2.304323e-04 2.268000e-05 3.948437e-07\n[21] 0.000000e+00\n\n\nSo what we are doing here is considering the possible parameters that govern our coin. Given that we observed 2 heads in 8 coin flips, it seems very unlikely that the coin weighted such that it lands on heads 80% of the time (e.g., the parameter of 0.8 is not likely). The idea that the coin is fair (0.5 probability) is more likely. The most likely parameter is 0.25 (because \\(\\frac{2}{8}=0.25\\)).\nYou can visualise this below:"
  },
  {
    "objectID": "lvp.html#a-slightly-more-formal-approach",
    "href": "lvp.html#a-slightly-more-formal-approach",
    "title": "Likelihood vs Probability",
    "section": "A slightly more formal approach",
    "text": "A slightly more formal approach\nLet \\(d\\) be our data (our observed outcome), and let \\(\\theta\\) be the parameters that govern the data generating process.\nWhen talking about “probability” we are talking about \\(P(d | \\theta)\\) for a given value of \\(\\theta\\).\nE.g. above we were talking about \\(P(\\text{2 heads in 8 flips}\\vert \\text{fair coin})\\).\nIn reality, we don’t actually know what \\(\\theta\\) is, but we do observe some data \\(d\\).\nGiven that we know that if we have a specific value for \\(\\theta\\), then \\(P(d \\vert \\theta)\\) will give us the probability of observing \\(d\\), we can ask “what value of \\(\\theta)\\) will maximise the probability of observing \\(d\\)?”.\nThis will sometimes get written as \\(\\mathcal{L}(\\theta \\vert d)\\) as the “likelihood function” of our unknown parameters \\(\\theta\\), conditioned upon our observed data \\(d\\)."
  },
  {
    "objectID": "lvp.html#footnotes",
    "href": "lvp.html#footnotes",
    "title": "Likelihood vs Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the typical frequentist stats view. There are other ways to do statistics (not covered in this course) - e.g., in Bayesian statistics, probability relates to the reasonable expectation (or “plausibility”) of a belief↩︎\nIf you really want to see them all, try running combn(8, 2) in your console.↩︎"
  }
]