```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
```

```{r simulate lab data, echo=F, eval=F}
set.seed(229)
nF=2 #number of factors
nV=10 #number of variables

Psi<-matrix(nrow=nF, ncol=nF,     # the nF by nF factor correlation matrix
            data=c(1.00,0.20,
                   0.20,1.00),byrow=T)


Lambda<-matrix(nrow=nV, ncol=nF,  # the nV by nF factor loading matrix
               #F1    F2
               data=c(0.65, 0.10, # item1
                      0.73, 0.08, # item2
                      0.65, 0.06, # item3
                      0.65, 0.10, # item4
                      0.84, 0.04, # item5
                      0.01, 0.65, # item6
                      0.10, 0.88, # item7
                      0.03, 0.92, #item8
                      0.10, 0.67, #item9
                      0.02, 0.65), #item10
               byrow=T)


Theta<-matrix(nrow=nV, ncol=nV, # the nV by nV residual matrix
              #item1 item2 item3 item4 item5 item6 item7 item8 item9 item10
              data=c(1-0.65^2-0.10^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item1
                     0.00, 1-0.73^2-0.08^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item2
                     0.00, 0.00, 1-0.65^2-0.06^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item3
                     0.00, 0.00, 0.00, 1-0.65^2-0.10^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item4
                     0.00, 0.00, 0.00, 0.00, 1-0.84^2-0.04^2, 0.00, 0.00, 0.00, 0.00, 0.00, #item5
                     0.00, 0.00, 0.00, 0.00, 0.00, 1-0.01^2-0.65^2, 0.00, 0.00, 0.00, 0.00, #item6
                     0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.10^2-0.88^2, 0.00, 0.00, 0.00, #item7
                     0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.03^2-0.92^2, 0.00, 0.00, #item8
                     0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.10^2-0.67^2, 0.00, #item9
                     0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.02^2-0.65^2), #item10
              byrow=T) 


#compute correlation matrix from Psi, Lambda and Theta

Sigma<-Lambda%*%Psi%*%t(Lambda)+Theta
#simulate data
library(MASS)
df<-as.data.frame(mvrnorm(n=600, mu=rep(0,10), Sigma=Sigma))
names(df)<-c('item1','item2','item3','item4','item5','item6','item7','item8','item9','item10')
```


# Confirmatory Factor Analysis (CFA)  

### Packages {-}  

+ lavaan  

### Lecture Slides {-}  

+ The lecture slides can be accessed [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/week8_cfa.pdf).   

## Quick recap {-}  

Last week we learned about two methods of data reduction: Principal Components Analysis (PCA) and Factor Analysis.  
In brief, PCA aims to summarise a set of measured variables into a set of uncorrelated (orthogonal) components, which are linear combinations (a weighted average) of the measured variables.  
Factor analysis, on the other hand, assumes that the relationships between a set of measured variables can be explained by a number of underlying latent factors. 
Note how the directions of the arrows in Figure \@ref(fig:pcafa) are different between PCA and FA - in PCA, each component $C_i$ is the weighted combination of the observed variables $y_1, ...,y_n$, whereas in FA, each measured variable $y_i$ is seen as generated by some latent factor $F$ plus some unexplained variance $u_i$. 

```{r pcafa, echo=FALSE, fig.cap="Path diagrams for PCA and FA", out.width="1000px"}
knitr::include_graphics("images/pcafadiag.png")
```

In Exploratory Factor Analysis (EFA), we tend to start with no hypothesis about either the number of latent factors or about the specific relationships between latent factors and measured variables (the *factor structure*).   
Typically, all variables will load on all factors, and a transformation method such as a rotation is used to help make the results more easily interpretable.  

When we have a clear hypothesis about relationships between measured variables and latent factors, we might want to impose a specific factor structure on the data. These sort of models are called Confirmatory Factor Analysis (CFA) models. 

The purpose of CFA can be seen of as twofold: Firstly to obtain parameter estimates (i.e., factor loadings, variances and covariances of factors, residual variances of measured variables), and secondly to assess whether the model provides a good fit to the data. 


## Data {-}

<div class="noteBox"> 
Last week we conducted an exploratory factor analysis of a dataset to try and identify an optimal factor structure for a new measure of conduct (i.e., antisocial behavioural) problems. This week, we'll conduct some confirmatory factor analyses (CFA) of the same inventory, using some new data collected by the researchers from n=600 adolescents.

The questionnaire items referred to the following 10 behaviours:

- item 1-	Stealing
- item 2-	Lying
- item 3-	Skipping school
- item 4-	Vandalism
- item 5-	Breaking curfew
- item 6-	Threatening others
- item 7-	Bullying
- item 8-	Spreading malicious rumours
- item 9-	Using a weapon 
- item 10-	Fighting

</div>

`r msmbstyle::question_begin(header="&#x25BA; Question 1")`
Begin by reading in the new data. It is available as a **.csv** at [https://edin.ac/32MGsFx](https://edin.ac/32MGsFx) 

`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`

```{r}
df <- read.csv("https://edin.ac/32MGsFx")
```

`r msmbstyle::solution_end()`

## One factor model {-}  

`r msmbstyle::question_begin(header="&#x25BA; Question 2a")`
Using lavaan syntax, specify a model in which all 10 items load on one latent variable.  

*Hint:* To specify items loading on a latent variable we use the '=~' operator. The latent variables goes on  the left hand side and the list of indicators (i.e., items used to measure the latent variable) go on the right hand side separated by '+'
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r model specification}
model1<-'CP=~item1+item2+item3+item4+item5+item6+item7+item8+item9+item10'
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 2b")`
Why is it not necessary to refer to **ALL** of your CFA parameters in your model specification if you use the `cfa()` function to estimate your model? Which parameters do you not need to include in the model specification?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
Some parameters are estimated or fixed by default when you estimate the model with the `cfa()` function. In this case, the residual factor variances and loadings are missing because the former are estimated by default and the latter are fixed to 1 by default. Latent factor variances are also estimated by default.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 2c")`
Estimate your model using the `cfa`()` function from the lavaan package. Scale your latent variable by fixing the latent variable variance to 1.  
  
Remember to load the lavaan library first.  
The default scaling constraint/identifcation constraints imposed when using the `cfa()` function are to fix the loading of the first item of each latent variable to 1.  
We can override this by setting `std.lv=TRUE`.  
This will instead scale the latent variables by fixing them to 1.  
It is helpful to save the results of `cfa()` to a new object so that we can later inspect that object (to look at the model fit and parameter estimates).
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r model estimation}
library(lavaan)
model1.est<-cfa(model1, data=df, std.lv=TRUE)
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 2d")`
Now examine the global fit of your model. Does it fit well?  

*Remember:* If $\textrm{RMSEA} < .05$, $\textrm{SRMR} < .05$, $\textrm{TLI} > .95$ and $\textrm{CFI} > .95$ then the model fits well. Smaller values of RMSEA and SRMR mean better fit while larger values of CFI and TLI mean better fit. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
To obtain the global fit measures, we can use the `summary()` function to inspect our estimated model, setting `fit.measures=TRUE`.

```{r global fit}
summary(model1.est, fit.measures=TRUE)
```

According to conventionally accepted criteria for RMSEA, SRMR, TLI and CFI, the model fits poorly. 
`r msmbstyle::solution_end()`

## Two factor model {-}

`r msmbstyle::question_begin(header="&#x25BA; Question 3a")`
Now let's try a different model. 
Specify a CFA model with two correlated latent factors.  
Consider items 1 to 5 as indicators of the first latent factor and items 6 to 10 as indicators of the second latent factor.
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can specify the model as below. 

```{r two factor model specification}
model2<-'LV1=~item1+item2+item3+item4+item5
LV2=~item6+item7+item8+item9+item10
LV1~~LV2'
```

We now define two latent variables 'Lv1' and 'Lv2' using the '=~' operator. We also introduce a new operator: '~~' in order to specify the covariance between the two latent variables. 
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3b")`
Estimate this model. Scale the latent variables using a reference indicator. Does the model fit well?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can estimate the model using the `cfa()` function. As it uses a reference indicator to scale the latent variables by default we only need to specify the name of the object with the model syntax and the name of the dataset.  

```{r estimate 2-factor model}
model2.est<-cfa(model2, data=df)
summary(model2.est, fit.measures=T)
```

This model fits well according to RMSEA, SRMR, CFI and TLI. The $\chi^2$ value is significant but we don't need to worry about this because the $\chi^2$  test  has a strong tendency to reject even trivially mis-specified models.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3c")`
Are there any areas of local mis-fit?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can look for local mis-fit by using the `modindices()` function. It gives us the modification indices (the expected improvement in $\chi^2$ if a parameter was added) and the expected parameter change associated with the addition of the parameter (an estimate of what the parameter estimate would be if the parameter was included in the model).

```{r modification indices}
modindices(model2.est, sort=T)
```

While the modification indices suggest that the model could be improved with the addition of parameters, none of the expected parameter changes are very large. If we included any additonal parameters here it is likely we would be capitalising on chance. We would also have to consider whether we could justify their inclusion on substantive grounds, i.e., we would ask ourselves 'does the addition of the parameter makes sense given our background knowledge of the construct?'.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3d")`
Take a look at the parameter estimates, are all of your loadings satisfactory? Which items are the best measures of the underlying latent variables? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
It helps to look at the standardised parameter estimates:

```{r stand ests}
summary(model2.est, standardized=T)
```

We can see that all loadings are statistically significant and all standardised loadings are >|.3|. This is good as it suggests all our items measure the relevant latent variable reasonably well. We can identify the 'best' indicators of our latent constructs according to those with the highest standardised loadings. We must, however, be aware that item loadings are model-specific: if we changed the model (e.g., the number of factors or which items we included) the loadings could change and so would our interpretation of the latent variables.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3e")`
 Using R, represent the model, including the standardised parameters as an SEM diagram
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can use the `semPaths()` function from the `semPlot` package to create a SEM diagram from the estimated model object. We can include the unstandardised estimates by setting `what='est'` and the standardised estimates by setting `what='stand'`.

```{r SEM diagram}
library(semPlot)
semPaths(model2.est, what='stand')

semPaths(model1.est, what='stand')

```

If we wanted to include a SEM diagram in a report the raw output from `semPaths()` would not usually meet publication standards. Instead, we can draw SEM diagrams in a programme such as powerpoint.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3f")`
Write a short paragraph summarising the method and results of the two factor model.
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
The main principle behind reporting any analysis is that you should be as  transparent as possible (e.g., reporting any model modifications made) and a reader should be able to reproduce your analysis based on your description. 

An example description would be:

*A two-factor model was tested. Items 1-5 loaded on a 'non-aggressive conduct problems' factor and items 6-10 loaded on an 'aggression' factor and these factors were allowed to correlate. Scaling and identification were achieved by fixing the loading of item 1 on the non-aggressive conduct problems factor and item 6 on the aggression factor to 1. The model was estimated using maximum likelihood estimation. The model fit well with CFI=.99, TLI=0.99,  RMSEA=.04, and SRMR=.04 (Hu & Bentler, 1999).  All loadings were statistically significant and >|.3| on the standardised scale. Modification indices and expected parameter changes were inspected but no modifications were made because no expected parameter changes were judged large enough to merit the inclusion of additional parameters given that there was little theoretical rationale for their inclusion. Overall, therefore, a two-factor oblique model was supported for the conduct problems items. The correlation between the factors was r=.38 (p<.001).*

When you write up a CFA for a report, you should also include the parameter estimates. This can be in the form of a table with the unstandardised loadings and factor covariances, their standard errors, p-values, and corresponding standardised values. 

Note: according to APA style, you should include a leading zero when a parameter can take values out of the 0-1 range (e.g., SD=0.99) but you should not include a leading zero when a parameter can only take values in the 0-1 range (e.g., r=.5). This is why we have TLI=0.99 but CFI=.99. The former can technically (though rarely does) take values >1 while the latter only takes values between 0 and 1.
`r msmbstyle::solution_end()`
