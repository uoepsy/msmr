<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Multilevel Modelling Recap</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);
e.style.display = ((e.style.display!='none') ? 'none' : 'block');
if(f.classList.contains('fa-chevron-circle-right')) {
    f.classList.add('fa-chevron-circle-down')
    f.classList.remove('fa-chevron-circle-right')
} else {
    f.classList.add('fa-chevron-circle-right')
    f.classList.remove('fa-chevron-circle-down')
}
}
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="assets/style-labs.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><strong>MSMR</strong></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Mixed Effects Models
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_intromlm.html">1: Introduction to Multilevel Models</a>
    </li>
    <li>
      <a href="02_lmm_log.html">2: Logistic Multilevel Models</a>
    </li>
    <li>
      <a href="03_nonlin.html">3: Longitudinal Nonlinear Models</a>
    </li>
    <li>
      <a href="04_ranef.html">4: Random Effect Structures</a>
    </li>
    <li>
      <a href="05_multilevel_recap.html">5: Multilevel Modelling Recap</a>
    </li>
    <li class="dropdown-header">FLW: Flexible Learning Week</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data Reduction &amp; SEM
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06_efapca.html">6: PCA | EFA</a>
    </li>
    <li>
      <a href="07_cfa.html">7: CFA</a>
    </li>
    <li>
      <a href="08_path.html">8: Path Analysis</a>
    </li>
    <li>
      <a href="09_sem1.html">9: SEM 1</a>
    </li>
    <li>
      <a href="10_sem2.html">10: SEM 2</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-info-circle"></span>
     
    Extras
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="zz_tidyverse_markdown.html">Tidyverse &amp; Markdown Recap</a>
    </li>
    <li>
      <a href="zz_binary_binomial.html">Binary vs Binomial Data</a>
    </li>
    <li>
      <a href="zz_quickcontrasts.html">Contrasts: a Quick Overview</a>
    </li>
    <li>
      <a href="zz_lvp.html">Likelihood vs Probability</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multilevel Modelling Recap</h1>

</div>


<div id="flashcards-lm-to-lmer" class="section level1">
<h1>Flashcards: <code>lm</code> to <code>lmer</code></h1>
<p>In a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.</p>
<p>Multi-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.</p>
<p>We can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).</p>
<div class="blue">
<p>Before you expand each of the boxes below, think about how comfortable you feel with each concept.<br />
This content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.</p>
</div>
<div class="optional-begin">
<span id="opt-start-1" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-1&#39;, &#39;opt-start-1&#39;)"> Simple Linear Regression</span>
</div>
<div id="opt-body-1" class="optional-body" style="display: none;">
<div class="frame">
<p><strong>Formula:</strong></p>
<ul>
<li><span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)</span></li>
</ul>
<p><strong>R command:</strong></p>
<ul>
<li><code>lm(outcome ~ predictor, data = dataframe)</code></li>
</ul>
<p><em>Note:</em> this is the same as <code>lm(outcome ~ 1 + predictor, data = dataframe)</code>. The <code>1 +</code> is always there unless we specify otherwise (e.g., by using <code>0 +</code>).</p>
</div>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-2-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-2" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-2&#39;, &#39;opt-start-2&#39;)"> Clustered (multi-level) data</span>
</div>
<div id="opt-body-2" class="optional-body" style="display: none;">
<p>When our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.</p>
<p>If we separate out our data to show an individual plot for each subject, we can see how the fitted regression line from <code>lm()</code> is assumed to be the same for each subject.</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-3" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-3&#39;, &#39;opt-start-3&#39;)"> Random intercepts</span>
</div>
<div id="opt-body-3" class="optional-body" style="display: none;">
<p>By including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.</p>
<div class="frame">
<p><strong>Formula:</strong><br />
Level 1:</p>
<ul>
<li><span class="math inline">\(y_{ij} = \beta_{0i} + \beta_{1i} x_{ij} + \epsilon_{ij}\)</span></li>
</ul>
<p>Level 2:</p>
<ul>
<li><span class="math inline">\(\beta_{0i} = \gamma_{00} + \zeta_{0i}\)</span></li>
</ul>
<p>Where the expected values of <span class="math inline">\(\zeta_{0}\)</span>, and <span class="math inline">\(\epsilon\)</span> are 0, and their variances are <span class="math inline">\(\sigma_{\zeta_{0}}^2\)</span> and <span class="math inline">\(\sigma_\epsilon^2\)</span> respectively. We will further assume that these are normally distributed.</p>
<p>We can now see that the intercept estimate <span class="math inline">\(\beta_{0i}\)</span> for a particular group <span class="math inline">\(i\)</span> is represented by the combination of a mean estimate for the parameter (<span class="math inline">\(\gamma_{00}\)</span>) and a random effect for that group (<span class="math inline">\(\zeta_{0i}\)</span>).</p>
<p><strong>R command:</strong></p>
<ul>
<li><code>lmer(outcome ~ predictor + (1 | grouping), data = dataframe)</code></li>
</ul>
</div>
<p>Notice how the fitted line of the random intercept model has an adjustment for each subject.<br />
Each subject’s line has been moved up or down accordingly.</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-4" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-4&#39;, &#39;opt-start-4&#39;)"> Shrinkage</span>
</div>
<div id="opt-body-4" class="optional-body" style="display: none;">
<p>If you think about it, we might have done a similar thing with the tools we already had at our disposal, by using <code>lm(y~x+subject)</code>.
This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to <code>lm(y~x*subject)</code> to give us an adjustment to the slope for each subject.</p>
<p>However, the estimate of these models will be slightly different:</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-5-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><strong>Why?</strong> One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on</p>
<ol style="list-style-type: lower-alpha">
<li>the level of across-cluster variation and<br />
</li>
<li>the number of datapoints in clusters.</li>
</ol>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-5" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-5&#39;, &#39;opt-start-5&#39;)"> Random slopes</span>
</div>
<div id="opt-body-5" class="optional-body" style="display: none;">
<div class="frame">
<p><strong>Formula:</strong><br />
Level 1:</p>
<ul>
<li><span class="math inline">\(y_{ij} = \beta_{0i} + \beta_{1i} x_{ij} + \epsilon_{ij}\)</span></li>
</ul>
<p>Level 2:</p>
<ul>
<li><span class="math inline">\(\beta_{0i} = \gamma_{00} + \zeta_{0i}\)</span><br />
</li>
<li><span class="math inline">\(\beta_{1i} = \gamma_{10} + \zeta_{1i}\)</span></li>
</ul>
<p>Where the expected values of <span class="math inline">\(\zeta_0\)</span>, <span class="math inline">\(\zeta_1\)</span>, and <span class="math inline">\(\epsilon\)</span> are 0, and their variances are <span class="math inline">\(\sigma_{\zeta_{0}}^2\)</span>, <span class="math inline">\(\sigma_{\zeta_{1}}^2\)</span>, <span class="math inline">\(\sigma_\epsilon^2\)</span> respectively. We will further assume that these are normally distributed.</p>
<p>As with the intercept <span class="math inline">\(\beta_{0i}\)</span>, the slope of the predictor <span class="math inline">\(\beta_{1i}\)</span> is now modelled by a mean <span class="math inline">\(\gamma_{10}\)</span> and a random effect for each group (<span class="math inline">\(\zeta_{1i}\)</span>).</p>
<p><strong>R command:</strong></p>
<ul>
<li><code>lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)</code></li>
</ul>
<p><em>Note:</em> this is the same as <code>lmer(outcome ~ predictor + (predictor | grouping), data = dataframe)</code> . Like in the fixed-effects part, the <code>1 +</code> is assumed in the random-effects part.</p>
</div>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-6" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-6&#39;, &#39;opt-start-6&#39;)"> Fixed effects</span>
</div>
<div id="opt-body-6" class="optional-body" style="display: none;">
<p>We can extract the <em>fixed effects</em> using the <code>fixef()</code> function:</p>
<p>These are the overall intercept and slope, <span class="math inline">\(\gamma_{00}\)</span> and <span class="math inline">\(\gamma_{10}\)</span>.</p>
<pre class="r"><code>fixef(random_slopes_model)</code></pre>
<pre><code>## (Intercept)          x1 
##     405.790      -0.672</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-7" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-7&#39;, &#39;opt-start-7&#39;)"> Random effects</span>
</div>
<div id="opt-body-7" class="optional-body" style="display: none;">
<p>The plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept &amp; slope):</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation <span class="math inline">\(\sigma_{\zeta_0}\)</span>. The standard deviation (and variance, which is <span class="math inline">\(\sigma_{\zeta_0}^2\)</span>) is what we see in the random effects part of our model summary (or using the <code>VarCorr()</code> function).</p>
<p><img src="images/varcors.PNG" width="400px" style="display: block; margin: auto;" /></p>
<p>In the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope.
We can extract the deviations for each group from the fixed effect estimates using the <code>ranef()</code> function.</p>
<p>These are the deviations from the overall intercept (<span class="math inline">\(\widehat \gamma_{00} = 405.79\)</span>) and slope (<span class="math inline">\(\widehat \gamma_{10} = -0.672\)</span>) for each subject <span class="math inline">\(i\)</span>.</p>
<pre class="r"><code>ranef(random_slopes_model)</code></pre>
<pre><code>## $subject
##         (Intercept)      x1
## sub_308       31.33 -1.4400
## sub_309      -28.83  0.4184
## sub_310        2.71  0.0599
## sub_330       59.40  0.3853
## sub_331       74.96  0.1739
## sub_332       91.09 -0.2346
## sub_333       97.85 -0.1906
## sub_334      -54.19 -0.5585
## sub_335      -16.90  0.9207
## sub_337       52.22 -1.1660
## sub_349      -67.76 -0.6844
## sub_350       -5.82 -1.2379
## sub_351       61.20  0.0550
## sub_352       -7.91 -0.6650
## sub_369      -47.64 -0.4681
## sub_370      -33.12 -1.1100
## sub_371       77.58 -0.2040
## sub_372      -36.39 -0.4583
## sub_373     -197.58  1.7990
## sub_374      -52.20  4.6051
## 
## with conditional variances for &quot;subject&quot;</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-8" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-8&#39;, &#39;opt-start-8&#39;)"> Group-level coefficients</span>
</div>
<div id="opt-body-8" class="optional-body" style="display: none;">
<p>We can also see the actual intercept and slope for each subject <span class="math inline">\(i\)</span> directly, using the <code>coef()</code> function.</p>
<pre class="r"><code>coef(random_slopes_model)</code></pre>
<pre><code>## $subject
##         (Intercept)     x1
## sub_308         437 -2.112
## sub_309         377 -0.254
## sub_310         409 -0.612
## sub_330         465 -0.287
## sub_331         481 -0.498
## sub_332         497 -0.907
## sub_333         504 -0.863
## sub_334         352 -1.231
## sub_335         389  0.248
## sub_337         458 -1.838
## sub_349         338 -1.357
## sub_350         400 -1.910
## sub_351         467 -0.617
## sub_352         398 -1.337
## sub_369         358 -1.140
## sub_370         373 -1.782
## sub_371         483 -0.876
## sub_372         369 -1.131
## sub_373         208  1.127
## sub_374         354  3.933
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
<p>Notice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.</p>
<pre class="r"><code>coef(random_intercept_model)</code></pre>
<pre><code>## $subject
##         (Intercept)     x1
## sub_308         384 -0.914
## sub_309         407 -0.914
## sub_310         422 -0.914
## sub_330         492 -0.914
## sub_331         498 -0.914
## sub_332         496 -0.914
## sub_333         505 -0.914
## sub_334         339 -0.914
## sub_335         440 -0.914
## sub_337         417 -0.914
## sub_349         320 -0.914
## sub_350         356 -0.914
## sub_351         479 -0.914
## sub_352         380 -0.914
## sub_369         349 -0.914
## sub_370         335 -0.914
## sub_371         484 -0.914
## sub_372         361 -0.914
## sub_373         294 -0.914
## sub_374         511 -0.914
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-9" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-9&#39;, &#39;opt-start-9&#39;)"> Visualising Model Fitted values</span>
</div>
<div id="opt-body-9" class="optional-body" style="display: none;">
<p>The model fitted (or “model predicted”) values can be obtained using <code>predict()</code> (returning just the values) or <code>broom.mixed::augment()</code> (returning the values attached to the data that is inputted to the model).</p>
<p>To plot, them, we would typically like to plot the fitted values for each group (e.g. subject)</p>
<pre class="r"><code>library(broom.mixed)
augment(random_slopes_model) %&gt;%
  ggplot(.,aes(x=x1, y=.fitted, group=subject))+
  geom_line()</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-10" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-10&#39;, &#39;opt-start-10&#39;)"> Visualising Fixed Effects</span>
</div>
<div id="opt-body-10" class="optional-body" style="display: none;">
<p>If we want to plot the fixed effects from our model, we have to do something else. Packages like <strong>sjPlot</strong> make it incredibly easy (but sometimes <em>too</em> easy), so a nice option is to use the <strong>effects</strong> package to construct a dataframe of the linear prediction accross the values of a predictor, plus standard errors and confidence intervals. We can then pass this to <code>ggplot()</code>, giving us all the control over the aesthetics.</p>
<pre class="r"><code>library(effects)
ef &lt;- as.data.frame(effect(term=&quot;x1&quot;,mod=random_slopes_model))
ggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+
  geom_line()+
  geom_ribbon(alpha=.3)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-14-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-11" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-11&#39;, &#39;opt-start-11&#39;)"> Plotting random effects</span>
</div>
<div id="opt-body-11" class="optional-body" style="display: none;">
<p>The quick and easy way to plot your random effects is to use the <code>dotplot.ranef.mer()</code> function in <code>lme4</code>.</p>
<pre class="r"><code>randoms &lt;- ranef(random_slopes_model, condVar=TRUE)
dotplot.ranef.mer(randoms)</code></pre>
<pre><code>## $subject</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-15-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="optional-begin">
<span id="opt-start-12" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-12&#39;, &#39;opt-start-12&#39;)"> Completely optional - extracting them for plotting in ggplot</span>
</div>
<div id="opt-body-12" class="optional-body" style="display: none;">
<p>Sometimes, however, we might want to have a bit more control over our plotting, we can extract the estimates and correlations for each subject:</p>
<pre class="r"><code>#we can get the random effects:
#(note that we use $subject because there might be other groupings, and the ranef() function will give us a list, with one element for each grouping variable)
randoms &lt;-
  ranef(random_slopes_model)$subject %&gt;%
  mutate(subject = row.names(.)) %&gt;%  # the subject IDs are stored in the rownames, so lets add them as a variable
  pivot_longer(cols=1:2, names_to=&quot;term&quot;,values_to=&quot;estimate&quot;) # finally, let&#39;s reshape it for plotting

#and the same for the standard errors (from the arm package):
randoms_se &lt;-
  arm::se.ranef(random_slopes_model)$subject %&gt;%
  as.data.frame() %&gt;%
  mutate(subject = row.names(.)) %&gt;%
  pivot_longer(cols=1:2, names_to=&quot;term&quot;,values_to=&quot;se&quot;)

# join them together:
ranefs_plotting &lt;- left_join(randoms, randoms_se)

# it&#39;s easier for plotting if we
ggplot(ranefs_plotting, aes(y=subject, x=estimate))+
  geom_errorbarh(aes(xmin=estimate-2*se, xmax=estimate+2*se))+
  facet_wrap(~term, scales=&quot;free_x&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-16-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-13" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-13&#39;, &#39;opt-start-13&#39;)"> Polynomials!</span>
</div>
<div id="opt-body-13" class="optional-body" style="display: none;">
<p>Sometimes, data have a clear non-linear pattern, such as a curvilinear trend. In such case, it is reasonable to try modelling the outcome <em>not</em> as a linear function of the variable, but as a curvilinear function of it.</p>
<p>The following plots show data (as black dots) where the outcome <span class="math inline">\(y\)</span> has a nonlinear and decreasing dependence on <span class="math inline">\(x\)</span>. That is, as <span class="math inline">\(x\)</span> varies from 1 to 10, the outcome <span class="math inline">\(y\)</span> decreases in a non-linear fashion.
Superimposed to the same data, you can see a linear fit (red line) and a cubic fit (blue).</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-17-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The residuals corresponding to each fit are:
<img src="05_multilevel_recap_files/figure-html/unnamed-chunk-18-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Clearly, a linear fit doesn’t capture the real trend in the data, and any leftover systematic pattern that the model doesn’t explicity account for always ends up in the residuals as the red points show.</p>
<p>On the other hand, once we account for the nonlinear trend, that systematic pattern in the residuals disappears.</p>
<p>The secret is to use instead of <span class="math inline">\(x\)</span> as a predictor, the corresponding polynomial up to a specific order:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \epsilon
\]</span></p>
<p>Consider the following example data. You can add polynomials up to order 3, for example, of a predictor “time” by saying:</p>
<pre><code>## # A tibble: 5 × 3
##   subject reaction  time
##     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
## 1       1    0.428     1
## 2       1    0.427     2
## 3       1    0.211     3
## 4       1    0.585     4
## 5       1    0.127     5</code></pre>
<pre class="r"><code>source(&quot;https://uoepsy.github.io/msmr/functions/code_poly.R&quot;)

code_poly(df, predictor = &#39;time&#39;, poly.order = 3, draw.poly = FALSE)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   subject reaction  time time.Index  poly1  poly2     poly3
##     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
## 1       1    0.428     1          1 -0.632  0.535 -3.16e- 1
## 2       1    0.427     2          2 -0.316 -0.267  6.32e- 1
## 3       1    0.211     3          3  0     -0.535 -4.10e-16
## 4       1    0.585     4          4  0.316 -0.267 -6.32e- 1
## 5       1    0.127     5          5  0.632  0.535  3.16e- 1</code></pre>
<p>and use those terms when specifying your linear model. We can, depending on how time is measured (e.g. if we have subjects over time) also include some (or all) of these terms as random effects, for example:</p>
<pre><code>lmer(reaction ~ poly1 + poly2 + poly3 + (1 + poly1 | subject))</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-14" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-14&#39;, &#39;opt-start-14&#39;)"> Nested and Crossed structures</span>
</div>
<div id="opt-body-14" class="optional-body" style="display: none;">
<p>The same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).</p>
<p>Consider the example where we have observations for each student in every class within a number of schools:</p>
<p><img src="images/structure_id.png" width="1200px" style="display: block; margin: auto;" /></p>
<p><strong>Question:</strong> Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?</p>
<p>No.<br />
The classes in one school are distinct from the classes in another <strong>even though they are named the same</strong>.</p>
<p>The classes-within-schools example is a good case of <strong>nested random effects</strong> - one factor level (one group in a grouping varible) appears <em>only within</em> a particular level of another grouping variable.</p>
<p>In R, we can specify this using:</p>
<p><code>(1 | school) + (1 | class:school)</code></p>
<p>or, more succinctly:</p>
<p><code>(1 | school/class)</code></p>
<p>Consider another example, where we administer the same set of tasks at multiple time-points for every participant.</p>
<p><strong>Question:</strong> Are tasks nested within participants?</p>
<p>No.<br />
Tasks are seen by multiple participants (and participants see multiple tasks).</p>
<p>We could visualise this as the below:<br />
<img src="images/structure_crossed.png" width="400px" style="display: block; margin: auto;" /></p>
<p>In the sense that these are not nested, they are <strong>crossed</strong> random effects.</p>
<p>In R, we can specify this using:</p>
<p><code>(1 | subject) + (1 | task)</code></p>
<div class="blue">
<p><strong>Nested vs Crossed</strong></p>
<p><em>Nested:</em> Each group belongs uniquely to a higher-level group.</p>
<p><em>Crossed:</em> Not-nested.</p>
</div>
<p>Note that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures <code>(1 | school) + (1 | class)</code> and <code>(1 | school/class)</code> would give the same results.<br />
<img src="images/structure_nested.png" width="1200px" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-15" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-15&#39;, &#39;opt-start-15&#39;)"> Inference for Multilevel Models</span>
</div>
<div id="opt-body-15" class="optional-body" style="display: none;">
<div class="imp">
<p>This is a copy from what we saw in Week 2, but it is important, so here it is again!<br />
The majority of the examples you have seen have been using likelihood ratio tests (using <code>anova(model1, model2)</code>) or using an approximation for the denominator degrees of freedom (the <strong>lmerTest</strong> package). But there are many other options, as detailed below (and this list is not remotely exhaustive).</p>
</div>
<div class="frame">
<p><strong>For these examples… </strong></p>
<p>For the following examples, we’re going to return to our dataset of various toys, and we are going to be concerned with whether practice (the <code>hrs_week</code> variable) is associated with changes in reading ages (<code>R_AGE</code> variable).<br />
To accommodate for the clustered nature of the data, we are going to fit a model with both intercepts and slopes varying by toy-type.</p>
<pre class="r"><code>toys_read &lt;- read_csv(&quot;https://uoepsy.github.io/data/toyexample.csv&quot;)
full_model &lt;- lmer(R_AGE ~ hrs_week + (1 + hrs_week | toy_type), data = toys_read)</code></pre>
</div>
<div class="optional-begin">
<span id="opt-start-16" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-16&#39;, &#39;opt-start-16&#39;)"> Use a normal approximation</span>
</div>
<div id="opt-body-16" class="optional-body" style="display: none;">
<p>Remember that the <span class="math inline">\(t\)</span> distribution starts to look more and more like the <span class="math inline">\(z\)</span> (“normal”) distribution when degrees of freedom increase? We could just assume we have infinite degrees of freedom in our test statistics, and pretend that the <span class="math inline">\(t\)</span>-values we get are actually <span class="math inline">\(z\)</span>-values. This is “anti-conservative” inasmuch as it is not a very cautious approach, and we are likely to have a higher false positive rate (e.g. more chance of saying “there <strong>is</strong> an effect!” when there actually isn’t.)</p>
<pre class="r"><code>coefs &lt;- as.data.frame(summary(full_model)$coefficients)
coefs$p.z &lt;- 2 * (1 - pnorm(abs(coefs[,3])))
coefs</code></pre>
<pre><code>##             Estimate Std. Error t value     p.z
## (Intercept)     1.76      0.961    1.83 0.06768
## hrs_week        1.14      0.296    3.87 0.00011</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-17" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-17&#39;, &#39;opt-start-17&#39;)"> Satterthwaite df approximation</span>
</div>
<div id="opt-body-17" class="optional-body" style="display: none;">
<p>There have been a couple of methods proposed to estimate the degrees of freedom in order to provide a better approximation to the null distribution of our tests. The way the Satterthwaite method has been implemented in R will just add a column for p-values to your <code>summary(model)</code> output).</p>
<p>Load the <strong>lmerTest</strong> package, refit the model, and voila!</p>
<pre class="r"><code>library(lmerTest)
full_model &lt;- lmer(R_AGE ~ hrs_week + (1 + hrs_week | toy_type), data = toys_read)
summary(full_model)$coefficients</code></pre>
<pre><code>##             Estimate Std. Error   df t value Pr(&gt;|t|)
## (Intercept)     1.76      0.961 16.2    1.83  0.08617
## hrs_week        1.14      0.296 17.5    3.87  0.00118</code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>To account for the extra uncertainty brought by the inclusion of random effects in the model, the degrees of freedom in the coefficients tests have been corrected via Satterthwaite’s method.<br />
…<br />
…<br />
Weekly hours of reading practice was associated increased reading age (<span class="math inline">\(\beta = 1.14,\ SE = 0.30,\ t(17.52^*) = 3.87,\ p = .001\)</span>).</p>
</div>
<p><strong>Note:</strong> if you have the <strong>lmerTest</strong> package loaded, then all the models you fit with <code>lmer()</code> will show p-values! If you want to stop this, then you will have to detach/unload the package, and refit the model.</p>
<pre class="r"><code>detach(&quot;package:lmerTest&quot;, unload=TRUE)</code></pre>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-18" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-18&#39;, &#39;opt-start-18&#39;)"> Kenward Rogers df approximations</span>
</div>
<div id="opt-body-18" class="optional-body" style="display: none;">
<p>The Kenward-Rogers approach is slightly more conservative than the Satterthwaite method, and has been implemented for model comparison between a full model and a restricted model (a model without the parameter of interest), using the KR adjustment for the denominator degrees of freedom in the <span class="math inline">\(F\)</span>-test.<br />
For this, models must be fitted with REML, <strong>not</strong> ML. The function <code>KRmodcomp()</code> will take care of this and re-fit them for you.</p>
<pre class="r"><code>library(pbkrtest)
restricted_model &lt;- lmer(R_AGE ~ 1 + (1 + hrs_week | toy_type), data = toys_read)
full_model &lt;- lmer(R_AGE ~ hrs_week + (1 + hrs_week | toy_type), data = toys_read)
KRmodcomp(full_model, restricted_model)</code></pre>
<pre><code>## large : R_AGE ~ hrs_week + (1 + hrs_week | toy_type)
## small : R_AGE ~ 1 + (1 + hrs_week | toy_type)
##       stat  ndf  ddf F.scaling p.value   
## Ftest 14.6  1.0 17.7         1  0.0013 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>To account for the extra uncertainty brought by the inclusion of random effects in the model, the denominator degrees of freedom in have been corrected via Kenward-Rogers’ method.<br />
…<br />
…<br />
Inclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (<span class="math inline">\(F(1,17.74^*) = 14.64,\ p = .001\)</span>).</p>
</div>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-19" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-19&#39;, &#39;opt-start-19&#39;)"> Likelihood Ratio Test (LRT)</span>
</div>
<div id="opt-body-19" class="optional-body" style="display: none;">
<p>Conduct a model comparison between your model and a restricted model (a model without the parameter of interest), evaluating the change in log-likelihood.</p>
<div class="statbox">
<p><strong>Likelihood</strong></p>
<p>“likelihood” is a function that associates to a parameter the probability (or probability density) of observing the given sample data.<br />
In simpler terms, the likelihood is the probability of the model given that we have this data.</p>
<p>The intuition behind likelihood:</p>
<ol style="list-style-type: decimal">
<li>I toss a coin 10 time and observed 8 Heads.<br />
</li>
<li>We can think of a ‘model’ of the process that governs the coin’s behaviour in terms of just one number: a parameter that indicates the probability of the coin landing on heads.<br />
I have two models:</li>
</ol>
<ul>
<li>Model 1: The coin will land on heads 20% of the time. <span class="math inline">\(P(Heads)=0.2\)</span><br />
</li>
<li>Model 2: The coin will land on heads 70% of the time. <span class="math inline">\(P(Heads)=0.7\)</span><br />
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Given the data I observe (see 1, above), we can (hopefully) intuit that Model 2 is more likely than Model 1.</li>
</ol>
<p>For a (slightly) more detailed explanation, see <a href="lvp.html">here</a>.</p>
</div>
<p>This method assumes that the ratio of two likelihoods will (as sample size increases) become closer to being <span class="math inline">\(\chi^2\)</span> distributed, and so may be unreliable for small samples.</p>
<p>Models must be fitted with ML, <strong>not</strong> REML. The function <code>anova()</code> will re-fit them for you.</p>
<pre class="r"><code>restricted_model &lt;- lmer(R_AGE ~ 1 + (1 + hrs_week | toy_type), data = toys_read)
full_model &lt;- lmer(R_AGE ~ hrs_week + (1 + hrs_week | toy_type), data = toys_read)
anova(restricted_model, full_model, test = &quot;Chisq&quot;)</code></pre>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: toys_read
## Models:
## restricted_model: R_AGE ~ 1 + (1 + hrs_week | toy_type)
## full_model: R_AGE ~ hrs_week + (1 + hrs_week | toy_type)
##                  npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## restricted_model    5 660 675   -325      650                        
## full_model          6 651 668   -319      639  11.8  1    0.00059 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>A likelihood ratio test indicated that the inclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (<span class="math inline">\(\chi^2(1) = 11.81, p &lt; .001\)</span>).</p>
</div>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-20" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-20&#39;, &#39;opt-start-20&#39;)"> Optional: Parametric Bootstrap LRT</span>
</div>
<div id="opt-body-20" class="optional-body" style="display: none;">
<p>There are also various “bootstrapping” methods which it is worth looking into. Think back to USMR when we first learned about hypothesis testing. Remember that we did lots of simulating data, so that we can compare what we actually observe with what we would expect if the null hypothesis were true? By doing this, we were essentially <em>creating</em> a null distribution, so that our calculating a p-value can become an issue of summarising data (e.g. calculate the proportion of our simulated null distribution that is more extreme than our observed statistic)</p>
<p>Instead of assuming that the likelihood ratio test statistics are <span class="math inline">\(\chi^2\)</span>-distributed, we can bootstrap this test instead. This approach simulates data from the simpler model, fits both the simple model and the complex model and evaluates the change in log-likelihood. By doing this over and over again, we build a distribution of what changes in log-likelihood we would be likely to see if the more complex model is not any better. In this way it actually constructs a distribution reflecting our null hypothesis, against which we can then compare our actual observed effect</p>
<pre class="r"><code>library(pbkrtest)
PBmodcomp(full_model, restricted_model, nsim=1000)</code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>A parametric bootstrap likelihood ratio test (R = 1000) indicated that the inclusion of weekly hours of reading practice as a predictor was associated with an improvement in model fit (<span class="math inline">\(LRT = 11.79, p = .004\)</span>).</p>
</div>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-21" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-21&#39;, &#39;opt-start-21&#39;)"> Parametric Bootstrap Confidence Intervals</span>
</div>
<div id="opt-body-21" class="optional-body" style="display: none;">
<p>Much the same as above, but with just one model we simulate data many times and refit the model, so that we get an empirical distribution that we can use to construct confidence intervals for our effects.</p>
<pre class="r"><code>confint(full_model, method=&quot;boot&quot;)  </code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>95% Confidence Intervals were obtained via parametric bootstrapping with 1000 iterations.<br />
…<br />
…<br />
Weekly hours of reading practice was associated increased reading age (<span class="math inline">\(\beta = 1.14,\ 95%\ CI\ [0.58 -- 1.73]\)</span>).</p>
</div>
</div>
<p class="optional-end">
</p>
<div class="optional-begin">
<span id="opt-start-22" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-22&#39;, &#39;opt-start-22&#39;)"> Case-based Bootstrap Confidence Intervals</span>
</div>
<div id="opt-body-22" class="optional-body" style="display: none;">
<p>It’s worth noting that there are many different types of bootstrapping that we can conduct. Different methods of bootstrapping vary with respect to the assumptions we will have to make when using them for drawing inferences. For instance, the parametric bootstrap discussed above assumes that explanatory variables are fixed and that model specification and the distributions such as <span class="math inline">\(\zeta_i \sim N(0,\sigma_{\zeta})\)</span> and <span class="math inline">\(\varepsilon_i \sim N(0,\sigma_{\varepsilon})\)</span> are correct.<br />
An alternative is to generate a distribution by <strong>resampling with replacement</strong> from our data, fitting our model to the resample, and then repeating this over and over. This doesn’t have to rely on assumptions about the shape of the distributions of <span class="math inline">\(\zeta_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> - we just need to ensure that we correctly specify the hierarchical dependency of data. It does, however, require the decision of at which levels to resample.</p>
<pre class="r"><code>full_model &lt;- lmer(R_AGE ~ 1 + hrs_week + (1 + hrs_week | toy_type), data = toys_read)
library(lmeresampler)
fullmod_bs &lt;- 
  bootstrap(full_model, # this is the model
            .f = fixef, # we want the fixef from each bootstrap
            type = &quot;case&quot;, # case based bootstrap
            B=2000, # do 2000 bootstraps
            resample = c(TRUE, TRUE) # resample both toy_types and individual toys
  ) 
confint(fullmod_bs, type = &quot;perc&quot;)</code></pre>
<pre><code>## # A tibble: 2 × 6
##   term        estimate  lower upper type  level
##   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;
## 1 (Intercept)     1.76 -1.35   3.65 perc   0.95
## 2 hrs_week        1.14  0.633  2.03 perc   0.95</code></pre>
<div class="int">
<p><strong>Reporting</strong></p>
<p>95% Confidence Intervals were obtained via case-based bootstrapping (resampling both toy types and individual toys) with 2000 iterations.<br />
…<br />
…<br />
Weekly hours of reading practice was associated increased reading age (<span class="math inline">\(\beta\)</span> = 1.144, 95% CI [0.633 – 2.032]).</p>
</div>
</div>
<p class="optional-end">
</p>
<div class="frame">
<p>If you want more information (not required reading for this course), then Julian Faraway has a page <a href="https://people.bath.ac.uk/jjf23/mixchange/index.html">here</a> with links to some worked examples, and Ben Bolker has a wealth of information on his <a href="http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have">GLMM FAQ pages</a>.</p>
</div>
</div>
<p class="optional-end">
</p>
</div>
<div id="exercises-more-crossed-ranefs" class="section level1">
<h1>Exercises: More Crossed Ranefs</h1>
<div class="frame">
<p><strong>Data: 11 Domain Tests</strong></p>
<p>44 participants across 4 groups A, B, C, &amp; W (between-subjects) were tested 5 times (waves) in 11 domains.
In each wave, participants received a score (on a 20-point scale) for each domain and a set of questions which they answered either correctly or incorrectly.</p>
<p>The data can be accessed using the following code, and a description of the variables in the data can be found in the table below:</p>
<pre class="r"><code>load(url(&quot;https://uoepsy.github.io/data/msmr_lab5.RData&quot;))</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Anonymous_Subject_ID
</td>
<td style="text-align:left;">
Participant Identifier
</td>
</tr>
<tr>
<td style="text-align:left;">
IndivDiff
</td>
<td style="text-align:left;">
?? (not sure but it’s not relevant for these questions!)
</td>
</tr>
<tr>
<td style="text-align:left;">
Wave
</td>
<td style="text-align:left;">
Study wave (timepoint), ranging from 1 to 5
</td>
</tr>
<tr>
<td style="text-align:left;">
Domain
</td>
<td style="text-align:left;">
Domain tested (one of 11 domains studied, including things such as animals (ANI), objects (OBJ), toys (TOY), vehicles (VEH)
</td>
</tr>
<tr>
<td style="text-align:left;">
Correct
</td>
<td style="text-align:left;">
Number of questions answered correctly
</td>
</tr>
<tr>
<td style="text-align:left;">
Error
</td>
<td style="text-align:left;">
Number of questions answered incorrectly
</td>
</tr>
<tr>
<td style="text-align:left;">
Group
</td>
<td style="text-align:left;">
Group (between-participants), A, B, C, or W
</td>
</tr>
<tr>
<td style="text-align:left;">
Score
</td>
<td style="text-align:left;">
Score
</td>
</tr>
</tbody>
</table>
</div>
<div class="question-begin">
Question A1
</div>
<div class="question-body">
<blockquote>
<p>Did the groups differ in overall performance?</p>
</blockquote>
<p>There are different ways to test this: use the 20-point score or the accuracy? Keep the domains separate or calculate an aggregate across all domains? Which way makes the most sense to you?</p>
<p>Make a plot that corresponds to the research question. Does it look like there’s a difference?</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-23" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-23&#39;, &#39;sol-start-23&#39;)"> Solution </span>
</div>
<div id="sol-body-23" class="solution-body" style="display: none;">
<p>Lots of options for this one, here is one that shows Group differences:</p>
<pre class="r"><code>ggplot(dat5, aes(Group, Score, color=Group)) +
  stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) +
  guides(color=&quot;none&quot;)+
  coord_flip()</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-37-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>And here is one that shows Group <em>and</em> Domain differences:</p>
<pre class="r"><code>ggplot(dat5, aes(Domain, Score, color=Group)) +
  stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) +
  coord_flip()</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-38-1.png" width="80%" style="display: block; margin: auto;" />
And another:</p>
<pre class="r"><code>library(ggridges)
ggplot(dat5, aes(x=Score,y=Domain,fill=Domain)) +
  geom_density_ridges()+
  facet_wrap(~Group)+
  viridis::scale_fill_viridis(discrete=T,option = &quot;plasma&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-39-1.png" width="80%" style="display: block; margin: auto;" />
Looks like there are group differences and domain differences, but not much in the way of group-by-domain differences.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A2
</div>
<div class="question-body">
<blockquote>
<p>Did the groups differ in overall performance?</p>
</blockquote>
<p>Use a mixed-effects model to test the difference.</p>
<ul>
<li>Will you use a linear or logistic model?</li>
<li>What should the fixed(s) effect be?</li>
<li>What should the random effect(s) be? We have observations clustered by subjects and by domains - are they nested?</li>
</ul>
<p><em>Tip:</em> For now, we can forget about the longitudinal aspect to the data, because the research question is only concerned with overall performance.</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-24" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-24&#39;, &#39;sol-start-24&#39;)"> Solution </span>
</div>
<div id="sol-body-24" class="solution-body" style="display: none;">
<p>We’re interested in the amount to which Groups vary in their overall performance, so we want a fixed effect of Group. Subjects and Domains are not nested - each subject sees different domains, and each domain is seen by multiple subjects.</p>
<pre class="r"><code># maximal model doesn&#39;t converge, removed random Group slopes for Domain
mod_grp &lt;- lmer(Score ~ Group + 
                   (1 | Anonymous_Subject_ID) + 
                   (1 | Domain), 
                 data=dat5, REML=FALSE)
summary(mod_grp)</code></pre>
<pre><code>## Linear mixed model fit by maximum likelihood  [&#39;lmerMod&#39;]
## Formula: Score ~ Group + (1 | Anonymous_Subject_ID) + (1 | Domain)
##    Data: dat5
## 
##      AIC      BIC   logLik deviance df.resid 
##    10398    10438    -5192    10384     2004 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -5.025 -0.498  0.064  0.634  3.278 
## 
## Random effects:
##  Groups               Name        Variance Std.Dev.
##  Anonymous_Subject_ID (Intercept) 19.49    4.41    
##  Domain               (Intercept)  1.06    1.03    
##  Residual                          9.12    3.02    
## Number of obs: 2011, groups:  Anonymous_Subject_ID, 44; Domain, 11
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)    15.83       1.12   14.12
## GroupB         -4.16       2.26   -1.84
## GroupC         -3.62       1.77   -2.05
## GroupW         -7.27       1.67   -4.34
## 
## Correlation of Fixed Effects:
##        (Intr) GroupB GroupC
## GroupB -0.457              
## GroupC -0.585  0.290       
## GroupW -0.618  0.306  0.392</code></pre>
<p>Yes, substantial Group differences: overall, group A does the best, groups C and B next, and group W does the worst.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A3
</div>
<div class="question-body">
<blockquote>
<p>Did performance change over time (across waves)? Did the groups differ in pattern of change?</p>
</blockquote>
<p>Make a plot that corresponds to the research question. Does it look like there was a change? A group difference?</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-25" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-25&#39;, &#39;sol-start-25&#39;)"> Solution </span>
</div>
<div id="sol-body-25" class="solution-body" style="display: none;">
<pre class="r"><code>ggplot(dat5, aes(Wave, Score, color=Group, fill=Group)) +
  stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, alpha=0.3, color=NA) +
  stat_summary(fun.y=mean, geom=&quot;line&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-41-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Yes, looks like groups A, C, and W are improving, but it looks like group B is getting worse.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A4
</div>
<div class="question-body">
<blockquote>
<p>Did performance change over time (across waves)? Did the groups differ in pattern of change?</p>
</blockquote>
<p>Use mixed-effects model(s) to test this.<br />
<br>
<em>Hint:</em> Fit a baseline model in which scores change over time (wave), then assess improvement in model fit due to inclusion of overall group effect and finally the interaction of group with time.</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-26" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-26&#39;, &#39;sol-start-26&#39;)"> Solution </span>
</div>
<div id="sol-body-26" class="solution-body" style="display: none;">
<p><em>Remember:</em> If we’re using a likelihood ratio test to compare models (like we are here), then the models should be fitted with ML, not REML. <code>anova(model1,model2)</code> will re-fit them automatically for us, but we specify it specifically for each model here anyway:</p>
<pre class="r"><code>mod_wv &lt;- lmer(Score ~ Wave + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = &quot;bobyqa&quot;))

mod_wv_grp &lt;- lmer(Score ~ Wave+Group + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = &quot;bobyqa&quot;))

mod_wv_x_grp &lt;- lmer(Score ~ Wave*Group + 
                   (1 + Wave | Anonymous_Subject_ID) + 
                   (1 + Wave | Domain), 
                 data=dat5, REML=FALSE,
                 lmerControl(optimizer = &quot;bobyqa&quot;))

anova(mod_wv, mod_wv_grp, mod_wv_x_grp)</code></pre>
<pre><code>## Data: dat5
## Models:
## mod_wv: Score ~ Wave + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain)
## mod_wv_grp: Score ~ Wave + Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain)
## mod_wv_x_grp: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave | Domain)
##              npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## mod_wv          9 9720 9770  -4851     9702                       
## mod_wv_grp     12 9710 9778  -4843     9686 15.35  3     0.0015 **
## mod_wv_x_grp   15 9710 9795  -4840     9680  5.81  3     0.1212   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>summary(mod_wv_x_grp)</code></pre>
<pre><code>## Linear mixed model fit by maximum likelihood  [&#39;lmerMod&#39;]
## Formula: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 +  
##     Wave | Domain)
##    Data: dat5
## Control: lmerControl(optimizer = &quot;bobyqa&quot;)
## 
##      AIC      BIC   logLik deviance df.resid 
##     9710     9795    -4840     9680     1996 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -5.005 -0.576  0.005  0.613  3.752 
## 
## Random effects:
##  Groups               Name        Variance Std.Dev. Corr 
##  Anonymous_Subject_ID (Intercept) 22.3660  4.729         
##                       Wave         0.7479  0.865    -0.34
##  Domain               (Intercept)  2.0533  1.433         
##                       Wave         0.0219  0.148    -0.99
##  Residual                          6.0686  2.463         
## Number of obs: 2011, groups:  Anonymous_Subject_ID, 44; Domain, 11
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  12.7728     1.2493   10.22
## Wave          1.2547     0.2389    5.25
## GroupB       -1.3648     2.4745   -0.55
## GroupC       -4.1467     1.9166   -2.16
## GroupW       -6.3185     1.8166   -3.48
## Wave:GroupB  -1.1423     0.5292   -2.16
## Wave:GroupC  -0.0369     0.3686   -0.10
## Wave:GroupW  -0.5089     0.3547   -1.43
## 
## Correlation of Fixed Effects:
##             (Intr) Wave   GroupB GroupC GroupW Wv:GrB Wv:GrC
## Wave        -0.412                                          
## GroupB      -0.444  0.175                                   
## GroupC      -0.574  0.226  0.290                            
## GroupW      -0.605  0.239  0.306  0.395                     
## Wave:GroupB  0.157 -0.436 -0.389 -0.102 -0.108              
## Wave:GroupC  0.225 -0.625 -0.114 -0.366 -0.155  0.282       
## Wave:GroupW  0.234 -0.650 -0.118 -0.153 -0.370  0.293  0.421</code></pre>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A5
</div>
<div class="question-body">
<p>Using <code>broom.mixed::augment()</code> for the model with a Wave*Group interaction, plot the average (<code>stat_summary()</code> perhaps?) model fitted values for each group across Waves. Add in the observed data too.</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-27" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-27&#39;, &#39;sol-start-27&#39;)"> Solution </span>
</div>
<div id="sol-body-27" class="solution-body" style="display: none;">
<pre class="r"><code>broom.mixed::augment(mod_wv_x_grp) %&gt;%
  ggplot(., aes(Wave, Score, color=Group)) +
  stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) +
  stat_summary(aes(y=.fitted), fun=mean, geom=&quot;line&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-43-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><em>We fit a linear model, but the model fit lines are not straight lines. Why is that?</em></p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A6
</div>
<div class="question-body">
<p>Create individual subject plots for the data and the model’s fitted values. Will these show straight lines?<br />
<br>
<em>Hint:</em> make use of <code>facet_wrap()</code> to create a different panel for each level of a grouping variable.</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-28" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-28&#39;, &#39;sol-start-28&#39;)"> Solution </span>
</div>
<div id="sol-body-28" class="solution-body" style="display: none;">
<pre class="r"><code>broom.mixed::augment(mod_wv_x_grp) %&gt;%
  ggplot(., aes(Wave, Score, color=Group)) +
  facet_wrap(~ Anonymous_Subject_ID) +
  stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) +
  stat_summary(aes(y=.fitted), fun.y=mean, geom=&quot;line&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-44-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The individual subject plots show linear fits, which is a better match to the model. But now we see the missing data – some participants only completed the first few waves.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A7
</div>
<div class="question-body">
<p>Make a plot of the actual (linear) model prediction.<br />
<br>
<em>Hint:</em> Use the <code>effect()</code> function from the <code>effects</code> package.</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-29" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-29&#39;, &#39;sol-start-29&#39;)"> Solution </span>
</div>
<div id="sol-body-29" class="solution-body" style="display: none;">
<pre class="r"><code>library(effects)
ef &lt;- as.data.frame(effect(&quot;Wave:Group&quot;, mod_wv_x_grp))
ggplot(ef, aes(Wave, fit, color=Group, fill=Group)) +
  geom_ribbon(aes(ymax=fit+se, ymin=fit-se), color=NA, alpha=0.1) +
  geom_line()</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-45-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A8
</div>
<div class="question-body">
<p>What important things are different between the plot from question A7 and that from question A5? (You can see the plots we created for these questions below).</p>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-46-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Why do you think these two plots differ?</p>
<div class="optional-begin">
<span id="opt-start-30" class="fa fa-chevron-circle-right optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-30&#39;, &#39;opt-start-30&#39;)"> Hint</span>
</div>
<div id="opt-body-30" class="optional-body" style="display: none;">
<p>The reason is visible here:
<img src="05_multilevel_recap_files/figure-html/unnamed-chunk-47-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p class="optional-end">
</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-31" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-31&#39;, &#39;sol-start-31&#39;)"> Solution </span>
</div>
<div id="sol-body-31" class="solution-body" style="display: none;">
<p><em>Group B was not actually getting worse. The appearance that it was getting worse is an artifact of selective drop-out: there’s only a few people in this group and the better-performing ones only did the first few waves so they are not represented in the later waves, but the worse-performing ones are contributing to the later waves. The model estimates how the better-performing ones would have done in later waves based on their early-wave performance and the pattern of performance of other participants in the study.</em></p>
<pre class="r"><code>summary(mod_wv_x_grp)$coefficients</code></pre>
<pre><code>##             Estimate Std. Error t value
## (Intercept)  12.7728      1.249  10.224
## Wave          1.2547      0.239   5.253
## GroupB       -1.3648      2.474  -0.552
## GroupC       -4.1467      1.917  -2.164
## GroupW       -6.3185      1.817  -3.478
## Wave:GroupB  -1.1423      0.529  -2.159
## Wave:GroupC  -0.0369      0.369  -0.100
## Wave:GroupW  -0.5089      0.355  -1.435</code></pre>
<p>Note that the Group A slope (coefficient for <code>Wave</code>) is 1.255 and, relative to that slope, the Group B slope is -1.142 (coefficient for <code>Wave:GroupB</code>). This means that the model-estimated slope for Group B is 0.112, which is very slightly positive, not strongly negative as appeared in the initial plots.</p>
<p>One of the valuable things about mixed-effects (aka multilevel) modeling is that individual-level and group-level trajectories are estimated. This helps the model overcome missing data in a sensible way. In fact, MLM/MLR models are sometimes used for imputing missing data. However, one has to think carefully about <em>why</em> data are missing. Group B is small and it might just be a coincidence that the better-performing participants dropped out after the first few waves, which would make it easier to generalize the patterns to them. On the other hand, it might be the case that there is something about the study that makes better-performing members of Group B drop out, which should make us suspicious of generalizing to them.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question A9
</div>
<div class="question-body">
<p>Create a plot of the subject and domain random effects.
Notice the pattern between the random intercept and random slope estimates for the 11 domains - what in our model is this pattern representing?</p>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-32" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-32&#39;, &#39;sol-start-32&#39;)"> Solution </span>
</div>
<div id="sol-body-32" class="solution-body" style="display: none;">
<pre class="r"><code>randoms &lt;- ranef(mod_wv_x_grp, condVar=TRUE)
dotplot.ranef.mer(randoms)</code></pre>
<pre><code>## $Anonymous_Subject_ID</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-50-1.png" width="80%" style="display: block; margin: auto;" /></p>
<pre><code>## 
## $Domain</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-50-2.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Notice that the domains with the lower relative intercept tend to have a higher relative slope (and vice versa). This is the negative correlation between random intercepts and slopes for domain in our model:</p>
<pre class="r"><code>VarCorr(mod_wv_x_grp)</code></pre>
<pre><code>##  Groups               Name        Std.Dev. Corr 
##  Anonymous_Subject_ID (Intercept) 4.729         
##                       Wave        0.865    -0.34
##  Domain               (Intercept) 1.433         
##                       Wave        0.148    -0.99
##  Residual                         2.463</code></pre>
<p>Try removing the correlation (hint: use the <code>||</code>) to see what happens. Does it make sense that these would be correlated? (Answer: we don’t really know enough about the study, but it’s something to think about!)</p>
</div>
<p class="solution-end">
</p>
<!-- Formatting -->
</div>
<div id="less-guided-exercise" class="section level1">
<h1>Less Guided Exercise</h1>
<div class="frame">
<p>Instead of step-by-step questions, this exercise is designed to get you thinking more, giving you practice for the report and for your future research. If it helps, you can find a (sort of) checklist for multilevel models <a href="../lectures/msmr_extra.html">here</a> (but please be aware that there is no ‘one-size-fits-all’ approach - this checklist may not always be appropriate for every research question with multi-level data)</p>
</div>
<div class="question-begin">
Question C1
</div>
<div class="question-body">
<blockquote>
<p>How does aggressive behaviour change over adolescence? How is this change dependent upon whether or not a child has siblings?</p>
</blockquote>
<div class="frame">
<p><strong>Data: Aggressive Behaviour in Adolescence</strong></p>
<p>Data was collected from 30 secondary schools across Scotland. A cohort of students were followed up every year from the age of 12 to 19. Each year, they completed the Aggressive Behaviour Scale (ABS). Data was also captured on the number of siblings each child had.<br />
The data can be accessed from <a href="https://uoepsy.github.io/data/schoolsabs.csv">https://uoepsy.github.io/data/schoolsabs.csv</a> . A description of the variables can be found in the table below.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
schoolid
</td>
<td style="text-align:left;">
School Identifier
</td>
</tr>
<tr>
<td style="text-align:left;">
ABS
</td>
<td style="text-align:left;">
Score on the Aggressive Behaviour Scale (Z-scored)
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:left;">
Age (in years) of child at observation
</td>
</tr>
<tr>
<td style="text-align:left;">
childid
</td>
<td style="text-align:left;">
Within-School Child Identifier
</td>
</tr>
<tr>
<td style="text-align:left;">
siblings
</td>
<td style="text-align:left;">
Sibling status (No/Yes)
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p class="question-end">
</p>
<div class="solution-begin">
<span id="sol-start-33" class="fa fa-chevron-circle-right solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-33&#39;, &#39;sol-start-33&#39;)"> Solution </span>
</div>
<div id="sol-body-33" class="solution-body" style="display: none;">
<pre class="r"><code>absdat &lt;- read_csv(&quot;https://uoepsy.github.io/data/schoolsabs.csv&quot;)</code></pre>
<p>Let’s start by just exploring the data a bit more.</p>
<pre><code>## [1] 30</code></pre>
<pre><code>## [1] 500</code></pre>
<p>We have 500 students from 30 different schools. Schools tend to have data from about 17 children. Some schools have data from as few as 13 children, some have data from as many as 20.</p>
<pre class="r"><code>absdat %&gt;% 
  group_by(schoolid) %&gt;% 
  summarise(
    nchildren = n_distinct(childid)
  ) %&gt;% summary</code></pre>
<p>Note that the <code>childid</code> variable does <strong>not</strong> uniquely identify each child. We need to also know which school child “1” comes from.</p>
<pre class="r"><code>absdat %&gt;% count(schoolid,childid) %&gt;% select(n) %&gt;% table</code></pre>
<p>We can see that for some children we have fewer than the 7 observations: some have 6, 5, 4, or 3.</p>
<p>There are lots of different ways we can cut up the data to plot it</p>
<pre class="r"><code>ggplot(absdat, aes(x = year, y = ABS,col=siblings)) +
  stat_summary(geom=&quot;pointrange&quot;,aes(group=siblings),alpha=.5)+
  stat_summary(geom=&quot;line&quot;,aes(group=siblings),alpha=.5)+
  labs(title=&quot;Average ABS scores over age by number of siblings&quot;, x = &quot;Age&quot;, y = &quot;Aggressive Behaviour Score&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-57-1.png" width="80%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(absdat, aes(x = year, y = ABS,col=siblings)) +
  stat_summary(geom=&quot;pointrange&quot;,aes(group=schoolid),alpha=.2)+
  stat_summary(geom=&quot;line&quot;,aes(group=schoolid),alpha=.2)+
  labs(title=&quot;School level mean ABS scores&quot;, x = &quot;Age&quot;, y = &quot;Aggressive Behaviour Score&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-57-2.png" width="80%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(absdat, aes(x = year, y = ABS,col=siblings)) +
  stat_summary(geom=&quot;line&quot;,aes(group=interaction(schoolid,childid)),alpha=.2)+
  facet_wrap(~siblings, labeller = label_both) +
  labs(title=&quot;Children&#39;s trajectories of ABS scores&quot;,x = &quot;Age&quot;, y = &quot;Aggressive Behaviour Score&quot;)</code></pre>
<p><img src="05_multilevel_recap_files/figure-html/unnamed-chunk-57-3.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Now let’s think a bit about the various considerations that go into fitting a multi-level model.</p>
<ul>
<li>We can group our data by whether or not they have siblings, by school, and by child.<br />
</li>
<li>We’re specifically interested in differences between the levels of the <code>siblings</code> variable, so that is going to be in our fixed effects.<br />
</li>
<li>We’re not interested in drawing inferences about specific schools or specific children. The schools and children in our dataset in this case are just a random sample of the wider population of schools and children. They will account for some variability in the outcome (e.g. some children are more aggressive than others, maybe some schools tend to have more aggressive children in them than others). So our model wants to account for this, but we don’t want to specifically estimate differences. These will be our random effects.<br />
</li>
<li>Schools and children is one of the most obvious cases of <em>nested</em> random effects. Each child belongs to one, and only one school. This means we are going to want to model the structure <code>(1 | School / Child)</code>.<br />
</li>
<li>The <code>childid</code> variable does <strong>not</strong> uniquely identify a single child. It just says “1”,“2”, etc. But child 1 from school 1 is a different child from child 1 in school 2. We need to remember this, and if we want to separate out our random effects of school and child, then we’ll need <code>(1 | School) + (1 | Child:School)</code>.<br />
Alternatively, we could make a new child identifier variable that <em>does</em> uniquely identify each child.<br />
</li>
<li>Along with being interested in differences between having/not-having siblings, we’re also interested in change over time. From our initial plotting, we see there is some curvature present, and this is most apparent in the group of children with siblings. Are we interested in differences between sibling groups at the first timepoint (age 12?). Possibly? This depends a lot on the research field.<br />
</li>
<li>We know that effects of time can vary by child, and so also by school. Children don’t change in whether or not they have siblings (well, they might, not often), so we can’t really think of “the effect of having siblings on aggressive behaviours for child <span class="math inline">\(i\)</span>” because child <span class="math inline">\(i\)</span> always has the same number of siblings. However, within a school, some children have siblings, some don’t, so it might be that this effect can vary by-school.</li>
</ul>
<pre class="r"><code>source(&quot;https://uoepsy.github.io/msmr/functions/code_poly.R&quot;)
absdat &lt;- code_poly(absdat, &quot;year&quot;, poly.order = 2, orthogonal = T, draw.poly = F)

basemod &lt;- 
  lmer(ABS ~ poly1 + poly2 + (1 + poly1 + poly2 | schoolid/childid), 
     data = absdat,
     control = lmerControl(optimizer = &quot;bobyqa&quot;))
isSingular(basemod)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>basemod &lt;- 
  lmer(ABS ~ poly1 + poly2 + (1 + poly1 | schoolid) +
         (1 + poly1 + poly2 | childid:schoolid), 
     data = absdat,
     control = lmerControl(optimizer = &quot;bobyqa&quot;))</code></pre>
<p>Let’s use effects coding for siblings, so that our estimated coefficients for poly1 and poly2 represent the average.</p>
<pre class="r"><code># we&#39;ll put yes first, because when we use contr.sum, it will set the last level to -1
absdat$siblings &lt;- factor(absdat$siblings, levels=c(&quot;yes&quot;,&quot;no&quot;))
contrasts(absdat$siblings) &lt;- contr.sum(2)
contrasts(absdat$siblings)</code></pre>
<pre><code>##     [,1]
## yes    1
## no    -1</code></pre>
<pre class="r"><code>linmod &lt;- 
  lmer(ABS ~ siblings*poly1 + poly2 + (1 + poly1 | schoolid) +
         (1 + poly1 + poly2 | childid:schoolid), 
     data = absdat,
     control = lmerControl(optimizer = &quot;bobyqa&quot;))
quadmod &lt;- 
  lmer(ABS ~ siblings*(poly1 + poly2) + (1 + poly1 | schoolid) +
         (1 + poly1 + poly2 | childid:schoolid), 
     data = absdat,
     control = lmerControl(optimizer = &quot;bobyqa&quot;))

anova(basemod,linmod,quadmod)</code></pre>
<pre><code>## Data: absdat
## Models:
## basemod: ABS ~ poly1 + poly2 + (1 + poly1 | schoolid) + (1 + poly1 + poly2 | childid:schoolid)
## linmod: ABS ~ siblings * poly1 + poly2 + (1 + poly1 | schoolid) + (1 + poly1 + poly2 | childid:schoolid)
## quadmod: ABS ~ siblings * (poly1 + poly2) + (1 + poly1 | schoolid) + (1 + poly1 + poly2 | childid:schoolid)
##         npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## basemod   13 8034 8113  -4004     8008                        
## linmod    15 8015 8106  -3992     7985 22.97  2      1e-05 ***
## quadmod   16 8016 8114  -3992     7984  0.65  1       0.42    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Let’s get some bootstrapped confidence intervals around our parameter estimates. This may take quite a while to run.</p>
<pre class="r"><code>confint(quadmod, method=&quot;boot&quot;)</code></pre>
<table style="width:69%;">
<colgroup>
<col width="31%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">term</th>
<th align="center">est</th>
<th align="center">2.5 %</th>
<th align="center">97.5 %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">(Intercept)</td>
<td align="center">0.011</td>
<td align="center">-0.145</td>
<td align="center">0.158</td>
</tr>
<tr class="even">
<td align="center">SiblingsY (Y=1,N=-1)</td>
<td align="center">-0.060</td>
<td align="center">-0.103</td>
<td align="center">-0.025</td>
</tr>
<tr class="odd">
<td align="center">Age (linear)</td>
<td align="center">0.526</td>
<td align="center">0.259</td>
<td align="center">0.800</td>
</tr>
<tr class="even">
<td align="center">Age (quadratic)</td>
<td align="center">-0.197</td>
<td align="center">-0.260</td>
<td align="center">-0.135</td>
</tr>
<tr class="odd">
<td align="center">SiblingsY:Age(lin)</td>
<td align="center">-0.202</td>
<td align="center">-0.315</td>
<td align="center">-0.092</td>
</tr>
<tr class="even">
<td align="center">SiblingsY:Age(quad)</td>
<td align="center">-0.025</td>
<td align="center">-0.085</td>
<td align="center">0.038</td>
</tr>
</tbody>
</table>
<div class="int">
<p>Aggressive behaviour scores show an increase over time (linear increase 0.526, 95% CI [0.259 – 0.8]), which gets more gradual as adolescence progresses (quadratic term -0.197 [-0.26 – -0.135]). Overall, having siblings was associated with lower scores on the Aggressive Behaviour Scale ( <span class="math inline">\(\beta\)</span> = -0.06 [-0.103 – -0.025]), and showed less increase with age ( <span class="math inline">\(\beta\)</span> = -0.202 [-0.315 – -0.092]). There was no evidence for the quadratic change over time being dependent upon sibling-status. The model fitted values and data are presented in Figure <a href="#fig:schoolsabs">1</a> below.</p>
</div>
<pre class="r"><code>broom.mixed::augment(quadmod) %&gt;%
  ggplot(.,aes(x=poly1, col=siblings))+
  stat_summary(aes(y=ABS),geom=&quot;pointrange&quot;, alpha=.5)+
  stat_summary(aes(y=.fitted),geom=&quot;line&quot;)+
  scale_x_continuous(&quot;age&quot;, breaks=poly(12:18,2)[,1], labels=c(12:18))</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:schoolsabs"></span>
<img src="05_multilevel_recap_files/figure-html/schoolsabs-1.png" alt="Means and standard errors for ABS scores over age (points), along with average model fitted values (lines)" width="80%" />
<p class="caption">
Figure 1: Means and standard errors for ABS scores over age (points), along with average model fitted values (lines)
</p>
</div>
</div>
<p class="solution-end">
</p>
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;">

</div>
</div>

<link rel="stylesheet" href="https://uoepsy.github.io/assets/css/ccfooter.css" />
<div class="ccfooter"></div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "site_libs/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
